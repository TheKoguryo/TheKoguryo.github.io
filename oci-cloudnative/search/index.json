[{"categories":null,"contents":"1. OKE(Oracle Container Engine for Kubernetes) 소개 \u0026hellip;\n","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/1.oke/","tags":["oke"],"title":"1.1 OKE(Oracle Container Engine for Kubernetes) 소개"},{"categories":null,"contents":"1.3.1 Cloud Shell로 OKE 클러스터 연결하기 Cloud Shell Cloud Shell은 공식 문서에서 설명하는 것 처럼 Oracle Cloud 콘솔에서 제공하는 웹 브라우저 기반 리눅스 터미널입니다. 작은 가상머신으로 구동된다고 이해하시면 되며, Oracle Cloud 콘솔에 접속한 유저에 대해 사전 인증된 OCI CLI를 제공하며, 추가적인 cli 및 설정들을 기본 구성하여 제공합니다.\nOKE 클러스터 접속을 위해 Kubernetes CLI인 kubectl로 기본 설치되어 제공합니다.\n무료로 사용할 수 있고, 인증 및 기본 툴 들이 구성되어 있기 때문 바로 사용할 수 있는 이점이 있습니다.\nCloud Shell로 OKE 클러스터 연결하기   생성한 OKE 클러스터 상세 페이지로 이동합니다.\n  Access Cluster를 클릭합니다.\n   두 가지 접근 방법 중에 Cloud Shell Access을 선택합니다.\n Cloud Shell: OCI에서 제공하는 Cloud Shell을 통해 접근합니다. 현재 접속한 사용자가 현 OCI Tenacy 환경에 작업하기 위한 기본 설정 및 관련 cli들이 구성되어 있습니다. Local Access: 로컬 PC 환경에서 처음 접속하기 위해 필요한 작업부터 시작하는 방법입니다.     Step #1. Launch Cloud Shell\nLaunch Cloud Shell를 클릭하거나, 우측 상단에 있는 링크를 클릭하여 Cloud Shell에 접속합니다.\n   접속한 환경에서 다음 명령을 실행해 보면 oci cli가 설치되어 있으며, 접속이 가능한 상태임을 알 수 있습니다.\noci -v oci os get ns    Step #2. kubeconfig 파일 생성하기\n생성된 OKE 클러스터 접속을 위한 kubeconfig을 생성하기 위해 Access Your Cluster의 두 번째 단계 내용을 Cloud Shell에서 실행합니다.\n 명령어에서 보듯이 Cloud Shell에서는 Kubernetes API를 Public Endpoint을 제공하는 경우에만 접근할 수 있습니다.     OKE 클러스터 연결 확인\nkubectl cluster-info 를 실행하면 생성된 kubeconfig를 통해 클러스터에 접속됨을 확인할 수 있습니다.\n   ","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/3.access-cluster/1.cloudshell-access/","tags":["oke"],"title":"1.3.1 Cloud Shell로 클러스터 연결하기"},{"categories":null,"contents":"1.3.2 로컬 환경에서 클러스터 연결하기 ","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/3.access-cluster/2.local-access/","tags":["oke"],"title":"1.3.2 로컬 환경에서 클러스터 연결하기"},{"categories":null,"contents":"1.6.1 Block Volume 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\n기본 설치된 Persistent Volume을 위한 StorageClass 확인 OKE는 OCI IaaS를 기반으로 제공되는 서비스로 OCI Block Volume 서비스를 이용하게 Persistent Volume을 제공합니다. 현 버전 기준으로 FlexVolume 볼륨 플러그인과 CSI(Container Storage Interface) 볼륨 플러그인의 두 가지를 사용하고 있습니다.\n  기본 StorageClass 확인\n oci: OCI Block Volume 서비스를 위한 FlexVolume 플러그인 사용 oci-bv: OCI Block Volume 서비스를 위한 CSI 플러그인 사용  oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 2d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d   CSI 볼륨 플러그인\n FlexVolume 플러그인 방식은 오픈소스 Kubernetes 1.2 버전때 부터 사용되어 더 오래되었지만, 업스트림 Kubernetes에서 CSI 플러그인 방식을 사용하는 흐름입니다. OKE에서도 Release Note 2020년 8월 1일, Support for the Container Storage Interface (CSI) volume plug-in 에 나와 있는 것 처럼 OKE에서도 신규 기능은 CSI 플러그인에 추가할 예정이며, FlexVolume은 유지 보수만 할 계획입니다. 그래서 이하 설명에서는 CSI 플러그인을 사용하는 oci-bv storageclass를 사용하겠습니다. Flex 플러그인을 사용하는 oci storageclass에 대한 사항은 공식 문서를 참조바랍니다.    OCI Block Volume용 CSI 플러그인을 사용하여 Persistent Volume 만들어 사용하기 Persitent Volume 테스트\n  아래와 같이 PV 요청 yaml을 사용하여 요청합니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:csi-bvs-pvcspec:storageClassName:\u0026#34;oci-bv\u0026#34;accessModes:- ReadWriteOnceresources:requests:storage:50Gi  테스트 앱 배포\n 요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱  apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-bvs-pvcname:nginx-bvs-pvcspec:replicas:1selector:matchLabels:app:nginx-bvs-pvctemplate:metadata:labels:app:nginx-bvs-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:csi-bvs-pvc  생성 결과\n 아래와 같이 정상적으로 PV 요청에 따라 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다.  oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 101s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 118s   Block Volume OCI 서비스 콘솔에서 Storage \u0026gt; Block Volumes 화면에서 보면 아래와 같이 PV용 Block Volume 만들어 졌으며, 특정 Worker Node에 부착된 걸 알 수 있습니다.\n     테스트\n  Persistent Volume에 쓰기\n아래와 같이 컨테이너 내부로 들어가 마운트 된 PV 내에 파일쓰기를 합니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vdp7c -- bash root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# echo \u0026#34;Hello PV\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# cat /usr/share/nginx/html/hello_world.txt Hello PV root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/#   POD 삭제 후 재생성\nPersistent Volume이 유지되는 것을 확인하기 위해 기존 POD를 삭제하고 재생성되도록 합니다. 이때 RWO 모드라 다른 Node에 POD가 생성되는 경우, Multi-Attach error가 일시적으로 발생합니다. 기존 POD가 삭제되었다는 것을 인지하는 데 시간이 걸리며, 조금 지난 후에 POD가 다시 재생성됩니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 6m53s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl delete pod nginx-bvs-pvc-7b574c9c5c-vdp7c pod \u0026#34;nginx-bvs-pvc-7b574c9c5c-vdp7c\u0026#34; deleted oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 0/1 ContainerCreating 0 17s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 1/1 Running 0 75s   신규 POD에서 Persistent Volume 확인\n앞서 변경한 파일을 신규 POD에서 다시 조회해 보면 기존 내용이 남아 있는 걸 확인할 수 있습니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vrptl -- cat /usr/share/nginx/html/hello_world.txt Hello PV     참고사항\n 앞선 테스트에서 처럼 ReadWriteOnce 접근 모드에서는 단일 Kubernetes Node에 있는 POD만 붙여서 사용할 수 있습니다. 다른 Node에 있는 POD에서 동일한 PV를 사용하려고 하면, 컨테이너 기동시 Multi-Attach 오류가 발생하며, 컨테이너가 기동이 되지 않습니다. 또한 Self-Healing으로 POD 재기동시 기존 POD가 해당 volume을 사용하고 있다고 생각하여 Multi-Attach 오류가 발생하고, 사용중인 POD가 없음을 인지하는 데 약간의 시간이 걸리게 됩니다.    ReadWriteMany 지원 여부 현재 버전 기준 CSI Driver for OCI Block Volume Service는 ReadWriteOnce만 지원합니다. 그래서 단일 Kuberenetes Node에 멀티 Pod까지만 지원됩니다. 또한 위 작업 내용을 accessMode를 ReadWriteMany로 변경후 동일하게 수행하면 pod가 생성되지 않고 아래와 같이 에러가 나게 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get events LAST SEEN TYPE REASON OBJECT MESSAGE ... 80s Warning ProvisioningFailed persistentvolumeclaim/mynginxclaim failed to provision volume with StorageClass \u0026quot;oci-bv\u0026quot;: rpc error: code = InvalidArgument desc = invalid volume capabilities requested. Only SINGLE_NODE_WRITER is supported ('accessModes.ReadWriteOnce' on Kubernetes) ","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/6.persistent-volume/1.block-volume/","tags":["oke"],"title":"1.6.1 Block Volume 사용하기"},{"categories":null,"contents":"1.6.2 File Storage 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)을 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 RWS 접근 모드로 사용하는 방법을 확인해 보겠습니다.\nFiles Storage 만들기 관련 문서를 참고하여 File Storage를 만듭니다.\n  https://docs.oracle.com/en-us/iaas/Content/File/home.htm\n  https://thekoguryo.github.io/oci/chapter08/\n    OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Storage \u0026gt; File Storage로 이동합니다\n  대상 Compartment를 확인합니다.\n  File Systems에서 Create File System을 클릭합니다.\n  기본 설정화면에서 간단히 아래 정보를 원하는 값이 맞게 알맞게 수정하고 생성합니다.\n File System Information:  Name   Mount Target Information:  New Mount Target Name Virtual Cloud Network Subnet       생성결과 확인\nFile Storage \u0026gt; Mount Target 에서 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\n Mount Target OCID: \u0026hellip;sc2mia IP Address: 예, 10.0.20.194 Export Path: 예) /OKE-FFS-Strorage     Security List 설정\nFile System 생성시 Mount Target의 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\n   File Storage 서비스를 이용하여 Persistent Volume 사용하기   Storage Class 만들기\n앞서 확인한 Mount Target OCID로 업데이트 후 적용\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:oci-fssprovisioner:oracle.com/oci-fssparameters:# Insert mount target from the FSS heremntTargetId:ocid1.mounttarget.oc1.ap_seoul_1.aaaaaa4np2szmmn5nfrw4llqojxwiotboawxgzlpovwc2mjnmfsc2mia  Persistent Volume (PV) 만들기\nMount Targe의 IP와 Export Path로 업데이트 후 적용\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumemetadata:name:oke-fss-pvspec:storageClassName:oci-fsscapacity:storage:100GiaccessModes:- ReadWriteManymountOptions:- nosuidnfs:# Replace this with the IP of your FSS file system in OCIserver:10.0.20.194# Replace this with the Path of your FSS file system in OCIpath:\u0026#34;/OKE-FFS-Storage\u0026#34;readOnly:false  Persistent Volume Claime(PVC) 만들기\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:oke-fss-pvcspec:storageClassName:oci-fssaccessModes:- ReadWriteManyresources:requests:storage:100GivolumeName:oke-fss-pv  PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\n앞선 예제와 달리 replica를 복수개로 지정할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-fss-pvcname:nginx-fss-pvcspec:replicas:3selector:matchLabels:app:nginx-fss-pvctemplate:metadata:labels:app:nginx-fss-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:oke-fss-pvc  실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oci-fss-storageclass.yaml storageclass.storage.k8s.io/oci-fss created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get sc,pv,pvc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/oci (default) oracle.com/oci Delete Immediate false 2d19h storageclass.storage.k8s.io/oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d19h storageclass.storage.k8s.io/oci-fss oracle.com/oci-fss Delete Immediate false 34s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 3h6m persistentvolume/oke-fss-pv 100Gi RWX Retain Bound default/oke-fss-pvc oci-fss 24s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/csi-bvs-pvc Bound csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO oci-bv 3h6m persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 100Gi RWX oci-fss 17s oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-9fb98454f-bc7hp 1/1 Running 0 24s 10.244.0.5 10.0.10.40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-bxw4x 1/1 Running 0 24s 10.244.1.18 10.0.10.15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-qm9tq 1/1 Running 0 24s 10.244.0.153 10.0.10.219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   파일 쓰기 테스트\n  아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.40\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bxw4x -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-qm9tq -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40     참고 문서 https://blogs.oracle.com/cloud-infrastructure/post/using-file-storage-service-with-container-engine-for-kubernetes\n","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/6.persistent-volume/2.file-storage/","tags":["oke"],"title":"1.6.2 File Storage 사용하기"},{"categories":null,"contents":"2. OKE 클러스터 만들기 Quick 모드로 클러스터 만들기 처음 OKE 클러스터를 만드는 단계로 실환경에서는 별도의 OKE 사용자 및 VCN 등 커스텀한 환경을 사용하겠지만, OKE를 이해하기 위한 처음 단계로 Administrator 유저를 통해 Quick 모드로 설치합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : **Kubernetes Clusters (OKE)**로 이동합니다.\n  List Scope에서 생성할 Compartment(예, oke-labs)를 선택합니다.\n  클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n  빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여생성된 OKE 클러스터를 통해 기분 구조를 이해하고자 합니다. 아래 Launch Workflow를 클릭합니다.\n   클러스터 생성 옵션   생성할 클러스터 이름을 입력합니다. 예, oke-cluster-1\n  설치될 Compartment를 선택합니다. 예, oke-labs\n  클러스터의 쿠버네티스 버전을 선택합니다. 예, v1.20.11\n  Kubernete API Endpoint\n Public API로 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kubernetes API를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Kubernetes API가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.    Kubernetes Worker Nodes\n Worker Nodes를 접속할 수 있는 방법을 선택합니다. 기본 선택된 Private Worker를 그대로 사용 생성되는 Worker Nodes를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Worker Nodes가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.     Shape Worker Node로 사용할 VM Shape를 지정합니다. 기본 선택된 Flex Shape에서 필요에 따라 OCPU, Memory를 조정합니다.\n   Number of nodes\nWorker Nodes 갯수를 지정합니다. 기본 값을 3개입니다.\n  Control Planes(Master Nodes)\nControl Plane은 OCI가 관리하는 영역으로 별도 크기 등을 지정하지 못하며, 기본적으로 다중화되어 있습니다.\n  Advanced Options\n Add an SSH Key  트러블 슈팅 등을 위해 Worker Node VM에 접근하기 위해서는 SSH Key 등록이 필요합니다. 사용할 SSH Key의 Public Key를 아래와 같이 등록합니다.       클러스터 생성 정보를 모두 입력하였습니다. 아래 Next를 클릭\n  앞서 입력한 값들을 리뷰한 후 Create Cluster를 클릭합니다.\n  클러스터 생성 및 관련 네트워크 자원\n아래 그림과 같이 Quick Create로 클러스터를 생성시 기본 네트워크 자원이 함께 생성되는 것을 볼수 있습니다.\n   클러스터 생성 확인   생성이 요청되면, 클러스터 생성, 노드 풀 생성, Worker Node 생성 및 구성 순으로 진행됩니다.\n  클러스터 상세정보에서 Resources \u0026gt; Node Pools를 보면 생성된 pool을 볼수 있습니다.\n   생성된 Node Pool인 pool1을 클릭하여 Node Pool 상세 정보로 이동합니다.\n  Node Pool 상세 정보에서 Resources \u0026gt; Nodes 정보를 보면 생성된 Worker Nodes를 확인할 수 있습니다. VM 생성후 쿠버네티스 구성 시간이 있어 Ready 상태가 될 때까지 약간의 시간이 걸립니다. 테스트 환경에서는 노드가 모두 Ready 될때 까지 5~6분 정도 걸렸습니다.\n   클러스터 및 네트워크 구성 확인 Quick Create \u0026amp; Public Endpoint \u0026amp; Private Workers Example Network Resource Configurations에 설명된 예시 처럼 Kubernetes API Endpoint, Worker Nodes, Service Load Balancer에 대해서 Private 또는 Public 서브넷을 조합하는 몇 가지 구성이 가능합니다. 여기서는 앞서 처럼 Quick Create 모드에서 Public Endpoint, Private Workers를 선택하였고, Service Load Balancer는 기본 생성시는 Public이며 Kubernetes에서 Load Balancer 생성시 선택할 수 있습니다.\n","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/2.install-quick-oke-cluster/","tags":["oke"],"title":"1.2 OKE 클러스터 만들기"},{"categories":null,"contents":"1.4 앱 배포 및 Load Balancer 사용하기 Docker Hub 이미지 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest   배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-78b9999646-xgtjp 1/1 Running 0 17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 130m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-78b9999646 1 1 1 20s   Load Balancer 테스트 Load Balancer 서비스 사용하기   클라이언트 서비스를 위해 LoadBalancer Type으로 서비스를 생성합니다.\n  서비스 생성 결과를 확인하면 아래와 같이 LoadBalancer 타입으로 생성되어 Public IP가 할당 된 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc service/nginx-docker-hub-svc exposed oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 135m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 49s   서비스 주소인 Public IP로 접속하면, 연결되는 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ curl http://152.67.200.29 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   OCI Service Console에서 Load Balancer 확인하기   콘솔에서 Networking \u0026gt; Load Balancer 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  아래 그림과 같이 kubernetes 상에 생성된 서비스와 동일한 Public IP로 Load Balancer가 생성된 것을 알 수 있습니다.\n   Load Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n  상세화면에서 좌측 하단 Resources \u0026gt; Listeners로 이동합니다.\nkubernetes에서 Load Balancer 생성시 지정한 80 포트로 Listen 하고 있는 걸 알 수 있습니다.\n   Resources \u0026gt; Backend Set으로 이동합니다. 생성된 Backend Set을 클릭하여 상세화면으로 이동합니다.\n  좌측 하단 Resources \u0026gt; Backends로 이동합니다.\n세 개의 백엔드 노드의 30610 포트로 부하 분산하는 것을 볼 수 있습니다.\n   다시 터미널로 이동하여 서비스와 노드 정보를 조회합니다.\nkubectl get svc kubectl get nodes -o wide   조회결과\n조회 해보면 OCI Load Balancer 가 Worker Nodes 3개로, 각 노드의 Node Port인 30610으로 부하 분산 되는 것을 알 수 있습니다. 이처럼 kubernetes에서 Load Balancer Type 서비스를 생성하면, OCI Load Balancer와 연동되어 자동으로 자원이 생성됩니다.\n  oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 160m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 25m oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.11 Ready node 155m v1.20.11 10.0.10.11 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.44 Ready node 155m v1.20.11 10.0.10.44 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.73 Ready node 155m v1.20.11 10.0.10.73 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   OCI Service Console에서 Security List 확인하기   콘솔에서 Networking \u0026gt; Virtual Cloud Networks 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  OKE 클러스터가 사용하는 VCN으로 이동합니다.\n  Subnet을 보면 그림과 같이, 3개의 서브넷이 있습니다.\n oke-k8sApiEndpoint-subnet~~: Kubernetes API Endpoint를 위한 서브넷 oke-svclbsubnet-~~: Load Balancer가 사용하는 서브넷 oke-nodesubnet-~~: Worker Node 들이 사용하는 서브넷     Resources \u0026gt; Security Lists로 이동하면, 위 3개의 서브넷이 사용하는 3개의 Security List가 있습니다.\n   Load Balancer용 서브넷 확인\n먼저 oke-svclbsubnet-~~ 의 상세화면으로 이동합니다. Ingress/Egress Rule을 확인해 보면, 외부에서 80 포트로 수신하고, Worker Node들의 Node Port인 30610로 송신할 수 있도록 자동으로 추가된 것을 볼수 있습니다.\n    다시 VCN 상세 화면으로 이동하여, Worker Nodes용 서브넷을 확인합니다.\nOCI Load Balancer에서 Node Port: 30610으로 요청을 수신할 수 있도록 자동으로 규칙이 추가된 것을 볼수 있습니다.\n   위와 같이 OKE 클러스터에 kubernetes 명령으로 Load Balancer 서비스 타입을 생성하면, 그에 따라 OCI Load Balancer가 생성되고, 관련 Security List에도 등록되는 것을 알 수 있습니다.\n  ","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/4.deploy-docker-hub-image-with-lb/","tags":["oke"],"title":"1.4 앱 배포 및 Load Balancer 사용하기"},{"categories":null,"contents":"1.5 OCIR 이미지 사용하여 앱 배포하기 OCIR에 이미지 등록하기 Oracle Cloud Infrastructure Registry(OCIR)는 오라클이 제공하는 관리형 컨테이너 레지스트리로 Docker V2 API를 지원하며, Open Container Initiate 호환 컨테이너 레지스트리입니다. docker cli를 통해 이미지를 Push, Pull 해서 사용할 수 있으며, Kubernetes 클러스터에서도 사용할 수 있습니다.\nOCIR에 이미지를 사용하기 위해서는 먼저 등록 작업이 필요하며, 앞서 예제에서 사용한 nginx 이미지를 아래 절차에 따라 등록해 봅니다.\nOCIR Repository 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, oke-labs)를 선택합니다.\n  이미지를 Push하기 전에 먼저 OCIR에 repository를 생성이 필요합니다.\nCreate repositoy를 클릭하여 아래와 같이 nginx repository를 생성합니다. Push, Pull 모두 인증 테스트를 위해 Access 모드를 Private으로 선택합니다.\n   생성 완료\n   Repository 화면에서 Namespace를 복사해 둡니다. Region내에 Tenant를 나타내는 tenancy-namespace: cnrlxx3w0wgq로 이후 로그인시 필요합니다.\n  OCI Auth Token 만들기 docker cli로 docker hub에 이미지를 등록하거나, 가져올때 username/password로 docker login을 통해 로그인을 합니다. OCIR에도 마찬가지로 로그인이 필요하며, password 대신 보안을 위해 Auth Token을 사용합니다.\n  우측 상단 사용자의 Profile 아이콘을 클릭하여 User Settings으로 이동합니다.\n 아래 그림상의 유저는 OCI local 유저로 username이 oke-admin입니다. 유저명이 oracleidentitycloudservice/~~~로 시작하면 Oracle Identiry Cloud Service의 유저입니다.     왼쪽 아래 Resources \u0026gt; Auth Token으로 이동합니다.\n  Auth Token 생성을 위해 Generate Token을 클릭합니다.\n  설명을 입력하고 생성합니다. Auth Token은 생성시에만 볼수 있으므로 복사해 둡니다.\n    OCIR 로그인 및 이미지 Push   앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 docker cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:~ (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   이미지 Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식으로 태그를 한 후 Push 하면 됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt;:\u0026lt;tag\u0026gt;   nginx:latest 예시  docker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCIR 확인\nOCI 서비스 콘솔로 다시 돌아가서 대상 Compartment 기준으로 Push한 이미지가 정상적으로 등록된 것을 알 수 있습니다.\n   실수를 막기 위한 참고 사항 다음과 같은 상황에서는 docker push하면 어떻게 될까요?\n  Push 전에 OCIR Repository를 사전에 만들지 않은 경우\n  dev/nginx:latest(또는 bitnami/nginx:latest)와 같이 이미지 이름 앞에 추가 적인 경로가 있는 경우에 OCIR Repoistory를 dev 로만 만든 경우\n=\u0026gt; 사전에 OCIR Repository를 만들지 않으면, 기본 설정에 의해 root compartment 쪽에 push 됩니다.\n=\u0026gt; dev/nginx 까지마 Repository 이름으로 해야 합니다. 그렇게 만들지 않는 경우 동일하게 root compartment 쪽에 push 됩니다.\n  Container Registry 우측 상단에 Settings를 클릭하여 설정정보를 보면 아래와 같이 대상 repository가 없는 경우 root compartment에 private repository를 자동으로 새로 만들고 push 하는 것이 기본 값으로 체크되어 있습니다.\n OCIR 이미지로 OKE 클러스터에 배포 OCIR 이미지 배포 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   배포 결과 아래와 같이 private repository로 인증문제로 이미지를 가져오는 오류가 발생한 것을 알수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ocir-6c9d554866-nqgjg 0/1 ErrImagePull 0 10s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe pod nginx-ocir-6c9d554866-nqgjg Name: nginx-ocir-6c9d554866-nqgjg ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning Failed 11s (x2 over 23s) kubelet Failed to pull image \u0026#34;ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest\u0026#34;: rpc error: code = Unknown desc = Error reading manifest latest in ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx: denied: Anonymous users are only allowed read access on public repos Warning Failed 11s (x2 over 23s) kubelet Error: ErrImagePull   OCIR Private Repository 이미지 배포 테스트 - imagepullsecret Private Repository에서 이미지를 가져와서 사용하려면 인증을 위한 secret을 등록해서 사용해야 합니다. 아래 절차에 따라 secret을 만들어 사용합니다.\n  앞서 Auth Token을 사용하여 docker login을 하였습니다. 로그인 하면 사용자 홈 밑에 .docker/config.json에 인증정보가 저장됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   위 인증 정보를 통해 로그인 합니다.\nkubectl create secret generic ocir-secret \\  --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\  --type=kubernetes.io/dockerconfigjson   또는 docker login 정보 없이 직접 secret을 만들 수도 있습니다.\nkubectl create secret docker-registry \u0026lt;secret-name\u0026gt; --docker-server=\u0026lt;region-key\u0026gt;.ocir.io --docker-username='\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;oci-username\u0026gt;' --docker-password='\u0026lt;oci-auth-token\u0026gt;' --docker-email='\u0026lt;email-address\u0026gt;'   아래와 같이 imagepullsecret을 사용하여 다시 배포합니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-ocirname:nginx-ocirspec:replicas:1selector:matchLabels:app:nginx-ocirtemplate:metadata:labels:app:nginx-ocirspec:containers:- name:nginximage:ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latestimagePullSecrets:- name:ocir-secret  아래와 같이 정상 배포되는 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret generic ocir-secret \\ \u0026gt; --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\ \u0026gt; --type=kubernetes.io/dockerconfigjson secret/ocir-secret created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-2tvwr kubernetes.io/service-account-token 3 24h ocir-secret kubernetes.io/dockerconfigjson 1 13s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f nginx-ocir-deployment.yaml deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-798957d964-9rddt 1/1 Running 0 7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 8s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-798957d964 1 1 1 7s   OCIR Private Repository 이미지 배포 테스트 - default imagepullsecret 매번 imagepullsecret을 지정하는 것이 불편한 경우 기본으로 사용할 Container Repository에 대한 인증을 default로 저장하여 사용할 수도 있습니다.\n  namespace에 default serviceaccount가 있는데, 여기에 아래와 같이 imagepullsecret을 추가합니다.\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39;   그 결과 아래와 같이 default serviceaccount에 기본적으로 사용할 imagesecret이 추가되었습니다\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39; serviceaccount/default patched oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get sa default -o yaml apiVersion: v1 imagePullSecrets: - name: ocir-secret kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2021-11-08T13:51:17Z\u0026#34; name: default namespace: default resourceVersion: \u0026#34;277559\u0026#34; uid: f72718f9-135c-46d2-b0ac-a1ea1b990863 secrets: - name: default-token-2tvwr   앞서 배포한 yaml을 삭제하고 인증정보가 없어 처음 실패한 명령으로 다시 배포합니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   결과확인하면 default imagepullsecret을 사용하여 정상 배포됨을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-6c9d554866-vmjtb 1/1 Running 0 5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 5s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-6c9d554866 1 1 1 5s   ","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/5.deploy-ocir-image/","tags":["oke"],"title":"1.5 OCIR 이미지 사용하기"},{"categories":null,"contents":"1.7 Kubernetes 지원 버전 및 업그레이드 Kubernetes 지원 버전 Kubernetes 버전은 x.y.z로 표현되며, 각각 x는 메이저, y는 마이너, z는 패치 버전을 뜻하며, 오픈소스 Kubernetes도 현재 버전과 그 이전 2개 버전까지를 지원하고 있습니다. OKE 또한 OKE가 지원하는 최신버전 기준, 그 이전 2개의 마이너 버전까지 지원하고 있습니다. 금일자 기준 지원하는 버전은 다음과 같습니다.\n 1.20.11 1.19.15 1.18.10  신규 버전 출시 후에 30일 동안만 그 이전 버전을 지원하고 있습니다. 예를 들어 OKE에서 1.20.11, 1.19.15가 2021년 10월 8일에 출시되어 기존 버전인 1.20.8, 1.19.12는 각각 30일후인 2021년 11월 7일까지만 지원합니다. 현재 지원 버전은 다음 링크를 참조합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm  버전 관리 단위 OKE 클러스터는 Control Plane과 Worker Node로 구성되며, Control Plane의 버전이 정해지면, Worker Node는 Control Plane과 같거나 그 이하 버전을 사용할 수 있습니다. 물론 호환되는 내에서 말입니다.\nWorker Node는 Node Pool 단위로 버전을 가질 수 있습니다. 여러 Node Pool을 만들어 각각 다른 버전을 가질 수 있습니다.\n버전 업그레이드 OKE 새 버전이 출시되면 버전 업그레이드는 다음 절차를 따릅니다.\n Control Plane 업그레이드  OCI 서비스 콘솔에서 OKE 클러스터 단위를 업그레이드하면 Control Plane만 업그레이드됨 오라클이 관리하는 영역으로 다운타임 없이 자동으로 업그레이드 됨 OKE 클러스터를 업그레이드 하면, 즉 Control Plane이 업그레이드하면 이전 버전으로 다시 다운그레이드 할 수 없음   Worker Node 업그레이드  OCI 서비스 콘솔에서 Node Pool 단위로 업그레이드 업그레이드 방식  in-place 업그레이드  대상이 되는 기존 Node Pool은 OCI 콘솔에서 버전 업그레이드, 실제 Node가 자동으로 업그레이드 되지 않음 kubectl drain 명령으로 특정 노드에 신규 컨테이너가 생기는 것을 방지함 OCI 서비스 콘솔에서 drain한 Node를 종료(Terminate) 시킴 Node 자가치유에 의해 신규 Node가 자동으로 생성되며, 생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전 기존 노드에 대해서 순서대로 모두 진행   out-of-place 업그레이드  신규 버전의 Node Pool 추가 생성 kubectl 명령으로 기존 Node Pool에 있는 Node 제거, Kubernetes에 의해 컨테이너가 모두 이동하면, Node 삭제하는 방식으로 하나씩 진행 기존 Node가 모두 제거되면, 기존 Node Pool 삭제        Node Pool 단위 버전 혼합 테스트를 위해 1.18.10, 1.19.12, 1.19.15, ,1.20.8, 1.20.11 버전이 사용 가능한 상태에서 1.19.12 버전의 OKE 클러스터를 기준으로 테스트를 진행합니다.\n 테스트 환경  OKE 클러스터 - oke-cluster-1 버전: 1.19.12 Control Plane 버전: 1.19.12 Node Pool - poo1 버전: 1.19.12      OKE 클러스터 상세화면으로 이동합니다.\n  왼쪽 아래 Resources \u0026gt; Node Pools 로 이동합니다.\n   현재 pool1이 1.19.12 버전이 있습니다. 추가 Node Pool 생성을 위해 Add Node Pool을 클릭합니다.\n  새 Node Pool 생성을 위한 정보를 입력합니다.\n Name: 새 Node Pool 이름 Version: 일반 버전 혼합을 확인하기 위해 1.18.10 버전은 선택합니다.  사용 가능한 버전을 보면, 현재 OKE 클러스터 버전 이하만 선택 가능한 걸 알 수 있습니다. 현재 OKE 클러스터 생성시 사용 가능한 1.19.15, ,1.20.8, 1.20.11 버전은 보이지 않습니다.      처음 생성시와 비슷하게 생성시 필요한 정보를 입력합니다. 다음은 예시입니다.  Shape: VM.Standard.E3.Flex Placement Configuration: Worker Node가 위치한 AD와 Node용 서브넷 지정 Advanced Options:  Add an SSH Key: Node에 SSH로 접근하기 위한 Publich Key        생성을 요청하면 실제 Node VM이 만들어지고, 준비되는 데 까지 앞서 설치시와 같이 약간의 시간이 걸립니다.\n  Node Pool을 추가 생성하면 그림과 같이 동일 OKE 클러스터에 두 가지 버전의 혼합을 지원하여, 앞서 Node Pool 추가시 본것 처럼 Pool 단위 VM 크기, 위치(AD, 서브넷)을 달리 할 수 있습니다.\n   kubectl로 노드를 조회해도 동일한 결과가 나옵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 23h v1.19.12 10.0.10.226 Ready node 23h v1.19.12 10.0.10.234 Ready node 6m53s v1.18.10 10.0.10.43 Ready node 6m30s v1.18.10 10.0.10.44 Ready node 23h v1.19.12 10.0.10.78 Ready node 6m41s v1.18.10   OKE 클러스터 버전 업그레이드 Control Plane 업그레이드 위와 같이 1.19.12 버전을 사용 중에 새로운 버전이 출시되었다고 가정합니다. 그러면 앞서 설명한 것과 같이 기술지원 정책에 따라 기존 버전은 30일간 지원하기 때문에, 그동안 버전 검증후 업그레이드가 필요합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm    업그레이드가 가능하면, OKE 클러스터 상세 화면에서 Upgrade Available 버튼이 활성화 됩니다.\n   Upgrade Available 버튼을 클릭하면 다음과 같이 안내 문구와 함께 업그레이드를 시작할 수 있습니다. 최신 버전인 v1.20.11을 선택하도록 하겠습니다.\n   버전을 선택하고 아래 Upgrade 버튼을 클릭하여 업그레이드를 시작합니다.\n  클러스터 상태가 UPDATING으로 표시되고 업그레이드가 진행됩니다. 오라클이 관리하는 Control Plane이 내부적으로 순차적으로 업그레이드가 발생합니다. 실제 애플리케이션이 동작하는 Worker Node는 서비스 중지 없이 업그레이드 됩니다.\n  테스트 시점에는 10~15분 후에 업그레이드 완료되었습니다.\n   Worker Node 업그레이드 - in-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. in-place 업그레이드 방식은 Node Pool 업그레이드 -\u0026gt; 기존 Node Drain \u0026gt; POD 이동 확인 -\u0026gt; 기존 Node VM 종료 순으로 업그레이드 합니다.\nNode Pool 업그레이드   업그레이드 하려는 Node Pool의 상세 페이지로 이동합니다.\n  수정을 위해 Edit를 클릭하면, 오른쪽에 수정 페이지가 뜹니다.\nVersion 항목에, 클러스터 버전과 Node Pool의 버전이 표시되며, 업그레이드 가능한 버전이 표시됩니다.\n   클러스터와 동일한 1.20.11로 선택하고 Save Change를 클릭하여 저장합니다.\n  Resources \u0026gt; Work Requests에 가서 보면, 2~3초 뒤에 Node Pool 업그레이드가 완료됩니다.\n   아직 실제 Worker Node가 업그레이드 된 것은 아닙니다.\nResources \u0026gt; Nodes에 가서 보면 기존 버전 그대로입니다.\n   Node Drain 시키기   kubectl 명령으로 Worker Node와 배포된 POD를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 24h v1.19.12 10.0.10.226 Ready node 24h v1.19.12 10.0.10.44 Ready node 24h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-b46nd 1/1 Running 0 72m 10.244.1.4 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 39m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-7tsjf 1/1 Running 0 39m 10.244.1.8 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 40m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl drain 10.0.10.44 --ignore-daemonsets node/10.0.10.44 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-7hc28, kube-system/kube-flannel-ds-b4dp6, kube-system/kube-proxy-f8qmp, kube-system/proxymux-client-6qs4x evicting pod default/nginx-fss-pvc-9fb98454f-7tsjf evicting pod default/nginx-bvs-pvc-7b574c9c5c-b46nd pod/nginx-bvs-pvc-7b574c9c5c-b46nd evicted pod/nginx-fss-pvc-9fb98454f-7tsjf evicted node/10.0.10.44 evicted   아래와 같이 44번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 25h v1.19.12 10.0.10.226 Ready node 25h v1.19.12 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-9smhh 1/1 Running 0 3m20s 10.244.0.138 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 45m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-8zkn2 1/1 Running 0 3m20s 10.244.0.6 10.0.10.185 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 46m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   종료할 Node의 노드 이름을 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -L displayName NAME STATUS ROLES AGE VERSION DISPLAYNAME 10.0.10.185 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-0 10.0.10.226 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-1 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-2   Node Pool 상세 화면에서 종료할 노드 이름을 클릭하여 Compute 인스턴스로 이동합니다.\n   Node에 해당되는 Compute 인스턴스를 종료합니다. boot volume도 같이 종료합니다.\n    Node 자가치유에 의해 신규 Node가 자동으로 생성됩니다. Work Requests를 보면 아래와 같이 재조정(NODEPOLL_RECONCILE) 작업이 발생되어 지정된 갯수에 맞게 노드가 다시 생성됩니다.\n   생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전으로 생성됩니다.\n  kubectl 명령으로 노드를 조회하면, 1.20.2 버전으로 신규 노드가 생성되었습니다. docker 런타임의 deprecate 예정으로 인해 참고로 1.20 부터는 컨테이너 런타임이 cri-o 변경되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.170 Ready node 7m25s v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.185 Ready node 25h v1.19.12 10.0.10.185 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11 10.0.10.226 Ready node 25h v1.19.12 10.0.10.226 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11   기존 노드에 대해서 순서대로 모두 진행합니다.\n  완료 결과\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.128 Ready node 6m10s v1.20.11 10.0.10.128 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.146 Ready node 35s v1.20.11 10.0.10.146 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.170 Ready node 23m v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   Worker Node 업그레이드 - out-of-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. out-of-place 업그레이드 방식은 업그레이드 버전의 Node Pool 신규 생성 -\u0026gt; 기존 Node Pool의 모든 노드 Drain -\u0026gt; 기존 Node Pool 삭제 순으로 업그레이드 합니다.\n새 버전의 Node Pool 만들기   OKE 클러스터 상세 페이지로 이동합니다.\n  Resources \u0026gt; Node Pools 로 이동합니다.\n  그림과 같이 기존 버전의 Node Pool이 있는 상태에서 신규 Node Pool 추가를 위해 Add Node Pool을 클릭합니다.\n   신규 Node Pool 정보를 입력하여 생성합니다.\n Name Version: 새 버전 선택 Shape: Node VM 유형 Number of nodes: 노드 수 Placement Configuration  Node가 위치할 AD, Subnet   Add an SSH key: Node VM에 SSH 접속시 사용할 키의 Private Key     추가 된 Node Pool을 OCI 서비스 콘솔 확인할 수 있습니다.\n  기존 Node Pool의 모든 노드 Drain   구동 중인 앱들이 기존 Node Pool에서 동작하고 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 18m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-2jbp8 1/1 Running 0 18m 10.244.0.5 10.0.10.242 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 19m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 18m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready node 71m v1.19.12 pool1 10.0.10.29 Ready node 71m v1.19.12 pool1 10.0.10.61 Ready node 71m v1.19.12 pool1 10.0.10.12 Ready node 4m25s v1.20.11 pool2 10.0.10.126 Ready node 4m48s v1.20.11 pool2 10.0.10.191 Ready node 4m42s v1.20.11 pool2   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl drain 10.0.10.242 --ignore-daemonsets node/10.0.10.242 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-l62xw, kube-system/kube-flannel-ds-7dv8l, kube-system/kube-proxy-zv4ks, kube-system/proxymux-client-24nmv evicting pod kube-system/coredns-c5d4bf466-wv8fb evicting pod default/nginx-fss-pvc-9fb98454f-2jbp8 evicting pod kube-system/coredns-c5d4bf466-h5rrm pod/coredns-c5d4bf466-wv8fb evicted pod/nginx-fss-pvc-9fb98454f-2jbp8 evicted pod/coredns-c5d4bf466-h5rrm evicted node/10.0.10.242 evicted   아래와 같이 242번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 76m v1.19.12 pool1 10.0.10.29 Ready node 76m v1.19.12 pool1 10.0.10.61 Ready node 76m v1.19.12 pool1 10.0.10.12 Ready node 9m15s v1.20.11 pool2 10.0.10.126 Ready node 9m38s v1.20.11 pool2 10.0.10.191 Ready node 9m32s v1.20.11 pool2 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 22m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5k8wg 1/1 Running 0 57s 10.244.3.2 10.0.10.126 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 23m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 22m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   나머지 기존 Node Pool에 있는 Node들도 drain합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.29 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.61 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.12 Ready node 11m v1.20.11 pool2 10.0.10.126 Ready node 12m v1.20.11 pool2 10.0.10.191 Ready node 12m v1.20.11 pool2   기존 Node Pool 삭제   기존 Node Pool에 있는 모든 Node들이 drain되어 더이상 사용되지 않습니다.\n  OCI 서비스 콘솔에서 OKE 클러스터 상세페이지로 이동합니다.\n  Resources \u0026gt; Node Pools로 이동하여 기존 Node Pool을 삭제합니다.\n   업그레이드가 완료되었습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.12 Ready node 18m v1.20.11 pool2 10.0.10.126 Ready node 19m v1.20.11 pool2 10.0.10.191 Ready node 19m v1.20.11 pool2   ","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/basic/7.supported-version-and-upgrade/","tags":["oke"],"title":"1.7 Kubernetes 버전 업그레이드"},{"categories":null,"contents":"","permalink":"https://thekoguryo.github.io/oci-cloudnative/search/","tags":null,"title":"Search"}]