[{"categories":null,"contents":"6.1 Kubernetes 클러스터에 Verrazzano 설치하기 쿠버네티스 클러스터 준비 Verrazzano는 Kubernetes Operator 방식을 사용하여 쿠버네티스 클러스터에 설치하는 방식입니다. 즉 설치할 쿠버네티스 클러스터가 필요합니다. 여기서는 OKE 클러스터 기준으로 진행합니다.\nContainer Engine for Kubernetes (OKE) 클러스터 준비 Quick Create 모드로 OKE 클러스터를 기본 설정으로 생성합니다.\n Verrazzano 상의 모든 컴포넌트와 예제 애플리케이션을 배포하기 위해 충분한 용량의 환경을 준비합니다.  Shape: VM.Standard2.4 이상 또는 VM.Standard.E2.4 이상 Number of nodes: 3개 이상 OKE 버전: 1.19  1.20, 1.21에서는 FluentD 파서 이슈로 Kibana 동작을 위한 추가 작업이 필요합니다.      Verrazzano 설치  Cloud Shell 또는 작업환경에서 kubectl로 생성한 OKE 클러스터에 접속합니다.  Verrazzano platform operator 설치   Verrazzano platform operator 설치\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.1.0/operator.yaml   설치 완료될때 까지 기다립니다.\nkubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator   successfully rolled out가 뜨고 설치가 잘 됐는지 확인합니다.\nkubectl -n verrazzano-install get pods   Verrazzano 설치 - OCI DNS \u0026amp; Let\u0026rsquo;s Encrypt 사용 설치시 dev, prod, managed-cluster 프로파일 중에 고를 수 있습니다. 여기서는 dev 프로파일을 사용합니다. 자세한 사항은 설치 프로파일 페이지를 참고하세요. 기본 프로파일로 설치하면 Self-Signed 인증서와 무료 와일드 카드 도메인(nip.io, sslip.io)을 사용하게 됩니다. 원하는 도메인과 인증서를 사용하기 위해서 OCI DNS와 Let\u0026rsquo;s Encrypt를 사용하는 방법을 확인해 봅니다.\n참고\n https://verrazzano.io/latest/docs/setup/install/customizing/dns/ https://verrazzano.io/latest/docs/setup/install/customizing/certificates/  OCI DNS Zone 생성 소유하고 있는 도메인을 OCI DNS에 Zone으로 등록합니다.\nOCI API Secret 생성 대상 쿠버네티스 클러스터에 API Private 정보를 Secret 정보로 등록합니다. 여기서는 OCI CLI가 이미 설정된 경우는 가정해 Verrazzano helper script를 통해 secret을 만듭니다.\n  helper script 다운로드\n 홈이 아닌경우 실행시 Hang이 걸리는 것 같으니 홈에서 실행합니다.  cd ~ curl \\  -o ./create_oci_config_secret.sh \\  https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/platform-operator/scripts/install/create_oci_config_secret.sh   KUBECONFIG 환경변수 설정 및 스크립트 실행\nchmod +x create_oci_config_secret.sh export KUBECONFIG=~/.kube/config ./create_oci_config_secret.sh   실행 예시 및 결과 확인\n[opc@bastion-host ~]$ chmod +x create_oci_config_secret.sh [opc@bastion-host ~]$ export KUBECONFIG=~/.kube/config [opc@bastion-host ~]$ [opc@bastion-host ~]$ ./create_oci_config_secret.sh secret/oci created [opc@bastion-host ~]$ kubectl get secret oci -n verrazzano-install NAME TYPE DATA AGE oci Opaque 1 35s [opc@bastion-host ~]$ kubectl get secret oci -o jsonpath=\u0026#34;{.data[\u0026#39;oci\\.yaml\u0026#39;]}\u0026#34; -n verrazzano-install | base64 -d auth: region: ap-seoul-1 tenancy: ocid1.tenancy.oc1..aaaaaaaa~~~ user: ocid1.user.oc1..aaaaaaaa~~~ key: | -----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCbKoBbV+xIDgeA ... K2jAF6UJZ/+BBKkHRXHSxoI= -----END PRIVATE KEY----- fingerprint: 99:c9:87:~~~   Verrazzano 설치   설치 yaml 샘플 다운로드\ncurl \\  -o ./install-oci.yaml \\  https://raw.githubusercontent.com/verrazzano/verrazzano/release-1.1/platform-operator/config/samples/install-oci.yaml   install-oci.yaml 파일 업데이트\n spec.environmentName: 설치된 환경에 대한 이름. myenv이고, zonename이 example.com인 경우 ingress 도메인이 myenv.example.com로 됨. spec.components.dns.oci.ociConfigSecret: oci, 앞서 생성한 secret name spec.components.dns.oci.dnsZoneCompartmentOCID: OCI DNS Zone으로 동록한 Zone이 있는 Compartment의 OCID spec.components.dns.oci.dnsZoneOCID: OCI DNS Zone으로 동록한 Zone의 OCID spec.components.dns.oci.dnsZoneName: OCI DNS Zone으로 동록한 Zone의 이름, 예, example.com  apiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:my-verrazzanospec:environmentName:myenvprofile:devcomponents:certManager:certificate:acme:provider:letsEncryptemailAddress:thekoguryo@gmail.comenvironment:staging dns:oci:ociConfigSecret:ocidnsZoneCompartmentOCID:ocid1.compartment.oc1..aaaaaaaa~~~dnsZoneOCID:ocid1.dns-zone.oc1..7974~~~dnsZoneName:thekoguryo.mlingress:type:LoadBalancer  install-oci.yaml 배포\nkubectl apply -f install-oci.yaml   설치 완료 확인\nkubectl wait \\  --timeout=20m \\  --for=condition=InstallComplete verrazzano/my-verrazzano   로그 확인\nkubectl logs -n verrazzano-install \\  -f $(kubectl get pod \\  -n verrazzano-install \\  -l app=verrazzano-platform-operator \\  -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) | grep \u0026#39;\u0026#34;operation\u0026#34;:\u0026#34;install\u0026#34;\u0026#39;   설치시 추가 확인\n OCI Trial로 인한 자원 부족 문제인지도 확인합니다.  kubectl get events --sort-by=.metadata.creationTimestamp -A   Verrazzano 설치 정보 확인   Console 주소 확인\nkubectl get verrazzano my-verrazzano -o yaml   결과 예시\nstatus: ... instance: consoleUrl: https://verrazzano.myenv.thekoguryo.ml elasticUrl: https://elasticsearch.vmi.system.myenv.thekoguryo.ml grafanaUrl: https://grafana.vmi.system.myenv.thekoguryo.ml keyCloakUrl: https://keycloak.myenv.thekoguryo.ml kialiUrl: https://kiali.vmi.system.myenv.thekoguryo.ml kibanaUrl: https://kibana.vmi.system.myenv.thekoguryo.ml prometheusUrl: https://prometheus.vmi.system.myenv.thekoguryo.ml rancherUrl: https://rancher.myenv.thekoguryo.ml state: Ready version: 1.1.0   Verrazzano 유저 암호 변경 KeyCloak에서 암호 변경   Verrazzano 관리자 유저(유저명: verrazzano)는 KeyCloak을 통해 관리되며 Single Sign-On(SSO)이 설정되어 있어, Verrazzano Console, Elasticsearch, Grafana, KeyCloak, Kiali, Kibana, Prometheus의 사용자를 KeyCloak을 통해 관리합니다.\n  verrazzano 유저의 초기 난수 암호는 kubernetes에서 확인이 가능하나, KeyCloak에서 변경 할 수 있습니다.\n  KeyCloack(예, https://keycloak.myenv.thekoguryo.ml)에 접속하여 Administration Console로 이동합니다.\n  KeyCloak Admin으로 로그인합니다.\n  KeyCloak admin user: keycloakadmin\n  KeyCloak admin password: 다음 명령으로 확인\nkubectl get secret \\  --namespace keycloak keycloak-http \\  -o jsonpath={.data.password} | base64 \\  --decode; echo     왼쪽 메뉴에서 Manage \u0026gt; Users 로 이동합니다.\n  View all users를 클릭하여 verrazzano 유저를 찾아, ID를 클릭합니다.\n  암호 변경을 위해 Credentials 탭을 클릭합니다.\n  Temporary는 OFF로 하고, 새 암호를 입력하고, Reset Password를 클릭합니다.\n   팝업이 뜨면 다시한번 Reset Password를 클릭합니다.\n  Kubernetes Secret에 반영   새 암호를 base64로 인코딩합니다.\n예)\necho -n \u0026#39;MyNewPwd\u0026#39; | base64   verrazzano가 설치된 kubernetes의 secret을 변경합니다.\nkubectl edit secret verrazzano -n verrazzano-system   콘솔 로그인   Verrazzano Console에 로그인합니다.\n예, https://verrazzano.myenv.thekoguryo.ml\n  System Telemetry 영역에 있는 툴은 SSO 구성이 되어 해당 링크로 verrazzano 유저로 접속할 수 있습니다.\n Kibana: 로그 모니터링 Grafana: 메트릭 모니터링 Promethues: 메트릭 수집 Elasticsearch: 로그 수집 Kiali: istio 서비스 메쉬 모니터링     KeyCloak: 유저 관리\n Admin 유저(KeyCloak Admin) 접속은 앞서와 동일합니다. 해당 링크를 클릭하면 KeyCloak 일반유저인 verrazzano 유저로 로그인 가능합니다.    Rancher: Kubernetes 모니터링\n  화면에 보이는 URL로 접속합니다.\n  User: admin\n  Password: 다음 명령으로 확인\nkubectl get secret \\  --namespace cattle-system rancher-admin-secret \\  -o jsonpath={.data.password} | base64 \\  --decode; echo     ","lastmod":"2021-12-28T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/verrazzano/1.install-verrazzano/","tags":["oke","verrazzano"],"title":"6.1 Verrazzano 설치"},{"categories":null,"contents":"5.1 OCI Service Operator for Kubernetes(OSOK) OCI Service Operator for Kubernetes는 OCI 자원을 Kubernetes API를 통해 관리할 수 있도록 도와주는 도구입니다. Autonomous Database 서비스를 Kubernetes API, kubectl을 통해 인스턴스를 생성, 삭제 등을 할 수 있게 해준다고 이해하면 됩니다. Kubernetes에서 사용하는 오픈소스 Operator Framework을 기반으로 작성되었습니다. 관련 참고 사이트는 아래와 같습니다.\n OCI Service Operator for Kubernetes GitHub OCI Service Operator for Kubernetes GitHub Documentation [OCI Docs Documentation] Adding OCI Service Operator for Kubernetes to Clusters  현재 v1.0.0 기준 지원하고 있는 OCI 서비스는 다음과 같습니다.\n Autonomous Database 서비스 MySQL Database 서비스 Streaming 서비스  OCI Service Operator for Kubernetes를 OKE Cluster에 설치 제품 설치문서를 따라 설치한 내용으로 자세한 사항은 아래 문서를 참고합니다.\n oci-service-operator/installation.md at main · oracle/oci-service-operator (github.com)  Operator SDK 설치 공식 설치 문서에 따라 설치합니다.\n Installation the Operator SDK CLI  Cloud Shell 기준 설치 명령 예시\n 아래 명령어로 설치하여 operator-sdk cli가 정상동작하는 지 확인합니다.  echo Download the release binary export ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac) export OS=$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;) export OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.15.0 curl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH} echo Verify the downloaded binary gpg --keyserver keyserver.ubuntu.com --recv-keys 052996E2A20B5C7E curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt.asc gpg -u \u0026#34;Operator SDK (release) \u0026lt;cncf-operator-sdk@cncf.io\u0026gt;\u0026#34; --verify checksums.txt.asc grep operator-sdk_${OS}_${ARCH} checksums.txt | sha256sum -c echo Install the release binary in your PATH chmod +x operator-sdk_${OS}_${ARCH} \u0026amp;\u0026amp; mv operator-sdk_${OS}_${ARCH} ~/.local/bin/operator-sdk operator-sdk version Operator Lifecycle Manager (OLM) 설치 아래 명령으로 현재 OKE 클러스터에 OLM 자원을 설치 및 확인합니다.\noperator-sdk olm install operator-sdk olm status OCI Service Operator for Kubernetes 설치 Instance Principal OKE Worker Node에 대한 Dynamic Group 만들기  OCI 콘솔에 로그인 하여 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동하여 OKE 클러스터가 있는 Compartment의 OCID를 복사합니다. 좌측 Dynamic Group 메뉴로 이동하여 복사한 OCID로 아래 규칙을 가진 Dynamic Group을 만듭니다.  Name: 예, oke-labs-dynamic-group Rule: instance.compartment.id = '\u0026lt;compartment-ocid\u0026gt;'    Dynamic Group을 위한 Policy 만들기   좌측 Policies 메뉴로 이동하여 만든 Dynamic Group에 아래와 같이 권한을 부여합니다.\nAllow dynamic-group \u0026lt;DYNAMICGROUP_NAME\u0026gt; to manage \u0026lt;OCI_SERVICE_1\u0026gt; in compartment \u0026lt;COMPARTMENT_NAME\u0026gt; ...   예시\n  Policy Name: oke-labs-osok-policy\n  COMPARTMENT_NAME: oke-labs\n  DYNAMICGROUP_NAME: oke-labs-dynamic-group\n  Autonomous Database Service\nAllow dynamic-group oke-labs-dynamic-group to manage autonomous-database-family in compartment oke-labs   MySQL DB System Service\nAllow dynamic-group oke-labs-dynamic-group to manage mysql-family in compartment oke-labs Allow dynamic-group oke-labs-dynamic-group to {SUBNET_READ, SUBNET_ATTACH, SUBNET_DETACH, VCN_READ, COMPARTMENT_INSPECT} in compartment oke-labs Allow dynamic-group oke-labs-dynamic-group to use tag-namespaces in compartment oke-labs     Enable User Principal   OSOK가 배포될 namespace를 만듭니다.\nkubectl create ns oci-service-operator-system kubectl label ns oci-service-operator-system control-plane=controller-manager   OCI Service Operator for Kubernetes가 OCI 서비스 및 자원을 생성, 관리할 사용자 정보에 대한 Kubernetes secret으로 만듭니다.\n  tenancy: 사용할 Tenancy OCID\n  user: 사용자의 OCID\n  privatekey: API_KEY로 등록한 Private Key 경로\n  fingerprint: API_KEY로 등록한 Public Key의 fingerprint\n  passphrase: Private Key 생성시 입력한 passphrase, 없으면 빈값\n  region: OKE 클러스터가 있는 region id, 예, ap-seoul-01\nkubectl -n oci-service-operator-system create secret generic ocicredentials \\ --from-literal=tenancy=\u0026lt;CUSTOMER_TENANCY_OCID\u0026gt; \\ --from-literal=user=\u0026lt;USER_OCID\u0026gt; \\ --from-literal=fingerprint=\u0026lt;USER_PUBLIC_API_KEY_FINGERPRINT\u0026gt; \\ --from-literal=region=\u0026lt;USER_OCI_REGION\u0026gt; \\ --from-literal=passphrase=\u0026lt;PASSPHRASE_STRING\u0026gt; \\ --from-file=privatekey=\u0026lt;PATH_OF_USER_PRIVATE_API_KEY\u0026gt;   예시\nkubectl -n oci-service-operator-system create secret generic ocicredentials \\ --from-literal=tenancy=ocid1.tenancy.oc1..aaaaaaaam~~~~~~~~~~~~~~~~~~ \\ --from-literal=user=ocid1.user.oc1..aaaaaaaaz~~~~~~~~~~~~~~~~~~ \\ --from-literal=fingerprint=a0:e1:fe:79:22:22:f0:b5:6b:29:72:5f:5d:~~:~~:~~ \\ --from-literal=region=ap-seoul-01 \\ --from-literal=passphrase= \\ --from-file=privatekey=/home/oke_admin/.oci/oci_api_key.pem     OCI Service Operator for Kubernetes(OSOK) 배포   OSOK Operator 설치\ndocker pull iad.ocir.io/oracle/oci-service-operator-bundle:1.0.0 operator-sdk run bundle iad.ocir.io/oracle/oci-service-operator-bundle:1.0.0   OSOK 설치후 OCI 서비스를 위한 CustomResource가 추가된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl api-resources --api-group=oci.oracle.com NAME SHORTNAMES APIVERSION NAMESPACED KIND autonomousdatabases oci.oracle.com/v1beta1 true AutonomousDatabases mysqldbsystems oci.oracle.com/v1beta1 true MySqlDbSystem streams oci.oracle.com/v1beta1 true Stream   OSOK로 Autonomous Database Service 관리하기 참고 문서\n oci-service-operator/adb.md at main · oracle/oci-service-operator (github.com)  ADB(Autonomous Database) Binding OCI 콘솔에서 만든 ADB 인스턴스를 Kubernetes의 자원으로 Binding하는 경우입니다.\n  OCI 콘솔에 로그인하여 바인딩할 Autonomous Database의 인스턴스를 생성합니다.\n  Binding을 위해 필요한 명세 yaml을 확인하여 작성합니다.\n spec.id: 기 존재하는, 바인딩할 ADB의 OCID를 입력 walletName: 바인딩후에 wallet이 저장될 kubernetes secret의 이름 입력 walletPassword.secret.secretName: wallet에 사용할 암호가 저장된 secret 이름, 바인딩 전에 미리 secret을 생성합니다.  apiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:\u0026lt;CR_OBJECT_NAME\u0026gt;spec:id:\u0026lt;AUTONOMOUS_DATABASE_OCID\u0026gt;wallet:walletName:\u0026lt;WALLET_SECRET_NAME\u0026gt;walletPassword:secret:secretName:\u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt;  실행 예시\n  walletPassword 생성\nkubectl create secret generic ociadb-wallet-password-secret --from-literal=walletPassword=\u0026#39;xxxxxxxxxxxx\u0026#39;   실행\ncat \u0026lt;\u0026lt;EOF \u0026gt; autonomousdatabases-bind.yamlapiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbspec:id:ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaahewyo25rujjz36bwsihfedmk3sxejchlq64krjylnhgawallet:walletName:ociadb-wallet-secretwalletPassword:secret:secretName:ociadb-wallet-password-secretEOFkubectl apply -f autonomousdatabases-bind.yaml    결과 확인\nkubectl describe 명령을 통해 에러없이 바인딩이 성공했는지 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get autonomousdatabases NAME DBWORKLOAD STATUS AGE ociadb Active 7m20s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe autonomousdatabases ociadb Name: ociadb ... Kind: AutonomousDatabases ... Status: Status: Conditions: Last Transition Time: 2021-12-22T09:29:25Z Message: AutonomousDatabase Bound success ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Success 7m54s AutonomousDatabases Finalizer is added to the object Normal Success 7m52s (x2 over 7m53s) AutonomousDatabases Create or Update of resource succeeded   wallet 확인\n바인딩 결과 ociadb-wallet-secret secret이 생성되며, 내용을 보면 wallet 상에 있는 파일들이 Base64로 인코딩된 형태로 있는 것을 확인할 수 있습니다. 애플리케이션 컨테이너에서 secret을 마운트하여 ADB 연결시 사용하면 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE ... ociadb-wallet-password-secret Opaque 1 9m40s ociadb-wallet-secret Opaque 8 9m32s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret ociadb-wallet-secret -o yaml apiVersion: v1 data: README: V2FsbGV0IE... cwallet.sso: ofhONgAAAA... ewallet.p12: MIIZ/AIBAz... keystore.jks: /u3+7QAAA... ojdbc.properties: IyBDb2... sqlnet.ora: V0FMTE... tnsnames.ora: b2NpYWR... truststore.jks: /u3+7QAAAA... kind: Secret metadata: ... name: ociadb-wallet-secret ... type: Opaque   ADB(Autonomous Database) Provisioning   Provisionig을 위해 필요한 명세 yaml을 확인하여 작성합니다.\n spec.compartmentId: 생성될 ADB가 위치할 Compartment의 OCID를 입력 walletName: 바인딩후에 wallet이 저장될 kubernetes secret의 이름 입력 walletPassword.secret.secretName: wallet에 사용할 암호가 저장된 secret 이름, 바인딩 전에 미리 secret을 생성합니다. 나머지 항목은 OCI 콘솔에서 ADB 생성시 입력하는 것과 동일하게 원하는 값 입력 - 항목 명세 참조  apiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:\u0026lt;CR_OBJECT_NAME\u0026gt;spec:compartmentId:\u0026lt;COMPARTMENT_OCID\u0026gt;displayName:\u0026lt;DISPLAY_NAME\u0026gt;dbName:\u0026lt;DB_NAME\u0026gt;dbWorkload:\u0026lt;OLTP/DW\u0026gt;isDedicated:\u0026lt;false/true\u0026gt;dbVersion:\u0026lt;ORABLE_DB_VERSION\u0026gt;dataStorageSizeInTBs:\u0026lt;SIZE_IN_TBs\u0026gt;cpuCoreCount:\u0026lt;COUNT\u0026gt;adminPassword:secret:secretName:\u0026lt;ADMIN_PASSWORD_SECRET_NAME\u0026gt;isAutoScalingEnabled:\u0026lt;true/false\u0026gt;isFreeTier:\u0026lt;false/true\u0026gt;licenseModel:\u0026lt;BRING_YOUR_OWN_LICENSE/LICENSE_INCLUDEE\u0026gt;wallet:walletName:\u0026lt;WALLET_SECRET_NAME\u0026gt;walletPassword:secret:secretName:\u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt;freeformTags:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;definedTags:\u0026lt;TAGNAMESPACE1\u0026gt;:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;  실행 예시\n  adminPassword, walletPassword 생성\nkubectl create secret generic ociadb-by-osok-admin-password-secret --from-literal=password=\u0026#39;xxxxxxxxxxxx\u0026#39; kubectl create secret generic ociadb-by-osok-wallet-password-secret --from-literal=walletPassword=\u0026#39;xxxxxxxxxxxx\u0026#39;   실행\ncat \u0026lt;\u0026lt;EOF \u0026gt; autonomousdatabases-provision.yamlapiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbbyosokspec:compartmentId:ocid1.compartment.oc1..aaaaaaaaa2jcbfqjyz24y4hbbqurdxjegmsp6eqhzq4r2gni5bocoh2axb4adisplayName:OCIADBbyOSOKdbName:ociadbbyosokdbWorkload:OLTPisDedicated:falsedbVersion:19cdataStorageSizeInTBs:1cpuCoreCount:1adminPassword:secret:secretName:ociadb-by-osok-admin-password-secretisAutoScalingEnabled:falseisFreeTier:falselicenseModel:LICENSE_INCLUDEDwallet:walletName:ociadb-by-osok-wallet-secretwalletPassword:secret:secretName:ociadb-by-osok-wallet-password-secretEOFkubectl apply -f autonomousdatabases-provision.yaml    결과 확인\nkubectl describe 명령을 통해 에러없이 바인딩이 성공했는지 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get autonomousdatabases NAME DBWORKLOAD STATUS AGE ociadbbyosok OLTP Active 5m30s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe autonomousdatabases ociadbbyosok Name: ociadbbyosok ... Kind: AutonomousDatabases ... Status: Status: Conditions: Last Transition Time: 2021-12-23T02:54:54Z Message: AutonomousDatabase Provisioning Status: True ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Success 6m14s AutonomousDatabases Finalizer is added to the object Normal Success 5m5s (x2 over 5m6s) AutonomousDatabases Create or Update of resource succeeded   wallet 확인\n바인딩 결과 ociadb-wallet-secret secret이 생성되며, 내용을 보면 wallet 상에 있는 파일들이 Base64로 인코딩된 형태로 있는 것을 확인할 수 있습니다. 애플리케이션 컨테이너에서 secret을 마운트하여 ADB 연결시 사용하면 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE ... ociadb-by-osok-admin-password-secret Opaque 1 23m ociadb-by-osok-wallet-password-secret Opaque 1 23m ociadb-by-osok-wallet-secret Opaque 8 20m oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret ociadb-by-osok-wallet-secret -o yaml apiVersion: v1 data: README: V2FsbGV0IE... cwallet.sso: ofhONgAAAA... ewallet.p12: MIIZ/AIBAz... keystore.jks: /u3+7QAAA... ojdbc.properties: IyBDb2... sqlnet.ora: V0FMTE... tnsnames.ora: b2NpYWR... truststore.jks: /u3+7QAAAA... kind: Secret metadata: ... name: ociadb-by-osok-wallet-secret ... type: Opaque   ADB(Autonomous Database) Update OCI API에서 제공하는 Autonomous Database에 대한 Update 지원 항목내에서 OSOK GitHub 문서의 예시를 참고합니다.\n  Oracle Cloud Infrastructure API Reference and Endpoints / UpdateAutonomousDatabaseDetails Reference\n  GitHub 문서 기준\napiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:\u0026lt;CR_OBJECT_NAME\u0026gt;spec:id:\u0026lt;AUTONOMOUS_DATABASE_OCID\u0026gt;displayName:\u0026lt;DISPLAY_NAME\u0026gt;dbName:\u0026lt;DB_NAME\u0026gt;dbWorkload:\u0026lt;OLTP/DW\u0026gt;isDedicated:\u0026lt;false/true\u0026gt;dbVersion:\u0026lt;ORABLE_DB_VERSION\u0026gt;dataStorageSizeInTBs:\u0026lt;SIZE_IN_TBs\u0026gt;cpuCoreCount:\u0026lt;COUNT\u0026gt;adminPassword:secret:secretName:\u0026lt;ADMIN_PASSWORD_SECRET_NAME\u0026gt;isAutoScalingEnabled:\u0026lt;true/false\u0026gt;isFreeTier:\u0026lt;false/true\u0026gt;licenseModel:\u0026lt;BRING_YOUR_OWN_LICENSE/LICENSE_INCLUDEE\u0026gt;wallet:walletName:\u0026lt;WALLET_SECRET_NAME\u0026gt;walletPassword:secret:secretName:\u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt;freeformTags:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;definedTags:\u0026lt;TAGNAMESPACE1\u0026gt;:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;  Binding 한 경우 기존 YAML 또는 배포된 YAML에 업데이트 항목을 추가 하여 반영합니다.\n  스토리지 증가 예시\n앞선 autonomousdatabases-bind.yaml 파일에 dataStorageSizeInTBs 항목을 추가하여 배포합니다.\napiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbspec:id:ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaahewyo25rujjz36bwsihfedmk3sxejchlq64krjylnhgawallet:walletName:ociadb-wallet-secretwalletPassword:secret:secretName:ociadb-wallet-password-secretdataStorageSizeInTBs:2  Provisioning 한 경우   스토리지 증가 예시\n앞선 autonomousdatabases-provision.yaml 파일에 생성된 ADB의 OCID를 spec.id에 추가합니다. 그리고 dataStorageSizeInTBs 값을 변경합니다.\napiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbbyosokspec:id:ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaaz6ydcc2epj236mgguz4kuc3udjontb5hetrwr4545x5acompartmentId:ocid1.compartment.oc1..aaaaaaaaa2jcbfqjyz24y4hbbqurdxjegmsp6eqhzq4r2gni5bocoh2axb4adisplayName:OCIADBbyOSOKdbName:ociadbbyosokdbWorkload:OLTPisDedicated:falsedbVersion:19cdataStorageSizeInTBs:2cpuCoreCount:1adminPassword:secret:secretName:ociadb-by-osok-admin-password-secretisAutoScalingEnabled:falseisFreeTier:falselicenseModel:LICENSE_INCLUDEDwallet:walletName:ociadb-by-osok-wallet-secretwalletPassword:secret:secretName:ociadb-by-osok-wallet-password-secret  업데이트 실행결과\n oke_admin@cloudshell:autonomousdatabases (ap-seoul-1)$ kubectl describe autonomousdatabases ociadbbyosok Name: ociadbbyosok ... Status: Status: Conditions: Last Transition Time: 2021-12-23T04:23:49Z Message: AutonomousDatabase Provisioning Status: True Type: Provisioning Last Transition Time: 2021-12-23T04:24:54Z Message: AutonomousDatabase OCIADBbyOSOK is Active Status: True Type: Active Last Transition Time: 2021-12-23T05:25:53Z Message: AutonomousDatabase Update success Status: True Type: Active Ocid: ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaaz6ydcc2epj236mgguz4kuc3udjontb5hetrwr4545x5a ...   ADB(Autonomous Database) Delete 현재 버전 기준으로 Delete 기능을 따로 제공하지 않아, OKE 클러스터에서 autonomousdatabases 자원을 kubectl delete 명령으로 삭제해도 실제 ADB 인스턴스가 OCI에서 삭제되지는 않습니다.\n","lastmod":"2021-12-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oracle-oss/1.oci-service-operator/","tags":["oke","opensource","osok"],"title":"5.1 OCI Service Operator for Kubernetes(OSOK)"},{"categories":null,"contents":"4.4.1 Service Mesh 없는 마이크로서비스 앱 배포 본 내용은 아래 Istio 문서 상에 있는 내용을 재 확인하는 내용으로 마이크로 서비스 앱을 사용하는 데 있어서 어떤 문제가 발생할 수 있는지, 왜 Istio 같은 Service Mesh 필요한지에 대해 알아보는 내용입니다.\n https://istio.io/latest/docs/examples/microservices-istio/bookinfo-kubernetes/ https://istio.io/latest/docs/examples/microservices-istio/add-new-microservice-version/  테스트 마이크로 서비스 앱(Bookinfo) 배포 테스트 앱\n  Product, Review, Details, Ratings의 4개 마이크로 서비스 앱으로 구성되어 있습니다.\n  먼저 여기서는 Reviews 서비스는 v1만 배포합니다.\n  아직 istio 설정이 안된 상태에서 배포 및 결과를 확인합니다.\n     앱 배포\nkubectl apply -l version!=v2,version!=v3 -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml   스케일\nkubectl scale deployments --all --replicas 3   외부 접근을 위한 서비스 오픈\n테스트 편의를 위해 Load Balancer 타입으로 오픈합니다.\nkubectl patch svc productpage -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39;   배포 상태 확인\n각 앱이 3개의 Pod이며 각 Pod는 Container 1개로 구성된 것을 확인할 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-29z48 1/1 Running 0 21s details-v1-79f774bdb9-d5lfq 1/1 Running 0 32s details-v1-79f774bdb9-sqp9p 1/1 Running 0 20s productpage-v1-6b746f74dc-drgbw 1/1 Running 0 21s productpage-v1-6b746f74dc-pc2h2 1/1 Running 0 32s productpage-v1-6b746f74dc-qzkqm 1/1 Running 0 20s ratings-v1-b6994bb9-6xlq7 1/1 Running 0 32s ratings-v1-b6994bb9-j2l78 1/1 Running 0 20s ratings-v1-b6994bb9-w748t 1/1 Running 0 20s reviews-v1-545db77b95-hg7j8 1/1 Running 0 32s reviews-v1-545db77b95-l42v4 1/1 Running 0 20s reviews-v1-545db77b95-lgl89 1/1 Running 0 20s   서비스 확인\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.96.175.211 \u0026lt;none\u0026gt; 9080/TCP 67s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16d productpage LoadBalancer 10.96.79.55 146.56.142.95 9080:32649/TCP 66s ratings ClusterIP 10.96.105.212 \u0026lt;none\u0026gt; 9080/TCP 66s reviews ClusterIP 10.96.197.86 \u0026lt;none\u0026gt; 9080/TCP 66s   메인 페이지 접속\n http://{productpage external ip}:9080/productpage 각 서비스가 호출되어 정상적인 페이지가 보이는 것을 알 수 있습니다. Review v1에서는 아직 Rating 서비스와 연동되지 않아 Rating 정보는 보이지 않습니다.     Product page 호출 코드 확인\n 소스 파일: https://github.com/istio/istio/blob/release-1.12/samples/bookinfo/src/productpage/productpage.py reviews 호출 주소를 보면 http://{reviewsHostname}.{servicesDomain}:9080/reviews 인걸 알 수 있습니다.     실제 호출 로그\n실제 로그를 보면 http://reviews:9080/reviews 로 정상 호출 되었으며, 배포시의 ClusterIP 타입의 reviews Service를 통해 같은 네임스페이스 상에서는 reviews를 주소로하여 정상 호출된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs -l app=productpage -f ... DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): reviews:9080 send: b\u0026#39;GET /reviews/0 HTTP/1.1\\r\\nHost: reviews:9080\\r\\nuser-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nConnection: keep-alive\\r\\nX-B3-TraceId: cd0101ed97c6b43f\\r\\nX-B3-SpanId: cd0101ed97c6b43f\\r\\nX-B3-Sampled: 1\\r\\n\\r\\n\u0026#39; reply: \u0026#39;HTTP/1.1 200 OK\\r\\n\u0026#39; header: X-Powered-By: Servlet/3.1 header: Content-Type: application/json header: Date: Mon, 20 Dec 2021 07:41:55 GMT header: Content-Language: en-US header: Content-Length: 295 DEBUG:urllib3.connectionpool:http://reviews:9080 \u0026#34;GET /reviews/0 HTTP/1.1\u0026#34; 200 295 INFO:werkzeug:10.244.1.128 - - [20/Dec/2021 07:41:55] \u0026#34;GET /productpage HTTP/1.1\u0026#34; 200     Reviews 새 버전 배포   Reviews Service 확인\nSelector에서 보이는 것처럼 app=reviews 레이블이 달린 Pod로 분배하고 있습니다. 현재 배포되어있는 Reviews v1의 세 개의 Pod의 Endpoints로 분배되고 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe svc reviews Name: reviews Namespace: default Labels: app=reviews service=reviews Annotations: \u0026lt;none\u0026gt; Selector: app=reviews Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.96.197.86 IPs: 10.96.197.86 Port: http 9080/TCP TargetPort: 9080/TCP Endpoints: 10.244.2.42:9080,10.244.2.44:9080,10.244.2.45:9080 Session Affinity: None Events: \u0026lt;none\u0026gt;   이 상태에서 Reviews 앱의 새로운 버전인 v2를 개발하여 배포하게 된다면, 아마 배포해서 정상동작 확인후 서비스 라우팅이 되도록 app=review를 추가하게 될 것입니다. 이후 일부 요청을 서비스 하도록 실제 운영하다 문제 발생시 원복시키거나, 문제가 없는 경우, v1로 가는 요청을 줄이고, v2로 가는 요청을 점진적으로 늘려가면서 실제 v2로 이관하게 될 것입니다.\n  Reviews v2 배포   Reviews v2 버전을 배포합니다.\n라우팅에서 일단 제외하기 위해 app: reviews_test로 레이블을 변경하여 배포합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | sed \u0026#39;s/app: reviews/app: reviews_test/\u0026#39; | kubectl apply -l app=reviews_test,version=v2 -f -   배포 결과 확인\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ... reviews-v1-545db77b95-hg7j8 1/1 Running 0 76m 10.244.2.42 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-545db77b95-l42v4 1/1 Running 0 76m 10.244.2.45 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-545db77b95-lgl89 1/1 Running 0 76m 10.244.2.44 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v2-7cd8b85558-c2jmw 1/1 Running 0 30s 10.244.2.47 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Reviews v2 Pod 테스트\nproductpage Pod에서 Reviews v2 Pod로 테스트하면 좋겠지만, productpage Pod에 테스트용 curl 없는 관계로 별도 클라이언트용 Pod를 배포하여 테스트합니다.\n  테스트 클라이언트 Pod 배포\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/sleep/sleep.yaml   테스트\nREVIEWS_V2_POD_IP=$(kubectl get pod -l app=reviews_test,version=v2 -o jsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;) echo $REVIEWS_V2_POD_IP kubectl exec $(kubectl get pod -l app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -- curl -sS \u0026#34;$REVIEWS_V2_POD_IP:9080/reviews/7\u0026#34;     테스트 결과\n아래와 같이 v2 버전이 잘 동작함을 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ REVIEWS_V2_POD_IP=$(kubectl get pod -l app=reviews_test,version=v2 -o jsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;) oke_admin@cloudshell:~ (ap-seoul-1)$ echo $REVIEWS_V2_POD_IP 10.244.2.47 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -- curl -sS \u0026#34;$REVIEWS_V2_POD_IP:9080/reviews/7\u0026#34; {\u0026#34;id\u0026#34;: \u0026#34;7\u0026#34;,\u0026#34;reviews\u0026#34;: [{ \u0026#34;reviewer\u0026#34;: \u0026#34;Reviewer1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;An extremely entertaining play by Shakespeare. The slapstick humour is refreshing!\u0026#34;, \u0026#34;rating\u0026#34;: {\u0026#34;stars\u0026#34;: 5, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}},{ \u0026#34;reviewer\u0026#34;: \u0026#34;Reviewer2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Absolutely fun and entertaining. The play lacks thematic depth when compared to other plays by Shakespeare.\u0026#34;, \u0026#34;rating\u0026#34;: {\u0026#34;stars\u0026#34;: 4, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}}]}   Reviews v2의 서비스 라우팅에 추가   version=v2 레이블이 달린 Review v2 Pod에 app=reviews 레이블을 추가합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl label pod -l version=v2 app=reviews --overwrite pod/reviews-v2-7cd8b85558-c2jmw labeled   Reviews Service 재확인\n아래와 같이 v2가 추가가 되어 Endpoints에 이전 3개에서 4개로 변경된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe svc reviews Name: reviews ... Selector: app=reviews ... Endpoints: 10.244.2.42:9080,10.244.2.44:9080,10.244.2.45:9080 + 1 more...   메인 페이지 접속\n  http://{productpage external ip}:9080/productpage\n  페이지를 새로고침하면 4번 중 1번은 Review v2 Pod로 라우팅되어 아래 그림과 같이 별점 표시가 되는 것을 볼 수 있습니다.\n     Reviews v2 문제 확인후 재배포   서비스 중에 문제가 발생한 경우 다음과 같이 삭제합니다.\n 위에서 서비스 라우팅을 위해 기존 Pod의 label을 변경했기 때문에, 변경된 Pod(app=reviews)외에 app=reviews_test의 Deployment도 아직 남아 있습니다. 둘다 삭제합니다. (실제로는 이렇게 하면 안되겠네요. 근데, Deployment 내에 정의된 Pod Template의 label은 변경이 안되네요\u0026hellip;)  kubectl delete deployment reviews-v2 kubectl delete pod -l app=reviews,version=v2   문제 해결후 재배포\n최종 배포시에는 아래와 같이 배포후 v2의 갯수를 늘리고 v1은 삭제하게 될 것입니다. 실제 상황에서는 점진적으로 v2을 늘리고 v1을 줄이는 요구사항도 있을 것입니다.\nkubectl apply -l app=reviews,version=v2 -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml kubectl scale deployment reviews-v2 --replicas=3 kubectl delete deployment reviews-v1   실제 배포시 대응 방법 실제 환경에서 마이크로 서비스 업데이트시의 안정적인 배포를 위해 새 버전 배포후 테스트, 점진적인 새 버전으로의 이관을 위한 여러가지 방안들이 나왔습니다. 그 대표적인 방법으로 가장 많이 사용하고 있는 것은 크게 Service Mesh와 Netflix\u0026rsquo;s Hystrix를 기반한 방법입니다.\n Service Mesh: Istio로 대표되는 서비스 메쉬를 사용하는 방법은 sidecar로 모듈을 Service Mesh 단에서 추가하는 방법입니다. Pod에 Application Container외에 Sidecar Container가 추가되고, Sidecar를 이용해 서비스 라우팅 및 추가 기능을 제공하는 방식입니다. 애플리케이션 코드에 별도 작업이 필요하지 않는 장점이 있습니다. 애플리케이션 코드 구현: Netflix\u0026rsquo;s Hystrix 예와 같이 서비스 라우팅, 보안, 모니터링 등에 필요한 기능을 라이브러리 모듈에서 제공하는 기능을 사용하는 방식입니다. 라이브러리를 사용하기 때문 코딩 자체량은 많지 않고, Annotation 등을 사용하게 됩니다. Spring 프레임워크에서는 Spring Cloud로 발전하여 기능을 제공하고 있습니다.  ","lastmod":"2021-12-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/service-mesh/1.sampleapp-without-istio/","tags":["oss","service mesh","istio"],"title":"4.4.1 Service Mesh 없는 마이크로서비스 앱 배포"},{"categories":null,"contents":"4.3.1 Prometheus\u0026amp;Grafana 설치하기 Prometheus 설치   설치용 namespace를 만듭니다.\nkubectl create ns monitoring   Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/kube-prometheus#parameters 아래 예시는 prometheus와 alertmanager 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamlprometheus:ingress:enabled:truehostname:prometheus.ingress.thekoguryo.mlpath:/annotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:truealertmanager:ingress:enabled:truehostname:alertmanager.ingress.thekoguryo.mlpath:/annotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  prometheus helm chart 설치\nhelm install prometheus -n monitoring -f values.yaml bitnami/kube-prometheus   설치결과\nPrometheus의 내부 DNS 정보는 이후 Grafana에서 연동할 때 사용됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ helm install prometheus -n monitoring -f values.yaml bitnami/kube-prometheus NAME: prometheus ... Prometheus can be accessed via port \u0026#34;9090\u0026#34; on the following DNS name from within your cluster: prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local ...   Grafana 설치   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/grafana#parameters grafana 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamladmin:password:\u0026#34;higrafana\u0026#34;ingress:enabled:truehostname:grafana.ingress.thekoguryo.mlpath:/annotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  grafana helm chart 설치\nhelm install grafana -n monitoring -f values.yaml bitnami/grafana   설치결과\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ helm install grafana -n monitoring -f values.yaml bitnami/grafana NAME: grafana ... 2. Get the admin credentials: echo \u0026#34;User: admin\u0026#34; echo \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34; oke_admin@cloudshell:grafana (ap-seoul-1)$ echo \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34;   Grafana 설정   Grafana 웹 UI를 접속합니다.\n  URL: values.yaml에서 설정한 ingress.hostname\n  관리자 계정\n  User: admin\n  Password: values.yaml에 설정한 admin.password 또는 미입력시 다음 명령으로 확인\necho \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34;        Prometheus 등록을 위해 왼쪽 메뉴에서 Configuration \u0026gt; Data sources 선택\n   Add data source 클릭\n  Prometheus 선택\n   Prometheus 연결정보 입력\n앞서 Prometheus 설치 로그에 확인한 Prometheus 내부 연결 정보를 입력합니다.\nhttp://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090    아래 쪽 Save \u0026amp; test 클릭\n  Grafana Dashboard 추가   Kubernetes 모니터링을 위해 공개된 Grafana Dashboard를 사용할 수 있습니다.\n https://grafana.com/grafana/dashboards/?search=kubernetes\u0026amp;dataSource=prometheus     위 사이트에서 검색된 대쉬보드 중 원하는 것을 선택합니다.\n  대쉬보드 상세 페이지에 ID를 확인합니다.\n   Dashboard 등록을 위해 왼쪽 메뉴에서 Dashboards \u0026gt; Manage 선택\n   Import 클릭\n  임포트할 대쉬보드 ID(예, 7249) 입력후 Load 클릭\n  data source를 앞서 등록한 Prometheus 선택후 Import 클릭\n   임포트된 대쉬보드를 통해 OKE 클러스터 모니터링\n   위와 같이 공개된 대쉬보드를 사용하거나, 원하는 대쉬보드를 작성하여 Prometheus에서 수집된 메트릭 정보를 이용하여, Kubernetes 클러스터를 모니터링 할 수 있습니다.\n  ","lastmod":"2021-12-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/monitoring/1.install-prometheus-grafana/","tags":["oss","monitoring","prometheus","grafana"],"title":"4.3.1 Prometheus\u0026Grafana 설치하기"},{"categories":null,"contents":"4.2.1 EFK 설치하기 Elastic Search + Kibana 설치   설치용 namespace를 만듭니다.\nkubectl create ns logging   Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/elasticsearch/#parameters 아래 예시는 elasticsearch 내장 kibana를 함께 설치하고, kibana 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamlglobal:kibanaEnabled:truekibana:ingress:enabled:truehostname:kibana.ingress.thekoguryo.mlannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  elasticsearch helm chart 설치\nhelm install elasticsearch -f values.yaml bitnami/elasticsearch -n logging   설치\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ helm install elasticsearch -f values.yaml bitnami/elasticsearch -n logging NAME: elasticsearch ... Elasticsearch can be accessed within the cluster on port 9200 at elasticsearch-coordinating-only.logging.svc.cluster.local To access from outside the cluster execute the following commands: kubectl port-forward --namespace logging svc/elasticsearch-coordinating-only 9200:9200 \u0026amp; curl http://127.0.0.1:9200/   설치된 elastic search 내부 주소와 포트를 확인합니다. 이후 Fluentd에서 로그 전송을 위해 사용할 주소입니다.\n 주소: elasticsearch-coordinating-only.logging.svc.cluster.local 포트: 9200    Fluentd 구성 참고 문서\n Kubernetes - Fluentd https://gist.github.com/agapoff/77d746b4588ee37a9e8074904533f6bc    Fluentd 설치를 위한 Service Account를 생성하고 관련 권한을 정의합니다.\n   configmap 추가 설정정의\n Fluentd 컨테이너 이미지에는 로그 파싱과 관련된 설정들이 컨테이너 이미지내에 /fluentd/etc/ 하위에 .conf 파일로 모두 정의 되어 있습니다. 이 파일들을 재정의 할 수 있습니다. 여기에서는 다른 설정들은 그대로 두고 Parser만 변경합니다. 기본 Parser는 Docker Engine이 런타임인 경우 잘 동작하지만, 최근 OSS 쿠버네티스의 기본 런타임인 containerd와 OKE에서 사용하고 있는 cri-o에서는 파싱 에러가 발생합니다. 정상 파싱을 위해 파서 설정(tail_container_parse.conf)만 아래와 같이 cri Parser로 변경합니다. https://github.com/fluent/fluentd-kubernetes-daemonset/issues/434#issuecomment-831801690     fluentd damonset 정의\n설정한 configmap 사용을 위해 Fluentd 문서상의 YAML을 일부 변경하셨습니다.\n   FluentD 설치\nkubectl apply -f fluentd-rbac.yaml kubectl apply -f fluentd-configmap-elasticsearch.yaml kubectl apply -f fluentd-daemonset-elasticsearch.yaml   Kibana 설정   설치한 kibana을 웹 브라우저로 접속합니다. ingress로 지정한 주소로 접속합니다.\n 예, https://kibana.ingress.thekoguryo.ml    Welcome 페이지의 Add Data를 클릭하여 홈으로 이동합니다.\n  왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n   Create index pattern을 클릭합니다.\n  인덱스 패턴을 생성합니다.\n오른쪽에서 보듯이 FluentD에서 전송된 로그는 logstash-로 시작합니다.\n Name: logstash-* Timestamp field: @timestamp     인덱스 패턴이 추가된 결과를 볼 수 있습니다.\n   왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n  생성한 인덱스 패턴을 통해 수집된 로그를 확인할 수 있습니다.\n 테스트 앱의 로그를 확인하기 위해 Add filter를 클릭하여 namespace_name=default 로 지정합니다.     테스트 앱을 접속합니다.\n 예, https://blue.ingress.thekogury.ml/ekf-test    로그 확인\n아래와 같이 kibana에서 테스트 앱의 로그를 확인할 수 있습니다.\n   EKF를 통해 OKE 상의 로그를 수집하는 예시였습니다. EKF에 대한 상세 내용은 제품 관련 홈페이지와 커뮤니티 사이트를 참고하기 바랍니다.\n  ","lastmod":"2021-12-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/logging-efk/1.install-efk/","tags":["oss","logging","efk"],"title":"4.2.1 EFK 설치하기"},{"categories":null,"contents":"4.1.1 NGINX Ingress Controller 사용하기 OKE에서 Kubernetes에서 사용가능한 여러가지 오픈 소스 ingress controller를 사용할 수 있습니다. 본 문서에서는 그중에서 가장 많이 사용되며, OKE 문서에서 예제로 설명하고 있는 nginx-ingress-controller를 테스트 해보겠습니다.\n  공식 문서\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm https://kubernetes.github.io/ingress-nginx/deploy/#oracle-cloud-infrastructure    설치 버전\n OKE 문서는 v0.44.0 기준이 NGINX Ingress Controller for Kubernetes 문서는 최신 버전인 v1.1.0 기준입니다. 본 문서는 최신 버전인 v1.1.0 기준이며, 해당 버전은 Kubernetes 1.22, 1.21, 1.20, 1.19을 지원하고 있습니다. NGINX Ingress Controller의 지원 버전  https://github.com/kubernetes/ingress-nginx#support-versions-table      NGINX Ingress Controller 설치 Ingress Controller 설치   kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\n  다음 명령으로 NGINX Cloud Deployment 설치\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml   설치 확인\ningress-nginx namespace에 아래와 같이 설치된 것을 확인할 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-x74zf 0/1 Completed 0 65s pod/ingress-nginx-admission-patch-f8x5k 0/1 Completed 0 65s pod/ingress-nginx-controller-69db7f75b4-vb84p 1/1 Running 0 65s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller LoadBalancer 10.96.212.64 132.226.225.240 80:31975/TCP,443:31756/TCP 65s service/ingress-nginx-controller-admission ClusterIP 10.96.95.133 \u0026lt;none\u0026gt; 443/TCP 65s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 65s NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-69db7f75b4 1 1 1 65s NAME COMPLETIONS DURATION AGE job.batch/ingress-nginx-admission-create 1/1 2s 65s job.batch/ingress-nginx-admission-patch 1/1 2s 65s   Load Balancer IP 확인   Ingress Controller 서비스의 로드밸런서 IP인 EXTERNAL-IP를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.96.212.64 132.226.225.240 80:31975/TCP,443:31756/TCP 5m31s   PATH 기반 기본 라우팅 테스트 가장 기본적인 라우팅으로 URL PATH에 따라 라우팅 서비스를 달리하는 경우입니다.\n  테스트를 위한 샘플 앱을 배포합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(path-basic.yaml)을 작성합니다.\n /blue 요청은 nginx-blue-svc 로 라우팅 /green 요청은 nginx-green-svc로 라우팅  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-path-basicannotations:kubernetes.io/ingress.class:nginxspec:rules:- http:paths:- path:/bluepathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- http:paths:- path:/greenpathType:Prefixbackend:service:name:nginx-green-svcport:number:80  작성한 path-basic.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f path-basic.yaml ingress.networking.k8s.io/ingress-path-basic created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-basic \u0026lt;none\u0026gt; * 132.226.225.240 80 49s   앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n  /blue 요청\n   /green 요청\n   POD 정보 확인\n정의한 PATH에 따라 각각 blue, green 앱이 배포된 POD로 라우팅 된 것을 웹페이지 배경색 및 POD IP로 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-blue-6fccd8fb49-q6qph 1/1 Running 0 13m 10.244.0.138 10.0.10.139 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-green-7f646c5c7f-snpxf 1/1 Running 0 13m 10.244.0.5 10.0.10.84 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;     Rewrite Target URL PATH 라우팅 결과를 보면 /blue, /green의 Path가 최종 라우팅 되어 실행되는 앱으로 그대로 전달되는 것을 알 수 있습니다. ingress controller에서 라우팅을 위해서만 사용하고, 실제 앱의 동작을 위해는 수정이 필요한 경우에 사용합니다.\n https://kubernetes.github.io/ingress-nginx/examples/rewrite/    ingress 설정 YAML(path-rewrite-target.yaml)을 작성합니다.\n path: /blue -\u0026gt; /blue(/|$)(.*) 로 변경 annotation 추가: nginx.ingress.kubernetes.io/rewrite-target: /$2 예시  ~~/blue -\u0026gt; ~~/ 로 앱으로 전달 ~~/blue/abc -\u0026gt; ~~/abc 로 앱으로 전달    apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-path-rewrite-targetannotations:kubernetes.io/ingress.class:nginxnginx.ingress.kubernetes.io/rewrite-target:/$2spec:rules:- http:paths:- path:/blue(/|$)(.*)pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- http:paths:- path:/green(/|$)(.*)pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  앞선 path-basic.yaml를 삭제하고 path-rewrite-target.yaml를 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl delete -f path-basic.yaml ingress.networking.k8s.io \u0026#34;ingress-path-basic\u0026#34; deleted oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f path-rewrite-target.yaml ingress.networking.k8s.io/ingress-path-rewrite-target created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-rewrite-target \u0026lt;none\u0026gt; * 132.226.225.240 80 43s   앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n  ~~/blue 요청\n 라우팅된 앱에서는 /blue가 빠지고 /로만 수신됨     ~~/blue/abc 요청\n 라우팅된 앱에서는 /blue가 빠지고 /abc로만 수신됨       ","lastmod":"2021-12-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/ingress-controller/1.install-nginx-ingress-controller/","tags":["oss","ingress-controller"],"title":"4.1.1 NGINX Ingress Controller 사용하기"},{"categories":null,"contents":"3.1 컨테이너 이미지 스캔 OCIR는 알려진 Common Vulnerabilities and Exposures (CVE) database를 기반으로 컨테이너 이미지에 대한 취약점 분석 기능을 제공하고 있습니다. 다음은 취약점 분석을 위한 이미지 스캔 기능을 확인해 봅니다.\n공식 문서\n Scanning Images for Vulnerabilities  관련 Policy 설정 스캔 서비스가 OCIR repository에 대한 권한을 부여합니다. 적용해야 하는 Policy는 위 공식 문서를 참조합니다. tenancy 전체 또는 compartment 에 대해서 지정할 수 있습니다.\n  전체 테넌시\n 이름 예, ocir-scanning-images-root-policy  allow service vulnerability-scanning-service to read repos in tenancy allow service vulnerability-scanning-service to read compartments in tenancy   특정 compartment\nallow service vulnerability-scanning-service to read repos in compartment \u0026lt;compartment-name\u0026gt; allow service vulnerability-scanning-service to read compartments in compartment \u0026lt;compartment-name\u0026gt;   설정 예시\n   이미지 스캐너 설정 이미지 스캐너는 repository 단위로 기능 추가, 삭제 할 수 있습니다. 테스트를 위해 먼저 이미지를 배포합니다.\n  OCIR 이미지 사용하여 앱 배포하기를 참고하여 OCIR에 이미지를 배포합니다.\n  OCIR에 이미지 배포\ndocker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, root)를 선택합니다.\n  스캐너를 설정할 Repository(예, nginx)를 선택하고 오른쪽 Actions 메뉴에서 Add scanner를 선택합니다.\n   생성할 스캐너의 compartment 위치를 확인하고, 처음 스캐너 사용이므로 Create new scan configuration으로 선택하여, 스캐너를 추가합니다. 현재는 스캐너별로 따라 추가 설정할 수 있는 기능이 없는 것으로 보여, 한번 만들고 다음부터는 만든 것을 사용하면 됩니다.\n   스캐너 결과 확인\n스캐너가 추가된 nginx repository에 대해서 tag 별로 컨테이너 이미지 상세 정보를 볼 수 있습니다. Scan results 탭을 보면 스캔 결과를 확인할 수 있으며, View details를 통해 상세 정보를 확인할 수 있습니다.\n   취약점 확인\n이미지 스캐닝 결과 확인 알려진 CVE 정보를 기반한 취약점을 확인 할 수 있으며, 취약점를 클릭하면, 실제 CVE 데이터 베이스로 이동하여, 상세 정보를 확인 할 수 있습니다.\n    ","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/ocir/1.scan-image/","tags":["container registry"],"title":"3.1 컨테이너 이미지 스캔"},{"categories":null,"contents":"1. OKE(Oracle Container Engine for Kubernetes) 소개 \u0026hellip;\n","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/1.oke/","tags":["oke"],"title":"1.1 OKE(Oracle Container Engine for Kubernetes) 소개"},{"categories":null,"contents":"1.3.1 Cloud Shell로 OKE 클러스터 연결하기 Cloud Shell Cloud Shell은 공식 문서에서 설명하는 것 처럼 Oracle Cloud 콘솔에서 제공하는 웹 브라우저 기반 리눅스 터미널입니다. 작은 가상머신으로 구동된다고 이해하시면 되며, Oracle Cloud 콘솔에 접속한 유저에 대해 사전 인증된 OCI CLI를 제공하며, 추가적인 cli 및 설정들을 기본 구성하여 제공합니다.\nOKE 클러스터 접속을 위해 Kubernetes CLI인 kubectl로 기본 설치되어 제공합니다.\n무료로 사용할 수 있고, 인증 및 기본 툴 들이 구성되어 있기 때문 바로 사용할 수 있는 이점이 있습니다.\nCloud Shell로 OKE 클러스터 연결하기   생성한 OKE 클러스터 상세 페이지로 이동합니다.\n  Access Cluster를 클릭합니다.\n   두 가지 접근 방법 중에 Cloud Shell Access을 선택합니다.\n Cloud Shell: OCI에서 제공하는 Cloud Shell을 통해 접근합니다. 현재 접속한 사용자가 현 OCI Tenacy 환경에 작업하기 위한 기본 설정 및 관련 cli들이 구성되어 있습니다. Local Access: 로컬 PC 환경에서 처음 접속하기 위해 필요한 작업부터 시작하는 방법입니다.     Step #1. Launch Cloud Shell\nLaunch Cloud Shell를 클릭하거나, 우측 상단에 있는 링크를 클릭하여 Cloud Shell에 접속합니다.\n   접속한 환경에서 다음 명령을 실행해 보면 oci cli가 설치되어 있으며, 접속이 가능한 상태임을 알 수 있습니다.\noci -v oci os get ns    Step #2. kubeconfig 파일 생성하기\n생성된 OKE 클러스터 접속을 위한 kubeconfig을 생성하기 위해 Access Your Cluster의 두 번째 단계 내용을 Cloud Shell에서 실행합니다.\n 명령어에서 보듯이 Cloud Shell에서는 Kubernetes API를 Public Endpoint을 제공하는 경우에만 접근할 수 있습니다.     OKE 클러스터 연결 확인\nkubectl cluster-info 를 실행하면 생성된 kubeconfig를 통해 클러스터에 접속됨을 확인할 수 있습니다.\n   ","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/3.access-cluster/1.cloudshell-access/","tags":["oke"],"title":"1.3.1 Cloud Shell로 클러스터 연결하기"},{"categories":null,"contents":"1.3.2 로컬 환경에서 클러스터 연결하기 OCI CLI 설치 및 환경 구성 OCI CLI 설치 공식 문서를 참고하여 OCI OCI를 설치합니다.\n https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cliinstall.htm  Oracle Linux 기준 예시\n  설치\nbash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;   설치 확인\noci --version   OCI CLI Config File설정   설정을 위해 필요한 정보 사전 확인\n  user OCID\n오른쪽 위 사용자 Profle에서 User Settings 클릭 후 사용자 OCID 확인   Tenancy OCID\n오른쪽 위 사용자 Profle에서 Tenancy 클릭 후 Tenancy OCID 확인   Region: 사용할 Region\n  API Signing Key: 여기서는 편의상 새로운 Private Key, Public Key를 생성하는 것으로 선택하겠습니다.\n      setup config 실행\noci setup config  실행 예시  [opc@bastion-host ~]$ oci setup config This command provides a walkthrough of creating a valid CLI config file. ... Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaazo6ilmezdaeozjcmsu6rcxnf5sjz2fau76kpdjvsbbakhqtw75sq Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaamd5zq7ohrxkmcsai23pp634k6i2eymnznv4d6ur7n5n4qj7jfhoq Enter a region by index or name(e.g. 1: ap-chiyoda-1, 2: ap-chuncheon-1, 3: ap-hyderabad-1, 4: ap-ibaraki-1, 5: ap-melbourne-1, 6: ap-mumbai-1, 7: ap-osaka-1, 8: ap-seoul-1, 9: ap-singapore-1, 10: ap-sydney-1, 11: ap-tokyo-1, 12: ca-montreal-1, 13: ca-toronto-1, 14: eu-amsterdam-1, 15: eu-frankfurt-1, 16: eu-marseille-1, 17: eu-zurich-1, 18: il-jerusalem-1, 19: me-dubai-1, 20: me-jeddah-1, 21: sa-santiago-1, 22: sa-saopaulo-1, 23: sa-vinhedo-1, 24: uk-cardiff-1, 25: uk-gov-cardiff-1, 26: uk-gov-london-1, 27: uk-london-1, 28: us-ashburn-1, 29: us-gov-ashburn-1, 30: us-gov-chicago-1, 31: us-gov-phoenix-1, 32: us-langley-1, 33: us-luke-1, 34: us-phoenix-1, 35: us-sanjose-1): 8 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: a0:e1:fe:79:22:22:f0:b5:6b:29:72:5f:5d:b6:22:32 Config written to /home/opc/.oci/config If you haven\u0026#39;t already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2   API Public Key 등록   OCI Config File에 등록한 사용자 상세 페이지로 이동\n  왼쪽 아래 Resources에서 API Keys 선택\n  Add Public Key 클릭\n    oci setup config 실행 후 생성된 API Public Key 확인\n[opc@bastion-host ~]$ cat ~/.oci/oci_api_key_public.pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAycclV86OzQ+x6I13HEbe ... gCO1GdLyJNS08Zv1uiW6j1IYIszrmr6XK482Vf3r48u8ZkFgBBlsPjU03R9H1x52 dwIDAQAB -----END PUBLIC KEY-----   생성된 API Public Key 내용을 API Public Key 추가\n   OCI CLI를 위한 API Public Key 등록 완료   연결 확인   다시 리눅스 호스트로 돌아가 oci os ns get을 실행하여 연결 확인\n[opc@bastion-host ~]$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; }   kubectl CLI 설치 및 환경 구성 kubectl CLI 설치 공식 문서를 참고하여 kubectl OCI를 설치합니다.\n https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/  Linux 기준 예시\n  설치\ncurl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl   kubeconfig 파일 생성하기   생성한 OKE 클러스터 상세 페이지에서 Access Cluster를 클릭합니다.\n  Local Access 를 클릭합니다.\n   PRIVATE_ENDPOINT\nKubernetes API 접근도 Private IP를 통해서 할 때 사용합니다. PRIVATE_ENDPOINT, 즉 Private IP로 접근을 해야 하므로, bastion host 등 내부 IP로 접근이 가능한 서버에서 수행할 때 사용합니다.\n bastion host는 외부에서 SSH로 접근 가능하게 22 포트 오픈이 필요하며, 내부적으로는 OKE 클러스터의 Kubernetes API 및 Worker Nodes 들에 접근이 가능해야 합니다. kubeconfig 파일 생성 및 클러스터에 접속 확인  [opc@bastion-host ~]$ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.ap-seoul-1.aaaaaaaair2g7crtxvgchbrfkvf5dz6j7ncrjepinzm2infw6cjy33uzxhyq --file $HOME/.kube/config --region ap-seoul-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT New config written to the Kubeconfig file /home/opc/.kube/config [opc@bastion-host ~]$ kubectl cluster-info Kubernetes control plane is running at https://10.0.0.4:6443 CoreDNS is running at https://10.0.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.   PUBLIC_ENDPOINT\nKubernetes API 접근도 Public IP를 통해서 할 때 사용합니다. OKE 클러스터 생성시 Kubernetes API에 Public IP를 부여한 경우에 사용 가능한 방법입니다. 실제 kubeconfig 파일 생성하는 명령(oci ce cluster crate-kubeconfig ~~)은 마지막 옵션값인 PUBLIC_ENPOINT이외는 동일한 명령합니다.\n  ","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/3.access-cluster/2.local-access/","tags":["oke"],"title":"1.3.2 로컬 환경에서 클러스터 연결하기"},{"categories":null,"contents":"6.2 Open Application Model 이해 및 애플리케이션 배포하기 참고\n https://verrazzano.io/latest/docs/guides/app-deployment/application-deployment-guide/  Open Application Model Open Application Model(OAM)은 컨테이너 또는 오케스트레이터 또는 인프라 보다는 개발자가 애플리케이션에 집중할 수 있도록 하고자 하는 모델입니다. 애플리케이션을 쿠버네티스상의 컨테이너로 배포하기 위해서 배포 외에도 모니터링, 트래픽 처리 등 추가 설정을 위해 여러가지 자원들을 배포해야 하고 그에 따른 YAML 파일들을 정의해야 합니다. 이 부분을 개발자는 애플리케이션에 초점을 맞추고, 운영, 관리 등을 위해 필요한 많은 부분은 템플릿 등을 통해 표준화하는 것을 지향하고 있습니다. Open Application Model은 현재 0.3 까지 나왔으며, Verrazzano에서는 Open Application Model 0.2.1을 기반으로 하고 있어 최신 버전과 약간의 문법 차이가 있습니다.\n주요 컨셉   https://verrazzano.io/latest/docs/concepts/verrazzanooam/\n   Component: 하나의 애플리케이션, 쉽게 컨테이너 앱라고 생각하면 될 것 같습니다. 이 컴포넌트에는 유형이 있을 수 있고, Verrazzano에서는 Helidon 프레임워크를 통해 개발한 앱이냐, WebLogic 앱이냐, Coherence 서버냐 등이 있을 것이고, 그 유형에 따라 배포시나 모니터링시의 특성이 있을 것입니다. 그래서 Component에서 어떤 Workload 타입인지를 지정하게 됩니다.\n  Workload: 어떤 타입인지 표시하는 것으로, WorkloadDefinition을 통해 정의할 수 있으며, VerrazzanoHelidonWorkload, VerrazzanoWebLogicWorkload 등 사전에 Verrazzano에서 정의한 유형 또는 Deployment, ConfigMap 등등 Kubernetes의 기본 자원형태을 그대로 사용 할 도 있습니다.\n  Application Configuration: 하나의 애플리케이션을 뜻하여, 여러 Component를 포함하게 됩니다. 하나의 컨테이너 앱에 관련된 설정을 가진 형태이거나, 여러 컨터이너를 Component로 가진 패키징된 앱의 형태일 수도 있습니다.\n  Trait: 하나의 Component에 운영, 모니터링 등의 필요에 의해 추가 설정하는 사항입니다. 사이트의 문서 예제에서 보면, 메트릭 수집 설정, Ingress 설정 등이 일반적인 예로 쓰입니다.\n  Scope: 여러 Component에 대한 한번에 설정하는 사항입니다.\n  Helidon 예제 애플리케이션 배포하기 Helidon 프레임워크를 사용해서 개발한 Java 애플리케이션을 배포하는 예제를 통해 Open Application Model을 사용해서 Verrazzano에서 배포하는 것을 확인해 봅니다. Java로 개발하여 컨테이너 이미지까지 빌드하는 방법은 어렵지 않으니, 그 이후에 Verrazzano 배포하는 부분을 확인해 봅니다.\n예제 애플리케이션: Helidon 샘플로, REST API로 /greet 로 요청하면, 환영 메시지가 오는 간단한 컨테이너 애플리케이션\nComponent 작성   Verrazzano에서 사용하는 Kubernetes CRD인 Component를 정의합니다.\n spec.workload: Component의 Workload를 정의합니다. spec.workload.kind: 일반유형이 아닌 Helidon 기반 컨테이너를 위해 Verrazzano가 사전에 정의한 VerrazzanoHelidonWorkload Workload Definition을 선택 spec.workload.spec: VerrazzanoHelidonWorkload의 스펙으로 Kubernetes Deployment 스펙을 그대로 사용  https://verrazzano.io/latest/docs/reference/api/oam/workloads/#verrazzanohelidonworkload    apiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-containerimage:\u0026#34;ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\u0026#34;ports:- containerPort:8080name:http  Application Configuration 작성   Verrazzano에서 사용하는 Kubernetes CRD인 Application Configuration을 정의합니다.\n spec.components.componentName: 방금 정의한 Component인 hello-helidon-component을 뜻합니다. spec.components 아래에 여러 Component가 추가될 수 있는 것을 알 수 있습니다. spec.components.traits.trait: 대상 Component에 운영, 관리상 추가할 부분을 정의합니다. 현재 IngressTrait, LoggingTrait, MetricsTrait를 제공하고 있습니다.  https://verrazzano.io/latest/docs/reference/api/oam/ingresstrait/    apiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:hello-helidon-appconfnamespace:hello-helidonannotations:version:v1.0.0description:\u0026#34;Hello Helidon application\u0026#34;spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitmetadata:name:hello-helidon-ingressspec:rules:- paths:- path:\u0026#34;/greet\u0026#34;pathType:Prefix  애플리케이션 배포   namespace 생성\nkubectl create namespace hello-helidon kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Component 배포\nkubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-comp.yaml   Application Configuration 배포\nkubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-app.yaml   배포된 앱의 istio를 통해 등록된 ingress DNS 확인\nkubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39;   실행예시\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ \u0026gt; -n hello-helidon \\ \u0026gt; -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39; hello-helidon-appconf.hello-helidon.myenv.thekoguryo.ml   애플리케이션 테스트\n확인된 https://{ingress DNS 주소}/greet 주소로 정상 호출되는 것을 알 수 있습니다.\n   모니터링 Verrazzano 콘솔   Verrazzano Console에 로그인합니다.\n예, https://verrazzano.myenv.thekoguryo.ml\n 접속 유저: verrazzano 암호: 설치시 초기화한 암호    왼쪽 Resources 항목에서 Application과 Component 항목에서 배포된 앱 정보를 확인할 수 있습니다.\n   로그 모니터링(Elasticsearch / Kibana)   Verrazzano 콘솔에서 Kibana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  내비게이션 메뉴에서 Stack Management를 선택합니다.\n   Index Pattern \u0026gt; + Create index pattern 을 클릭합니다.\n  가능한 패턴에 예제앱을 위해 추가한 namespace를 포함하여 여러 소스가 보입니다.\n   표현식을 사용해도 되지만, 여기서는 예제 namespace 이름을 패턴 이름으로 입력합니다. 예, verrazzano-namespace-hello-helidon\n   Time field로 @timestamp 선택\n   추가완료\n   내비게이션 메뉴에서 Kibana \u0026gt; Discover 메뉴로 이동합니다.\n  생성한 인덱스 패턴에 대해 수집된 로그, 대상 namespace 상의 전체 수집된 로그가 보입니다.\n   필터 적용후 정확하게 로그를 확인할 수 있습니다.\n$ kubectl logs -n hello-helidon hello-helidon-deployment-c4d7859-nqt7l -c hello-helidon-container ... 2021.12.29 03:22:44 INFO io.helidon.microprofile.server.ServerCdiExtension Thread[main,5,main]: Server started on http://localhost:8080 (and all other host addresses) in 3075 milliseconds (since JVM startup). http://localhost:8080/greet 2021.12.29 03:22:44 INFO io.helidon.common.HelidonFeatures Thread[features-thread,5,main]: Helidon MP 2.3.2 features: [CDI, Config, Fault Tolerance, Health, JAX-RS, Metrics, Open API, REST Client, Security, Server, Tracing]    메트릭 모니터링(Prometheus / Grafana)   Verrazzano 콘솔에서 Grafana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  내비게이션 메뉴에서 Dashboard \u0026gt; Manage를 선택합니다.\n  기본 제공하고 있는 대쉬보드 중에 Helidon 대쉬보드를 선택합니다.\n   Helidon 대쉬보드에서 앞서 배포한 앱의 상태를 확인할 수 있습니다.\n   서비스 메쉬 모니터링(Kiali)   Verrazzano 콘솔에서 Kiali 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  왼쪽 메뉴에서 Graph를 클릭합니다.\n  대상 Namespace를 선택하면, 아래와 같이 상호 호출 관계를 볼 수 있습니다. 단일 서비스로 구성된 예제로 간단히 보입니다.\n   추가 참고사항   오픈소스 클라이언트 툴인 octant를 ApplicationConfiguration과 관계되 자원을 확인하였습니다.\n ApplicationConfiguration과 Component에서 직접 명시한 내용 자원에 추가하여 Deployment(하위 포함), Service, istio Gateway, Virtual Service가 배포되는 것을 확인할 수 있습니다.     ","lastmod":"2021-12-28T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/verrazzano/2.deploy-app/","tags":["oke","verrazzano"],"title":"6.2 Open Application Model 이해 및 애플리케이션 배포하기"},{"categories":null,"contents":"4.4.2 마이크로서비스 앱에 Service Mesh - Istio 적용하기 istioctl, Helm 또는 매뉴얼로 설치가 가능합니다. 여기서는 OKE 문서에서 예시로 설명하고 있는 istioctl 기준으로 설치하고 Istio 문서에 따라 서비스에 적용하는 것을 확인해보겠습니다.\n 공식 문서  https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengistio-intro-topic.htm https://istio.io/latest/docs/examples/microservices-istio/add-istio/    Istio 설치   Cloud Shell 또는 작업환경에 접속합니다.\n  Istio 다운로드\ncurl -L https://istio.io/downloadIstio | sh -   Istio 경로로 이동\ncd istio-1.12.1   PATH 환경 변수에 추가\nexport PATH=$PWD/bin:$PATH   사전검증 실행\nistioctl x precheck   설치\nistioctl install --set profile=demo   Istio Enable productpage 서비스 Istio Enable   Istio sidecar 추가하여 재배포\nistioctl kube-inject 명령으로 productpage 앱에만 sidecar를 추가합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | sed \u0026#39;s/replicas: 1/replicas: 3/g\u0026#39; | kubectl apply -l app=productpage,version=v1 -f -   결과 확인\nproduct Pod내에 istio-proxy 컨테이너가 추가된 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-29z48 1/1 Running 0 89m details-v1-79f774bdb9-d5lfq 1/1 Running 0 89m details-v1-79f774bdb9-sqp9p 1/1 Running 0 89m productpage-v1-c6885474-6g754 2/2 Running 0 16s productpage-v1-c6885474-r9l8l 2/2 Running 0 19s productpage-v1-c6885474-sz9mf 2/2 Running 0 14s ratings-v1-b6994bb9-6xlq7 1/1 Running 0 89m ratings-v1-b6994bb9-j2l78 1/1 Running 0 89m ratings-v1-b6994bb9-w748t 1/1 Running 0 89m reviews-v2-7bf8c9648f-4rh4z 1/1 Running 0 7m16s reviews-v2-7bf8c9648f-6vhlq 1/1 Running 0 7m18s reviews-v2-7bf8c9648f-8zjcw 1/1 Running 0 7m16s sleep-557747455f-jxk5z 1/1 Running 0 12m oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe pod productpage-v1-c6885474-6g75 Name: productpage-v1-c6885474-6g754 Namespace: default ... Containers: productpage: Container ID: cri-o://95f68b2fcb5e8168879e1a5703bf520a94729664cc410edbe0d3255113344149 Image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2 ... istio-proxy: Container ID: cri-o://74f2707caa99b8b12b3e293a129f74234c5ac1ab36729f70045de640eeb2b4b4 Image: docker.io/istio/proxyv2:1.12.1 ...   앞서와 동일하게 productpage 페이지를 접속합니다.\n http://{productpage external ip}:9080/productpage    istio-proxy 로그 확인\n웹페이지가 이전과 동일하게 접속이 되고 추가적으로 istio-proxy를 통해서 거쳐가는 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs -l app=productpage -c istio-proxy -f ... [2021-12-20T09:16:54.826Z] \u0026#34;GET /details/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 178 2 2 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u0026#34; \u0026#34;adeb5dbb-261c-9982-9d66-523cc3890e2b\u0026#34; \u0026#34;details:9080\u0026#34; \u0026#34;10.244.2.43:9080\u0026#34; outbound|9080||details.default.svc.cluster.local 10.244.2.149:41122 10.96.175.211:9080 10.244.2.149:55324 - default [2021-12-20T09:16:54.842Z] \u0026#34;GET /reviews/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 379 591 591 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u0026#34; \u0026#34;adeb5dbb-261c-9982-9d66-523cc3890e2b\u0026#34; \u0026#34;reviews:9080\u0026#34; \u0026#34;10.244.2.51:9080\u0026#34; outbound|9080||reviews.default.svc.cluster.local 10.244.2.149:45512 10.96.197.86:9080 10.244.2.149:35752 - default [2021-12-20T09:16:54.820Z] \u0026#34;GET /productpage HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 5183 616 615 \u0026#34;10.179.87.76\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u0026#34; \u0026#34;adeb5dbb-261c-9982-9d66-523cc3890e2b\u0026#34; \u0026#34;146.56.142.95:9080\u0026#34; \u0026#34;10.244.2.149:9080\u0026#34; inbound|9080|| 127.0.0.6:41867 10.244.2.149:9080 10.179.87.76:0 - default   전체 서비스 Istio Enable   다음 명령을 통해 Review v2와 나머지 서비스들에도 Istio를 활성화합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app!=reviews -f - curl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v2 -f -   실행결과\nIstio-proxy가 추가 되어 Pod당 컨테이너가 2개로 보이면, bookinfo.yaml이 재적용되어 replica=1임을 참고합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-5c6d488c49-ltw8r 2/2 Running 0 69s productpage-v1-c6885474-6g754 2/2 Running 0 15h ratings-v1-7dfc8d4bbf-p8n2f 2/2 Running 0 68s reviews-v2-5ff75858c7-zhn57 2/2 Running 0 64s sleep-557747455f-jxk5z 1/1 Running 0 16h   참고\n아래와 같이 istio-injection 레이블을 달면 해당 네임스페이스에 배포되는 pod는 자동으로 istio sidecar가 추가됩니다.\nkubectl label namespace \u0026lt;NAMESPACE_NAME\u0026gt; istio-injection=enabled   Istio Ingress Gateway 설정 앞서 다른 장에서 Ingress Controller로 nginx ingress controller를 사용하였습니다. 마이크로 서비스들에 대한 컨트롤을 위해 서비스들에 대해서 Istio의 Ingress Gateway를 사용하도록 설정합니다.\n  참고문서\n https://istio.io/latest/docs/examples/microservices-istio/istio-ingress-gateway/    Gateway: 서비스 메쉬로 들어오고 나가는 트래픽에 대한 로드 밸런서 정보를 기술합니다.\n  VirtualService: 트래픽에 대한 라우팅 규칙을 지정합니다.\n    아래 내용으로 예제인 book-info에 대한 Istio Ingress Gateway를 생성합니다.\nkubectl apply -f - \u0026lt;\u0026lt;EOFapiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# use Istio default gateway implementationservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \u0026#34;*\u0026#34;gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- uri:exact:/logout- uri:prefix:/staticroute:- destination:host:productpageport:number:9080EOF  Istio Ingress Gateway를 통한 접속을 위해 IP를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.96.59.144 \u0026lt;none\u0026gt; 80/TCP,443/TCP 66m istio-ingressgateway LoadBalancer 10.96.150.51 146.56.186.52 15021:32250/TCP,80:32707/TCP,443:32572/TCP,31400:32532/TCP,15443:31522/TCP 66m istiod ClusterIP 10.96.188.42 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 66m   접속 확인\n 예시, http://146.56.186.52:80/productpage     Metric 모니터링   Prometheus \u0026amp; Grafana 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/grafana.yaml   설치된 Grafana 대쉬보드를 외부에서 접근\n 옵션 #1:  istioctl dashboard grafana 명령으로 로컬 프락시를 통해 접속할 수 있습니다. Cloud Shell는 Public IP 및 외부 접속을 허용하지 않아, 외부 접속이 필요하면 관련 설정이 필요합니다.   옵션 #2:  Istio Ingress Gateway를 통한 외부 접속을 설정할 수 있습니다. 설정을 통해 외부 접속 설정- https://istio.io/latest/docs/tasks/observability/gateways/      공식 문서를 참고하여 외부 접속을 설정합니다.\n https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http    외부 접속 설정 예시\n  설정을 위한 도메인 설정\n  Istio Ingress Gateway의 External IP를 확인하여, DNS 서버에 설정하거나, 클라이언트의 /etc/hosts 파일의 설정합니다. 사용할 도메인 주소를 입력합니다.\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.ml\u0026#34;     외부 접속 오픈\n  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:grafana-gatewaynamespace:istio-systemspec:selector:istio:ingressgatewayservers:- port:number:80name:http-grafanaprotocol:HTTPhosts:- \u0026#34;grafana.${INGRESS_DOMAIN}\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:grafana-vsnamespace:istio-systemspec:hosts:- \u0026#34;grafana.${INGRESS_DOMAIN}\u0026#34;gateways:- grafana-gatewayhttp:- route:- destination:host:grafanaport:number:3000---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:grafananamespace:istio-systemspec:host:grafanatrafficPolicy:tls:mode:DISABLE---EOF  테스트 데이타 발생\n productpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다.  테스트 앱 페이지 예시, http://146.56.186.52:80/productpage      Grafana 접속\n  예, http://grafana.istio.thekogury.ml\n  설정한 주소로 접속하면 Istio에서 제공하고 있는 기본 대쉬보드를 확인할 수 있습니다.\n    Grafana 대쉬보드를 보면 각 서비스의 처리 청보를 확인할 수 있습니다.     분산 추적(Distributed Tracing) 마이크로 서비스는 클라이언트의 요청을 분산된 서비스를 통해 처리한 후 클라이언트에게 응답합니다. 예시로 사용하고 있는 Bookinfo 앱도 productpage 서비스가 뒷단 서비스에서 취합한 정보를 클라이언트에게 제공하고 있습니다. 이러한 분산환경에서 클라이언트의 요청에 대한 서비스 들간의 호출 정보를 추적하기 위한 여러가지 툴들이 있습니다. Istio에서는 Envoy의 분산 추적에 대한 기능을 활용하여, Jaeger, Zipkin, LightStep 등의 툴을 활용할 수 있습니다. 여기서는 Zipkin을 통한 방법을 확인해 보겠습니다. 그외 툴들은 관련 페이지를 참고하세요.\n https://istio.io/latest/docs/tasks/observability/distributed-tracing/overview/  Zipkin   Zipkin 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/extras/zipkin.yaml   설치된 Zipkin 페이지를 외부에서 접근\n Grafana와 동일하게 옵션 #2: 외부 접속을 사용합니다. 공식 문서를 참고하여 외부 접속을 설정합니다.  https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http      외부 접속 설정 예시\n  설정을 위한 도메인 설정\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.ml\u0026#34;   외부 접속 오픈\n  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:tracing-gatewaynamespace:istio-systemspec:selector:istio:ingressgatewayservers:- port:number:80name:http-tracingprotocol:HTTPhosts:- \u0026#34;tracing.${INGRESS_DOMAIN}\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:tracing-vsnamespace:istio-systemspec:hosts:- \u0026#34;tracing.${INGRESS_DOMAIN}\u0026#34;gateways:- tracing-gatewayhttp:- route:- destination:host:tracingport:number:80---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:tracingnamespace:istio-systemspec:host:tracingtrafficPolicy:tls:mode:DISABLE---EOF  테스트 데이타 발생\n productpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다.  테스트 앱 페이지 예시, http://146.56.186.52:80/productpage      Zipkin 페이지 접속합니다.\n 예, http://tracing.istio.thekogury.ml    서비스 추적 테스트\n Find a trace 화면에서 빨간 플러스 기호를 클릭하여 검색할 서비스를 설정하고 쿼리를 실행합니다.     조회 결과를 확인하고 추적할 건에 대해서 오른쪽 SHOW를 클릭합니다.\n   해당 요청건에 대한 서비스간 호출 관계, 시간 및 각 서비스에 대한 태그 정보를 볼 수 있습니다.\n     Trace ID로 추적\n  각 요청건에 대해서 Span ID(Trace ID)를 통해 추적할 수 있습니다. 로그에서 확인할 수 있습니다.\n   화면 오른쪽 상단의 Trace ID 검색창을 통해서 바로 검색하면, 해당 요청건을 바로 확인할 수 있습니다.\n     서비스 메쉬 시각화 마이크로 서비스는 클라이언트의 요청을 분산된 서비스를 통해 처리한 후 클라이언트에게 응답합니다. 예시로 사용하고 있는 Bookinfo 앱도 productpage 서비스가 뒷단 서비스에서 취합한 정보를 클라이언트에게 제공하고 있습니다. 이러한 분산환경에서 클라이언트의 요청에 대한 서비스 들간의 호출 정보를 시각화를 Kiali를 통해 제공하고 있습니다.\n https://istio.io/latest/docs/tasks/observability/kiali/    Kiali 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/kiali.yaml   설치된 Kiali 페이지를 외부에서 접근\n Grafana와 동일하게 옵션 #2: 외부 접속을 사용합니다. 공식 문서를 참고하여 외부 접속을 설정합니다.  https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http      외부 접속 설정 예시\n  설정을 위한 도메인 설정\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.ml\u0026#34;   외부 접속 오픈\n  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:kiali-gatewaynamespace:istio-systemspec:selector:istio:ingressgatewayservers:- port:number:80name:http-kialiprotocol:HTTPhosts:- \u0026#34;kiali.${INGRESS_DOMAIN}\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:kiali-vsnamespace:istio-systemspec:hosts:- \u0026#34;kiali.${INGRESS_DOMAIN}\u0026#34;gateways:- kiali-gatewayhttp:- route:- destination:host:kialiport:number:20001---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:kialinamespace:istio-systemspec:host:kialitrafficPolicy:tls:mode:DISABLE---EOF  테스트 데이타 발생\n productpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다.  테스트 앱 페이지 예시, http://146.56.186.52:80/productpage      Kiali 대쉬보드 접속합니다.\n 예, http://kiali.istio.thekoguryo.ml    내비게이션 메뉴에서 Graph를 클릭하면, 서비스 간의 호출 정보를 시각화해서 볼 수 있습니다.\n   Reviews 새 버전 배포 Reviews v3 배포   Reviews v3 버전을 배포합니다.\nIstio가 활성화 된 상태에서 버전 업데이트를 가정하여 v3를 배포합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v3 -f -   배포 결과 확인\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE ... reviews-v2-5ff75858c7-hwrgv 2/2 Running 0 5h15m reviews-v3-5484f6cbd6-685b6 2/2 Running 0 3m8s ...   productpage의 Istio Ingress Gateway를 통해 접속하여 테스트합니다.\n 테스트 앱 페이지 예시, http://146.56.186.52:80/productpage Reviews v2과 Reviews v3는 각각 Pod 하나로 분배규칙을 따라 설정하지 않았기 때문에, 한번씩 라우팅됩니다. Reviews v2(검은 별점)과 Reviews v3(빨간 별점)이 한번시 표시되는 것을 볼수 있습니다.     가중치 기반 라우팅 신규 버전의 완전한 서비스 전에 검증을 위해 가중치 기반의 일부 요청만 라우팅되도록 하는 시나리오를 확인 해 봅니다.\n  배포된 Reviews Pod의 버전 레이블을 확인합니다. 각각 version=v2, version=v2가 할당되어 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -L version NAME READY STATUS RESTARTS AGE VERSION details-v1-5c6d488c49-m5t7j 2/2 Running 0 5h28m v1 productpage-v1-c6885474-qkpx9 2/2 Running 0 5h28m v1 ratings-v1-7dfc8d4bbf-97p6z 2/2 Running 0 5h28m v1 reviews-v2-5ff75858c7-hwrgv 2/2 Running 0 5h28m v2 reviews-v3-5484f6cbd6-685b6 2/2 Running 0 15m v3 sleep-557747455f-jxk5z 1/1 Running 0 22h   레이블 기준으로 라우팅을 위해 Istio에 필요한 아래 설정을 배포합니다.\n DestinationRule: 서비스 엔드포인트에 대해서 labels로 필터링. 테스트 예제에서는 Service Type이 아닌, Pod에만 version label이 있음. Pod의 label로 하는 것 같음 VirtualService: 정의한 destination에 대해서 weight 기준으로 분배함. weight의 총합은 100이어야함. 100번 중에 90번, 10번 이렇게 정확한 분배가 아닌, 확률로 추측됨.  kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 weight: 90 - destination: host: reviews subset: v3 weight: 10 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 EOF   아래와 같이 반복 테스트합니다.\ncounter=1; \\ while [ $counter -lt 20 ]; \\ do curl -s http://146.56.186.52/productpage | grep color | head -1; counter=$(( $counter + 1 )); \\ done   테스트 결과\n20중에 18번 검은색 별표, 2번 빨간색 별표임을 알수 있습니다. 확률로 정확이 딱 안 맞을 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ counter=1; \\ \u0026gt; while [ $counter -lt 20 ]; \\ \u0026gt; do \u0026gt; curl -s http://146.56.186.52/productpage | grep color | head -1; \u0026gt; counter=$(( $counter + 1 )); \\ \u0026gt; done \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt;   Kiali에 호출 흐름 확인\n아래그림에서 보듯이 Reviews v2, v3에 설정한 가중치에 맞게 분배되는 것을 볼 수 있습니다.\n   ","lastmod":"2021-12-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/service-mesh/2.sampleapp-with-istio/","tags":["oss","service mesh","istio"],"title":"4.4.2 마이크로서비스 앱에 Service Mesh - Istio 적용하기"},{"categories":null,"contents":"4.1.2 NGINX Ingress Controller에서 host 기반 라우팅(feat. OCI DNS) Ingress Controller에서 도메인 네임을 기반하여 라우팅하기 위해 OCI DNS를 사용하는 방법을 확인합니다.\nOCI DNS 서비스 사용하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 freenom 사이트에서 발급받은 무료 Domain Name(thekoguryo.ml)을 사용하였습니다.\nOCI DNS 서비스 설정   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Networking \u0026gt; DNS Management \u0026gt; Zones로 이동합니다.\n  Create Zone 클릭\n   생성정보 입력\n사용한 Domain Name을 바탕으로 Zone을 생성합니다.\n  METHOD: Manual\n  ZONE NAME: 가지고 있는 Domain Name 입력\n  COMPARTMENT: 생성할 대상 Compartment\n  ZONE TYPE: Primary\n     Zone 내부에 NS 유형과 SOA 유형의 레코드가 생성되어 있습니다. NS는 네임 서버 레코드, SOA는 권한 시작 레코드입니다. Add Record를 클릭합니다.   추가할 레코드를 입력하고 제출합니다.\n  Record Type: A - IPv4 Address\n  NAME: *.ingress\n 와일드 카드 형식으로 ingress controller가 사용할 서브 Domain Name을 입력합니다.    TTL: 300, 우측 자물쇠는 클릭하여 잠금 해제 후 TTL 값 입력\n  RDATA MODE: Basic\n  ADDRESS: 매핑할 IP, 여기서는 앞서 만든 nginx ingress controller의 Load Balancer의 IP 입력\n     반영하기 위해 Publish Change 클릭\n  확인 창이 뜨면 한번 더 Publish Change 클릭\n   레코드 추가 및 반영 완료\n   레코드 중 NS 유형인 네임서버 주소를 모두 복사합니다.\n  Domain Name 제공 측에 설정 이제 Domain Name을 구입한 사이트에서 설정이 필요합니다. 아래 과정은 freenom 기준 설정입니다. 구입한 사이트에서 비슷한 방식으로 설정합니다.\n  freenom 사이트에 접속하여 My Domain에서 사용할 도메인 네임 우측의 Manage Domain 클릭   위쪽 Management Tools \u0026gt; Nameservers를 선택한 다음 **Use custom nameservers (enter below)**를 선택합니다.\n  앞서 OCI DNS Zone 에서 복사해둔 네임서버 주소를 차례대로 입력한 후 Change Nameservers 클릭   DNS 테스트   nslookup 툴로 등록한 DNS를 테스트 해봅니다. 잘 등록된 것을 알 수 있습니다.\nC:\\\u0026gt;nslookup *.ingress.thekoguryo.ml 서버: kns.kornet.net Address: 168.126.63.1 권한 없는 응답: 이름: *.ingress.thekoguryo.ml Address: 132.226.225.240   HOST 기반 라우팅 테스트 HOST 이름에 따라 라우팅 서비스를 달리하는 경우입니다.\n  테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(host-basic.yaml)을 작성합니다.\n blue.ingress.thekoguryo.ml 요청은 nginx-blue-svc 로 라우팅 green.ingress.thekoguryo.ml 요청은 nginx-green-svc로 라우팅  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-host-basicannotations:kubernetes.io/ingress.class:nginxspec:rules:- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  작성한 host-basic.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f host-basic.yaml ingress.networking.k8s.io/ingress-host-basic created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-host-basic \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80 6s   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n  blue.ingress.thekoguryo.ml 요청\n   green.ingress.thekoguryo.ml 요청\n   와일드 카드 주소로 DNS에 등록한 Ingress Controller의 Load Balancer를 거쳐 접속한 host의 FQDN에 따라 대상 서비스에 라우팅 되는 것을 확인할 수 있습니다.\n    ","lastmod":"2021-12-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/ingress-controller/2.nginx-ingress-host/","tags":["oss","ingress-controller"],"title":"4.1.2 NGINX Ingress Controller에서 host 기반 라우팅(feat. OCI DNS)"},{"categories":null,"contents":"3.2 Helm Chart Repostory로 사용하기 OCIR은OCI(Open Container Initiative) Registry로 Helm 3에서는 현재 OCI Registry를 실험적(experimental)인 기능으로 지원하고 있습니다.\n https://helm.sh/docs/topics/registries/  Helm CLI 환경 준비   helm cli를 사용할 Cloud Shell 또는 작업환경에 접속합니다.\n  helm cli 3.7 설치\nOCI Registry인 OCIR에 차트를 등록하기 위해서는 helm 3.7에서 제공하는 helm push 명령을 사용해야 원활히 됩니다. 문서 작성일 기준으로 현재 Cloud Shell에 기본 설치된 helm cli 버전은 3.5.4 여서 3.7을 추가 설치합니다.\nwget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz tar -zxvf helm-v3.7.1-linux-amd64.tar.gz linux-amd64/helm mv linux-amd64/helm ~/.local/bin/   .bashrc의 PATH에 등록\nPATH=$HOME/.local/bin:$HOME/bin:$PATH   OCI Support 활성화\nhelm cli에서 OCI Registry 지원은 실험적 기능으로 사용을 위해 다음 환경변수를 설정이 필요합니다.\nexport HELM_EXPERIMENTAL_OCI=1   Helm Chart 생성후 등록하기 샘플 차트 만들기 Helm Chart Template Guide 예제를 따라 만든 샘플 차트를\nOCIR 등록 해봅니다.\n  테스트를 위해 차트를 만듭니다.\noke_admin@cloudshell:helm (ap-seoul-1)$ helm create mychart Creating mychart   차트 작성\n생성된 차트는 nginx를 배포하는 샘플 차트입니다. 실제 차트 작성을 위해서는 앱에 맞게 수정하겠지만, 지금은 배포 테스트로 수정없이 그냥 사용합니다.\n  차트 패키징\nhelm package 명령으로 패키징합니다.\noke_admin@cloudshell:helm (ap-seoul-1)$ cd mychart oke_admin@cloudshell:mychart (ap-seoul-1)$ helm package . Successfully packaged chart and saved it to: /home/oke_admin/works/helm/mychart/mychart-0.1.0.tgz oke_admin@cloudshell:mychart (ap-seoul-1)$ ls charts Chart.yaml mychart-0.1.0.tgz templates values.yaml   OCIR 로그인 및 Helm Chart Push OCIR에 docker cli로 로그인 할때와 동일하게 사용자와 Auth Token을 사용해 로그인합니다. 이전 내용을 참고합니다.\n  앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 helm cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:mychart (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:mychart (ap-seoul-1)$ helm registry login -u cnrlxx3w0wgq/oke-admin ap-seoul-1.ocir.io Password: Login Succeeded   Helm Chart Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식 Push 하면 됩니다. 그러면 repo-prefix/ 을 포함하여 repository 가 생성됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-prefix\u0026gt;   mychart 예시  oke_admin@cloudshell:mychart (ap-seoul-1)$ helm push ./mychart-0.1.0.tgz oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm Pushed: ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart:0.1.0 Digest: sha256:842486615ee4398001092a9b2f931320add0ecd13885e039427ce6f7926b493b   OCIR 확인\nOCI 서비스 콘솔에서 OCIR을 보면 Push한 차트가 정상적으로 등록된 것을 알 수 있습니다. 예제는 편의상 root compartment로 push 하였습니다.\n 하위 compartment로 push하는 경우 사전에 repository를 만들어야 합니다. 예, helm/mychart repository를 push 전에 만들것     Helm Chart를 OKE 클러스터에 배포하기   Cloud Shell 또는 작업 환경에 접속합니다.\n  등록한 Chart로 배포합니다.\nhelm install mychart oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart --version 0.1.0   배포 예시\noke_admin@cloudshell:mychart (ap-seoul-1)$ helm install mychart oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart --version 0.1.0 --set service.type=LoadBalancer NAME: mychart LAST DEPLOYED: Fri Dec 3 15:17:51 2021 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running \u0026#39;kubectl get --namespace default svc -w mychart\u0026#39; export SERVICE_IP=$(kubectl get svc --namespace default mychart --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) echo http://$SERVICE_IP:80 oke_admin@cloudshell:mychart (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/mychart-76677cc888-rl4v6 1/1 Running 0 55s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 32m service/mychart LoadBalancer 10.96.198.136 146.56.152.14 80:30926/TCP 56s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mychart 1/1 1 1 56s NAME DESIRED CURRENT READY AGE replicaset.apps/mychart-76677cc888 1 1 1 55s   배포 앱 접속 확인\n   ","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/ocir/2.helm-chart/","tags":["container registry","helm chart"],"title":"3.2 Helm Chart Repostory로 사용하기"},{"categories":null,"contents":"2.1 DevOps 서비스를 이용한 Spring Boot 앱을 OKE에 배포 자동화하기 DevOps 서비스 사용을 위한 IAM Policy 설정 DevOps 서비스를 사용하기 위해서는 DevOps 자원들에 권한 설정이 필요합니다. 공식 문서를 참조하여 권한 설정을 위한 Dynamic Group 및 Group에 대한 Policy를 설정합니다.\n https://docs.oracle.com/en-us/iaas/Content/devops/using/devops_iampolicies.htm#build_policies  아래 Dynamic Group 및 Policy는 위 문서의 예제를 기준으로 작성한 내용으로 요구사항에 따라 일부 변경이 될 수 있습니다.\nDynamic Group 만들기 주어진 Compartment 내에서 DevOps 서비스를 사용할 수 있도록 Compartment에 대한 Dynamic Group을 먼저 생성합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\n  DevOps를 사용할 Compartment로 이동하여 OCID를 복사해 둡니다.\n  Identity \u0026gt; Dynamic Groups로 이동합니다.\n  Create Dynamic Group을 클릭합니다.\n  복사해둔 Compartment OCID를 이용해 필요한 Dynamic Group을 만듭니다.\n  oke-labs-DeployDynamicGroup\nALL {resource.type = \u0026#39;devopsrepository\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   oke-labs-CoderepoDynamicGroup\nALL {resource.type = \u0026#39;devopsbuildpipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   oke-labs-BuildDynamicGroup\nAll {resource.type = \u0026#39;devopsdeploypipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}     DevOps 서비스를 위한 Policy 설정하기   Identity \u0026gt; Policies로 이동합니다.\n  Create Policy을 클릭하여 새 Policy를 만듭니다.\n  Compartment 레벨로 다음 Policy를 만듭니다.\n Name: 예, oke-labs-DevOps-policy  Allow dynamic-group oke-labs-CoderepoDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to manage repos in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to read secret-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to manage generic-artifacts in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to use ons-topics in compartment \u0026lt;YourCompartmentName\u0026gt;   Root Compartment 레벨로 다음 Policy를 만듭니다.\n Name: 예, oke-labs-DevOps-root-policy  Allow dynamic-group oke-labs-CoderepoDynamicGroup to manage all-resources in tenancy Allow dynamic-group oke-labs-CoderepoDynamicGroup to read secret-family in tenancy Allow dynamic-group oke-labs-DeployDynamicGroup to manage all-resources in tenancy   DevOps 서비스를 통한 CI/CD 배포 자동화 하기 DevOps 프로젝트 만들기 Notification Topic 만들기 DevOps 파이프 라인 실행이 발생하는 주요 이벤트를 알려주기 위한 용도로 Notification Topic 설정이 필요합니다. DevOps 프로젝트 생성시 필수 요구 사항이라 미리 만듭니다\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Application Integration \u0026gt; Notifications으로 이동합니다.\n  Create Topic을 클릭하여 Topic을 생성합니다.\n Name: 예, oke-labs-devops-topic     Notification을 위해 생성한 Topic 이벤트를 가져갈 Subscrition을 일단 생략합니다. 필요시 구성하시면 됩니다.\n  DevOps 프로젝트 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; DevOps로 이동합니다.\n  프로젝트 생성을 위해 Projects로 이동하여 Create DevOps project를 클릭합니다.\n  생성 정보를 입력하여 프로젝트를 만듭니다.\n Project name: 예, my-devops-project Notification Topic: 앞서 생성한 Topic 선택     프로젝트 생성완료\n   Enable Logging 프로젝트 생성 직후 Enable Logging 관련 정보가 보이는 것을 볼 수 있습니다. 설명문에서 보는 것 처럼 Logging을 활성화하지 않을 경우, 파이프라인 실행 화면에서 오른쪽에 보이는 실행 로그가 안보입니다. 그래서 Enable Logging은 필수입니다.\n   Project Overview에서 Enable Log을 클릭하거나 왼쪽 메뉴에서 Logs를 클릭합니다.\n   로그를 활성화 버튼을 토글합니다.\n   대상 Compartment에 이미 Log Group이 있는 경우 나열된 것 중에 선택이 가능합니다. 미리 생성된 Log Group이 없는 경우 아래와 같이 자동입력된 정보를 바탕으로 Enable Log 버튼 클릭시 새로 Log Group과 Log가 만들어 지게 됩니다. 필요시 설정을 수정하고 그렇치 않으면, Enable Log 버튼을 클릭합니다.\n   애플리케이션 배포 자동화 하기 Code Repository를 사용하여 애플리케이션 코드 관리하기 샘플로 Spring Boot Helloword 앱을 만들어 테스트하겠습니다.\n  코드 저장소 생성을 위해 왼쪽 메뉴에서 Code Repositories를 클릭합니다.\n  Create repository를 클릭하여 저장소를 만듭니다.\n Repository name: 예, spring-boot-hello-repo     생성된 코드 저장소 입니다. 일반적인 Git Repository입니다.\n   실제 개발 작업은 git 명령을 통해 개발 PC에서 진행하면 됩니다. 저장소 상세정보 위에 있는 Clone 버튼을 하면 Clone 명령어가 아래 그림처럼 뜨게 됩니다. 여기서는 Clone with HTTPS를 사용하겠습니다.\n   개발 PC에 복사한 주소를 사용해 git clone 명령어를 통해 복제합니다.\ngit clone \u0026lt;YourClonewithHTTPS URL\u0026gt;   이때 사용자 인증이 필요합니다. HTTPS기반 사용자 인증시 아래 유저명 형식과 AuthToken을 사용합니다.\n  인증 유저명\n Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-name\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-name\u0026gt;/\u0026lt;username\u0026gt; 이전 가이드들과 달리 tenancy-namespace가 아닌 tenacy-name인 것에 주의합니다.    AuthToken: 생성에 대한 내용은 이전 가이드들을 참고합니다.\n  Code Repository의 HTTPS 인증관련 문서\n https://docs.oracle.com/en-us/iaas/Content/devops/using/clone_repo.htm#https_auth    예시\n  $ git clone https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com/namespaces/cnrlxx3w0wgq/projects/my-devops-project/repositories/spring-boot-hello-repo Cloning into \u0026#39;spring-boot-hello-repo\u0026#39;... Username for \u0026#39;https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: thekoguryo/oke-developer Password for \u0026#39;https://oreozz/oke-admin@devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: remote: Counting objects: 2, done remote: Finding sources: 100% (2/2) remote: Getting sizes: 100% (1/1) Unpacking objects: 100% (2/2), done. remote: Total 2 (delta 0), reused 2 (delta 0)   현재 복제된 저장소는 비어 있습니다. 아래 가이드를 통해 spring-boot-hello 샘플 코드를 작성합니다.\n https://spring.io/guides/gs/spring-boot-docker/    작성된 코드를 git 명령어를 통해서 Code Repository에 저장합니다.\n  예시\ngit add . git commit -m \u0026#34;init\u0026#34; git push     코드 작성 및 반영 완료\n   Build Pipeline 만들기 CI/CD 중에 코드를 빌드하여 배포 산출물을 만드는 CI 과정에 해당되는 부분을 Build Pipeline을 통해 구성이 가능합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Build Pipelines로 이동합니다.\n  Create build pipeline을 클릭하여 파이프라인을 생성합니다.\n  Name: 예, spring-boot-hello-build-pipeline\n     생성된 파이프라인을 클릭합니다.\n  그림과 같이 Stage를 추가하여 파이프라인 흐름을 구성할 수 있습니다. Add Stage를 클릭합니다.\n   제공 Stage\n Managed Build: 빌드스펙에 정의된 내용에 따라 빌드 과정을 실행합니다. Delivery Artifacts: 빌드 산출물(예, 컨테이너 이미지)를 Artifact Repository에 저장합니다. Trigger Deployment: 빌드가 끝나고 Deployment Pipeline을 호출합니다. Wait: 일정시간 대기합니다.     Build Stage 만들기   빌드를 위해 먼저 Managed Build Stage를 추가합니다.\n  Managed Build Stage 설정\n Stage name: 예, build-stage Build Spec File Path: 빌드 스크립트 경로를 지정합니다. 기본적으로 소스 루트에 있는 build_spec.yaml을 파일을 사용합니다. Primary Code Repository: 빌드할 메인 소스가 있는 코드 저장소를 지정합니다.     Primary Code Repository 설정 화면\n 대상 소스 코드가 있는 저장소를 지정합니다.       설정된 Stage를 Add를 클릭하여 추가합니다.\n  아래 그림과 같이 build-stage가 추가되었습니다. Start Manual Run을 클릭하면 테스트를 해 볼수 있습니다.\n   테스트처럼 소스 코드상의 Build Spec의 정의가 필요합니다.\n   Build Spec은 다음 문서를 참조합니다.\n  https://docs.oracle.com/en-us/iaas/Content/devops/using/build_specs.htm\n  문서에 있는 Example 2 기준 예시\n steps: 실행할 스크립트를 정의하는 부분입니다. 예제이는 Build Source, Dockerizer 2개의 step이 정의되어 있고 각각 command에서 실행할 스크립트를 정의하고 있습니다. 정의된 순서대로 실행됩니다. env.exportedVariables: 전역으로 선언된 환경변수로 이전 step에서 값을 변경하면 그다음 step에도 적용됩니다. Deployment Pipeline을 호출시에도 전달됩니다. outputArtifacts: 빌드 산출물의 정의하는 부분으로, 이후 Delivery Artifact Stage를 통해 Artifact Storage에 저장할 때 여기서 정의된 이름을 통해 지정 가능합니다.  version:0.1component:buildtimeoutInSeconds:6000shell:bashenv:exportedVariables:- BuildServiceDemoVersionsteps:- type:Commandname:\u0026#34;Build Source\u0026#34;timeoutInSeconds:4000command:|echo $PATH mvn clean install- type:CommandtimeoutInSeconds:400name:\u0026#34;Dockerizer\u0026#34;command:|BuildServiceDemoVersion=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` echo $BuildServiceDemoVersion docker build -t build-service-demo .outputArtifacts:- name:build-service-demotype:DOCKER_IMAGElocation:build-service-demo- name:build-service-demo-kube-manifesttype:BINARYlocation:deployment/app.yml    Build Spec 정의\n  개발한 spring-boot-hello 소스 코드의 root 경로에 build_spec.yaml을 다음과 같이 정의하고 코드 저장소에 저장합니다.\n  build_spec.yaml\nversion:0.1component:buildtimeoutInSeconds:6000shell:bashenv:# these are local variables to the build configvariables:appName:\u0026#34;spring-boot-hello\u0026#34;# exportedVariables are made available to use as parameters in sucessor Build Pipeline stagesexportedVariables:- APP_NAME- OCIR_PATH- TAGsteps:- type:Commandname:\u0026#34;Init exportedVariables\u0026#34;timeoutInSeconds:4000command:|APP_NAME=$appName - type:Commandname:\u0026#34;Build Source\u0026#34;timeoutInSeconds:4000command:|mvn clean install- type:CommandtimeoutInSeconds:400name:\u0026#34;Build Source - Post\u0026#34;command:|mkdir -p target/dependency \u0026amp;\u0026amp; (cd target/dependency; jar -xf ../*.jar)- type:Commandname:\u0026#34;Define Image Tag - Commit ID\u0026#34;timeoutInSeconds:30command:|COMMIT_ID=`echo ${OCI_TRIGGER_COMMIT_HASH} | cut -c 1-7` BUILDRUN_HASH=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` [ -z \u0026#34;$COMMIT_ID\u0026#34; ] \u0026amp;\u0026amp; TAG=$BUILDRUN_HASH || TAG=$COMMIT_ID- type:Commandname:\u0026#34;Define OCIR Path\u0026#34;timeoutInSeconds:30command:|TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` REPO_NAME=$appName OCIR_PATH=$OCI_RESOURCE_PRINCIPAL_REGION.ocir.io/$TENANCY_NAMESPACE/$REPO_NAME- type:CommandtimeoutInSeconds:400name:\u0026#34;Containerize\u0026#34;command:|docker build -t new-generated-image . docker images- type:Commandname:\u0026#34;Check exportedVariables\u0026#34;timeoutInSeconds:30command:| [-z \u0026#34;$APP_NAME\u0026#34; ] \u0026amp;\u0026amp; APP_NAME=unknown [-z \u0026#34;$OCIR_PATH\u0026#34; ] \u0026amp;\u0026amp; OCIR_PATH=unknown [-z \u0026#34;$TAG\u0026#34; ] \u0026amp;\u0026amp; TAG=unknownecho \u0026#34;APP_NAME: \u0026#34; $APP_NAME echo \u0026#34;OCIR_PATH: \u0026#34; $OCIR_PATHecho \u0026#34;TAG: \u0026#34; $TAGoutputArtifacts:- name:output-imagetype:DOCKER_IMAGElocation:new-generated-image     Start Manual Run을 통해 다시 실행하면 아래와 같이 스크립트가 수행되는 것을 볼 수 있습니다.\n   ExportVariables 확인\n실행 결과 화면에서 오른쪽 위쪽 점3개를 클릭하여 상세 화면으로 이동하면 Build Output에서 실행결과로 나온 변수값을 볼 수 있습니다. 이 변수들은 이후 Stage 또는 연결되어 호출된 Deployment Pipeline으로 전달되어 사용할 수 있게 됩니다.\n   컨테이너 이미지 OCIR 등록 Stage 만들기   Build Pipeline 탭으로 이동합니다.\n  플러스 버튼을 클릭하여 build-stage 다음에 stage를 추가합니다.\n   Delivery Artifact Stage를 선택합니다.\n  stage 이름을 입력하고 Create Artifact를 선택합니다.\n   Container image 유형으로 Artifact 추가합니다.\n 이미지 경로: docker tag를 달때 사용하는 이미지 경로입니다. 직접 입력해도 되지만 여기서는 build-stage에서 넘어온 exportedVariable을 사용하여 ${OCIR_PATH}:${TAG} 과 같이 입력합니다.     같은 방식으로 하나 더 추가 합니다.\n Name: generated_image_with_latest Image Path: ${OCIR_PATH}:latest    Artifact 매핑\n  Associate Artifact에서 방금 추가한 2개의 Artifact에 실제 컨테이너 이미지 파일을 매핑해 줍니다. 앞서 build-stage에서 build_spec.yaml에서 정의한 outputArtifacts의 이름을 입력합니다.\noutputArtifacts:- name:output-imagetype:DOCKER_IMAGElocation:new-generated-image      이제 delivery stage까지 추가하였습니다.\n  파이프라인을 다시 실행해 봅니다. 실제 소스코드로 빌드된 컨테이너 이미지가 OCIR에 자동으로 등록됩니다.\n   Deploy Pipeline 만들기 CI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\n  Create pipeline을 클릭하여 파이프라인을 생성합니다.\n  Name: 예, spring-boot-hello-deployment-pipeline\n     생성된 파이프라인을 클릭합니다.\n  Add Stage를 클릭하여 Stage를 추가합니다.\n  제공 Stage\n  Deploy: OKE, Compute 인스턴스 배포, Oracle Function에 배포 기능을 제공합니다.\n  Control: 승인 대기, 트래픽 변경, 대기 등을 지원합니다.\n  Integration: 커스텀 로직 수행을 위한 Oracle Function 실행을 지원합니다.\n     Kubernetes에 배포할 manifest 파일 준비 Kubernetes에 배포할 Stage 유형을 사용하기 위해서는 사전에 배포할 manifest yaml 파일을 준비해야 합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Artifacts로 이동합니다.\n  Artifacts로 앞서 빌드 파이프라인 만들때 등록한 2개가 있는 것을 볼수 있습니다. 여기에 등록된 Artifact는 재사용이 가능합니다.\n   manifest 파일을 등록하기 위해 Add artifact를 클릭합니다.\n  4 가지 등록 유형을 제공합니다. 이중에 Kubernetes manifest를 선택합니다.\n   Kubernetes manifest 유형에는 Artifact Source로 2가지 유형을 제고합니다.\n Artifact Registry Repository: Container Registry로 OCIR을 제공하고 있듯시 Artifact Registry를 서비스로 제공하고 있습니다. 그곳에 있는 자원을 참조할 경우에 선택합니다. Inline: 인라인은 현재 DevOps 프로젝트에 있는 여기 Artifact에 직접 입력하는 것을 말합니다.    Artifact Source로 Inline 유형으로 다음과 같이 등록합니다.\n Name: 예, k8s_spring_boot_deploy_template     Value\n앞 서와 같이 build-stage에서 export한 변수값들을 사용할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:${APP_NAME}name:${APP_NAME}spec:replicas:1selector:matchLabels:app:${APP_NAME}template:metadata:labels:app:${APP_NAME}spec:containers:- name:${APP_NAME}image:${OCIR_PATH}:${TAG}---apiVersion:v1kind:Servicemetadata:name:${APP_NAME}-serviceannotations:service.beta.kubernetes.io/oci-load-balancer-shape:\u0026#34;10Mbps\u0026#34;spec:type:LoadBalancerports:- port:80protocol:TCPtargetPort:8080selector:app:${APP_NAME}    Kubernetes Enviroment 등록하기   my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Enviroments로 이동하여 배포할 OKE 환경을 등록합니다.\n  OKE 유형을 선택합니다.\n   배포할 클러스터를 선택합니다.\n   Kubernetes manifest 배포 Stage 만들기   등록한 Deployment Pipeline(spring-boot-hello-deployment-pipeline) 설정 페이지로 이동합니다.\n  Add Stage를 클릭하여 Apply manifest to your Kubernetes cluster Stage를 추가합니다.\n  배포할 환경 및 manifest 파일을 선택합니다\n   파이프라인 완성\n   Build Pipeline에서 Deployment Pipeline 호출하기 앞서 만든 Build Pipeline에서 컨테이너 이미지 까지 OCIR에 등록하고 나면, OKE에 배포할 Deployment Pipeline을 기동되어야 전체 빌드에서 배포까지가 완료됩니다. 이제 Deployment Pipeline을 등록하였으므로, Build Pipeline에서 호출할 수 있습니다.\n  앞서 만든 **Build Pipelines(spring-boot-hello-build-pipeline)**으로 이동합니다.\n  파이프라인 마지막에 Stage를 추가합니다.\n  Trigger Deployment 유형을 선택합니다.\n  설정한 Deployment Pipeline을 지정합니다.\n   전체 흐름이 완료되었습니다.\n   Trigger 설정하기 지금 까지는 테스트를 하기 위해 Build Pipeline에서 Start Manual Run을 통해 시작하였습니다. 실제로는 개발자가 코드를 코드 저장소에 반영이 될 때 자동으로 빌드, 배포 파이프라인이 동작할 필요가 있습니다. Trigger는 코드 저장소에 발생한 이벤트를 통해 빌드 파이프라인을 시작하게 하는 역할을 하게 됩니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Trigger로 이동합니다.\n  Create trigger을 클릭합니다.\n  Trigger를 설정합니다.\n Name: 예, spring-boot-hello-trigger Source Code Repository: OCI Code Repository, GitHub, GitLab 연동을 지원하며, 예제에서는 앞서 만든 OCI Code Repository상의 spring-boot-hello-repo를 선택합니다. Actions: 트리거링 되었을 때 호출하는 액션으로 작성한 빌드 파이프라인인 spring-boot-hello-build-pipeline을 선택합니다.     설정이 완료되었습니다.\n  테스트   Trigger에서 지정한 spring-boot-hello 소스 코드에 임의의 변경사항을 발생시키고 Code Repository에 반영합니다.\n  저는 Application.java에 있는 응답메시지를 \u0026ldquo;Hello OCI DevOps\u0026quot;로 변경하고 반영하셨습니다.\n  빌드 실행 내역을 보면, 그림과 같이 Trigger 된것은 Commit ID가 함께 보이며, Code Repository와 링크되어 있습니다.\n   Commit ID를 클릭하면 Code Repository상의 코드 변경 분을 확인할 수 있습니다.\n     빌드 파이프라인이 정상적으로 코드 빌드 부터 컨테이너 이미지 생성, 배포 파이프라인 호출까지 실행되었습니다.\n   배포 파이프라인도 정상 실행되었습니다.\n   OKE 클러스터를 조회해 보면 정상 배포 되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/spring-boot-hello-54848fcfd5-5jpxh 1/1 Running 0 5m39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16h service/spring-boot-hello-service LoadBalancer 10.96.186.158 146.56.186.172 80:32224/TCP 41m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spring-boot-hello 1/1 1 1 15h NAME DESIRED CURRENT READY AGE replicaset.apps/spring-boot-hello-54557d9c47 0 0 0 41m replicaset.apps/spring-boot-hello-54848fcfd5 1 1 1 5m39s   서비스 주소로 접속시 정상 동작을 확인할 수 있습니다.\n   ","lastmod":"2021-11-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/devops/1.deploy-app-on-oke-using-devops/","tags":["devops"],"title":"2.1 DevOps 서비스를 통해 Spring Boot 앱 배포 자동화하기"},{"categories":null,"contents":"1.6.1 Block Volume 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\n기본 설치된 Persistent Volume을 위한 StorageClass 확인 OKE는 OCI IaaS를 기반으로 제공되는 서비스로 OCI Block Volume 서비스를 이용하게 Persistent Volume을 제공합니다. 현 버전 기준으로 FlexVolume 볼륨 플러그인과 CSI(Container Storage Interface) 볼륨 플러그인의 두 가지를 사용하고 있습니다.\n  기본 StorageClass 확인\n oci: OCI Block Volume 서비스를 위한 FlexVolume 플러그인 사용 oci-bv: OCI Block Volume 서비스를 위한 CSI 플러그인 사용  oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 2d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d   CSI 볼륨 플러그인\n FlexVolume 플러그인 방식은 오픈소스 Kubernetes 1.2 버전때 부터 사용되어 더 오래되었지만, 업스트림 Kubernetes에서 CSI 플러그인 방식을 사용하는 흐름입니다. OKE에서도 Release Note 2020년 8월 1일, Support for the Container Storage Interface (CSI) volume plug-in 에 나와 있는 것 처럼 OKE에서도 신규 기능은 CSI 플러그인에 추가할 예정이며, FlexVolume은 유지 보수만 할 계획입니다. 그래서 이하 설명에서는 CSI 플러그인을 사용하는 oci-bv storageclass를 사용하겠습니다. Flex 플러그인을 사용하는 oci storageclass에 대한 사항은 공식 문서를 참조바랍니다.    OCI Block Volume용 CSI 플러그인을 사용하여 Persistent Volume 만들어 사용하기 Persitent Volume 테스트\n  아래와 같이 PV 요청 yaml을 사용하여 요청합니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:csi-bvs-pvcspec:storageClassName:\u0026#34;oci-bv\u0026#34;accessModes:- ReadWriteOnceresources:requests:storage:50Gi  테스트 앱 배포\n 요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱  apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-bvs-pvcname:nginx-bvs-pvcspec:replicas:1selector:matchLabels:app:nginx-bvs-pvctemplate:metadata:labels:app:nginx-bvs-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:csi-bvs-pvc  생성 결과\n 아래와 같이 정상적으로 PV 요청에 따라 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다.  oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 101s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 118s   Block Volume OCI 서비스 콘솔에서 Storage \u0026gt; Block Volumes 화면에서 보면 아래와 같이 PV용 Block Volume 만들어 졌으며, 특정 Worker Node에 부착된 걸 알 수 있습니다.\n     테스트\n  Persistent Volume에 쓰기\n아래와 같이 컨테이너 내부로 들어가 마운트 된 PV 내에 파일쓰기를 합니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vdp7c -- bash root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# echo \u0026#34;Hello PV\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# cat /usr/share/nginx/html/hello_world.txt Hello PV root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/#   POD 삭제 후 재생성\nPersistent Volume이 유지되는 것을 확인하기 위해 기존 POD를 삭제하고 재생성되도록 합니다. 이때 RWO 모드라 다른 Node에 POD가 생성되는 경우, Multi-Attach error가 일시적으로 발생합니다. 기존 POD가 삭제되었다는 것을 인지하는 데 시간이 걸리며, 조금 지난 후에 POD가 다시 재생성됩니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 6m53s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl delete pod nginx-bvs-pvc-7b574c9c5c-vdp7c pod \u0026#34;nginx-bvs-pvc-7b574c9c5c-vdp7c\u0026#34; deleted oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 0/1 ContainerCreating 0 17s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 1/1 Running 0 75s   신규 POD에서 Persistent Volume 확인\n앞서 변경한 파일을 신규 POD에서 다시 조회해 보면 기존 내용이 남아 있는 걸 확인할 수 있습니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vrptl -- cat /usr/share/nginx/html/hello_world.txt Hello PV     참고사항\n 앞선 테스트에서 처럼 ReadWriteOnce 접근 모드에서는 단일 Kubernetes Node에 있는 POD만 붙여서 사용할 수 있습니다. 다른 Node에 있는 POD에서 동일한 PV를 사용하려고 하면, 컨테이너 기동시 Multi-Attach 오류가 발생하며, 컨테이너가 기동이 되지 않습니다. 또한 Self-Healing으로 POD 재기동시 기존 POD가 해당 volume을 사용하고 있다고 생각하여 Multi-Attach 오류가 발생하고, 사용중인 POD가 없음을 인지하는 데 약간의 시간이 걸리게 됩니다.    ReadWriteMany 지원 여부 현재 버전 기준 CSI Driver for OCI Block Volume Service는 ReadWriteOnce만 지원합니다. 그래서 단일 Kuberenetes Node에 멀티 Pod까지만 지원됩니다. 또한 위 작업 내용을 accessMode를 ReadWriteMany로 변경후 동일하게 수행하면 pod가 생성되지 않고 아래와 같이 에러가 나게 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get events LAST SEEN TYPE REASON OBJECT MESSAGE ... 80s Warning ProvisioningFailed persistentvolumeclaim/mynginxclaim failed to provision volume with StorageClass \u0026#34;oci-bv\u0026#34;: rpc error: code = InvalidArgument desc = invalid volume capabilities requested. Only SINGLE_NODE_WRITER is supported (\u0026#39;accessModes.ReadWriteOnce\u0026#39; on Kubernetes) ","lastmod":"2021-11-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/6.persistent-volume/1.block-volume/","tags":["oke"],"title":"1.6.1 Block Volume 사용하기"},{"categories":null,"contents":"1.6.2 File Storage 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)을 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 RWS 접근 모드로 사용하는 방법을 확인해 보겠습니다.\nFiles Storage 만들기 관련 문서를 참고하여 File Storage를 만듭니다.\n  https://docs.oracle.com/en-us/iaas/Content/File/home.htm\n  https://thekoguryo.github.io/oci/chapter08/\n    OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Storage \u0026gt; File Storage로 이동합니다\n  대상 Compartment를 확인합니다.\n  File Systems에서 Create File System을 클릭합니다.\n  기본 설정화면에서 간단히 아래 정보를 원하는 값이 맞게 알맞게 수정하고 생성합니다.\n File System Information:  Name   Mount Target Information:  New Mount Target Name Virtual Cloud Network Subnet       생성결과 확인\nFile Storage \u0026gt; Mount Target 에서 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\n Mount Target OCID: \u0026hellip;sc2mia IP Address: 예, 10.0.20.194 Export Path: 예) /OKE-FFS-Strorage     Security List 설정\nFile System 생성시 Mount Target의 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\n   File Storage 서비스를 이용하여 Persistent Volume 사용하기   Storage Class 만들기\n앞서 확인한 Mount Target OCID로 업데이트 후 적용\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:oci-fssprovisioner:oracle.com/oci-fssparameters:# Insert mount target from the FSS heremntTargetId:ocid1.mounttarget.oc1.ap_seoul_1.aaaaaa4np2szmmn5nfrw4llqojxwiotboawxgzlpovwc2mjnmfsc2mia  Persistent Volume (PV) 만들기\nMount Targe의 IP와 Export Path로 업데이트 후 적용\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumemetadata:name:oke-fss-pvspec:storageClassName:oci-fsscapacity:storage:100GiaccessModes:- ReadWriteManymountOptions:- nosuidnfs:# Replace this with the IP of your FSS file system in OCIserver:10.0.20.194# Replace this with the Path of your FSS file system in OCIpath:\u0026#34;/OKE-FFS-Storage\u0026#34;readOnly:false  Persistent Volume Claime(PVC) 만들기\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:oke-fss-pvcspec:storageClassName:oci-fssaccessModes:- ReadWriteManyresources:requests:storage:100GivolumeName:oke-fss-pv  PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\n앞선 예제와 달리 replica를 복수개로 지정할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-fss-pvcname:nginx-fss-pvcspec:replicas:3selector:matchLabels:app:nginx-fss-pvctemplate:metadata:labels:app:nginx-fss-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:oke-fss-pvc  실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oci-fss-storageclass.yaml storageclass.storage.k8s.io/oci-fss created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get sc,pv,pvc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/oci (default) oracle.com/oci Delete Immediate false 2d19h storageclass.storage.k8s.io/oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d19h storageclass.storage.k8s.io/oci-fss oracle.com/oci-fss Delete Immediate false 34s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 3h6m persistentvolume/oke-fss-pv 100Gi RWX Retain Bound default/oke-fss-pvc oci-fss 24s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/csi-bvs-pvc Bound csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO oci-bv 3h6m persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 100Gi RWX oci-fss 17s oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-9fb98454f-bc7hp 1/1 Running 0 24s 10.244.0.5 10.0.10.40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-bxw4x 1/1 Running 0 24s 10.244.1.18 10.0.10.15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-qm9tq 1/1 Running 0 24s 10.244.0.153 10.0.10.219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   파일 쓰기 테스트\n  아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.40\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bxw4x -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-qm9tq -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40     참고 문서 https://blogs.oracle.com/cloud-infrastructure/post/using-file-storage-service-with-container-engine-for-kubernetes\n","lastmod":"2021-11-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/6.persistent-volume/2.file-storage/","tags":["oke"],"title":"1.6.2 File Storage 사용하기"},{"categories":null,"contents":"2. OKE 클러스터 만들기 Quick 모드로 클러스터 만들기 처음 OKE 클러스터를 만드는 단계로 실환경에서는 별도의 OKE 사용자 및 VCN 등 커스텀한 환경을 사용하겠지만, OKE를 이해하기 위한 처음 단계로 Administrator 유저를 통해 Quick 모드로 설치합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n  List Scope에서 생성할 Compartment(예, oke-labs)를 선택합니다.\n  클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n  빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여생성된 OKE 클러스터를 통해 기분 구조를 이해하고자 합니다. 아래 Launch Workflow를 클릭합니다.\n   클러스터 생성 옵션   생성할 클러스터 이름을 입력합니다. 예, oke-cluster-1\n  설치될 Compartment를 선택합니다. 예, oke-labs\n  클러스터의 쿠버네티스 버전을 선택합니다. 예, v1.20.11\n  Kubernete API Endpoint\n Public API로 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kubernetes API를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Kubernetes API가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.    Kubernetes Worker Nodes\n Worker Nodes를 접속할 수 있는 방법을 선택합니다. 기본 선택된 Private Worker를 그대로 사용 생성되는 Worker Nodes를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Worker Nodes가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.     Shape Worker Node로 사용할 VM Shape를 지정합니다. 기본 선택된 Flex Shape에서 필요에 따라 OCPU, Memory를 조정합니다.\n   Number of nodes\nWorker Nodes 갯수를 지정합니다. 기본 값을 3개입니다.\n  Control Planes(Master Nodes)\nControl Plane은 OCI가 관리하는 영역으로 별도 크기 등을 지정하지 못하며, 기본적으로 다중화되어 있습니다.\n  Advanced Options\n Add an SSH Key  트러블 슈팅 등을 위해 Worker Node VM에 접근하기 위해서는 SSH Key 등록이 필요합니다. 사용할 SSH Key의 Public Key를 아래와 같이 등록합니다.       클러스터 생성 정보를 모두 입력하였습니다. 아래 Next를 클릭\n  앞서 입력한 값들을 리뷰한 후 Create Cluster를 클릭합니다.\n  클러스터 생성 및 관련 네트워크 자원\n아래 그림과 같이 Quick Create로 클러스터를 생성시 기본 네트워크 자원이 함께 생성되는 것을 볼수 있습니다.\n   클러스터 생성 확인   생성이 요청되면, 클러스터 생성, 노드 풀 생성, Worker Node 생성 및 구성 순으로 진행됩니다.\n  클러스터 상세정보에서 Resources \u0026gt; Node Pools를 보면 생성된 pool을 볼수 있습니다.\n   생성된 Node Pool인 pool1을 클릭하여 Node Pool 상세 정보로 이동합니다.\n  Node Pool 상세 정보에서 Resources \u0026gt; Nodes 정보를 보면 생성된 Worker Nodes를 확인할 수 있습니다. VM 생성후 쿠버네티스 구성 시간이 있어 Ready 상태가 될 때까지 약간의 시간이 걸립니다. 테스트 환경에서는 노드가 모두 Ready 될때 까지 5~6분 정도 걸렸습니다.\n   클러스터 및 네트워크 구성 확인 Quick Create \u0026amp; Public Endpoint \u0026amp; Private Workers Example Network Resource Configurations에 설명된 예시 처럼 Kubernetes API Endpoint, Worker Nodes, Service Load Balancer에 대해서 Private 또는 Public 서브넷을 조합하는 몇 가지 구성이 가능합니다. 여기서는 앞서 처럼 Quick Create 모드에서 Public Endpoint, Private Workers를 선택하였고, Service Load Balancer는 기본 생성시는 Public이며 Kubernetes에서 Load Balancer 생성시 선택할 수 있습니다.\n","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/2.install-quick-oke-cluster/","tags":["oke"],"title":"1.2 OKE 클러스터 만들기"},{"categories":null,"contents":"6.3 Multicluster Verrazzano 설치하기 Verrazzano 설치 admin-cluster에 Verrazzano 설치 단일 클러스터 환경에 Verrazzano는 설치하는 것과 동일하게 설치합니다. dev 또는 prod 프로파일로 설치합니다. 다만, 설치 yaml 파일(예, install-oci.yaml)에 mananged-cluster와 구분하기 위해 환경 이름을 지정합니다.\n  예시\n...spec:environmentName:adminprofile:dev...  managed-cluster에 Verrazzano 설치 단일 클러스터 환경에 Verrazzano는 설치하는 것과 동일하게 설치합니다. managed-cluster 프로파일로 설치합니다. 다만, 설치 yaml 파일(예, install-oci.yaml)에 mananged-cluster와 구분하기 위해 환경 이름을 지정합니다.\n  예시\n...spec:environmentName:mc-1profile:managed-cluster...    설치 결과 확인\nkubectl get verrazzano my-verrazzano -o yaml   결과 예시\n managed-cluster 프로파일로 Managed Cluster를 위한 컴포넌트만 설치됩니다.  status: components: cert-manager: ... instance: consoleUrl: https://verrazzano.mc-1.thekoguryo.ml prometheusUrl: https://prometheus.vmi.system.mc-1.thekoguryo.ml state: Ready version: 1.1.0   Managed Cluster 등록하기 준비 단계 Admin Cluster 설정   Context 변경\n$ kubectl config use-context admin-cluster Switched to context \u0026#34;admin-cluster\u0026#34;. $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * admin-cluster cluster-cbrgetu5uwa user-cbrgetu5uwa managed-cluster-1 cluster-cpcnsgcmqsq user-cpcnsgcmqsq managed-cluster-2 cluster-cvoyziqrrzq user-cvoyziqrrzq   API Server 주소 확인\n$ kubectl cluster-info Kubernetes control plane is running at https://129.154.60.75:6443   확인한 주소로 ConfigMap 생성\nkubectl apply -f \u0026lt;\u0026lt;EOF - apiVersion: v1 kind: ConfigMap metadata: name: verrazzano-admin-cluster namespace: verrazzano-mc data: server: \u0026#34;https://129.154.60.75:6443\u0026#34; EOF   등록 단계 - LetsEncrypt staging 기준 Admin Verrazzano와 Managed Cluster간에 https로 통신을 합니다. 알려진 CA 인증서 외에, Self-Signed 인증서, Let\u0026rsquo;s Encrypt Staging을 사용하는 경우 Admin Cluster와 Managed Cluster 간의 원활한 통신을 위해 Managed Cluster의 CA 인증서를 Admin Cluster에 등록해 주는 절차가 필요합니다. 관련 절차는 아래 문서를 참조합니다.\n 필요시 관련 문서 참조 - https://verrazzano.io/latest/docs/setup/install/multicluster/#prerequisites  ※ Let\u0026rsquo;s Encrypt Staging의 경우 DST Root CA의 만료로 인해 관련 업데이트하지 않는 클라이언트 환경으로 인해 추가적인 작업이 더 필요합니다.\n Let\u0026rsquo;s Encrypt Staging  https://letsencrypt.org/docs/staging-environment/    Admin Cluster 설정   Context 변경\nkubectl config use-context admin-cluster   LetsEncrypt staging CA 등록\n# (STAGING) Artificial Apricot R3 curl -O https://letsencrypt.org/certs/staging/letsencrypt-stg-int-r3.pem # (STAGING) Pretend Pear X1 curl -O https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem MGD_CA_CERT=$(cat *.pem) echo $MGD_CA_CERT kubectl create secret generic \u0026#34;ca-secret-managed-cluster-1\u0026#34; \\  -n verrazzano-mc \\  --from-literal=cacrt=\u0026#34;$MGD_CA_CERT\u0026#34; \\  --dry-run=client \\  -o yaml \u0026gt; ca-secret-managed-cluster-1.yaml kubectl apply -f ca-secret-managed-cluster-1.yaml   Managed Cluster로 등록\n 등록 클러스터 이름이 managed-cluster-1 인 경우  kubectl apply -f \u0026lt;\u0026lt;EOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed-cluster-1 namespace: verrazzano-mc spec: description: \u0026#34;VerrazzanoManagedCluster object\u0026#34; caSecret: ca-secret-managed-cluster-1 EOF   등록 대기\nkubectl wait --for=condition=Ready \\  vmc managed-cluster-1 -n verrazzano-mc   Managed Cluster용 등록 파일 생성\nkubectl get secret verrazzano-cluster-managed-cluster-1-manifest \\  -n verrazzano-mc \\  -o jsonpath={.data.yaml} | base64 --decode \u0026gt; managed-cluster-1-register.yaml   Managed Cluster 설정   Context 변경\nkubectl config use-context managed-cluster-1   등록\nkubectl apply -f managed-cluster-1-register.yaml   등록 결과\nManaged Cluster를 Admin Cluster에 등록하게 되면, Managed Cluster에서 주기적으로 Admin Cluster에 앞서 VerrazzanoManagedCluster 유형으로 등록한 자원(managed-cluster-1)의 상태를 주기적으로 업데이트 하게 됩니다.\n  ranch agent 오류 확인\n아래와 같이 x509: certificate signed by unknown authority 인증서 오류로 인해 POD가 기동하지 않습니다.\n[opc@bastion-host t (⎈ |managed-cluster-1:default)]$ kubectl get pod -n cattle-system NAME READY STATUS RESTARTS AGE cattle-cluster-agent-5dd4dd9594-v9tf8 0/1 CrashLoopBackOff 3 2m8s [opc@bastion-host t (⎈ |managed-cluster-1:default)]$ kubectl logs -n cattle-system cattle-cluster-agent-5dd4dd9594-v9tf8 ... INFO: https://rancher.admin.thekoguryo.ml/ping is accessible ... time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate details from https://rancher.admin.thekoguryo.ml\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate #0 (https://rancher.admin.thekoguryo.ml)\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Subject: CN=*.admin.thekoguryo.ml\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Issuer: CN=(STAGING) Artificial Apricot R3,O=(STAGING) Let\u0026#39;s Encrypt,C=US\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;IsCA: false\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;DNS Names: [*.admin.thekoguryo.ml]\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;IPAddresses: \u0026lt;none\u0026gt;\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;NotBefore: 2022-01-04 04:52:39 +0000 UTC\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;NotAfter: 2022-04-04 04:52:38 +0000 UTC\u0026#34; .. time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate #1 (https://rancher.admin.thekoguryo.ml)\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Subject: CN=(STAGING) Artificial Apricot R3,O=(STAGING) Let\u0026#39;s Encrypt,C=US\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Issuer: CN=(STAGING) Pretend Pear X1,O=(STAGING) Internet Security Research Group,C=US\u0026#34; ... time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate #2 (https://rancher.admin.thekoguryo.ml)\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Subject: CN=(STAGING) Pretend Pear X1,O=(STAGING) Internet Security Research Group,C=US\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Issuer: CN=(STAGING) Doctored Durian Root CA X3,O=(STAGING) Internet Security Research Group,C=US\u0026#34; ... time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=fatal msg=\u0026#34;Certificate chain is not complete, please check if all needed intermediate certificates are included in the server certificate (in the correct order) and if the cacerts setting in Rancher either contains the correct CA certificate (in the case of using self signed certificates) or is empty (in the case of using a certificate signed by a recognized CA). Certificate information is displayed above. error: Get \\\u0026#34;https://rancher.admin.thekoguryo.ml\\\u0026#34;: x509: certificate signed by unknown authority\u0026#34;   cattle-cluster-agent를 수정하여 반영합니다.\n  수정\nkubectl edit deploy cattle-cluster-agent -n cattle-system   추가 사항\ninitContainer를 통해 별도로 새 인증서를 다운로드 받아 공유 볼륨을 통해 cluster-register 컨테이너에 전달합니다.\napiVersion:apps/v1kind:Deployment...containers:- env:...name:cluster-register...volumeMounts:- mountPath:/cattle-credentialsname:cattle-credentialsreadOnly:true- mountPath:/etc/pki/tls/certs/name:certsinitContainers:- name:prepare-to-fix-ca-expireimage:ghcr.io/verrazzano/rancher-agent:v2.5.9-20211209021347-2e57ce2a4command:- /bin/sh- -c- |echo start curl -k https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem -o /etc/pki/ca-trust/source/anchors/letsencrypt-stg-root-x1.pem update-ca-trust cp /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem /certs/ca-bundle.crt cp /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt /certs/ca-bundle.trust.crt echo endvolumeMounts:- mountPath:/certs/name:certs ...volumes:- name:cattle-credentialssecret:defaultMode:320secretName:cattle-credentials-2b150a3- name:certsemptyDir:{}status...  결과 재 확인\n[opc@bastion-host tmp (⎈ |managed-cluster-1:default)]$ kubectl get pod -n cattle-system NAME READY STATUS RESTARTS AGE cattle-cluster-agent-6b785ffd86-vshsw 1/1 Running 0 42s   위 POD가 정상으로 기동해야 Admin Cluster의 Rancher에서 Managed Cluster가 Active 상태로 등록된 것을 확인할 수 있습니다.\n    등록 결과 확인   admin cluster로 context 변경\nkubectl config use-context admin-cluster   앞서 등록한 Managed Cluster 자원의 현재 상태 확인\n status.lastAgentConnectTime 값으로 마지막 연결된 시간을 통해 연결되었음을 확인합니다. managed-cluster-1의 apiUrl과 prometheusHost 주소도 등록된 것을 알 수 있습니다.  $ kubectl get vmc managed-cluster-1 -n verrazzano-mc -o yaml apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: ... spec: ... status: apiUrl: https://verrazzano.mc-1.thekoguryo.ml conditions: - lastTransitionTime: \u0026#34;2021-12-31T06:48:59Z\u0026#34; message: Ready status: \u0026#34;True\u0026#34; type: Ready lastAgentConnectTime: \u0026#34;2021-12-31T06:57:06Z\u0026#34; prometheusHost: prometheus.vmi.system.mc-1.thekoguryo.ml rancherRegistration: message: Registration of managed cluster completed successfully status: Completed state: Active   최종 확인 Promethus   Verrazzano Admin Cluster의 Prometheus로 접속합니다.\n 예, https://prometheus.vmi.system.admin.thekoguryo.ml/    샘플 쿼리 node_disk_io_time_seconds_total 로 하면 그림과 같이 Verrazzano Managed Cluster의 메트릭 정보도 함께 조회됨을 알 수 있습니다.\n   Kibana   Verrazzano Admin Cluster의 Kibana로 접속합니다.\n 예, https://kibana.vmi.system.admin.thekoguryo.ml/    아직 배포된 앱이 없다면, 모든 Verrazzano Cluster에 있는 namespace 기준(예, cert-manager)로 인덱스 패턴을 생성합니다.\n  선택 가능한 필드에서 cluster_name을 클릭합니다. 아래와 같이 Admin Cluster(local) 및 등록된 Managed Cluster에서도 로그를 가져오는 것을 알 수 있습니다.\n   Rancher   Verrazzano Admin Cluster의 Prometheus로 접속합니다.\n 예, https://rancher.admin.thekoguryo.ml/    아래와 같이 Managed Cluster가 정상적으로 등록된 것을 볼 수 있습니다.\n   Explorer를 클릭하여 클러스터로 화면에서도 잘 보이는 것을 알 수 있습니다.\n   ","lastmod":"2022-01-04T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/verrazzano/3.multicluster-verrazzano/","tags":["oke","verrazzano","multicluster"],"title":"6.3 Multicluster Verrazzano 설치하기"},{"categories":null,"contents":"4.1.3 NGINX Ingress Controller에서 TLS termination(feats. Let\u0026rsquo;s Encrypt) Ingress Controller에서 외부 수신을 SSL로 하기 위한 설정을 확인합니다.\nSelf-Signed 인증서 사용하기 테스트 목적으로 Self-Signed 인증서를 만들어 사용하는 방법을 확인해 봅니다. 실제 환경에서는 공인 인증기관에서 발급받은 인증서를 사용합니다. Self-Signed 인증서 발급 절차만 대체되어 TLS Secret 등록과정부터는 동일하게 수행됩니다.\n참고 문서\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm  인증서 만들기   Cloud Shell 또는 작업환경에서 다음 명령으로 인증서를 생성합니다. 공인 인증기관에서 발급받은 인증서 사용시 하지 않아도 됩니다.\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34;   TLS Secret을 만듭니다.\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt   실행결과\noke_admin@cloudshell:~ (ap-seoul-1)$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; Generating a 2048 bit RSA private key *************************************************************************************************************************************************************+++++ ****************************************************************************************************************************+++++ writing new private key to \u0026#39;tls.key\u0026#39; ----- oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret tls tls-secret --key tls.key --cert tls.crt secret/tls-secret created   TLS Ingress 자원 배포   테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(tls-termination.yaml)을 작성합니다.\n spec.tls.secretName으로 앞서 생성한 Self-Signed 인증서 이름을 사용합니다.  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-tls-terminationannotations:kubernetes.io/ingress.class:nginxspec:tls:- secretName:tls-secretrules:- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  기존 테스트 ingress는 삭제하고, 작성한 tls-termination.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f tls-termination.yaml ingress.networking.k8s.io/ingress-tls-termination created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80, 443 5s   TLS 적용 결과 검증   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서로 인한 경고 메시지가 뜹니다.\n   고급을 클릭하고 해당 페이지로 이동을 선택합니다.\n   브라우저 주소창 메뉴를 통해 인증서 정보를 확인합니다. Self-Signed 인증서로 루트 인증서가 신뢰할 수 없다는 경고를 확인할 수 있습니다.\n    Let\u0026rsquo;s Encrypt \u0026amp; Cert Manager 사용하기 Let\u0026rsquo;s Encrypt는 무료 인증서 발급 사이트로 TLS에 사용할 인증서를 발급 받을 수 있습니다. 대신 90일 동안만 유효하며 만료전에 갱신해야 합니다. Kubernetes에서는 Cert Manager를 통해 자동으로 갱신할 수 있습니다.\n https://letsencrypt.org/2015/11/09/why-90-days.html  설치 참고 문서\n https://cert-manager.io/docs/tutorials/acme/ingress/  Cert Manager 배포   Cloud Shell 또는 작업환경에서 Cert Manager를 배포합니다.\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml   설치 확인\ncert-manager namespace에 자원들이 정상 실행중인 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all -n cert-manager NAME READY STATUS RESTARTS AGE pod/cert-manager-55658cdf68-sk5nj 1/1 Running 0 18s pod/cert-manager-cainjector-967788869-b77w2 1/1 Running 0 18s pod/cert-manager-webhook-7b86bc6578-pnxtg 1/1 Running 0 18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager ClusterIP 10.96.100.11 \u0026lt;none\u0026gt; 9402/TCP 19s service/cert-manager-webhook ClusterIP 10.96.212.15 \u0026lt;none\u0026gt; 443/TCP 19s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 19s deployment.apps/cert-manager-cainjector 1/1 1 1 19s deployment.apps/cert-manager-webhook 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-55658cdf68 1 1 1 19s replicaset.apps/cert-manager-cainjector-967788869 1 1 1 19s replicaset.apps/cert-manager-webhook-7b86bc6578 1 1 1 18s   Let\u0026rsquo;s Encrypt Issuer 구성 본 예제에서는 Let\u0026rsquo;s Encrypt에서 제공하는 Staging Issuer, Production Issuer을 사용할 수 있습니다. 여기서는 테스트용도로 Staging Issuer를 사용하겠습니다.\n  Let\u0026rsquo;s Encrypt Staging Issuer 설정\nhttps://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml 파일에서 email 부분만 본인 것으로 수정하여 반영합니다.\n네임스페이스에만 사용되는 Issuer가 아닌 전체 쿠버네티스 클러스터에 사용하기 위해 ClusterIssuer 유형을 사용합니다.\napiVersion:cert-manager.io/v1kind:ClusterIssuermetadata:name:letsencrypt-stagingspec:acme:# The ACME server URLserver:https://acme-staging-v02.api.letsencrypt.org/directory# Email address used for ACME registrationemail:user@example.com# Name of a secret used to store the ACME account private keyprivateKeySecretRef:name:letsencrypt-staging# Enable the HTTP-01 challenge providersolvers:- http01:ingress:class:nginx  Let\u0026rsquo;s Encrypt Production Issuer\nProduction Issuer도 https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml 파일을 이용해 동일한 방식으로 설치할 수 있습니다. 다만 사용 limit로 인해 삭제, 생성을 반복할 경우 Rate Limit에 걸릴 수 있습니다.\n  설정 적용\noke_admin@cloudshell:$ (ap-seoul-1)$ kubectl create --edit -f https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml issuer.cert-manager.io/letsencrypt-staging created   TLS Ingress 자원 배포   테스트 앱은 이전 그대로 사용합니다.\n  ingress 설정 YAML(tls-termination-cert-manager.yaml)을 작성합니다.\n 문서 작성일 기준 (STAGING) Doctored Durian Root CA의 만료로 인해 staging issuer 사용시에도 유효하지 않은 인증서라고 나올 수 있습니다. cert manager issuer는 production issuer를 사용하겠습니다. 대신 production issuer는 생성을 반복할 경우 limit에 걸릴 수 있습니다. cert-manager.io/cluster-issuer: 방금 생성한 letsencrypt-staging 설정, issuer가 아닌 cluster-issuer를 사용합니다. spec.tls 하위에 tls 저장할 저장소 이름 및 발급받아 사용할 도메인 이름을 지정합니다  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-tls-termination-cert-managerannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:\u0026#34;letsencrypt-staging\u0026#34;spec:tls:- secretName:ingress-thekoguryo-ml-tlshosts:- green.ingress.thekoguryo.ml- blue.ingress.thekoguryo.mlrules:- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80  기존 테스트 ingress는 삭제하고, 작성한 tls-termination-cert-manager.yaml을 배포합니다.\noke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl apply -f tls-termination-cert-manager.yaml ingress.networking.k8s.io/ingress-tls-termination-cert-manager created oke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination-cert-manager \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80, 443 9s   인증서 발급 확인\n지정한 spec.tls.secretName으로 secret이 만들어지고, certificate 상태(READY)가 True가 되면 정상 발급되었습니다.\noke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-jbv7p kubernetes.io/service-account-token 3 3d19h ingress-thekoguryo-ml-tls kubernetes.io/tls 2 15m letsencrypt-staging Opaque 1 16m oke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get certificate NAME READY SECRET AGE ingress-thekoguryo-ml-tls True ingress-thekoguryo-ml-tls 16m   TLS 적용 결과 검증   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서와 달리 경고 없이 유효한 인증서로 표시됩니다.\n    DNS 대체 주소에 요청한 host가 모두 등록되어, blue 앱도 인증에러 없이 접속됩니다.\n   Let\u0026rsquo;s Encrypt Root CA 변경으로 인해 인증오류 해결   Staging Issuer를 사용할 경우, (STAGING) Doctored Durian Root CA X3 만료로 인해 웹브라우저 접속했을 때 인증 오류가 발생하는 경우가 있습니다. Production Issuer는 해당 문제가 발생하지 않습니다.\n   해당 에러가 발생하는 경우 변경된 새 Root CA를 let\u0026rsquo;s encrypt 사이트에서 다운 받아 브라우저에 등록해 줍니다.\n  파일 다운로드\nhttps://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem\n  브라우저에 다운받은 Root CA 추가(크롬 브라우저 기준)\n  크롬 브라우저 \u0026gt; 설정 \u0026gt; 개인정보 및 보안 \u0026gt; 인증서 관리 로 이동\n  인증서 가져오기 클릭\n   다운받은 파일 선택\n   인증서 설치\n   인증서 등록후 확인\n신뢰할 수 있는 루트 인증 기관에 방금 등록한 (STAGING) 인증서가 보임\n   인증서가 등록후 다시 앱의 웹페이지를 접속하면 인증오류가 발생하지 않고, 인증 경로가 아래와 같이 보이게 됩니다.\n       참고 링크\nhttps://github.com/vancluever/terraform-provider-acme/issues/161\n  ","lastmod":"2021-12-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/ingress-controller/3.nginx-ingress-tls-termination/","tags":["oss","ingress-controller"],"title":"4.1.3 NGINX Ingress Controller에서 TLS termination"},{"categories":null,"contents":"6.4 Multicluster에 애플리케이션 배포하기 Helidon 예제 애플리케이션 배포하기 Helidon 프레임워크를 사용해서 개발한 Java 애플리케이션을, Open Application Model을 사용해서 Verrazzano에서 배포하는 것을 앞서 확인하였습니다. 단일 Kubernetes 클러스터에 대해서 배포하는 것을 확인했습니다.\n이번에는 멀티 클러스터 환경에서 배포하는 것을 확인해 보겠습니다. Verrazzano가 배포된 3개의 클러스터를 기준으로 확인해 봅니다.\n Admin Cluster  admin-cluster   Managed Cluster  managed-cluster-1 managed-cluster-2    애플리케이션 배포   namespace 생성\n  단일 클러스터 기준 (참고)\n앞서 예제에서 배포할 namespace를 생성하고 라벨을 사전에 설정해야 했습니다.\nkubectl create namespace hello-helidon kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   멀티 클러스터를 위해 namespace를 생성(즉, Project를 생성)\n 애플리케이션이 배포될 멀티클러스터에 걸쳐있는 namespace를 관리하기 위해서 VerrazzanoProject 유형을 제공합니다. Admin Cluster에 다음 명령을 수행하면, Admin Cluster 및 대상 Managed Cluster에 해당 namespace가 생성됩니다. 아래 설정을 admin-cluster에 배포합니다.  apiVersion:clusters.verrazzano.io/v1alpha1kind:VerrazzanoProjectmetadata:name:hello-helidonnamespace:verrazzano-mcspec:template:namespaces:- metadata:name:hello-helidonplacement:clusters:- name:managed-cluster-1- name:managed-cluster-2   결과 확인\nVerrazzanoProject 만들면 아래와 같이 대상 클러스터들에 namespace가 만들어지고 자동으로 라벨이 부여됩니다.\n[opc@bastion-host ~ ( )]$ kubectl config use-context admin-cluster Switched to context \u0026#34;admin-cluster\u0026#34;. [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl get ns hello-helidon --show-labels NAME STATUS AGE LABELS hello-helidon Active 4m50s istio-injection=enabled,verrazzano-managed=true [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl config use-context managed-cluster-1 Switched to context \u0026#34;managed-cluster-1\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl get ns hello-helidon --show-labels NAME STATUS AGE LABELS hello-helidon Active 4m50s istio-injection=enabled,verrazzano-managed=true [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl config use-context managed-cluster-2 Switched to context \u0026#34;managed-cluster-2\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-2:default)]$ kubectl get ns hello-helidon --show-labels NAME STATUS AGE LABELS hello-helidon Active 5m3s istio-injection=enabled,verrazzano-managed=true     Component 배포\n단일 클러스터, 멀티 클러스터 차이가 없습니다. 설정 파일은 이전과 동일하고 대신 admin-cluster에 배포합니다.\nkubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-comp.yaml   Application Configuration 배포\n  멀티 클러스터 배포를 위한 YAML 예시\n멀티 클러스터 배포시에는 MultiClusterApplicationConfiguration 유형을 사용합니다. 기존 ApplicationConfiguration에서 정의된 부분이 MultiClusterApplicationConfiguration의 하위 spec으로 들어간 것을 알 수 있습니다. 또한 멀티클러스터 배포를 위한 placement 구문이 추가됩니다.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon spec: template: metadata: annotations: version: v1.0.0 description: \u0026#34;Hello Helidon application\u0026#34; spec: components: - componentName: hello-helidon-component traits: ... placement: clusters: - name: managed-cluster-1   멀티 클러스터 배포용 Application Configuration을 각자 placement에 맞게 수정하여 배포합니다.\nwget https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/multicluster/hello-helidon/mc-hello-helidon-app.yaml # placement 수정 kubectl apply -f mc-hello-helidon-app.yaml   필요하면 이전 단일클러스터 배포시 사용한 아래 파일과 직접 비교해봅니다.\nhttps://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-app.yaml       배포결과\nadmin-cluster에 Componet와 Application을 배포하면, 아래 결과에서 보듯이 placement로 지정한 클러스터에 배포되는 것을 알 수 있습니다.\n[opc@bastion-host t (⎈ |admin-cluster:default)]$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-comp.yaml component.core.oam.dev/hello-helidon-component created [opc@bastion-host t (⎈ |admin-cluster:default)]$ kubectl apply -f mc-hello-helidon-app.yaml multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf created [opc@bastion-host t (⎈ |admin-cluster:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon NAME AGE multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf 11m NAME WORKLOAD-KIND AGE component.core.oam.dev/hello-helidon-component VerrazzanoHelidonWorkload 17m [opc@bastion-host t (⎈ |admin-cluster:default)]$ kubectl config use-context managed-cluster-1 Switched to context \u0026#34;managed-cluster-1\u0026#34;. [opc@bastion-host t (⎈ |managed-cluster-1:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon NAME AGE multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf 11m NAME WORKLOAD-KIND AGE component.core.oam.dev/hello-helidon-component VerrazzanoHelidonWorkload 11m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-helidon-deployment 1/1 1 1 11m [opc@bastion-host t (⎈ |managed-cluster-1:default)]$ kubectl config use-context managed-cluster-2 Switched to context \u0026#34;managed-cluster-2\u0026#34;. [opc@bastion-host t (⎈ |managed-cluster-2:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon No resources found in hello-helidon namespace.   배포된 앱의 istio를 통해 등록된 ingress DNS 확인\nkubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39;   실행예시\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ \u0026gt; -n hello-helidon \\ \u0026gt; -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39; hello-helidon-appconf.hello-helidon.mc-1.thekoguryo.ml   애플리케이션 테스트\n확인된 https://{ingress DNS 주소}/greet 주소로 정상 호출되는 것을 알 수 있습니다.\n   모니터링 Verrazzano 콘솔   Verrazzano Console에 로그인합니다.\n예, https://verrazzano.myenv.thekoguryo.ml\n 접속 유저: verrazzano 암호: 설치시 초기화한 암호    왼쪽 Resources 항목에서 Application과 Component 항목에서 배포된 앱 정보를 확인할 수 있습니다.\n   로그 모니터링(Elasticsearch / Kibana)   Verrazzano 콘솔에서 Kibana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  단일 클러스터와 배포시와 동일한 방법으로 verrazzano-namespace-hello-helidon 네임스페이스로 인덱스 패턴을 생성합니다.\n  생성한 인덱스 패턴에 대해 수집된 로그, 대상 namespace 상의 전체 수집된 로그가 보입니다. admin-cluster는 등록된 Managed Cluster 모두의 로그를 수집합니다. 앞서 배포된 앱는 managed-cluster-1에만 배포했기 때문에 실제 수집된 로그도 동일함을 알수 있습니다.\n   메트릭 모니터링(Prometheus / Grafana)   Verrazzano 콘솔에서 Grafana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  기본 제공하고 있는 대쉬보드 중에 Helidon 대쉬보드를 선택합니다.\n  Helidon 대쉬보드에서 앞서 배포한 앱의 상태를 확인할 수 있습니다. 모든 클러스터에 수집된 메트릭을 볼 수 있으면, 실제 managed-cluster-1에 있기 때문에 수집된 데이터 기준으로 보이게 됩니다.\n   ","lastmod":"2022-01-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/verrazzano/4.deploy-app-to-multicluster/","tags":["oke","verrazzano","multicluster"],"title":"6.4 Multicluster에 애플리케이션 배포하기"},{"categories":null,"contents":"1.4 앱 배포 및 Load Balancer 사용하기 Docker Hub 이미지 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest   배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-78b9999646-xgtjp 1/1 Running 0 17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 130m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-78b9999646 1 1 1 20s   Load Balancer 테스트 Load Balancer 서비스 사용하기   클라이언트 서비스를 위해 LoadBalancer Type으로 서비스를 생성합니다.\n  서비스 생성 결과를 확인하면 아래와 같이 LoadBalancer 타입으로 생성되어 Public IP가 할당 된 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc service/nginx-docker-hub-svc exposed oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 135m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 49s   서비스 주소인 Public IP로 접속하면, 연결되는 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ curl http://152.67.200.29 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   OCI Service Console에서 Load Balancer 확인하기   콘솔에서 Networking \u0026gt; Load Balancer 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  아래 그림과 같이 kubernetes 상에 생성된 서비스와 동일한 Public IP로 Load Balancer가 생성된 것을 알 수 있습니다.\n   Load Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n  상세화면에서 좌측 하단 Resources \u0026gt; Listeners로 이동합니다.\nkubernetes에서 Load Balancer 생성시 지정한 80 포트로 Listen 하고 있는 걸 알 수 있습니다.\n   Resources \u0026gt; Backend Set으로 이동합니다. 생성된 Backend Set을 클릭하여 상세화면으로 이동합니다.\n  좌측 하단 Resources \u0026gt; Backends로 이동합니다.\n세 개의 백엔드 노드의 30610 포트로 부하 분산하는 것을 볼 수 있습니다.\n   다시 터미널로 이동하여 서비스와 노드 정보를 조회합니다.\nkubectl get svc kubectl get nodes -o wide   조회결과\n조회 해보면 OCI Load Balancer 가 Worker Nodes 3개로, 각 노드의 Node Port인 30610으로 부하 분산 되는 것을 알 수 있습니다. 이처럼 kubernetes에서 Load Balancer Type 서비스를 생성하면, OCI Load Balancer와 연동되어 자동으로 자원이 생성됩니다.\n  oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 160m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 25m oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.11 Ready node 155m v1.20.11 10.0.10.11 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.44 Ready node 155m v1.20.11 10.0.10.44 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.73 Ready node 155m v1.20.11 10.0.10.73 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   OCI Service Console에서 Security List 확인하기   콘솔에서 Networking \u0026gt; Virtual Cloud Networks 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  OKE 클러스터가 사용하는 VCN으로 이동합니다.\n  Subnet을 보면 그림과 같이, 3개의 서브넷이 있습니다.\n oke-k8sApiEndpoint-subnet~~: Kubernetes API Endpoint를 위한 서브넷 oke-svclbsubnet-~~: Load Balancer가 사용하는 서브넷 oke-nodesubnet-~~: Worker Node 들이 사용하는 서브넷     Resources \u0026gt; Security Lists로 이동하면, 위 3개의 서브넷이 사용하는 3개의 Security List가 있습니다.\n   Load Balancer용 서브넷 확인\n먼저 oke-svclbsubnet-~~ 의 상세화면으로 이동합니다. Ingress/Egress Rule을 확인해 보면, 외부에서 80 포트로 수신하고, Worker Node들의 Node Port인 30610로 송신할 수 있도록 자동으로 추가된 것을 볼수 있습니다.\n    다시 VCN 상세 화면으로 이동하여, Worker Nodes용 서브넷을 확인합니다.\nOCI Load Balancer에서 Node Port: 30610으로 요청을 수신할 수 있도록 자동으로 규칙이 추가된 것을 볼수 있습니다.\n   위와 같이 OKE 클러스터에 kubernetes 명령으로 Load Balancer 서비스 타입을 생성하면, 그에 따라 OCI Load Balancer가 생성되고, 관련 Security List에도 등록되는 것을 알 수 있습니다.\n  ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/4.deploy-docker-hub-image-with-lb/","tags":["oke"],"title":"1.4 앱 배포 및 Load Balancer 사용하기"},{"categories":null,"contents":"1.5 OCIR 이미지 사용하여 앱 배포하기 OCIR에 이미지 등록하기 Oracle Cloud Infrastructure Registry(OCIR)는 오라클이 제공하는 관리형 컨테이너 레지스트리로 Docker V2 API를 지원하며, Open Container Initiate 호환 컨테이너 레지스트리입니다. docker cli를 통해 이미지를 Push, Pull 해서 사용할 수 있으며, Kubernetes 클러스터에서도 사용할 수 있습니다.\nOCIR에 이미지를 사용하기 위해서는 먼저 등록 작업이 필요하며, 앞서 예제에서 사용한 nginx 이미지를 아래 절차에 따라 등록해 봅니다.\nOCIR Repository 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, oke-labs)를 선택합니다.\n  이미지를 Push하기 전에 먼저 OCIR에 repository를 생성이 필요합니다.\nCreate repositoy를 클릭하여 아래와 같이 nginx repository를 생성합니다. Push, Pull 모두 인증 테스트를 위해 Access 모드를 Private으로 선택합니다.\n   생성 완료\n   Repository 화면에서 Namespace를 복사해 둡니다. Region내에 Tenant를 나타내는 tenancy-namespace: cnrlxx3w0wgq로 이후 로그인시 필요합니다.\n  OCI Auth Token 만들기 docker cli로 docker hub에 이미지를 등록하거나, 가져올때 username/password로 docker login을 통해 로그인을 합니다. OCIR에도 마찬가지로 로그인이 필요하며, password 대신 보안을 위해 Auth Token을 사용합니다.\n  우측 상단 사용자의 Profile 아이콘을 클릭하여 User Settings으로 이동합니다.\n 아래 그림상의 유저는 OCI local 유저로 username이 oke-admin입니다. 유저명이 oracleidentitycloudservice/~~~로 시작하면 Oracle Identiry Cloud Service의 유저입니다.     왼쪽 아래 Resources \u0026gt; Auth Token으로 이동합니다.\n  Auth Token 생성을 위해 Generate Token을 클릭합니다.\n  설명을 입력하고 생성합니다. Auth Token은 생성시에만 볼수 있으므로 복사해 둡니다.\n    OCIR 로그인 및 이미지 Push   앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 docker cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:~ (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   이미지 Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식으로 태그를 한 후 Push 하면 됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt;:\u0026lt;tag\u0026gt;   nginx:latest 예시  docker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCIR 확인\nOCI 서비스 콘솔로 다시 돌아가서 대상 Compartment 기준으로 Push한 이미지가 정상적으로 등록된 것을 알 수 있습니다.\n   실수를 막기 위한 참고 사항 다음과 같은 상황에서는 docker push하면 어떻게 될까요?\n  Push 전에 OCIR Repository를 사전에 만들지 않은 경우\n  dev/nginx:latest(또는 bitnami/nginx:latest)와 같이 이미지 이름 앞에 추가 적인 경로가 있는 경우에 OCIR Repoistory를 dev 로만 만든 경우\n=\u0026gt; 사전에 OCIR Repository를 만들지 않으면, 기본 설정에 의해 root compartment 쪽에 push 됩니다.\n=\u0026gt; dev/nginx 까지마 Repository 이름으로 해야 합니다. 그렇게 만들지 않는 경우 동일하게 root compartment 쪽에 push 됩니다.\n  Container Registry 우측 상단에 Settings를 클릭하여 설정정보를 보면 아래와 같이 대상 repository가 없는 경우 root compartment에 private repository를 자동으로 새로 만들고 push 하는 것이 기본 값으로 체크되어 있습니다.\n OCIR 이미지로 OKE 클러스터에 배포 OCIR 이미지 배포 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   배포 결과 아래와 같이 private repository로 인증문제로 이미지를 가져오는 오류가 발생한 것을 알수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ocir-6c9d554866-nqgjg 0/1 ErrImagePull 0 10s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe pod nginx-ocir-6c9d554866-nqgjg Name: nginx-ocir-6c9d554866-nqgjg ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning Failed 11s (x2 over 23s) kubelet Failed to pull image \u0026#34;ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest\u0026#34;: rpc error: code = Unknown desc = Error reading manifest latest in ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx: denied: Anonymous users are only allowed read access on public repos Warning Failed 11s (x2 over 23s) kubelet Error: ErrImagePull   OCIR Private Repository 이미지 배포 테스트 - imagepullsecret Private Repository에서 이미지를 가져와서 사용하려면 인증을 위한 secret을 등록해서 사용해야 합니다. 아래 절차에 따라 secret을 만들어 사용합니다.\n  앞서 Auth Token을 사용하여 docker login을 하였습니다. 로그인 하면 사용자 홈 밑에 .docker/config.json에 인증정보가 저장됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   위 인증 정보를 통해 로그인 합니다.\nkubectl create secret generic ocir-secret \\  --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\  --type=kubernetes.io/dockerconfigjson   또는 docker login 정보 없이 직접 secret을 만들 수도 있습니다.\nkubectl create secret docker-registry \u0026lt;secret-name\u0026gt; --docker-server=\u0026lt;region-key\u0026gt;.ocir.io --docker-username=\u0026#39;\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;oci-username\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;oci-auth-token\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email-address\u0026gt;\u0026#39;   아래와 같이 imagepullsecret을 사용하여 다시 배포합니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-ocirname:nginx-ocirspec:replicas:1selector:matchLabels:app:nginx-ocirtemplate:metadata:labels:app:nginx-ocirspec:containers:- name:nginximage:ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latestimagePullSecrets:- name:ocir-secret  아래와 같이 정상 배포되는 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret generic ocir-secret \\ \u0026gt; --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\ \u0026gt; --type=kubernetes.io/dockerconfigjson secret/ocir-secret created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-2tvwr kubernetes.io/service-account-token 3 24h ocir-secret kubernetes.io/dockerconfigjson 1 13s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f nginx-ocir-deployment.yaml deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-798957d964-9rddt 1/1 Running 0 7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 8s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-798957d964 1 1 1 7s   OCIR Private Repository 이미지 배포 테스트 - default imagepullsecret 매번 imagepullsecret을 지정하는 것이 불편한 경우 기본으로 사용할 Container Repository에 대한 인증을 default로 저장하여 사용할 수도 있습니다.\n  namespace에 default serviceaccount가 있는데, 여기에 아래와 같이 imagepullsecret을 추가합니다.\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39;   그 결과 아래와 같이 default serviceaccount에 기본적으로 사용할 imagesecret이 추가되었습니다\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39; serviceaccount/default patched oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get sa default -o yaml apiVersion: v1 imagePullSecrets: - name: ocir-secret kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2021-11-08T13:51:17Z\u0026#34; name: default namespace: default resourceVersion: \u0026#34;277559\u0026#34; uid: f72718f9-135c-46d2-b0ac-a1ea1b990863 secrets: - name: default-token-2tvwr   앞서 배포한 yaml을 삭제하고 인증정보가 없어 처음 실패한 명령으로 다시 배포합니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   결과확인하면 default imagepullsecret을 사용하여 정상 배포됨을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-6c9d554866-vmjtb 1/1 Running 0 5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 5s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-6c9d554866 1 1 1 5s   ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/5.deploy-ocir-image/","tags":["oke"],"title":"1.5 OCIR 이미지 사용하기"},{"categories":null,"contents":"1.7 Kubernetes 지원 버전 및 업그레이드 Kubernetes 지원 버전 Kubernetes 버전은 x.y.z로 표현되며, 각각 x는 메이저, y는 마이너, z는 패치 버전을 뜻하며, 오픈소스 Kubernetes도 현재 버전과 그 이전 2개 버전까지를 지원하고 있습니다. OKE 또한 OKE가 지원하는 최신버전 기준, 그 이전 2개의 마이너 버전까지 지원하고 있습니다. 금일자 기준 지원하는 버전은 다음과 같습니다.\n 1.20.11 1.19.15 1.18.10  신규 버전 출시 후에 30일 동안만 그 이전 버전을 지원하고 있습니다. 예를 들어 OKE에서 1.20.11, 1.19.15가 2021년 10월 8일에 출시되어 기존 버전인 1.20.8, 1.19.12는 각각 30일후인 2021년 11월 7일까지만 지원합니다. 현재 지원 버전은 다음 링크를 참조합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm  버전 관리 단위 OKE 클러스터는 Control Plane과 Worker Node로 구성되며, Control Plane의 버전이 정해지면, Worker Node는 Control Plane과 같거나 그 이하 버전을 사용할 수 있습니다. 물론 호환되는 내에서 말입니다.\nWorker Node는 Node Pool 단위로 버전을 가질 수 있습니다. 여러 Node Pool을 만들어 각각 다른 버전을 가질 수 있습니다.\n버전 업그레이드 OKE 새 버전이 출시되면 버전 업그레이드는 다음 절차를 따릅니다.\n Control Plane 업그레이드  OCI 서비스 콘솔에서 OKE 클러스터 단위를 업그레이드하면 Control Plane만 업그레이드됨 오라클이 관리하는 영역으로 다운타임 없이 자동으로 업그레이드 됨 OKE 클러스터를 업그레이드 하면, 즉 Control Plane이 업그레이드하면 이전 버전으로 다시 다운그레이드 할 수 없음   Worker Node 업그레이드  OCI 서비스 콘솔에서 Node Pool 단위로 업그레이드 업그레이드 방식  in-place 업그레이드  대상이 되는 기존 Node Pool은 OCI 콘솔에서 버전 업그레이드, 실제 Node가 자동으로 업그레이드 되지 않음 kubectl drain 명령으로 특정 노드에 신규 컨테이너가 생기는 것을 방지함 OCI 서비스 콘솔에서 drain한 Node를 종료(Terminate) 시킴 Node 자가치유에 의해 신규 Node가 자동으로 생성되며, 생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전 기존 노드에 대해서 순서대로 모두 진행   out-of-place 업그레이드  신규 버전의 Node Pool 추가 생성 kubectl 명령으로 기존 Node Pool에 있는 Node 제거, Kubernetes에 의해 컨테이너가 모두 이동하면, Node 삭제하는 방식으로 하나씩 진행 기존 Node가 모두 제거되면, 기존 Node Pool 삭제        Node Pool 단위 버전 혼합 테스트를 위해 1.18.10, 1.19.12, 1.19.15, ,1.20.8, 1.20.11 버전이 사용 가능한 상태에서 1.19.12 버전의 OKE 클러스터를 기준으로 테스트를 진행합니다.\n 테스트 환경  OKE 클러스터 - oke-cluster-1 버전: 1.19.12 Control Plane 버전: 1.19.12 Node Pool - poo1 버전: 1.19.12      OKE 클러스터 상세화면으로 이동합니다.\n  왼쪽 아래 Resources \u0026gt; Node Pools 로 이동합니다.\n   현재 pool1이 1.19.12 버전이 있습니다. 추가 Node Pool 생성을 위해 Add Node Pool을 클릭합니다.\n  새 Node Pool 생성을 위한 정보를 입력합니다.\n Name: 새 Node Pool 이름 Version: 일반 버전 혼합을 확인하기 위해 1.18.10 버전은 선택합니다.  사용 가능한 버전을 보면, 현재 OKE 클러스터 버전 이하만 선택 가능한 걸 알 수 있습니다. 현재 OKE 클러스터 생성시 사용 가능한 1.19.15, ,1.20.8, 1.20.11 버전은 보이지 않습니다.      처음 생성시와 비슷하게 생성시 필요한 정보를 입력합니다. 다음은 예시입니다.  Shape: VM.Standard.E3.Flex Placement Configuration: Worker Node가 위치한 AD와 Node용 서브넷 지정 Advanced Options:  Add an SSH Key: Node에 SSH로 접근하기 위한 Publich Key        생성을 요청하면 실제 Node VM이 만들어지고, 준비되는 데 까지 앞서 설치시와 같이 약간의 시간이 걸립니다.\n  Node Pool을 추가 생성하면 그림과 같이 동일 OKE 클러스터에 두 가지 버전의 혼합을 지원하여, 앞서 Node Pool 추가시 본것 처럼 Pool 단위 VM 크기, 위치(AD, 서브넷)을 달리 할 수 있습니다.\n   kubectl로 노드를 조회해도 동일한 결과가 나옵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 23h v1.19.12 10.0.10.226 Ready node 23h v1.19.12 10.0.10.234 Ready node 6m53s v1.18.10 10.0.10.43 Ready node 6m30s v1.18.10 10.0.10.44 Ready node 23h v1.19.12 10.0.10.78 Ready node 6m41s v1.18.10   OKE 클러스터 버전 업그레이드 Control Plane 업그레이드 위와 같이 1.19.12 버전을 사용 중에 새로운 버전이 출시되었다고 가정합니다. 그러면 앞서 설명한 것과 같이 기술지원 정책에 따라 기존 버전은 30일간 지원하기 때문에, 그동안 버전 검증후 업그레이드가 필요합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm    업그레이드가 가능하면, OKE 클러스터 상세 화면에서 Upgrade Available 버튼이 활성화 됩니다.\n   Upgrade Available 버튼을 클릭하면 다음과 같이 안내 문구와 함께 업그레이드를 시작할 수 있습니다. 최신 버전인 v1.20.11을 선택하도록 하겠습니다.\n   버전을 선택하고 아래 Upgrade 버튼을 클릭하여 업그레이드를 시작합니다.\n  클러스터 상태가 UPDATING으로 표시되고 업그레이드가 진행됩니다. 오라클이 관리하는 Control Plane이 내부적으로 순차적으로 업그레이드가 발생합니다. 실제 애플리케이션이 동작하는 Worker Node는 서비스 중지 없이 업그레이드 됩니다.\n  테스트 시점에는 10~15분 후에 업그레이드 완료되었습니다.\n   Worker Node 업그레이드 - in-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. in-place 업그레이드 방식은 Node Pool 업그레이드 -\u0026gt; 기존 Node Drain \u0026gt; POD 이동 확인 -\u0026gt; 기존 Node VM 종료 순으로 업그레이드 합니다.\nNode Pool 업그레이드   업그레이드 하려는 Node Pool의 상세 페이지로 이동합니다.\n  수정을 위해 Edit를 클릭하면, 오른쪽에 수정 페이지가 뜹니다.\nVersion 항목에, 클러스터 버전과 Node Pool의 버전이 표시되며, 업그레이드 가능한 버전이 표시됩니다.\n   클러스터와 동일한 1.20.11로 선택하고 Save Change를 클릭하여 저장합니다.\n  Resources \u0026gt; Work Requests에 가서 보면, 2~3초 뒤에 Node Pool 업그레이드가 완료됩니다.\n   아직 실제 Worker Node가 업그레이드 된 것은 아닙니다.\nResources \u0026gt; Nodes에 가서 보면 기존 버전 그대로입니다.\n   Node Drain 시키기   kubectl 명령으로 Worker Node와 배포된 POD를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 24h v1.19.12 10.0.10.226 Ready node 24h v1.19.12 10.0.10.44 Ready node 24h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-b46nd 1/1 Running 0 72m 10.244.1.4 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 39m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-7tsjf 1/1 Running 0 39m 10.244.1.8 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 40m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl drain 10.0.10.44 --ignore-daemonsets node/10.0.10.44 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-7hc28, kube-system/kube-flannel-ds-b4dp6, kube-system/kube-proxy-f8qmp, kube-system/proxymux-client-6qs4x evicting pod default/nginx-fss-pvc-9fb98454f-7tsjf evicting pod default/nginx-bvs-pvc-7b574c9c5c-b46nd pod/nginx-bvs-pvc-7b574c9c5c-b46nd evicted pod/nginx-fss-pvc-9fb98454f-7tsjf evicted node/10.0.10.44 evicted   아래와 같이 44번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 25h v1.19.12 10.0.10.226 Ready node 25h v1.19.12 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-9smhh 1/1 Running 0 3m20s 10.244.0.138 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 45m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-8zkn2 1/1 Running 0 3m20s 10.244.0.6 10.0.10.185 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 46m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   종료할 Node의 노드 이름을 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -L displayName NAME STATUS ROLES AGE VERSION DISPLAYNAME 10.0.10.185 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-0 10.0.10.226 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-1 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-2   Node Pool 상세 화면에서 종료할 노드 이름을 클릭하여 Compute 인스턴스로 이동합니다.\n   Node에 해당되는 Compute 인스턴스를 종료합니다. boot volume도 같이 종료합니다.\n    Node 자가치유에 의해 신규 Node가 자동으로 생성됩니다. Work Requests를 보면 아래와 같이 재조정(NODEPOLL_RECONCILE) 작업이 발생되어 지정된 갯수에 맞게 노드가 다시 생성됩니다.\n   생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전으로 생성됩니다.\n  kubectl 명령으로 노드를 조회하면, 1.20.2 버전으로 신규 노드가 생성되었습니다. docker 런타임의 deprecate 예정으로 인해 참고로 1.20 부터는 컨테이너 런타임이 cri-o 변경되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.170 Ready node 7m25s v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.185 Ready node 25h v1.19.12 10.0.10.185 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11 10.0.10.226 Ready node 25h v1.19.12 10.0.10.226 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11   기존 노드에 대해서 순서대로 모두 진행합니다.\n  완료 결과\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.128 Ready node 6m10s v1.20.11 10.0.10.128 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.146 Ready node 35s v1.20.11 10.0.10.146 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.170 Ready node 23m v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   Worker Node 업그레이드 - out-of-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. out-of-place 업그레이드 방식은 업그레이드 버전의 Node Pool 신규 생성 -\u0026gt; 기존 Node Pool의 모든 노드 Drain -\u0026gt; 기존 Node Pool 삭제 순으로 업그레이드 합니다.\n새 버전의 Node Pool 만들기   OKE 클러스터 상세 페이지로 이동합니다.\n  Resources \u0026gt; Node Pools 로 이동합니다.\n  그림과 같이 기존 버전의 Node Pool이 있는 상태에서 신규 Node Pool 추가를 위해 Add Node Pool을 클릭합니다.\n   신규 Node Pool 정보를 입력하여 생성합니다.\n Name Version: 새 버전 선택 Shape: Node VM 유형 Number of nodes: 노드 수 Placement Configuration  Node가 위치할 AD, Subnet   Add an SSH key: Node VM에 SSH 접속시 사용할 키의 Private Key     추가 된 Node Pool을 OCI 서비스 콘솔 확인할 수 있습니다.\n  기존 Node Pool의 모든 노드 Drain   구동 중인 앱들이 기존 Node Pool에서 동작하고 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 18m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-2jbp8 1/1 Running 0 18m 10.244.0.5 10.0.10.242 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 19m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 18m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready node 71m v1.19.12 pool1 10.0.10.29 Ready node 71m v1.19.12 pool1 10.0.10.61 Ready node 71m v1.19.12 pool1 10.0.10.12 Ready node 4m25s v1.20.11 pool2 10.0.10.126 Ready node 4m48s v1.20.11 pool2 10.0.10.191 Ready node 4m42s v1.20.11 pool2   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl drain 10.0.10.242 --ignore-daemonsets node/10.0.10.242 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-l62xw, kube-system/kube-flannel-ds-7dv8l, kube-system/kube-proxy-zv4ks, kube-system/proxymux-client-24nmv evicting pod kube-system/coredns-c5d4bf466-wv8fb evicting pod default/nginx-fss-pvc-9fb98454f-2jbp8 evicting pod kube-system/coredns-c5d4bf466-h5rrm pod/coredns-c5d4bf466-wv8fb evicted pod/nginx-fss-pvc-9fb98454f-2jbp8 evicted pod/coredns-c5d4bf466-h5rrm evicted node/10.0.10.242 evicted   아래와 같이 242번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 76m v1.19.12 pool1 10.0.10.29 Ready node 76m v1.19.12 pool1 10.0.10.61 Ready node 76m v1.19.12 pool1 10.0.10.12 Ready node 9m15s v1.20.11 pool2 10.0.10.126 Ready node 9m38s v1.20.11 pool2 10.0.10.191 Ready node 9m32s v1.20.11 pool2 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 22m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5k8wg 1/1 Running 0 57s 10.244.3.2 10.0.10.126 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 23m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 22m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   나머지 기존 Node Pool에 있는 Node들도 drain합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.29 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.61 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.12 Ready node 11m v1.20.11 pool2 10.0.10.126 Ready node 12m v1.20.11 pool2 10.0.10.191 Ready node 12m v1.20.11 pool2   기존 Node Pool 삭제   기존 Node Pool에 있는 모든 Node들이 drain되어 더이상 사용되지 않습니다.\n  OCI 서비스 콘솔에서 OKE 클러스터 상세페이지로 이동합니다.\n  Resources \u0026gt; Node Pools로 이동하여 기존 Node Pool을 삭제합니다.\n   업그레이드가 완료되었습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.12 Ready node 18m v1.20.11 pool2 10.0.10.126 Ready node 19m v1.20.11 pool2 10.0.10.191 Ready node 19m v1.20.11 pool2   ","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/7.supported-version-and-upgrade/","tags":["oke"],"title":"1.7 Kubernetes 버전 업그레이드"},{"categories":null,"contents":"1.8 애플리케이션 로그 모니터링 OKE 상에 배포되어 있는 애플리케이션 로그 모니터링을 OCI Logging 서비스를 통해 모니터링할 수 있습니다.\nOCI Logging 서비스 사용 권한 설정 Worker Node에 대한 Dynamic Group 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\n  OKE 클러스터가 있는 Compartment의 OCID를 확인합니다.\n   좌측 Dynamic Group 메뉴로 이동하여 아래 규칙을 가진 Dynamic Group을 만듭니다.\ninstance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39;    Dynamic Group에 대한 OCI Logging 서비스 권한 부여하기   좌측 Policy 메뉴로 이동하여 아래 규칙을 가진 Policy을 만듭니다. 방금 생성한 Dynamic Group에 대한 Policy를 만듭니다.\nallow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use log-content in compartment \u0026lt;compartment-name\u0026gt;    컨테이너를 위한 Custom Log 설정하기 Log Group 만들기 Log Group은 로그들을 관리하는 말 그대로 로그의 묶음 단위 입니다. 커스텀 로그를 만들기 위해 먼저 만듭니다.\n  좌측 상단 햄버거 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging \u0026gt; Log Groups로 이동합니다.\n  Create Log Group을 클릭하여 로그 그룹을 만듭니다.\n   Custom Log 만들기 Custom Log는 커스텀 애플리케이션에서 수집하는 로그에 매핑되는 것입니다. Custom Log를 정의하고, 이에 대한 로그 수집기를 정의합니다.\n  Resources \u0026gt; Logs 메뉴로 이동하여 Create custom log를 클릭합니다.\n  로그 이름과 보관 주기 등을 설정하여 custom log를 만듭니다.\n   이해를 돕고자 Agent는 별도로 설정합니다. 여기서는 일단 Add configuration later 선택\n   Agent Configuration 설정 Agent Configuration는 로그를 수집하는 agent를 설정하는 부분입니다.\n  Logging \u0026gt; Agent Configurations 메뉴로 이동하여 Create agent log를 클릭합니다.\n  Agent 이름 및 대상 Host Group을 앞서 만든 Dynamic Group으로 지정합니다.\n   Agent 설정 부분에서 로그가 위치한 경로 및 수집된 로그의 전달 위치를 지정합니다.\n  log input: /var/log/containers/*.log\n앞서 지정한 Dynamic Group상에 있는 VM, 여기서는 OKE 클러스터 Worker Node VM 상에 수집할 로그의 위치를 지정합니다.입력하고 엔터키를 꼭 칩니다.\n  log destination: 수집한 로그를 전달한 앞서 생성한 custom log 이름을 지정합니다.\n     참고: Worker Node VM상에 컨테이너 로그 위치   Worker Node VM에 SSH로 접속이 가능한 환경, 예, bastion host에서 Worker Node에 접속해 보면 컨테이너 로그 위치는 다음과 같습니다.\n[opc@bastion-host ~]$ ssh opc@10.0.10.175 Last login: Tue Nov 16 06:43:43 2021 from bastion-host.suba22926d1b.okecluster1.oraclevcn.com [opc@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 ~]$ sudo su [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 opc]# cd /var/log/containers/ [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# ls -la total 16 drwxr-xr-x. 2 root root 4096 Nov 15 14:05 . drwxr-xr-x. 13 root root 4096 Nov 15 13:55 .. lrwxrwxrwx. 1 root root 100 Nov 15 13:52 coredns-c5d4bf466-qdgrh_kube-system_coredns-a533d7375a4bd90b894d533e637bae6ce38e2e7d89cd0ff16e34bd120111c7e4.log -\u0026gt; /var/log/pods/kube-system_coredns-c5d4bf466-qdgrh_24b226a0-0fb3-4ede-b02f-17b177e6c248/coredns/0.log ... lrwxrwxrwx. 1 root root 101 Nov 15 14:05 nginx-bvs-pvc-7b574c9c5c-vdpdh_default_nginx-cc996c8fd2281d1bec7fdad75518c66a2ec6f03adc214f8bbd97c26967e8d0e7.log -\u0026gt; /var/log/pods/default_nginx-bvs-pvc-7b574c9c5c-vdpdh_aab712c6-81e1-434c-b051-f3e8fe50fa24/nginx/0.log lrwxrwxrwx. 1 root root 106 Nov 15 13:52 proxymux-client-cb86k_kube-system_proxymux-client-471c1c80fe2dc01c5e1413819af58d968796541cc744384f1b2daa566873d8ba.log -\u0026gt; /var/log/pods/kube-system_proxymux-client-cb86k_153b7b7a-a777-4be9-b971-657eb8ecdddd/proxymux-client/0.log   /var/log/containers/*.log는 위에서 보는 것처럼 링크라서 kubernetes namespace 기준으로 하고 싶다면, 로그 경로를 default namespace인 경우/var/log/pods/default_*/*/*.log 이렇게 해도 되겠습니다.\n  로깅 테스트   애플리케이션 로그 확인을 위해 이전 가이드에 샘플로 배포된 nginx 앱을 접속해 봅니다.\n   발생한 POD 로그는 다음과 같습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs nginx-bvs-pvc-7b574c9c5c-vdpdh -f ... 10.244.0.128 - - [16/Nov/2021:08:10:46 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   동일한 로그가 Worker Node VM 상에서도 로그를 조회해 보면 /var/log/containers/*.log 위치에 발생하는 것을 확인 할 수 있습니다.\n[root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# pwd /var/log/containers [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# tail -f nginx-bvs-pvc-7b574c9c5c-vdpdh_default_nginx-cc996c8fd2281d1bec7fdad75518c66a2ec6f03adc214f8bbd97c26967e8d0e7.log ... 2021-11-16T08:10:46.961696444+00:00 stdout F 10.244.0.128 - - [16/Nov/2021:08:10:46 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   OCI 서비스 콘솔에서 Logging 화면으로 다시 돌아갑니다.\n  Agent, Logs, Log Groups 각 화면에서 Resources \u0026gt; Explore Log에서 로그를 조회 할 수 있습니다. 검색을 위해서는 로그목록 오른쪽위에 있는 Explore with Log Search를 클릭합니다.\n   Custom filters 항목에서 POD 이름 또는 앞서 테스트 URL에 있는 customlogtest 같이 검색값으로 조회하면 됩니다. Custom filters에 값을 입력하고 엔터키를 꼭 칩니다.\n   검색된 로그 데이터를 확인할 수 있습니다.\n   ","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/8.monitor-application-log/","tags":["oke"],"title":"1.8 애플리케이션 로그 모니터링"},{"categories":null,"contents":"","lastmod":null,"permalink":"https://thekoguryo.github.io/oci-cloudnative/search/","tags":null,"title":"Search"}]