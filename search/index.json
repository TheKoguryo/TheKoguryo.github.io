[{"categories":null,"contents":"1.1 OKE(Oracle Cloud Infrastructure Container Engine for Kubernetes) 소개 Oracle Cloud Infrastructure Container Engine for Kubernetes는 OCI에서 제공하는 관리형 쿠버네티스 서비스 입니다. 줄여서 OKE라고 부며 CNCF 인증된 Kubernetes 버전을 사용하고 있습니다.\nQuick Start 버전으로 클러스터를 생성하게 되면 OCI 자원들을 사용하여 아래와 같은 구조를 가지는 쿠버네티스 클러스터가 생성됩니다.\n  Control Plane Node:\n 쿠버네티스는 Control Plane은 OCI가 관리하는 영역으로 사용자에게 직접 노출이 되지 않습니다. API Server 접속을 위한 정보만 VNIC을 통해 사용자의 VCN상의 API Subnet으로 노출하게 됩니다. 생성 옵션에 따라 Public, Private 모두 지원    Data Plane Nodes\n 실제 사용자의 컨테이너가 구동되는 Worker Node 들을 VM 또는 베어메탈 서버를 사용합니다. 개발자가 직접 Node 에 접속할 수 있습니다. 생성 옵션에 따라 Public, Private 모두 지원    OCI 서비스 연계\n 컨테이너 배포하고 서비스 하기 위해 필요한 Container Image Registry, Persistent Volume, Load Balancer, IAM, Audit 등 OCI 다른 서비스들과 연계된 기능을 제공합니다.    OKE 클러스터 자체에 대한 비용은 무료이며, 클라스터가 사용하는 OCI 자원(예, VM Node, 네트워크 사용량, 스토리지 사용량)에 대한 비용만 청구하는 방식입니다.\n   ","lastmod":"2022-01-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/1.oke/","tags":["oke"],"title":"1.1 OKE(OCI Container Engine for Kubernetes) 소개"},{"categories":null,"contents":"15.1 Resource Manager 사용하기 Resource Manager는 OCI 자원 Provisioning을 자동화하는 기능으로 Terraform Configuration을 등록해서 실행하는 서비스입니다. 클라이언트에 Terraform을 설치하는 것이 아닌, Resource Manager에서 Terraform Configuration 파일들을 등록해서 OCI 콘솔 또는 OCI CLI로 실행할 수 있습니다.\nTerraform CLI를 별도로 설치할 필요가 없고, Terraform 실행 State를 Resource Manager가 관리한다는 차이가 있습니다.\n  Stack\n스택은 Terraform Configuration 묶음을 등록하여 생성하는 Provisioning 단위입니다.\n  Job\nJob은 스택으로 등록된 Terraform Configuration의 실행 작업이며, Terraform에서 경험한 것 처럼 Terraform Plan, Apply, Destroy이 실행되는 작업입니다.\n  Resource Manager 사용하기   Terraform Configuration 만들기 15.2 Terraform Configurations 작성하기에서 사용한 VCN 만들기 설정파일을 그대로 사용해 보겠습니다. 다만 차이는 oci provider에서 region만 필수 필드이고, 나머지 항목들은 필수가 아닙니다. 등록한 Resource Manager의 Tenancy에서 실행되고, 인증받은 유저가 실행하기 때문에 나머지 항목은 필수가 아닙니다.\n  provider.tf\nvariable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { region = \u0026#34;${var.region}\u0026#34; }   vcn.tf\nvariable \u0026#34;compartment_ocid\u0026#34; {} resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; dns_label = \u0026#34;vcn1\u0026#34; compartment_id = \u0026#34;${var.compartment_ocid}\u0026#34; display_name = \u0026#34;vcn1\u0026#34; } output \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_virtual_network.vcn1.id}\u0026#34;] }    다음 변수들을 OCI 콘솔 정보를 통해 자동으로 계산됩니다.  tenancy_ocid compartment_ocid region current_user_ocid https://docs.oracle.com/en-us/iaas/Content/ResourceManager/Concepts/terraformconfigresourcemanager.htm#configvar      Terraform Configuration을 Resource Manager 용으로 변경하기\n  OCI Console에서 Resource Manager를 사용하기 때문에 UI가 존재합니다. Terraform 설정값을 UI에서 설정할 수 있게 확장할 수 있는 스키마를 제공합니다.\n Extend Console Pages Using Schema Documents    provider.tf\nvariable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { region = \u0026#34;${var.region}\u0026#34; }   vcn.tf 변경분\nvariable \u0026#34;compartment_ocid\u0026#34; {} variable \u0026#34;vcn_name\u0026#34; {} variable \u0026#34;vcn_dns_label\u0026#34; {} variable \u0026#34;vcn_cidr_block\u0026#34; {} resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { cidr_block = var.vcn_cidr_block dns_label = var.vcn_dns_label compartment_id = var.compartment_ocid display_name = var.vcn_name } output \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_virtual_network.vcn1.id}\u0026#34;] }     schema.yaml\nStack 설명구 추가, 변수 타이틀, 설명문 추가, 변수 UI 배열 구문 등을 아래와 같이 추가할 수 있습니다.\ntitle:\u0026#34;Resource Manager Example VCN by TheKoguryo\u0026#34;description:\u0026#34;Resource Manager로 VCN을 만드는 예제입니다.\u0026#34;schemaVersion:1.0.0version:\u0026#34;20220119\u0026#34;locale:\u0026#34;en\u0026#34;groupings:- title:\u0026#34;Basic Hidden\u0026#34;visible:falsevariables:- compartment_ocid- tenancy_ocid- region- title:\u0026#34;Basic Configuration\u0026#34;variables:- vcn_name- vcn_dns_label- title:\u0026#34;Configure VCN\u0026#34;variables:- vcn_cidr_block variables:vcn_name:type:stringtitle:VCN Namerequired:truedefault:example_vcnvcn_dns_label:type:stringtitle:VCN DNS Labeldescription:\u0026#34;Only letters and numbers, starting with a letter. 15 characters max.\u0026#34;required:truedefault:examplevcn vcn_cidr_block:type:list(string)title:VCN IPv4 CIDR Blockdescription:\u0026#34;Example: 10.0.0.0/16\u0026#34;required:truedefault:[\u0026#34;10.0.0.0/16\u0026#34;]    OCI 콘솔에서 내비게이션 메뉴의 Developer Services \u0026raquo; Resource Manager \u0026raquo; Stacks을 클릭합니다.\n  Create Stack 을 클릭합니다.\n  Stack 생성 설정\n  첫 번째 My Configuration으로 작성한 파일이 위치한 폴더를 Drop a folder로 드래그 앤 드랍합니다.\n   설정 로딩이 되어 Stack의 기본 설명이 아래와 같이 보이게 됩니다.\n   다음 페이지로 넘어갑니다.\n    변수 값 입력\n작성한 schema.yaml을 바탕으로 그림과 같이 표현됩니다.\n   값을 변경해 보고 리뷰후 Stack을 생성합니다.\n   terraform plan 명령처럼 Plan 버튼을 클릭해봅니다.\n  Terraform CLI에서 terraform plan을 수행했던 것과 동일한 결과가 로그로 확인됩니다.\n   다시 Stack 상세화면으로 돌아가 Apply를 클릭하여 실행해 봅니다.\n  생성이 성공하면 Resouces \u0026raquo; Job Resources 에서 결과물로 생성된 자원을 확인할 수 있습니다.\n   링크를 따라 가면 실제 VCN이 생성된 것을 확인할 수 있습니다.\n  Stack 상세 정보로 돌아가 Destroy 버튼을 클릭합니다.\n  실행이 끝나고 VCN 리스트 화면으로 이동해 보면, 앞서 생성된 example_vcn이 삭제된 것을 알수 있습니다.\n  Stack의 지금까지 Plan, Apply, Destroy를 실행한 모든 작업 결과 내역은 Stack 상세 화면에서 볼 수 있습니다.\n   ","lastmod":"2022-01-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter15/1/","tags":["resource manage","terraform"],"title":"15.1 Resource Manager 사용하기"},{"categories":null,"contents":"14.1.2.1 Object Storage를 위한 IAM Policy 설정 Object Storage를 위한 Policy 설정 IAM Policy는 OCI 자원에 대한 접근 정책입니다. 하나의 정책은 일반적으로 사용자 그룹에게 특정 Compartment에 속한 특정 타입의 자원에 대한 권한을 허용하는 것을 정의하는 것으로 생각하면 하면 됩니다.\n  Object Storage, Archive Storage, and Data Transfer 레퍼런스\n https://docs.cloud.oracle.com/iaas/Content/Identity/Reference/objectstoragepolicyreference.htm    예시\n  Object 목록 조회\nAllow group \u0026lt;group_name\u0026gt; to inspect objects in compartment \u0026lt;compartment_name\u0026gt;   Object 읽기\nAllow group \u0026lt;group_name\u0026gt; to read objects in compartment \u0026lt;compartment_name\u0026gt;   Object 사용(읽기 \u0026amp; 업데이트)\nAllow group \u0026lt;group_name\u0026gt; to use objects in compartment \u0026lt;compartment_name\u0026gt;   Object 관리(사용, 생성, 삭제, 복구)\nAllow group \u0026lt;group_name\u0026gt; to manage objects in compartment \u0026lt;compartment_name\u0026gt;     Step 1. Bucket 생성  사용자가 쓸 Object Storage Bucket 을 해당 Compartment에 미리 생성합니다.  Name: 예) ExampleBucketForCLI    Step 2. API 접근 사용자에 권한 설정   관리자로 OCI 콘솔에 로그인합니다.\n  Policy 설정\n  oci cli로 연결되는 유저, 즉 oci setup config에서 설정한 유저가 속한 그룹에 다음 Policy가 필요합니다.\nAllow group \u0026lt;group_name\u0026gt; to inspect buckets in compartment \u0026lt;compartment_name\u0026gt; Allow group \u0026lt;group_name\u0026gt; to manage objects in compartment \u0026lt;compartment_name\u0026gt;   예시\n  이름: ObjectStorageToolPolicy\n  규칙\nAllow group ObjectStorageToolGroup to inspect buckets in compartment Sandbox Allow group ObjectStorageToolGroup to manage objects in compartment Sandbox       ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/2/1/","tags":["object storage","policy"],"title":"14.1.2.1 Object Storage를 위한 IAM Policy 설정"},{"categories":null,"contents":"14.2.1 Terraform 설치하기 HashiCorp 다운로드 페이지에서 OS 맞는 설치파일을 다운받습니다. 압축을 풀면 terraform.exe 또는 terraform 파일 하나이므로 사용할 폴더에 복사하고 PATH에 등록하면 바로 사용할 수 있습니다.\n   Linux 예시\nwget https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip unzip terraform_1.1.3_linux_amd64.zip sudo mv terraform /usr/local/bin terraform -v   ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/1/","tags":["terraform"],"title":"14.2.1 Terraform 설치하기"},{"categories":null,"contents":"13.1 Autoscaling을 위한 준비 Autoscaling에 사용할 인스턴스 만들기 Autoscaling을 사용하려면 기준 이미지가 필요합니다. Autoscaling을 통해 서버 확장이 Apache가 설치된 인스턴스 한 대가 추가 기동 될 수 있도록, Load Balancer 장에서 사용한 Oracle Linux에 Apache가 설치된 이미지를 만들어 보겠습니다. 또한 추가 인스턴스가 실행할 최신 소스를 가져오는 방법을 고려하여 cloud-init을 사용해봅니다. 이때 추가 인스턴스임을 확인하기 쉽게 하기 위해 index.html에 생성된 호스트의 Private IP가 보이도록 합니다.\n  Compute Instance 생성 화면으로 이동합니다.\n  아래쪽 Show advanced options 클릭\n  cloud-init 을 사용하여 인스턴스 생성시 시작할 스크립트를 설정할 수 있습니다. 여기서는 bash 스크립트를 사용하여 Apache 설치 및 index.html 생성작업을 실행합니다.\n#!/bin/bash sudo yum -y install httpd sudo firewall-offline-cmd --add-port=80/tcp sudo systemctl enable httpd MY_IP=$(hostname -I) sudo bash -c \u0026#34;echo \u0026#39;Hello Apache on \u0026#39;$MY_IP\u0026gt;\u0026gt; /var/www/html/index.html\u0026#34; sudo reboot    아래쪽 Create를 클릭하여 인스턴스를 만듭니다.\n  cloud-init 로그 /var/log/messages 파일에서 cloud-init의 실행로그를 확인할 수 있습니다.\n  cloud-init 로그 예시\n... Jan 17 07:03:31 autoscalingwebserver-694639 cloud-init: Installed: Jan 17 07:03:31 autoscalingwebserver-694639 cloud-init: httpd.x86_64 0:2.4.6-97.0.5.el7_9.2 Jan 17 07:03:31 autoscalingwebserver-694639 cloud-init: Dependency Installed: Jan 17 07:03:31 autoscalingwebserver-694639 cloud-init: apr.x86_64 0:1.4.8-7.el7 apr-util.x86_64 0:1.5.2-6.0.1.el7 Jan 17 07:03:31 autoscalingwebserver-694639 cloud-init: httpd-tools.x86_64 0:2.4.6-97.0.5.el7_9.2 mailcap.noarch 0:2.1.41-2.el7 Jan 17 07:03:31 autoscalingwebserver-694639 cloud-init: Complete! Jan 17 07:03:32 autoscalingwebserver-694639 cloud-init: success Jan 17 07:03:32 autoscalingwebserver-694639 cloud-init: Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. ...      주의사항 cloud-init에서 앞서 Load Balancer 실습에서 사용한 firewall-cmd 구문을 그대로 사용하면 cloud-init시 다음 에러가 발생합니다. 그래서 대신 firewall-offline-cmd 구문을 사용합니다. Mar 5 05:50:46 instance-20190305-1447 cloud-init: ERROR:dbus.proxies:Introspect error on :1.3:/org/fedoraproject/FirewallD1: dbus.exceptions.DBusException: org.freedesktop.DBus.Error.NoReply: Did not receive a reply. Possible causes include: the remote application did not send a reply, the message bus security policy blocked the reply, the reply timeout expired, or the network connection was broken.   Instance Configuration 만들기 Autoscaling시 만들어진 Instance의 구성정보를 만들기 위해 Instance Configuration이 필요합니다. 다음과 같은 순서로 만듭니다\n  사용할 인스턴스의 상세 화면으로 이동합니다.\n  추가 액션 메뉴 중 Create instance configuration 클릭   Instance Configuration 이름 입력 후 Create 클릭   생성 완료\n  Instance Pool 만들기 Instance Pool은 Instance Configuration을 통해 만들어진 같은 구성기반의 인스턴스들의 모음입니다.\n  Instance Pool에서 Load Balancer를 사용하기 위해서는 사전에 만들어야 합니다. 이전 장에서 만든 Load Balancer를 사용하거나, 없는 경우 미리 만듭니다.\n  앞서 만든 Instance Configuration에서 Create instance pool을 클릭   Instance Pool 생성 기본 정보\n Name: 원하는 이름 입력 Number of instances: Pool 생성과 함께 시작할 인스턴스 수, 일단 1 설정     위치 정보\n  생성될 VM 들이 위치할 도메인 및 VCN, Subnet을 선택합니다.\n     Load Balancer 설정\n 인스턴스들간의 분배를 위해 Load Balacner가 필요하므로 Attach a load balancer을 체크합니다. Load Balancer 장에서 만든 것을 그대로 사용합니다. 없으면 새로 만듭니다. Load balancer: 사용할 Load Balancer 선택 Backend set: Load Balancer의 분배 대상이 되어 Backend Set 선택 Port: 수신할 포트 입력 VNIC: Primary VNIC만 있으므로 선택     선택한 설정을 확인후 Create 합니다.\n  Instance Pool이 만들어지고 인스턴스 1개가 만들어짐   Resources \u0026raquo; Load balancer 로 이동하여 연관된 Load Balancer 클릭   LB 상세정보에서 Backend Set으로 이동하여 Instance Pool이 사용하는 Backend Set 클릭\n  Backend Set에 자동으로 추가된 것을 확인할 수 있습니다. 또한 Health가 OK인 상태로 생성된 VM에 있는 Apache 서버와 정상 통신 된 것을 알 수 있습니다.   브라우저를 통해 LB의 Public IP로 접속합니다. 정상적으로 구성되었음을 알 수 있습니다.   ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter13/1/","tags":["autoscailing","cloud-init"],"title":"13.1 Autoscaling을 위한 준비"},{"categories":null,"contents":"11.1 Domain Name과 매핑하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 freenom 사이트에서 발급받은 무료 Domain Name(thekoguryo.ml)을 사용하였습니다.\n앞선 과정에서 Load Balancer를 사용해 Apache 웹서버와 연동하였습니다. 가지고 있는 Domain Name을 Load Balancer의 IP와 매핑하도록 하겠습니다.\nOCI DNS 서비스 설정   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; DNS Management \u0026raquo; Zones 항목으로 이동합니다.\n  Create Zone 클릭   생성정보 입력\n METHOD: Manual ZONE NAME: 가지고 있는 Domain Name 입력 ZONE TYPE: Primary     Create 클릭\n  Resources \u0026raquo; Records로 이동하면 생성된 항목을 볼 수 있습니다. Zone 내부에 NS 유형과 SOA 유형의 레코드가 생성되어 있습니다. NS는 네임 서버 레코드, SOA는 권한 시작 레코드입니다.   j앞서 웹서버를 Load Balancer의 IP 대신 특정 주소(www.thekoguryo.ml)로 접속이 되게 레코드를 추가해 봈니다.\n Record Type: A - IPv4 Address NAME: www TTL: 300, 우측 자물쇠는 클릭하여 잠금 해제 후 TTL 값 입력 RDATA MODE: Basic ADDRESS: 매핑할 IP, 여기서는 앞서 만든 Load Balancer의 IP 입력     Submit 클릭\n  변경분을 반영하기 위해 Publish Change 클릭\n  확인 창이 뜨면 한번 더 Publish Change 클릭   레코드 추가 및 반영 완료   레코드 중 NS 유형인 네임서버 주소를 모두 복사합니다.\n  Domain Name 제공 측에 설정 이제 Domain Name을 구입한 사이트에서 설정이 필요합니다. 아래 과정은 freenom 기준 설정입니다. 구입한 사이트에서 비슷한 방식으로 설정합니다.\n freenom 사이트에 접속하여 Services \u0026raquo; My Domains으로 이동합니다. My Domain에서 사용할 도메인 네임 우측의 [Manage Domain] 클릭  Management Tools \u0026raquo; Nameservers를 선택한 다음 **Use custom nameservers (enter below)**를 선택합니다. 앞서 OCI DNS Zone 에서 복사해둔 네임서버 주소를 차례대로 입력한 후 Change Nameservers 클릭   테스트   브라우저를 통해 Domain Name으로 접속해 봅니다.\n  그림과 같이 Domain Name을 통해 접속되는 것을 확인할 수 있습니다.   ","lastmod":"2022-01-16T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter11/1/","tags":["DNS","domain name"],"title":"11.1 Domain Name과 매핑하기"},{"categories":null,"contents":"12.1 Compute 인스턴스 Monitoring 활성화 Monitoring 활성화 아래는 OracleCloudAgent가 이미 설치된 최신 Oracle Linux 이미지를 사용하였습니다.\n  모니터링하려는 Compute 인스턴스의 상세 페이지로 이동합니다.\n  왼쪽 아래 Resources \u0026raquo; Metrics 선택\n  모니터링 활성화를 위해 Enable Monitoring 클릭   Boot Volumes, Block Volumes, VNICs Compute를 제외한 모니터링을 지원하는 다른 서비스들을 기본적으로 메트릭 정보가 취합됩니다. Compute에 장착된 자원에 대한 모니터링을 확인합니다.\n  Compute 인스턴스의 왼쪽 아래 Resources \u0026raquo; Boot Volume 선택후 장착된 자원 클릭   Compute 인스턴스의 왼쪽 아래 Resources \u0026raquo; Attached VNICs 선택후 장착된 자원 클릭   장착된 Block Volumes이 있는 경우 동일하게 모니터링됩니다.\n  ","lastmod":"2022-01-16T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/1/","tags":["monitoring"],"title":"12.1 Compute 인스턴스 Monitoring 활성화"},{"categories":null,"contents":"9.1 기본 제공하고 있는 OS Image Compute Instance 생성을 위한 OS Image 및 애플리케이션 Image를 제공하며, 사용자가 만든 Custom Image, Boot Volumes을 통해 Instance를 생성할 수 도 있습니다. 그림처럼 Compute Instance 생성시 오른쪽 이미지 선택 메뉴에서 고를 수 있는 현재 기본 제공하고 있는 OS Image 목록입니다.  제공 이미지   Platform Images\n   Oracle Images\n   Partner Images\n   ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter09/1/","tags":["os image","image"],"title":"9.1 기본 제공하고 있는 OS Image"},{"categories":null,"contents":"7.1 Bucket 생성하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Object Storage \u0026amp; Archive Storage \u0026raquo; Buckets 항목으로 이동합니다.\n  Bucket을 생성할 Compartment를 선택합니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. [Core Infrastructure] \u0026raquo; [Object Storage] \u0026raquo; [Storage] 항목으로 이동합니다.\n  Create Bucket을 클릭합니다.\n   생성 정보 입력\n Bucket Name: 생성할 Bucket 이름을 입력합니다, 예) \u0026ldquo;ExampleBucket\u0026rdquo; Default Storage Tier:  Standard: 표준 Object Storage로 빠르게 자주 사용할 데이터를 저장하기 위해 사용합니다. Archive: 가끔 사용하지만, 장기간 보관 해야 할 데이터를 저장하기 위해 사용합니다. STARNDARD 보다는 저렴하지만, 데이터 사용시 로딩 시간이 조금 더 걸립니다.   추가 옵션: 현재는 기본값 사용     Create을 클릭합니다.\n  생성이 완료되면 상세 정보를 보기 위해 이름을 클릭합니다.\n  생성된 Bucket 상세 정보\n   Bucket이 생성되었습니다. 이제 Object를 추가해서 사용할 수 있습니다.\n  ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/1/","tags":["object storage","bucket"],"title":"7.1 Bucket 생성하기"},{"categories":null,"contents":"4.1 Reserved Public IP 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; IP Management \u0026raquo; Reserved Public IPs 항목으로 이동합니다.\n  Reserve Public IP Address를 클릭합니다.\n   생성 정보 입력\n  Name: 생성할 Reserved Public IP의 이름 입력\n  Compartment: 위치할 Compartment 선택, 예, \u0026ldquo;Sandbox\u0026rdquo; 선택\n  IP Address Source: 사용자가 가진 BYOIP가 있는 경우 선택할 수 있겠으나, 따로 없으면 Oracle로 선택\n     Reserve Public IP Address 클릭\n  IP가 예약되었습니다. 반납(삭제)하지 않는 동안 원하는 Instance에 부여하여 사용할 수 있습니다.\n   ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter04/1/","tags":["reserved public ip"],"title":"4.1 Reserved Public IP 만들기"},{"categories":null,"contents":"3.1 작업 Compartment 만들기 Compartment는 자원들을 쉽게 관리할 수 있도록 하는 논리적인 개념으로 폴더 구조라고 생각하면 됩니다. Tenancy가 생성되면 최초로 Root Compartment 하나가 만들어져 있으며, 관리자가 Root Compartment 하위로 새로운 Compartment를 추가할 수 있습니다. 모든 OCI 자원들은 특정 Compartment에 속하게 되며 Compartment 단위로 사용자들의 접근 정책을 관리할 수 있습니다.\nCompartment, 사용자 관리, 접근 정책 관리 등은 Identity and Access Management에서 이후에 알아 보겠습니다.\n여기서는 Compute VM 생성작업을 위한 전 단계로 Compartment만 만들도록 하겠습니다.\nStep 1. Sandbox Compartment 만들기  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Compartments 항목으로 이동합니다. Create Compartment 클릭 생성 정보 입력  Name: \u0026ldquo;Sandbox\u0026rdquo; 입력 Description: 설명 입력, 예, 이름과 동일하게 \u0026ldquo;Sandbox\u0026rdquo; Parent Compartment: 상위 Compartment 지정, Root Compartment 선택   Create Compartment 클릭 만들어진 Sandbox Compartment가 일정의 폴더로 이후 생성할 자원들을 관리할 공간이라고 생각하면 됩니다.  ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/1/","tags":["oci user"],"title":"3.1 작업 Compartment 만들기"},{"categories":null,"contents":"3.5.1 Linux, Mac 에서 접속하기 ssh-keygen로 생성한 PEM(Privacy Enhanced Mail) 파일 형식의 키는 Linux, Mac에서는 바로 사용 가능합니다. PuTTY Key Generator로 생성한 .ppk은 변환기를 통해 PEM 형식으로 변환하여 사용합니다.\n유닉스 스타일 시스템에서 Linux 인스턴스 접속하기   개인키 권한 변경\n$ chmod 400 \u0026lt;private_key\u0026gt;  \u0026lt;private_key\u0026gt;: Linux 인스턴스에 등록된 공개키에 매칭되는 개인키    SSH 명령을 통해 접속\n$ ssh –i \u0026lt;private_key\u0026gt; \u0026lt;username\u0026gt;@\u0026lt;public-ip-address\u0026gt;  \u0026lt;private_key\u0026gt;: Linux 인스턴스에 등록된 공개키에 매칭되는 개인키 : 인스턴스의 디폴트 사용자입니다. : 인스턴스의 Public IP입니다. OCI 콘솔에서 확인할 수 있습니다.    디폴트 접속 사용자    OS Image Default User Name     Oracle Linux, CentOS opc   ubuntu ubuntu    접속 예시 - 유닉스 스타일 시스템 ubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i privateKey opc@146.56.171.40 The authenticity of host \u0026#39;146.56.171.40 (146.56.171.40)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:NWD7RMc4Mb1azKCAN24f6hNUh55YKAygfB97WS9EpP0. Are you sure you want to continue connecting (yes/no/[fingerprint])? yes Warning: Permanently added \u0026#39;146.56.171.40\u0026#39; (ECDSA) to the list of known hosts. [opc@examplelinuxinstance ~]$ ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/1/","tags":["linux","ssh"],"title":"3.5.1 Linux, Mac 에서 접속하기"},{"categories":null,"contents":"3.6.1 Block Volume 생성하기 Block Volume은 OCI Compute Instance와 함께 사용하는 네트워크 스토리지입니다. Block Volume을 생성하여 인스턴스에 장착하고 마운트하면 컴퓨터에 물리적인 하드 드라이브를 연결해서 사용하는 것처럼 사용할 수 있습니다. 한 번에 한 Instance에 장착될 수 있으면, 장착해제 후 다른 Instance에 장착할 수 있습니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Block Storage \u0026raquo; Block Volumes 항목으로 이동합니다.\n  Create Block Volume을 클릭합니다.\n   생성 정보 입력\n  Name: 원하는 이름 입력\n  Create in Compartment: 현재 사용중인 Compartment가 기본으로 보입니다. 원하는 Compartment을 선택합니다.\n  Availability Domain: Compute Instance가 속한 Availability Domain을 선택합니다. Volume과 Instance는 반드시 같은 AD여야 합니다.\n  Size: 사이즈 설정, 최소 사이즈 50GB\n   Backup Policies: 선택 안함.\n  Cross Region Replication: Free Tier는 단일 Region 이므로 여기서는 OFF\n  Encryption: 지금은 OCI 제공 키 사용\n    Create Block Volume 클릭\n  생성 완료\nBlock Volume이 PROVISIONING 상태로 보입니다. 완료되면 AVAILABLE 상태로 되며, Instance에 장착할 수 있게 됩니다.\n   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/1/","tags":["block volume"],"title":"3.6.1 Block Volume 생성하기"},{"categories":null,"contents":"3.7.1 Block Volume Full 백업하기 이전 가이드에서 Block Volume을 삭제한 경우, 다시 Block Volume을 만듭니다.\n기본 Block Volume 백업하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Block Storage \u0026raquo; Block Volumes 항목으로 이동합니다.\n  백업하려는 Block Volume의 이름을 클릭합니다.\n  Block Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Block Volume Backups을 클릭합니다.\n  Create Block Volume Backup 을 클릭합니다.   백업 생성화면에서 이름을 입력하고 하단의 Create Block Volume Backup을 클릭합니다.\nBACKUP TYPE으로 풀 백업과 증분 백업을 설정할 수 있습니다.\n Full Backup: 볼륨 생성이후 모든 변경사항을 백업합니다. Incremental Backup: 마지막 Backup 이후의 모든 변경사항을 백업합니다. 볼륨 생성이후 첫 백업이 Incremental Backup인 경우 Full Backup이 발생됩니다.     백업이 완료되면 AVAILBLE 상태로 표시됩니다.\n   매뉴얼 증분 백업하기   Block Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Block Volume Backups을 클릭합니다.\n  Create Block Volume Backup 을 클릭합니다.\n  백업 생성화면에서 이름을 입력하고 BACKUP TYPE으로 증분 백업을 선택합니다. 그리고 하단의 Create Block Volume Backup을 클릭합니다.\n   백업이 완료되면 AVAILBLE 상태로 표시됩니다.\n   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/1/","tags":["block volume","backup"],"title":"3.7.1 Block Volume 백업하기"},{"categories":null,"contents":"3.8.1 Boot Volume 백업하기 Boot Volume 백업 기능은 Block Volume 백업 기능과 거의 같습니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Boot Volume 상세 페이지로 이동합니다.\n 방법 #1  Compute \u0026raquo; Instances 항목으로 이동합니다. 대상 인스턴스 VM을 클릭합니다. Resources \u0026raquo; Boot volume으로 이동하여 Attached 된 Boot Volume을 클릭하여 상세 페이지로 이동합니다.   방법 #2  Storage \u0026raquo; Block Volume 항목으로 이동합니다. 왼쪽 메뉴에서 Boot Volumes로 이동하여 원하는 Boot Volume을 클릭하여 상세 페이지로 이동합니다.      Boot Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Boot Volume Backups를 클릭합니다.\n  Create Boot Volume Backup 을 클릭합니다.\n   백업 생성화면에서 이름을 입력하고 하단의 Create Boot Volume Backup을 클릭합니다.\nBACKUP TYPE으로 풀 백업과 증분 백업을 설정할 수 있습니다.\n  Full Backup: 볼륨 생성이후 모든 변경사항을 백업합니다.\n  Incremental Backup: 마지막 Backup 이후의 모든 변경사항을 백업합니다. 볼륨 생성이후 첫 백업이 Incremental Backup인 경우 Full Backup이 발생됩니다.\n     백업이 완료되면 AVAILABLE 상태로 표시됩니다.\n   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/8/1/","tags":["boot volume","backup"],"title":"3.8.1 Boot Volume 백업하기"},{"categories":null,"contents":"1.1 Regions, Availability Domains OCI(Oracle Cloud Infrastructure)는 Region과 Availability Domain 안에서 호스팅 됩니다. Region은 지리적인 영역이며 Availability Domain은 한 Region 내에 위치한 데이터 센터입니다. 단일 Region은 하나 이상의 Availability Domain으로 구성됩니다.\nAvailability Domain들은 동시에 결함이 발생하거나, 다른 Availability Domain의 결함에 영향을 받지 않도록 서로 분리되어 있습니다. Availability Domain은 파워, 쿨링 및 내부 네트워크 같은 자원들을 공유하지 않아 다른 Availability Domain에 영향이 없도록 설계되어 있습니다.\nRegion 내의 모든 Availability Domain은 상호 간 높은 가용성을 제공하기 위해 서로 간에 low latency, high bandwidth network로 연결되어 있습니다.\nRegion은 다른 Region과 완전히 독립되어 있으며, Region에서 발생할 수 있는 장애를 대비해 각 Region은 다른 나라 또는 다른 대륙의 멀리 떨어진 위치에 구성되어 있으며, 다양한 요구에 따라 다른 Regions의 서비스를 사용할 수도 있습니다.\n Region 단위로 발생할 수 있는 위험, 즉 기상 이변, 지진 등 장애 대비 법적인 문제나 다른 다양한 비즈니스적으로 충족하기 위해  Region 목록 현재 36개의 Region이 있으며 2022년 말까지 8개 Region을 추가하는 계획을 가지고 있습니다. 대한민국에서는 서울, 춘천 두 개의 Region이 있습니다. Region 전체 리스트 및 제공 서비스 등 자세한 사항은 아래 링크를 참고하세요.\n https://www.oracle.com/cloud/data-regions/#apac https://www.oracle.com/cloud/architecture-and-regions/  2022년 1월 기준\n https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm  Oracle is adding multiple cloud regions around the world to provide local access to cloud resources for our customers. To accomplish this quickly, we’ve chosen to launch regions in new geographies with one availability domain.\n=\u0026gt; 최근에는 고객 요구에 맞춘 빠른 확장을 위해 단일 가용 도메인을 가진 Region을 만들고 있으며, 단일 국가에 Region 2개 또는 바로 지리적으로 인접한 국가에 Region을 통해 가용성을 제공하고 있습니다. 물론 단일 가용 도메인 내에서는 Fault Domain을 통해 물리적인 가용성을 지원하고 있습니다. 이러한 정책으로 초기 설립된 Region 4개외에는 모두 가동 도메인 하나이며 서울, 춘천 Region 또한 가용 도메인 하나입니다.\n   Region Name Region Identifier Region Location Region Key Realm Key Availability Domains     Australia East (Sydney) ap-sydney-1 Sydney, Australia SYD OC1 1   Australia Southeast (Melbourne) ap-melbourne-1 Melbourne, Australia MEL OC1 1   Brazil East (Sao Paulo) sa-saopaulo-1 Sao Paulo, Brazil GRU OC1 1   Brazil Southeast (Vinhedo) sa-vinhedo-1 Vinhedo, Brazil VCP OC1 1   Canada Southeast (Montreal) ca-montreal-1 Montreal, Canada YUL OC1 1   Canada Southeast (Toronto) ca-toronto-1 Toronto, Canada YYZ OC1 1   Chile (Santiago) sa-santiago-1 Santiago, Chile SCL OC1 1   France South (Marseille) eu-marseille-1 Marseille, France MRS OC1 1   Germany Central (Frankfurt) eu-frankfurt-1 Frankfurt, Germany FRA OC1 3   India South (Hyderabad) ap-hyderabad-1 Hyderabad, India HYD OC1 1   India West (Mumbai) ap-mumbai-1 Mumbai, India BOM OC1 1   Israel Central (Jerusalem) il-jerusalem-1 Jerusalem, Israel MTZ OCI 1   Italy Northwest (Milan) eu-milan-1 Milan, Italy LIN OC1 1   Japan Central (Osaka) ap-osaka-1 Osaka, Japan KIX OC1 1   Japan East (Tokyo) ap-tokyo-1 Tokyo, Japan NRT OC1 1   Netherlands Northwest (Amsterdam) eu-amsterdam-1 Amsterdam, Netherlands AMS OC1 1   Saudi Arabia West (Jeddah) me-jeddah-1 Jeddah, Saudi Arabia JED OC1 1   Singapore (Singapore) ap-singapore-1 Singapore,Singapore SIN OC1 1   South Africa Central (Johannesburg) af-johannesburg-1 Johannesburg, South Africa JNB OC1 1   South Korea Central (Seoul) ap-seoul-1 Seoul, South Korea ICN OC1 1   South Korea North (Chuncheon) ap-chuncheon-1 Chuncheon, South Korea YNY OC1 1   Sweden Central (Stockholm) eu-stockholm-1 Stockholm, Sweden ARN OC1 1   Switzerland North (Zurich) eu-zurich-1 Zurich, Switzerland ZRH OC1 1   UAE Central (Abu Dhabi) me-abudhabi-1 Abu Dhabi, UAE AUH OC1 1   UAE East (Dubai) me-dubai-1 Dubai, UAE DXB OC1 1   UK South (London) uk-london-1 London, United Kingdom LHR OC1 3   UK West (Newport) uk-cardiff-1 Newport, United Kingdom CWL OC1 1   US East (Ashburn) us-ashburn-1 Ashburn, VA IAD OC1 3   US West (Phoenix) us-phoenix-1 Phoenix, AZ PHX OC1 3   US West (San Jose) us-sanjose-1 San Jose, CA SJC OC1 1    ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/1/","tags":["region","availibilty domain","AD"],"title":"1.1 Regions, Availability Domains"},{"categories":null,"contents":"2.1 Oracle Cloud Free Tier 계정 생성 Oracle Cloud Infrastructure을 무료로 사용해 보기 위한 30일 동안 US 300$ 한도 내에서 사용할 수 있는 Trial을 제공하고 있습니다. 또한 Always Free 항목으로 제공하는 자원에 대해서 30일 이후도 항시 무료로 사용할 수 있습니다.\n  https://www.oracle.com/cloud/free/\n     30-day free trial에서 이용 가능한 서비스 목록\n https://www.oracle.com/cloud/free/#free-cloud-trial    Oracle Cloud Free Tier 계정 생성 절차   Free Tier 신청\n  https://www.oracle.com/kr/cloud/free/ 에 접속하여 무료로 시작하기를 클릭하여 신청합니다.\n     계정 정보 입력\n   기본 정보를 입력하고 내 전자메일 확인을 클릭하면 입력한 이메일로 링크가 전달됩니다.\n   메일 수신함을 확인합니다. Click here를 클릭하면 Free Tier 등록 링크로 이동하게 됩니다.\n   등록 링크로 이동하여 추가 계정 정보를 입력합니다.\n  비밀번호를 입력\n   클라우드 계정 이름(Cloud Account Name)\n  홈 영역\n Free Tier는 Region 하나로 Limit가 걸려있습니다. 추가 수정이 불가하니 잘 선택합니다.     이용 약관 동의\n   주소 정보\n  휴대폰 전화번호\n   지급 검증\n      무료 체험 시작\n     Free Tier 생성중\n   Free Tier Account 생성중\n   Free Tier Account 준비 완료 메일 수신\n   ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/1/","tags":["trial","cloud account"],"title":"2.1 Oracle Cloud Free Tier 계정 생성"},{"categories":null,"contents":"6.1 Kubernetes 클러스터에 Verrazzano 설치하기 쿠버네티스 클러스터 준비 Verrazzano는 Kubernetes Operator 방식을 사용하여 쿠버네티스 클러스터에 설치하는 방식입니다. 즉 설치할 쿠버네티스 클러스터가 필요합니다. 여기서는 OKE 클러스터 기준으로 진행합니다.\nContainer Engine for Kubernetes (OKE) 클러스터 준비 Quick Create 모드로 OKE 클러스터를 기본 설정으로 생성합니다.\n Verrazzano 상의 모든 컴포넌트와 예제 애플리케이션을 배포하기 위해 충분한 용량의 환경을 준비합니다.  Shape: VM.Standard2.4 이상 또는 VM.Standard.E2.4 이상 Number of nodes: 3개 이상 OKE 버전: 1.19  1.20, 1.21에서는 FluentD 파서 이슈로 Kibana 동작을 위한 추가 작업이 필요합니다.      Verrazzano 설치  Cloud Shell 또는 작업환경에서 kubectl로 생성한 OKE 클러스터에 접속합니다.  Verrazzano platform operator 설치   Verrazzano platform operator 설치\nkubectl apply -f https://github.com/verrazzano/verrazzano/releases/download/v1.1.0/operator.yaml   설치 완료될때 까지 기다립니다.\nkubectl -n verrazzano-install rollout status deployment/verrazzano-platform-operator   successfully rolled out가 뜨고 설치가 잘 됐는지 확인합니다.\nkubectl -n verrazzano-install get pods   Verrazzano 설치 - OCI DNS \u0026amp; Let\u0026rsquo;s Encrypt 사용 설치시 dev, prod, managed-cluster 프로파일 중에 고를 수 있습니다. 여기서는 dev 프로파일을 사용합니다. 자세한 사항은 설치 프로파일 페이지를 참고하세요. 기본 프로파일로 설치하면 Self-Signed 인증서와 무료 와일드 카드 도메인(nip.io, sslip.io)을 사용하게 됩니다. 원하는 도메인과 인증서를 사용하기 위해서 OCI DNS와 Let\u0026rsquo;s Encrypt를 사용하는 방법을 확인해 봅니다.\n참고\n https://verrazzano.io/latest/docs/setup/install/customizing/dns/ https://verrazzano.io/latest/docs/setup/install/customizing/certificates/  OCI DNS Zone 생성 소유하고 있는 도메인을 OCI DNS에 Zone으로 등록합니다.\nOCI API Secret 생성 대상 쿠버네티스 클러스터에 API Private 정보를 Secret 정보로 등록합니다. 여기서는 OCI CLI가 이미 설정된 경우는 가정해 Verrazzano helper script를 통해 secret을 만듭니다.\n  helper script 다운로드\n 홈이 아닌경우 실행시 Hang이 걸리는 것 같으니 홈에서 실행합니다.  cd ~ curl \\  -o ./create_oci_config_secret.sh \\  https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/platform-operator/scripts/install/create_oci_config_secret.sh   KUBECONFIG 환경변수 설정 및 스크립트 실행\nchmod +x create_oci_config_secret.sh export KUBECONFIG=~/.kube/config ./create_oci_config_secret.sh   실행 예시 및 결과 확인\n[opc@bastion-host ~]$ chmod +x create_oci_config_secret.sh [opc@bastion-host ~]$ export KUBECONFIG=~/.kube/config [opc@bastion-host ~]$ [opc@bastion-host ~]$ ./create_oci_config_secret.sh secret/oci created [opc@bastion-host ~]$ kubectl get secret oci -n verrazzano-install NAME TYPE DATA AGE oci Opaque 1 35s [opc@bastion-host ~]$ kubectl get secret oci -o jsonpath=\u0026#34;{.data[\u0026#39;oci\\.yaml\u0026#39;]}\u0026#34; -n verrazzano-install | base64 -d auth: region: ap-seoul-1 tenancy: ocid1.tenancy.oc1..aaaaaaaa~~~ user: ocid1.user.oc1..aaaaaaaa~~~ key: | -----BEGIN PRIVATE KEY----- MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCbKoBbV+xIDgeA ... K2jAF6UJZ/+BBKkHRXHSxoI= -----END PRIVATE KEY----- fingerprint: 99:c9:87:~~~   Verrazzano 설치   설치 yaml 샘플 다운로드\ncurl \\  -o ./install-oci.yaml \\  https://raw.githubusercontent.com/verrazzano/verrazzano/release-1.1/platform-operator/config/samples/install-oci.yaml   install-oci.yaml 파일 업데이트\n spec.environmentName: 설치된 환경에 대한 이름. myenv이고, zonename이 example.com인 경우 ingress 도메인이 myenv.example.com로 됨. spec.components.dns.oci.ociConfigSecret: oci, 앞서 생성한 secret name spec.components.dns.oci.dnsZoneCompartmentOCID: OCI DNS Zone으로 동록한 Zone이 있는 Compartment의 OCID spec.components.dns.oci.dnsZoneOCID: OCI DNS Zone으로 동록한 Zone의 OCID spec.components.dns.oci.dnsZoneName: OCI DNS Zone으로 동록한 Zone의 이름, 예, example.com  apiVersion:install.verrazzano.io/v1alpha1kind:Verrazzanometadata:name:my-verrazzanospec:environmentName:myenvprofile:devcomponents:certManager:certificate:acme:provider:letsEncryptemailAddress:thekoguryo@gmail.comenvironment:staging dns:oci:ociConfigSecret:ocidnsZoneCompartmentOCID:ocid1.compartment.oc1..aaaaaaaa~~~dnsZoneOCID:ocid1.dns-zone.oc1..7974~~~dnsZoneName:thekoguryo.mlingress:type:LoadBalancer  install-oci.yaml 배포\nkubectl apply -f install-oci.yaml   설치 완료 확인\nkubectl wait \\  --timeout=20m \\  --for=condition=InstallComplete verrazzano/my-verrazzano   로그 확인\nkubectl logs -n verrazzano-install \\  -f $(kubectl get pod \\  -n verrazzano-install \\  -l app=verrazzano-platform-operator \\  -o jsonpath=\u0026#34;{.items[0].metadata.name}\u0026#34;) | grep \u0026#39;\u0026#34;operation\u0026#34;:\u0026#34;install\u0026#34;\u0026#39;   설치시 추가 확인\n OCI Trial로 인한 자원 부족 문제인지도 확인합니다.  kubectl get events --sort-by=.metadata.creationTimestamp -A   Verrazzano 설치 정보 확인   Console 주소 확인\nkubectl get verrazzano my-verrazzano -o yaml   결과 예시\nstatus: ... instance: consoleUrl: https://verrazzano.myenv.thekoguryo.ml elasticUrl: https://elasticsearch.vmi.system.myenv.thekoguryo.ml grafanaUrl: https://grafana.vmi.system.myenv.thekoguryo.ml keyCloakUrl: https://keycloak.myenv.thekoguryo.ml kialiUrl: https://kiali.vmi.system.myenv.thekoguryo.ml kibanaUrl: https://kibana.vmi.system.myenv.thekoguryo.ml prometheusUrl: https://prometheus.vmi.system.myenv.thekoguryo.ml rancherUrl: https://rancher.myenv.thekoguryo.ml state: Ready version: 1.1.0   Verrazzano 유저 암호 변경 KeyCloak에서 암호 변경   Verrazzano 관리자 유저(유저명: verrazzano)는 KeyCloak을 통해 관리되며 Single Sign-On(SSO)이 설정되어 있어, Verrazzano Console, Elasticsearch, Grafana, KeyCloak, Kiali, Kibana, Prometheus의 사용자를 KeyCloak을 통해 관리합니다.\n  verrazzano 유저의 초기 난수 암호는 kubernetes에서 확인이 가능하나, KeyCloak에서 변경 할 수 있습니다.\n  KeyCloack(예, https://keycloak.myenv.thekoguryo.ml)에 접속하여 Administration Console로 이동합니다.\n  KeyCloak Admin으로 로그인합니다.\n  KeyCloak admin user: keycloakadmin\n  KeyCloak admin password: 다음 명령으로 확인\nkubectl get secret \\  --namespace keycloak keycloak-http \\  -o jsonpath={.data.password} | base64 \\  --decode; echo     왼쪽 메뉴에서 Manage \u0026gt; Users 로 이동합니다.\n  View all users를 클릭하여 verrazzano 유저를 찾아, ID를 클릭합니다.\n  암호 변경을 위해 Credentials 탭을 클릭합니다.\n  Temporary는 OFF로 하고, 새 암호를 입력하고, Reset Password를 클릭합니다.\n   팝업이 뜨면 다시한번 Reset Password를 클릭합니다.\n  Kubernetes Secret에 반영   새 암호를 base64로 인코딩합니다.\n예)\necho -n \u0026#39;MyNewPwd\u0026#39; | base64   verrazzano가 설치된 kubernetes의 secret을 변경합니다.\nkubectl edit secret verrazzano -n verrazzano-system   콘솔 로그인   Verrazzano Console에 로그인합니다.\n예, https://verrazzano.myenv.thekoguryo.ml\n  System Telemetry 영역에 있는 툴은 SSO 구성이 되어 해당 링크로 verrazzano 유저로 접속할 수 있습니다.\n Kibana: 로그 모니터링 Grafana: 메트릭 모니터링 Promethues: 메트릭 수집 Elasticsearch: 로그 수집 Kiali: istio 서비스 메쉬 모니터링     KeyCloak: 유저 관리\n Admin 유저(KeyCloak Admin) 접속은 앞서와 동일합니다. 해당 링크를 클릭하면 KeyCloak 일반유저인 verrazzano 유저로 로그인 가능합니다.    Rancher: Kubernetes 모니터링\n  화면에 보이는 URL로 접속합니다.\n  User: admin\n  Password: 다음 명령으로 확인\nkubectl get secret \\  --namespace cattle-system rancher-admin-secret \\  -o jsonpath={.data.password} | base64 \\  --decode; echo     ","lastmod":"2021-12-28T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/verrazzano/1.install-verrazzano/","tags":["oke","verrazzano"],"title":"6.1 Verrazzano 설치"},{"categories":null,"contents":"5.1 OCI Service Operator for Kubernetes(OSOK) OCI Service Operator for Kubernetes는 OCI 자원을 Kubernetes API를 통해 관리할 수 있도록 도와주는 도구입니다. Autonomous Database 서비스를 Kubernetes API, kubectl을 통해 인스턴스를 생성, 삭제 등을 할 수 있게 해준다고 이해하면 됩니다. Kubernetes에서 사용하는 오픈소스 Operator Framework을 기반으로 작성되었습니다. 관련 참고 사이트는 아래와 같습니다.\n OCI Service Operator for Kubernetes GitHub OCI Service Operator for Kubernetes GitHub Documentation [OCI Docs Documentation] Adding OCI Service Operator for Kubernetes to Clusters  현재 v1.0.0 기준 지원하고 있는 OCI 서비스는 다음과 같습니다.\n Autonomous Database 서비스 MySQL Database 서비스 Streaming 서비스  OCI Service Operator for Kubernetes를 OKE Cluster에 설치 제품 설치문서를 따라 설치한 내용으로 자세한 사항은 아래 문서를 참고합니다.\n oci-service-operator/installation.md at main · oracle/oci-service-operator (github.com)  Operator SDK 설치 공식 설치 문서에 따라 설치합니다.\n Installation the Operator SDK CLI  Cloud Shell 기준 설치 명령 예시\n  아래 명령어로 설치하여 operator-sdk cli가 정상동작하는 지 확인합니다.\necho Download the release binary export ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac) export OS=$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;) export OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.15.0 curl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH} echo Verify the downloaded binary gpg --keyserver keyserver.ubuntu.com --recv-keys 052996E2A20B5C7E curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt.asc gpg -u \u0026#34;Operator SDK (release) \u0026lt;cncf-operator-sdk@cncf.io\u0026gt;\u0026#34; --verify checksums.txt.asc grep operator-sdk_${OS}_${ARCH} checksums.txt | sha256sum -c echo Install the release binary in your PATH chmod +x operator-sdk_${OS}_${ARCH} \u0026amp;\u0026amp; mv operator-sdk_${OS}_${ARCH} ~/.local/bin/operator-sdk operator-sdk version   Operator Lifecycle Manager (OLM) 설치 아래 명령으로 현재 OKE 클러스터에 OLM 자원을 설치 및 확인합니다.\noperator-sdk olm install operator-sdk olm status OCI Service Operator for Kubernetes 설치 Instance Principal OKE Worker Node에 대한 Dynamic Group 만들기  OCI 콘솔에 로그인 하여 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동하여 OKE 클러스터가 있는 Compartment의 OCID를 복사합니다. 좌측 Dynamic Group 메뉴로 이동하여 복사한 OCID로 아래 규칙을 가진 Dynamic Group을 만듭니다.  Name: 예, oke-labs-dynamic-group Rule: instance.compartment.id = '\u0026lt;compartment-ocid\u0026gt;'    Dynamic Group을 위한 Policy 만들기   좌측 Policies 메뉴로 이동하여 만든 Dynamic Group에 아래와 같이 권한을 부여합니다.\nAllow dynamic-group \u0026lt;DYNAMICGROUP_NAME\u0026gt; to manage \u0026lt;OCI_SERVICE_1\u0026gt; in compartment \u0026lt;COMPARTMENT_NAME\u0026gt; ...   예시\n  Policy Name: oke-labs-osok-policy\n  COMPARTMENT_NAME: oke-labs\n  DYNAMICGROUP_NAME: oke-labs-dynamic-group\n  Autonomous Database Service\nAllow dynamic-group oke-labs-dynamic-group to manage autonomous-database-family in compartment oke-labs   MySQL DB System Service\nAllow dynamic-group oke-labs-dynamic-group to manage mysql-family in compartment oke-labs Allow dynamic-group oke-labs-dynamic-group to {SUBNET_READ, SUBNET_ATTACH, SUBNET_DETACH, VCN_READ, COMPARTMENT_INSPECT} in compartment oke-labs Allow dynamic-group oke-labs-dynamic-group to use tag-namespaces in compartment oke-labs     Enable User Principal   OSOK가 배포될 namespace를 만듭니다.\nkubectl create ns oci-service-operator-system kubectl label ns oci-service-operator-system control-plane=controller-manager   OCI Service Operator for Kubernetes가 OCI 서비스 및 자원을 생성, 관리할 사용자 정보에 대한 Kubernetes secret으로 만듭니다.\n  tenancy: 사용할 Tenancy OCID\n  user: 사용자의 OCID\n  privatekey: API_KEY로 등록한 Private Key 경로\n  fingerprint: API_KEY로 등록한 Public Key의 fingerprint\n  passphrase: Private Key 생성시 입력한 passphrase, 없으면 빈값\n  region: OKE 클러스터가 있는 region id, 예, ap-seoul-01\nkubectl -n oci-service-operator-system create secret generic ocicredentials \\ --from-literal=tenancy=\u0026lt;CUSTOMER_TENANCY_OCID\u0026gt; \\ --from-literal=user=\u0026lt;USER_OCID\u0026gt; \\ --from-literal=fingerprint=\u0026lt;USER_PUBLIC_API_KEY_FINGERPRINT\u0026gt; \\ --from-literal=region=\u0026lt;USER_OCI_REGION\u0026gt; \\ --from-literal=passphrase=\u0026lt;PASSPHRASE_STRING\u0026gt; \\ --from-file=privatekey=\u0026lt;PATH_OF_USER_PRIVATE_API_KEY\u0026gt;   예시\nkubectl -n oci-service-operator-system create secret generic ocicredentials \\ --from-literal=tenancy=ocid1.tenancy.oc1..aaaaaaaam~~~~~~~~~~~~~~~~~~ \\ --from-literal=user=ocid1.user.oc1..aaaaaaaaz~~~~~~~~~~~~~~~~~~ \\ --from-literal=fingerprint=a0:e1:fe:79:22:22:f0:b5:6b:29:72:5f:5d:~~:~~:~~ \\ --from-literal=region=ap-seoul-01 \\ --from-literal=passphrase= \\ --from-file=privatekey=/home/oke_admin/.oci/oci_api_key.pem     OCI Service Operator for Kubernetes(OSOK) 배포   OSOK Operator 설치\ndocker pull iad.ocir.io/oracle/oci-service-operator-bundle:1.0.0 operator-sdk run bundle iad.ocir.io/oracle/oci-service-operator-bundle:1.0.0   OSOK 설치후 OCI 서비스를 위한 CustomResource가 추가된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl api-resources --api-group=oci.oracle.com NAME SHORTNAMES APIVERSION NAMESPACED KIND autonomousdatabases oci.oracle.com/v1beta1 true AutonomousDatabases mysqldbsystems oci.oracle.com/v1beta1 true MySqlDbSystem streams oci.oracle.com/v1beta1 true Stream   OSOK로 Autonomous Database Service 관리하기 참고 문서\n oci-service-operator/adb.md at main · oracle/oci-service-operator (github.com)  ADB(Autonomous Database) Binding OCI 콘솔에서 만든 ADB 인스턴스를 Kubernetes의 자원으로 Binding하는 경우입니다.\n  OCI 콘솔에 로그인하여 바인딩할 Autonomous Database의 인스턴스를 생성합니다.\n  Binding을 위해 필요한 명세 yaml을 확인하여 작성합니다.\n spec.id: 기 존재하는, 바인딩할 ADB의 OCID를 입력 walletName: 바인딩후에 wallet이 저장될 kubernetes secret의 이름 입력 walletPassword.secret.secretName: wallet에 사용할 암호가 저장된 secret 이름, 바인딩 전에 미리 secret을 생성합니다.  apiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:\u0026lt;CR_OBJECT_NAME\u0026gt;spec:id:\u0026lt;AUTONOMOUS_DATABASE_OCID\u0026gt;wallet:walletName:\u0026lt;WALLET_SECRET_NAME\u0026gt;walletPassword:secret:secretName:\u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt;  실행 예시\n  walletPassword 생성\nkubectl create secret generic ociadb-wallet-password-secret --from-literal=walletPassword=\u0026#39;xxxxxxxxxxxx\u0026#39;   실행\ncat \u0026lt;\u0026lt;EOF \u0026gt; autonomousdatabases-bind.yamlapiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbspec:id:ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaahewyo25rujjz36bwsihfedmk3sxejchlq64krjylnhgawallet:walletName:ociadb-wallet-secretwalletPassword:secret:secretName:ociadb-wallet-password-secretEOFkubectl apply -f autonomousdatabases-bind.yaml    결과 확인\nkubectl describe 명령을 통해 에러없이 바인딩이 성공했는지 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get autonomousdatabases NAME DBWORKLOAD STATUS AGE ociadb Active 7m20s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe autonomousdatabases ociadb Name: ociadb ... Kind: AutonomousDatabases ... Status: Status: Conditions: Last Transition Time: 2021-12-22T09:29:25Z Message: AutonomousDatabase Bound success ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Success 7m54s AutonomousDatabases Finalizer is added to the object Normal Success 7m52s (x2 over 7m53s) AutonomousDatabases Create or Update of resource succeeded   wallet 확인\n바인딩 결과 ociadb-wallet-secret secret이 생성되며, 내용을 보면 wallet 상에 있는 파일들이 Base64로 인코딩된 형태로 있는 것을 확인할 수 있습니다. 애플리케이션 컨테이너에서 secret을 마운트하여 ADB 연결시 사용하면 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE ... ociadb-wallet-password-secret Opaque 1 9m40s ociadb-wallet-secret Opaque 8 9m32s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret ociadb-wallet-secret -o yaml apiVersion: v1 data: README: V2FsbGV0IE... cwallet.sso: ofhONgAAAA... ewallet.p12: MIIZ/AIBAz... keystore.jks: /u3+7QAAA... ojdbc.properties: IyBDb2... sqlnet.ora: V0FMTE... tnsnames.ora: b2NpYWR... truststore.jks: /u3+7QAAAA... kind: Secret metadata: ... name: ociadb-wallet-secret ... type: Opaque   ADB(Autonomous Database) Provisioning   Provisionig을 위해 필요한 명세 yaml을 확인하여 작성합니다.\n spec.compartmentId: 생성될 ADB가 위치할 Compartment의 OCID를 입력 walletName: 바인딩후에 wallet이 저장될 kubernetes secret의 이름 입력 walletPassword.secret.secretName: wallet에 사용할 암호가 저장된 secret 이름, 바인딩 전에 미리 secret을 생성합니다. 나머지 항목은 OCI 콘솔에서 ADB 생성시 입력하는 것과 동일하게 원하는 값 입력 - 항목 명세 참조  apiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:\u0026lt;CR_OBJECT_NAME\u0026gt;spec:compartmentId:\u0026lt;COMPARTMENT_OCID\u0026gt;displayName:\u0026lt;DISPLAY_NAME\u0026gt;dbName:\u0026lt;DB_NAME\u0026gt;dbWorkload:\u0026lt;OLTP/DW\u0026gt;isDedicated:\u0026lt;false/true\u0026gt;dbVersion:\u0026lt;ORABLE_DB_VERSION\u0026gt;dataStorageSizeInTBs:\u0026lt;SIZE_IN_TBs\u0026gt;cpuCoreCount:\u0026lt;COUNT\u0026gt;adminPassword:secret:secretName:\u0026lt;ADMIN_PASSWORD_SECRET_NAME\u0026gt;isAutoScalingEnabled:\u0026lt;true/false\u0026gt;isFreeTier:\u0026lt;false/true\u0026gt;licenseModel:\u0026lt;BRING_YOUR_OWN_LICENSE/LICENSE_INCLUDEE\u0026gt;wallet:walletName:\u0026lt;WALLET_SECRET_NAME\u0026gt;walletPassword:secret:secretName:\u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt;freeformTags:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;definedTags:\u0026lt;TAGNAMESPACE1\u0026gt;:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;  실행 예시\n  adminPassword, walletPassword 생성\nkubectl create secret generic ociadb-by-osok-admin-password-secret --from-literal=password=\u0026#39;xxxxxxxxxxxx\u0026#39; kubectl create secret generic ociadb-by-osok-wallet-password-secret --from-literal=walletPassword=\u0026#39;xxxxxxxxxxxx\u0026#39;   실행\ncat \u0026lt;\u0026lt;EOF \u0026gt; autonomousdatabases-provision.yamlapiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbbyosokspec:compartmentId:ocid1.compartment.oc1..aaaaaaaaa2jcbfqjyz24y4hbbqurdxjegmsp6eqhzq4r2gni5bocoh2axb4adisplayName:OCIADBbyOSOKdbName:ociadbbyosokdbWorkload:OLTPisDedicated:falsedbVersion:19cdataStorageSizeInTBs:1cpuCoreCount:1adminPassword:secret:secretName:ociadb-by-osok-admin-password-secretisAutoScalingEnabled:falseisFreeTier:falselicenseModel:LICENSE_INCLUDEDwallet:walletName:ociadb-by-osok-wallet-secretwalletPassword:secret:secretName:ociadb-by-osok-wallet-password-secretEOFkubectl apply -f autonomousdatabases-provision.yaml    결과 확인\nkubectl describe 명령을 통해 에러없이 바인딩이 성공했는지 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get autonomousdatabases NAME DBWORKLOAD STATUS AGE ociadbbyosok OLTP Active 5m30s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe autonomousdatabases ociadbbyosok Name: ociadbbyosok ... Kind: AutonomousDatabases ... Status: Status: Conditions: Last Transition Time: 2021-12-23T02:54:54Z Message: AutonomousDatabase Provisioning Status: True ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Success 6m14s AutonomousDatabases Finalizer is added to the object Normal Success 5m5s (x2 over 5m6s) AutonomousDatabases Create or Update of resource succeeded   wallet 확인\n바인딩 결과 ociadb-wallet-secret secret이 생성되며, 내용을 보면 wallet 상에 있는 파일들이 Base64로 인코딩된 형태로 있는 것을 확인할 수 있습니다. 애플리케이션 컨테이너에서 secret을 마운트하여 ADB 연결시 사용하면 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE ... ociadb-by-osok-admin-password-secret Opaque 1 23m ociadb-by-osok-wallet-password-secret Opaque 1 23m ociadb-by-osok-wallet-secret Opaque 8 20m oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret ociadb-by-osok-wallet-secret -o yaml apiVersion: v1 data: README: V2FsbGV0IE... cwallet.sso: ofhONgAAAA... ewallet.p12: MIIZ/AIBAz... keystore.jks: /u3+7QAAA... ojdbc.properties: IyBDb2... sqlnet.ora: V0FMTE... tnsnames.ora: b2NpYWR... truststore.jks: /u3+7QAAAA... kind: Secret metadata: ... name: ociadb-by-osok-wallet-secret ... type: Opaque   ADB(Autonomous Database) Update OCI API에서 제공하는 Autonomous Database에 대한 Update 지원 항목내에서 OSOK GitHub 문서의 예시를 참고합니다.\n  Oracle Cloud Infrastructure API Reference and Endpoints / UpdateAutonomousDatabaseDetails Reference\n  GitHub 문서 기준\napiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:\u0026lt;CR_OBJECT_NAME\u0026gt;spec:id:\u0026lt;AUTONOMOUS_DATABASE_OCID\u0026gt;displayName:\u0026lt;DISPLAY_NAME\u0026gt;dbName:\u0026lt;DB_NAME\u0026gt;dbWorkload:\u0026lt;OLTP/DW\u0026gt;isDedicated:\u0026lt;false/true\u0026gt;dbVersion:\u0026lt;ORABLE_DB_VERSION\u0026gt;dataStorageSizeInTBs:\u0026lt;SIZE_IN_TBs\u0026gt;cpuCoreCount:\u0026lt;COUNT\u0026gt;adminPassword:secret:secretName:\u0026lt;ADMIN_PASSWORD_SECRET_NAME\u0026gt;isAutoScalingEnabled:\u0026lt;true/false\u0026gt;isFreeTier:\u0026lt;false/true\u0026gt;licenseModel:\u0026lt;BRING_YOUR_OWN_LICENSE/LICENSE_INCLUDEE\u0026gt;wallet:walletName:\u0026lt;WALLET_SECRET_NAME\u0026gt;walletPassword:secret:secretName:\u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt;freeformTags:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;definedTags:\u0026lt;TAGNAMESPACE1\u0026gt;:\u0026lt;KEY1\u0026gt;:\u0026lt;VALUE1\u0026gt;  Binding 한 경우 기존 YAML 또는 배포된 YAML에 업데이트 항목을 추가 하여 반영합니다.\n  스토리지 증가 예시\n앞선 autonomousdatabases-bind.yaml 파일에 dataStorageSizeInTBs 항목을 추가하여 배포합니다.\napiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbspec:id:ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaahewyo25rujjz36bwsihfedmk3sxejchlq64krjylnhgawallet:walletName:ociadb-wallet-secretwalletPassword:secret:secretName:ociadb-wallet-password-secretdataStorageSizeInTBs:2  Provisioning 한 경우   스토리지 증가 예시\n앞선 autonomousdatabases-provision.yaml 파일에 생성된 ADB의 OCID를 spec.id에 추가합니다. 그리고 dataStorageSizeInTBs 값을 변경합니다.\napiVersion:oci.oracle.com/v1beta1kind:AutonomousDatabasesmetadata:name:ociadbbyosokspec:id:ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaaz6ydcc2epj236mgguz4kuc3udjontb5hetrwr4545x5acompartmentId:ocid1.compartment.oc1..aaaaaaaaa2jcbfqjyz24y4hbbqurdxjegmsp6eqhzq4r2gni5bocoh2axb4adisplayName:OCIADBbyOSOKdbName:ociadbbyosokdbWorkload:OLTPisDedicated:falsedbVersion:19cdataStorageSizeInTBs:2cpuCoreCount:1adminPassword:secret:secretName:ociadb-by-osok-admin-password-secretisAutoScalingEnabled:falseisFreeTier:falselicenseModel:LICENSE_INCLUDEDwallet:walletName:ociadb-by-osok-wallet-secretwalletPassword:secret:secretName:ociadb-by-osok-wallet-password-secret  업데이트 실행결과\n oke_admin@cloudshell:autonomousdatabases (ap-seoul-1)$ kubectl describe autonomousdatabases ociadbbyosok Name: ociadbbyosok ... Status: Status: Conditions: Last Transition Time: 2021-12-23T04:23:49Z Message: AutonomousDatabase Provisioning Status: True Type: Provisioning Last Transition Time: 2021-12-23T04:24:54Z Message: AutonomousDatabase OCIADBbyOSOK is Active Status: True Type: Active Last Transition Time: 2021-12-23T05:25:53Z Message: AutonomousDatabase Update success Status: True Type: Active Ocid: ocid1.autonomousdatabase.oc1.ap-seoul-1.anuwgljrwtbe3zaaz6ydcc2epj236mgguz4kuc3udjontb5hetrwr4545x5a ...   ADB(Autonomous Database) Delete 현재 버전 기준으로 Delete 기능을 따로 제공하지 않아, OKE 클러스터에서 autonomousdatabases 자원을 kubectl delete 명령으로 삭제해도 실제 ADB 인스턴스가 OCI에서 삭제되지는 않습니다.\n","lastmod":"2021-12-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oracle-oss/1.oci-service-operator/","tags":["oke","opensource","osok"],"title":"5.1 OCI Service Operator for Kubernetes(OSOK)"},{"categories":null,"contents":"4.4.1 Service Mesh 없는 마이크로서비스 앱 배포 본 내용은 아래 Istio 문서 상에 있는 내용을 재 확인하는 내용으로 마이크로 서비스 앱을 사용하는 데 있어서 어떤 문제가 발생할 수 있는지, 왜 Istio 같은 Service Mesh 필요한지에 대해 알아보는 내용입니다.\n https://istio.io/latest/docs/examples/microservices-istio/bookinfo-kubernetes/ https://istio.io/latest/docs/examples/microservices-istio/add-new-microservice-version/  테스트 마이크로 서비스 앱(Bookinfo) 배포 테스트 앱\n  Product, Review, Details, Ratings의 4개 마이크로 서비스 앱으로 구성되어 있습니다.\n  먼저 여기서는 Reviews 서비스는 v1만 배포합니다.\n  아직 istio 설정이 안된 상태에서 배포 및 결과를 확인합니다.\n     앱 배포\nkubectl apply -l version!=v2,version!=v3 -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml   스케일\nkubectl scale deployments --all --replicas 3   외부 접근을 위한 서비스 오픈\n테스트 편의를 위해 Load Balancer 타입으로 오픈합니다.\nkubectl patch svc productpage -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39;   배포 상태 확인\n각 앱이 3개의 Pod이며 각 Pod는 Container 1개로 구성된 것을 확인할 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-29z48 1/1 Running 0 21s details-v1-79f774bdb9-d5lfq 1/1 Running 0 32s details-v1-79f774bdb9-sqp9p 1/1 Running 0 20s productpage-v1-6b746f74dc-drgbw 1/1 Running 0 21s productpage-v1-6b746f74dc-pc2h2 1/1 Running 0 32s productpage-v1-6b746f74dc-qzkqm 1/1 Running 0 20s ratings-v1-b6994bb9-6xlq7 1/1 Running 0 32s ratings-v1-b6994bb9-j2l78 1/1 Running 0 20s ratings-v1-b6994bb9-w748t 1/1 Running 0 20s reviews-v1-545db77b95-hg7j8 1/1 Running 0 32s reviews-v1-545db77b95-l42v4 1/1 Running 0 20s reviews-v1-545db77b95-lgl89 1/1 Running 0 20s   서비스 확인\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.96.175.211 \u0026lt;none\u0026gt; 9080/TCP 67s kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16d productpage LoadBalancer 10.96.79.55 146.56.142.95 9080:32649/TCP 66s ratings ClusterIP 10.96.105.212 \u0026lt;none\u0026gt; 9080/TCP 66s reviews ClusterIP 10.96.197.86 \u0026lt;none\u0026gt; 9080/TCP 66s   메인 페이지 접속\n http://{productpage external ip}:9080/productpage 각 서비스가 호출되어 정상적인 페이지가 보이는 것을 알 수 있습니다. Review v1에서는 아직 Rating 서비스와 연동되지 않아 Rating 정보는 보이지 않습니다.     Product page 호출 코드 확인\n 소스 파일: https://github.com/istio/istio/blob/release-1.12/samples/bookinfo/src/productpage/productpage.py reviews 호출 주소를 보면 http://{reviewsHostname}.{servicesDomain}:9080/reviews 인걸 알 수 있습니다.     실제 호출 로그\n실제 로그를 보면 http://reviews:9080/reviews 로 정상 호출 되었으며, 배포시의 ClusterIP 타입의 reviews Service를 통해 같은 네임스페이스 상에서는 reviews를 주소로하여 정상 호출된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs -l app=productpage -f ... DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): reviews:9080 send: b\u0026#39;GET /reviews/0 HTTP/1.1\\r\\nHost: reviews:9080\\r\\nuser-agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nConnection: keep-alive\\r\\nX-B3-TraceId: cd0101ed97c6b43f\\r\\nX-B3-SpanId: cd0101ed97c6b43f\\r\\nX-B3-Sampled: 1\\r\\n\\r\\n\u0026#39; reply: \u0026#39;HTTP/1.1 200 OK\\r\\n\u0026#39; header: X-Powered-By: Servlet/3.1 header: Content-Type: application/json header: Date: Mon, 20 Dec 2021 07:41:55 GMT header: Content-Language: en-US header: Content-Length: 295 DEBUG:urllib3.connectionpool:http://reviews:9080 \u0026#34;GET /reviews/0 HTTP/1.1\u0026#34; 200 295 INFO:werkzeug:10.244.1.128 - - [20/Dec/2021 07:41:55] \u0026#34;GET /productpage HTTP/1.1\u0026#34; 200     Reviews 새 버전 배포   Reviews Service 확인\nSelector에서 보이는 것처럼 app=reviews 레이블이 달린 Pod로 분배하고 있습니다. 현재 배포되어있는 Reviews v1의 세 개의 Pod의 Endpoints로 분배되고 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe svc reviews Name: reviews Namespace: default Labels: app=reviews service=reviews Annotations: \u0026lt;none\u0026gt; Selector: app=reviews Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.96.197.86 IPs: 10.96.197.86 Port: http 9080/TCP TargetPort: 9080/TCP Endpoints: 10.244.2.42:9080,10.244.2.44:9080,10.244.2.45:9080 Session Affinity: None Events: \u0026lt;none\u0026gt;   이 상태에서 Reviews 앱의 새로운 버전인 v2를 개발하여 배포하게 된다면, 아마 배포해서 정상동작 확인후 서비스 라우팅이 되도록 app=review를 추가하게 될 것입니다. 이후 일부 요청을 서비스 하도록 실제 운영하다 문제 발생시 원복시키거나, 문제가 없는 경우, v1로 가는 요청을 줄이고, v2로 가는 요청을 점진적으로 늘려가면서 실제 v2로 이관하게 될 것입니다.\n  Reviews v2 배포   Reviews v2 버전을 배포합니다.\n라우팅에서 일단 제외하기 위해 app: reviews_test로 레이블을 변경하여 배포합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | sed \u0026#39;s/app: reviews/app: reviews_test/\u0026#39; | kubectl apply -l app=reviews_test,version=v2 -f -   배포 결과 확인\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ... reviews-v1-545db77b95-hg7j8 1/1 Running 0 76m 10.244.2.42 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-545db77b95-l42v4 1/1 Running 0 76m 10.244.2.45 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-545db77b95-lgl89 1/1 Running 0 76m 10.244.2.44 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v2-7cd8b85558-c2jmw 1/1 Running 0 30s 10.244.2.47 10.0.10.9 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   Reviews v2 Pod 테스트\nproductpage Pod에서 Reviews v2 Pod로 테스트하면 좋겠지만, productpage Pod에 테스트용 curl 없는 관계로 별도 클라이언트용 Pod를 배포하여 테스트합니다.\n  테스트 클라이언트 Pod 배포\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/sleep/sleep.yaml   테스트\nREVIEWS_V2_POD_IP=$(kubectl get pod -l app=reviews_test,version=v2 -o jsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;) echo $REVIEWS_V2_POD_IP kubectl exec $(kubectl get pod -l app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -- curl -sS \u0026#34;$REVIEWS_V2_POD_IP:9080/reviews/7\u0026#34;     테스트 결과\n아래와 같이 v2 버전이 잘 동작함을 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ REVIEWS_V2_POD_IP=$(kubectl get pod -l app=reviews_test,version=v2 -o jsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;) oke_admin@cloudshell:~ (ap-seoul-1)$ echo $REVIEWS_V2_POD_IP 10.244.2.47 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -- curl -sS \u0026#34;$REVIEWS_V2_POD_IP:9080/reviews/7\u0026#34; {\u0026#34;id\u0026#34;: \u0026#34;7\u0026#34;,\u0026#34;reviews\u0026#34;: [{ \u0026#34;reviewer\u0026#34;: \u0026#34;Reviewer1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;An extremely entertaining play by Shakespeare. The slapstick humour is refreshing!\u0026#34;, \u0026#34;rating\u0026#34;: {\u0026#34;stars\u0026#34;: 5, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}},{ \u0026#34;reviewer\u0026#34;: \u0026#34;Reviewer2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Absolutely fun and entertaining. The play lacks thematic depth when compared to other plays by Shakespeare.\u0026#34;, \u0026#34;rating\u0026#34;: {\u0026#34;stars\u0026#34;: 4, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}}]}   Reviews v2의 서비스 라우팅에 추가   version=v2 레이블이 달린 Review v2 Pod에 app=reviews 레이블을 추가합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl label pod -l version=v2 app=reviews --overwrite pod/reviews-v2-7cd8b85558-c2jmw labeled   Reviews Service 재확인\n아래와 같이 v2가 추가가 되어 Endpoints에 이전 3개에서 4개로 변경된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe svc reviews Name: reviews ... Selector: app=reviews ... Endpoints: 10.244.2.42:9080,10.244.2.44:9080,10.244.2.45:9080 + 1 more...   메인 페이지 접속\n  http://{productpage external ip}:9080/productpage\n  페이지를 새로고침하면 4번 중 1번은 Review v2 Pod로 라우팅되어 아래 그림과 같이 별점 표시가 되는 것을 볼 수 있습니다.\n     Reviews v2 문제 확인후 재배포   서비스 중에 문제가 발생한 경우 다음과 같이 삭제합니다.\n 위에서 서비스 라우팅을 위해 기존 Pod의 label을 변경했기 때문에, 변경된 Pod(app=reviews)외에 app=reviews_test의 Deployment도 아직 남아 있습니다. 둘다 삭제합니다. (실제로는 이렇게 하면 안되겠네요. 근데, Deployment 내에 정의된 Pod Template의 label은 변경이 안되네요\u0026hellip;)  kubectl delete deployment reviews-v2 kubectl delete pod -l app=reviews,version=v2   문제 해결후 재배포\n최종 배포시에는 아래와 같이 배포후 v2의 갯수를 늘리고 v1은 삭제하게 될 것입니다. 실제 상황에서는 점진적으로 v2을 늘리고 v1을 줄이는 요구사항도 있을 것입니다.\nkubectl apply -l app=reviews,version=v2 -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml kubectl scale deployment reviews-v2 --replicas=3 kubectl delete deployment reviews-v1   실제 배포시 대응 방법 실제 환경에서 마이크로 서비스 업데이트시의 안정적인 배포를 위해 새 버전 배포후 테스트, 점진적인 새 버전으로의 이관을 위한 여러가지 방안들이 나왔습니다. 그 대표적인 방법으로 가장 많이 사용하고 있는 것은 크게 Service Mesh와 Netflix\u0026rsquo;s Hystrix를 기반한 방법입니다.\n Service Mesh: Istio로 대표되는 서비스 메쉬를 사용하는 방법은 sidecar로 모듈을 Service Mesh 단에서 추가하는 방법입니다. Pod에 Application Container외에 Sidecar Container가 추가되고, Sidecar를 이용해 서비스 라우팅 및 추가 기능을 제공하는 방식입니다. 애플리케이션 코드에 별도 작업이 필요하지 않는 장점이 있습니다. 애플리케이션 코드 구현: Netflix\u0026rsquo;s Hystrix 예와 같이 서비스 라우팅, 보안, 모니터링 등에 필요한 기능을 라이브러리 모듈에서 제공하는 기능을 사용하는 방식입니다. 라이브러리를 사용하기 때문 코딩 자체량은 많지 않고, Annotation 등을 사용하게 됩니다. Spring 프레임워크에서는 Spring Cloud로 발전하여 기능을 제공하고 있습니다.  ","lastmod":"2021-12-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/service-mesh/1.sampleapp-without-istio/","tags":["oss","service mesh","istio"],"title":"4.4.1 Service Mesh 없는 마이크로서비스 앱 배포"},{"categories":null,"contents":"4.3.1 Prometheus\u0026amp;Grafana 설치하기 Prometheus 설치   설치용 namespace를 만듭니다.\nkubectl create ns monitoring   Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/kube-prometheus#parameters 아래 예시는 prometheus와 alertmanager 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamlprometheus:ingress:enabled:truehostname:prometheus.ingress.thekoguryo.mlpath:/annotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:truealertmanager:ingress:enabled:truehostname:alertmanager.ingress.thekoguryo.mlpath:/annotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  prometheus helm chart 설치\nhelm install prometheus -n monitoring -f values.yaml bitnami/kube-prometheus   설치결과\nPrometheus의 내부 DNS 정보는 이후 Grafana에서 연동할 때 사용됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ helm install prometheus -n monitoring -f values.yaml bitnami/kube-prometheus NAME: prometheus ... Prometheus can be accessed via port \u0026#34;9090\u0026#34; on the following DNS name from within your cluster: prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local ...   Grafana 설치   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/grafana#parameters grafana 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamladmin:password:\u0026#34;higrafana\u0026#34;ingress:enabled:truehostname:grafana.ingress.thekoguryo.mlpath:/annotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  grafana helm chart 설치\nhelm install grafana -n monitoring -f values.yaml bitnami/grafana   설치결과\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ helm install grafana -n monitoring -f values.yaml bitnami/grafana NAME: grafana ... 2. Get the admin credentials: echo \u0026#34;User: admin\u0026#34; echo \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34; oke_admin@cloudshell:grafana (ap-seoul-1)$ echo \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34;   Grafana 설정   Grafana 웹 UI를 접속합니다.\n  URL: values.yaml에서 설정한 ingress.hostname\n  관리자 계정\n  User: admin\n  Password: values.yaml에 설정한 admin.password 또는 미입력시 다음 명령으로 확인\necho \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34;        Prometheus 등록을 위해 왼쪽 메뉴에서 Configuration \u0026gt; Data sources 선택\n   Add data source 클릭\n  Prometheus 선택\n   Prometheus 연결정보 입력\n앞서 Prometheus 설치 로그에 확인한 Prometheus 내부 연결 정보를 입력합니다.\nhttp://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090    아래 쪽 Save \u0026amp; test 클릭\n  Grafana Dashboard 추가   Kubernetes 모니터링을 위해 공개된 Grafana Dashboard를 사용할 수 있습니다.\n https://grafana.com/grafana/dashboards/?search=kubernetes\u0026amp;dataSource=prometheus     위 사이트에서 검색된 대쉬보드 중 원하는 것을 선택합니다.\n  대쉬보드 상세 페이지에 ID를 확인합니다.\n   Dashboard 등록을 위해 왼쪽 메뉴에서 Dashboards \u0026gt; Manage 선택\n   Import 클릭\n  임포트할 대쉬보드 ID(예, 7249) 입력후 Load 클릭\n  data source를 앞서 등록한 Prometheus 선택후 Import 클릭\n   임포트된 대쉬보드를 통해 OKE 클러스터 모니터링\n   위와 같이 공개된 대쉬보드를 사용하거나, 원하는 대쉬보드를 작성하여 Prometheus에서 수집된 메트릭 정보를 이용하여, Kubernetes 클러스터를 모니터링 할 수 있습니다.\n  ","lastmod":"2021-12-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/monitoring/1.install-prometheus-grafana/","tags":["oss","monitoring","prometheus","grafana"],"title":"4.3.1 Prometheus\u0026Grafana 설치하기"},{"categories":null,"contents":"4.2.1 EFK 설치하기 Elastic Search + Kibana 설치   설치용 namespace를 만듭니다.\nkubectl create ns logging   Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/elasticsearch/#parameters 아래 예시는 elasticsearch 내장 kibana를 함께 설치하고, kibana 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamlglobal:kibanaEnabled:truekibana:ingress:enabled:truehostname:kibana.ingress.thekoguryo.mlannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  elasticsearch helm chart 설치\nhelm install elasticsearch -f values.yaml bitnami/elasticsearch -n logging   설치\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ helm install elasticsearch -f values.yaml bitnami/elasticsearch -n logging NAME: elasticsearch ... Elasticsearch can be accessed within the cluster on port 9200 at elasticsearch-coordinating-only.logging.svc.cluster.local To access from outside the cluster execute the following commands: kubectl port-forward --namespace logging svc/elasticsearch-coordinating-only 9200:9200 \u0026amp; curl http://127.0.0.1:9200/   설치된 elastic search 내부 주소와 포트를 확인합니다. 이후 Fluentd에서 로그 전송을 위해 사용할 주소입니다.\n 주소: elasticsearch-coordinating-only.logging.svc.cluster.local 포트: 9200    Fluentd 구성 참고 문서\n Kubernetes - Fluentd https://gist.github.com/agapoff/77d746b4588ee37a9e8074904533f6bc    Fluentd 설치를 위한 Service Account를 생성하고 관련 권한을 정의합니다.\n   configmap 추가 설정정의\n Fluentd 컨테이너 이미지에는 로그 파싱과 관련된 설정들이 컨테이너 이미지내에 /fluentd/etc/ 하위에 .conf 파일로 모두 정의 되어 있습니다. 이 파일들을 재정의 할 수 있습니다. 여기에서는 다른 설정들은 그대로 두고 Parser만 변경합니다. 기본 Parser는 Docker Engine이 런타임인 경우 잘 동작하지만, 최근 OSS 쿠버네티스의 기본 런타임인 containerd와 OKE에서 사용하고 있는 cri-o에서는 파싱 에러가 발생합니다. 정상 파싱을 위해 파서 설정(tail_container_parse.conf)만 아래와 같이 cri Parser로 변경합니다. https://github.com/fluent/fluentd-kubernetes-daemonset/issues/434#issuecomment-831801690     fluentd damonset 정의\n설정한 configmap 사용을 위해 Fluentd 문서상의 YAML을 일부 변경하셨습니다.\n   FluentD 설치\nkubectl apply -f fluentd-rbac.yaml kubectl apply -f fluentd-configmap-elasticsearch.yaml kubectl apply -f fluentd-daemonset-elasticsearch.yaml   Kibana 설정   설치한 kibana을 웹 브라우저로 접속합니다. ingress로 지정한 주소로 접속합니다.\n 예, https://kibana.ingress.thekoguryo.ml    Welcome 페이지의 Add Data를 클릭하여 홈으로 이동합니다.\n  왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n   Create index pattern을 클릭합니다.\n  인덱스 패턴을 생성합니다.\n오른쪽에서 보듯이 FluentD에서 전송된 로그는 logstash-로 시작합니다.\n Name: logstash-* Timestamp field: @timestamp     인덱스 패턴이 추가된 결과를 볼 수 있습니다.\n   왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n  생성한 인덱스 패턴을 통해 수집된 로그를 확인할 수 있습니다.\n 테스트 앱의 로그를 확인하기 위해 Add filter를 클릭하여 namespace_name=default 로 지정합니다.     테스트 앱을 접속합니다.\n 예, https://blue.ingress.thekogury.ml/ekf-test    로그 확인\n아래와 같이 kibana에서 테스트 앱의 로그를 확인할 수 있습니다.\n   EFK를 통해 OKE 상의 로그를 수집하는 예시였습니다. EKF에 대한 상세 내용은 제품 관련 홈페이지와 커뮤니티 사이트를 참고하기 바랍니다.\n  ","lastmod":"2021-12-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/logging-efk/1.install-efk/","tags":["oss","logging","efk"],"title":"4.2.1 EFK 설치하기"},{"categories":null,"contents":"4.1.1 NGINX Ingress Controller 사용하기 OKE에서 Kubernetes에서 사용가능한 여러가지 오픈 소스 ingress controller를 사용할 수 있습니다. 본 문서에서는 그중에서 가장 많이 사용되며, OKE 문서에서 예제로 설명하고 있는 nginx-ingress-controller를 테스트 해보겠습니다.\n  공식 문서\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm https://kubernetes.github.io/ingress-nginx/deploy/#oracle-cloud-infrastructure    설치 버전\n OKE 문서는 v0.44.0 기준이 NGINX Ingress Controller for Kubernetes 문서는 최신 버전인 v1.1.0 기준입니다. 본 문서는 최신 버전인 v1.1.0 기준이며, 해당 버전은 Kubernetes 1.22, 1.21, 1.20, 1.19을 지원하고 있습니다. NGINX Ingress Controller의 지원 버전  https://github.com/kubernetes/ingress-nginx#support-versions-table      NGINX Ingress Controller 설치 Ingress Controller 설치   kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\n  다음 명령으로 NGINX Cloud Deployment 설치\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml   설치 확인\ningress-nginx namespace에 아래와 같이 설치된 것을 확인할 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-x74zf 0/1 Completed 0 65s pod/ingress-nginx-admission-patch-f8x5k 0/1 Completed 0 65s pod/ingress-nginx-controller-69db7f75b4-vb84p 1/1 Running 0 65s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller LoadBalancer 10.96.212.64 132.226.225.240 80:31975/TCP,443:31756/TCP 65s service/ingress-nginx-controller-admission ClusterIP 10.96.95.133 \u0026lt;none\u0026gt; 443/TCP 65s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 65s NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-69db7f75b4 1 1 1 65s NAME COMPLETIONS DURATION AGE job.batch/ingress-nginx-admission-create 1/1 2s 65s job.batch/ingress-nginx-admission-patch 1/1 2s 65s   Load Balancer IP 확인   Ingress Controller 서비스의 로드밸런서 IP인 EXTERNAL-IP를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.96.212.64 132.226.225.240 80:31975/TCP,443:31756/TCP 5m31s   PATH 기반 기본 라우팅 테스트 가장 기본적인 라우팅으로 URL PATH에 따라 라우팅 서비스를 달리하는 경우입니다.\n  테스트를 위한 샘플 앱을 배포합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(path-basic.yaml)을 작성합니다.\n /blue 요청은 nginx-blue-svc 로 라우팅 /green 요청은 nginx-green-svc로 라우팅  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-path-basicannotations:kubernetes.io/ingress.class:nginxspec:rules:- http:paths:- path:/bluepathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- http:paths:- path:/greenpathType:Prefixbackend:service:name:nginx-green-svcport:number:80  작성한 path-basic.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f path-basic.yaml ingress.networking.k8s.io/ingress-path-basic created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-basic \u0026lt;none\u0026gt; * 132.226.225.240 80 49s   앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n  /blue 요청\n   /green 요청\n   POD 정보 확인\n정의한 PATH에 따라 각각 blue, green 앱이 배포된 POD로 라우팅 된 것을 웹페이지 배경색 및 POD IP로 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-blue-6fccd8fb49-q6qph 1/1 Running 0 13m 10.244.0.138 10.0.10.139 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-green-7f646c5c7f-snpxf 1/1 Running 0 13m 10.244.0.5 10.0.10.84 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;     Rewrite Target URL PATH 라우팅 결과를 보면 /blue, /green의 Path가 최종 라우팅 되어 실행되는 앱으로 그대로 전달되는 것을 알 수 있습니다. ingress controller에서 라우팅을 위해서만 사용하고, 실제 앱의 동작을 위해는 수정이 필요한 경우에 사용합니다.\n https://kubernetes.github.io/ingress-nginx/examples/rewrite/    ingress 설정 YAML(path-rewrite-target.yaml)을 작성합니다.\n path: /blue -\u0026gt; /blue(/|$)(.*) 로 변경 annotation 추가: nginx.ingress.kubernetes.io/rewrite-target: /$2 예시  ~~/blue -\u0026gt; ~~/ 로 앱으로 전달 ~~/blue/abc -\u0026gt; ~~/abc 로 앱으로 전달    apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-path-rewrite-targetannotations:kubernetes.io/ingress.class:nginxnginx.ingress.kubernetes.io/rewrite-target:/$2spec:rules:- http:paths:- path:/blue(/|$)(.*)pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- http:paths:- path:/green(/|$)(.*)pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  앞선 path-basic.yaml를 삭제하고 path-rewrite-target.yaml를 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl delete -f path-basic.yaml ingress.networking.k8s.io \u0026#34;ingress-path-basic\u0026#34; deleted oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f path-rewrite-target.yaml ingress.networking.k8s.io/ingress-path-rewrite-target created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-rewrite-target \u0026lt;none\u0026gt; * 132.226.225.240 80 43s   앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n  ~~/blue 요청\n 라우팅된 앱에서는 /blue가 빠지고 /로만 수신됨     ~~/blue/abc 요청\n 라우팅된 앱에서는 /blue가 빠지고 /abc로만 수신됨       ","lastmod":"2021-12-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/ingress-controller/1.install-nginx-ingress-controller/","tags":["oss","ingress-controller"],"title":"4.1.1 NGINX Ingress Controller 사용하기"},{"categories":null,"contents":"3.1 컨테이너 이미지 스캔 OCIR는 알려진 Common Vulnerabilities and Exposures (CVE) database를 기반으로 컨테이너 이미지에 대한 취약점 분석 기능을 제공하고 있습니다. 다음은 취약점 분석을 위한 이미지 스캔 기능을 확인해 봅니다.\n공식 문서\n Scanning Images for Vulnerabilities  관련 Policy 설정 스캔 서비스가 OCIR repository에 대한 권한을 부여합니다. 적용해야 하는 Policy는 위 공식 문서를 참조합니다. tenancy 전체 또는 compartment 에 대해서 지정할 수 있습니다.\n  전체 테넌시\n 이름 예, ocir-scanning-images-root-policy  allow service vulnerability-scanning-service to read repos in tenancy allow service vulnerability-scanning-service to read compartments in tenancy   특정 compartment\nallow service vulnerability-scanning-service to read repos in compartment \u0026lt;compartment-name\u0026gt; allow service vulnerability-scanning-service to read compartments in compartment \u0026lt;compartment-name\u0026gt;   설정 예시\n   이미지 스캐너 설정 이미지 스캐너는 repository 단위로 기능 추가, 삭제 할 수 있습니다. 테스트를 위해 먼저 이미지를 배포합니다.\n  OCIR 이미지 사용하여 앱 배포하기를 참고하여 OCIR에 이미지를 배포합니다.\n  OCIR에 이미지 배포\ndocker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, root)를 선택합니다.\n  스캐너를 설정할 Repository(예, nginx)를 선택하고 오른쪽 Actions 메뉴에서 Add scanner를 선택합니다.\n   생성할 스캐너의 compartment 위치를 확인하고, 처음 스캐너 사용이므로 Create new scan configuration으로 선택하여, 스캐너를 추가합니다. 현재는 스캐너별로 따라 추가 설정할 수 있는 기능이 없는 것으로 보여, 한번 만들고 다음부터는 만든 것을 사용하면 됩니다.\n   스캐너 결과 확인\n스캐너가 추가된 nginx repository에 대해서 tag 별로 컨테이너 이미지 상세 정보를 볼 수 있습니다. Scan results 탭을 보면 스캔 결과를 확인할 수 있으며, View details를 통해 상세 정보를 확인할 수 있습니다.\n   취약점 확인\n이미지 스캐닝 결과 확인 알려진 CVE 정보를 기반한 취약점을 확인 할 수 있으며, 취약점를 클릭하면, 실제 CVE 데이터 베이스로 이동하여, 상세 정보를 확인 할 수 있습니다.\n    ","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/ocir/1.scan-image/","tags":["container registry"],"title":"3.1 컨테이너 이미지 스캔"},{"categories":null,"contents":"1.6.1 Block Volume 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\n기본 설치된 Persistent Volume을 위한 StorageClass 확인 OKE는 OCI IaaS를 기반으로 제공되는 서비스로 OCI Block Volume 서비스를 이용하게 Persistent Volume을 제공합니다. 현 버전 기준으로 FlexVolume 볼륨 플러그인과 CSI(Container Storage Interface) 볼륨 플러그인의 두 가지를 사용하고 있습니다.\n  기본 StorageClass 확인\n oci: OCI Block Volume 서비스를 위한 FlexVolume 플러그인 사용 oci-bv: OCI Block Volume 서비스를 위한 CSI 플러그인 사용  oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 2d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d   CSI 볼륨 플러그인\n FlexVolume 플러그인 방식은 오픈소스 Kubernetes 1.2 버전때 부터 사용되어 더 오래되었지만, 업스트림 Kubernetes에서 CSI 플러그인 방식을 사용하는 흐름입니다. OKE에서도 Release Note 2020년 8월 1일, Support for the Container Storage Interface (CSI) volume plug-in 에 나와 있는 것 처럼 OKE에서도 신규 기능은 CSI 플러그인에 추가할 예정이며, FlexVolume은 유지 보수만 할 계획입니다. 그래서 이하 설명에서는 CSI 플러그인을 사용하는 oci-bv storageclass를 사용하겠습니다. Flex 플러그인을 사용하는 oci storageclass에 대한 사항은 공식 문서를 참조바랍니다.    OCI Block Volume용 CSI 플러그인을 사용하여 Persistent Volume 만들어 사용하기 Persitent Volume 테스트\n  아래와 같이 PV 요청 yaml을 사용하여 요청합니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:csi-bvs-pvcspec:storageClassName:\u0026#34;oci-bv\u0026#34;accessModes:- ReadWriteOnceresources:requests:storage:50Gi  테스트 앱 배포\n 요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱  apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-bvs-pvcname:nginx-bvs-pvcspec:replicas:1selector:matchLabels:app:nginx-bvs-pvctemplate:metadata:labels:app:nginx-bvs-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:csi-bvs-pvc  생성 결과\n 아래와 같이 정상적으로 PV 요청에 따라 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다.  oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 101s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 118s   Block Volume OCI 서비스 콘솔에서 Storage \u0026gt; Block Volumes 화면에서 보면 아래와 같이 PV용 Block Volume 만들어 졌으며, 특정 Worker Node에 부착된 걸 알 수 있습니다.\n     테스트\n  Persistent Volume에 쓰기\n아래와 같이 컨테이너 내부로 들어가 마운트 된 PV 내에 파일쓰기를 합니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vdp7c -- bash root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# echo \u0026#34;Hello PV\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# cat /usr/share/nginx/html/hello_world.txt Hello PV root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/#   POD 삭제 후 재생성\nPersistent Volume이 유지되는 것을 확인하기 위해 기존 POD를 삭제하고 재생성되도록 합니다. 이때 RWO 모드라 다른 Node에 POD가 생성되는 경우, Multi-Attach error가 일시적으로 발생합니다. 기존 POD가 삭제되었다는 것을 인지하는 데 시간이 걸리며, 조금 지난 후에 POD가 다시 재생성됩니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 6m53s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl delete pod nginx-bvs-pvc-7b574c9c5c-vdp7c pod \u0026#34;nginx-bvs-pvc-7b574c9c5c-vdp7c\u0026#34; deleted oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 0/1 ContainerCreating 0 17s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 1/1 Running 0 75s   신규 POD에서 Persistent Volume 확인\n앞서 변경한 파일을 신규 POD에서 다시 조회해 보면 기존 내용이 남아 있는 걸 확인할 수 있습니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vrptl -- cat /usr/share/nginx/html/hello_world.txt Hello PV     참고사항\n 앞선 테스트에서 처럼 ReadWriteOnce 접근 모드에서는 단일 Kubernetes Node에 있는 POD만 붙여서 사용할 수 있습니다. 다른 Node에 있는 POD에서 동일한 PV를 사용하려고 하면, 컨테이너 기동시 Multi-Attach 오류가 발생하며, 컨테이너가 기동이 되지 않습니다. 또한 Self-Healing으로 POD 재기동시 기존 POD가 해당 volume을 사용하고 있다고 생각하여 Multi-Attach 오류가 발생하고, 사용중인 POD가 없음을 인지하는 데 약간의 시간이 걸리게 됩니다.    ReadWriteMany 지원 여부 현재 버전 기준 CSI Driver for OCI Block Volume Service는 ReadWriteOnce만 지원합니다. 그래서 단일 Kuberenetes Node에 멀티 Pod까지만 지원됩니다. 또한 위 작업 내용을 accessMode를 ReadWriteMany로 변경후 동일하게 수행하면 pod가 생성되지 않고 아래와 같이 에러가 나게 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get events LAST SEEN TYPE REASON OBJECT MESSAGE ... 80s Warning ProvisioningFailed persistentvolumeclaim/mynginxclaim failed to provision volume with StorageClass \u0026#34;oci-bv\u0026#34;: rpc error: code = InvalidArgument desc = invalid volume capabilities requested. Only SINGLE_NODE_WRITER is supported (\u0026#39;accessModes.ReadWriteOnce\u0026#39; on Kubernetes) ","lastmod":"2021-11-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/6.persistent-volume/1.block-volume/","tags":["oke"],"title":"1.6.1 Block Volume 사용하기"},{"categories":null,"contents":"1.3.1 Cloud Shell로 OKE 클러스터 연결하기 Cloud Shell Cloud Shell은 공식 문서에서 설명하는 것 처럼 Oracle Cloud 콘솔에서 제공하는 웹 브라우저 기반 리눅스 터미널입니다. 작은 가상머신으로 구동된다고 이해하시면 되며, Oracle Cloud 콘솔에 접속한 유저에 대해 사전 인증된 OCI CLI를 제공하며, 추가적인 cli 및 설정들을 기본 구성하여 제공합니다.\nOKE 클러스터 접속을 위해 Kubernetes CLI인 kubectl로 기본 설치되어 제공합니다.\n무료로 사용할 수 있고, 인증 및 기본 툴 들이 구성되어 있기 때문 바로 사용할 수 있는 이점이 있습니다.\nCloud Shell로 OKE 클러스터 연결하기   생성한 OKE 클러스터 상세 페이지로 이동합니다.\n  Access Cluster를 클릭합니다.\n   두 가지 접근 방법 중에 Cloud Shell Access을 선택합니다.\n Cloud Shell: OCI에서 제공하는 Cloud Shell을 통해 접근합니다. 현재 접속한 사용자가 현 OCI Tenacy 환경에 작업하기 위한 기본 설정 및 관련 cli들이 구성되어 있습니다. Local Access: 로컬 PC 환경에서 처음 접속하기 위해 필요한 작업부터 시작하는 방법입니다.     Step #1. Launch Cloud Shell\nLaunch Cloud Shell를 클릭하거나, 우측 상단에 있는 링크를 클릭하여 Cloud Shell에 접속합니다.\n   접속한 환경에서 다음 명령을 실행해 보면 oci cli가 설치되어 있으며, 접속이 가능한 상태임을 알 수 있습니다.\noci -v oci os get ns    Step #2. kubeconfig 파일 생성하기\n생성된 OKE 클러스터 접속을 위한 kubeconfig을 생성하기 위해 Access Your Cluster의 두 번째 단계 내용을 Cloud Shell에서 실행합니다.\n 명령어에서 보듯이 Cloud Shell에서는 Kubernetes API를 Public Endpoint을 제공하는 경우에만 접근할 수 있습니다.     OKE 클러스터 연결 확인\nkubectl cluster-info 를 실행하면 생성된 kubeconfig를 통해 클러스터에 접속됨을 확인할 수 있습니다.\n   ","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/3.access-cluster/1.cloudshell-access/","tags":["oke"],"title":"1.3.1 Cloud Shell로 클러스터 연결하기"},{"categories":null,"contents":"1.3.2 로컬 환경에서 클러스터 연결하기 OCI CLI 설치 및 환경 구성 OCI CLI 설치 공식 문서를 참고하여 OCI OCI를 설치합니다.\n https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cliinstall.htm  Oracle Linux 기준 예시\n  설치\nbash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;   설치 확인\noci --version   OCI CLI Config File설정   설정을 위해 필요한 정보 사전 확인\n  user OCID\n오른쪽 위 사용자 Profle에서 User Settings 클릭 후 사용자 OCID 확인   Tenancy OCID\n오른쪽 위 사용자 Profle에서 Tenancy 클릭 후 Tenancy OCID 확인   Region: 사용할 Region\n  API Signing Key: 여기서는 편의상 새로운 Private Key, Public Key를 생성하는 것으로 선택하겠습니다.\n      setup config 실행\noci setup config  실행 예시  [opc@bastion-host ~]$ oci setup config This command provides a walkthrough of creating a valid CLI config file. ... Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaazo6ilmezdaeozjcmsu6rcxnf5sjz2fau76kpdjvsbbakhqtw75sq Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaamd5zq7ohrxkmcsai23pp634k6i2eymnznv4d6ur7n5n4qj7jfhoq Enter a region by index or name(e.g. 1: ap-chiyoda-1, 2: ap-chuncheon-1, 3: ap-hyderabad-1, 4: ap-ibaraki-1, 5: ap-melbourne-1, 6: ap-mumbai-1, 7: ap-osaka-1, 8: ap-seoul-1, 9: ap-singapore-1, 10: ap-sydney-1, 11: ap-tokyo-1, 12: ca-montreal-1, 13: ca-toronto-1, 14: eu-amsterdam-1, 15: eu-frankfurt-1, 16: eu-marseille-1, 17: eu-zurich-1, 18: il-jerusalem-1, 19: me-dubai-1, 20: me-jeddah-1, 21: sa-santiago-1, 22: sa-saopaulo-1, 23: sa-vinhedo-1, 24: uk-cardiff-1, 25: uk-gov-cardiff-1, 26: uk-gov-london-1, 27: uk-london-1, 28: us-ashburn-1, 29: us-gov-ashburn-1, 30: us-gov-chicago-1, 31: us-gov-phoenix-1, 32: us-langley-1, 33: us-luke-1, 34: us-phoenix-1, 35: us-sanjose-1): 8 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: a0:e1:fe:79:22:22:f0:b5:6b:29:72:5f:5d:b6:22:32 Config written to /home/opc/.oci/config If you haven\u0026#39;t already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2   API Public Key 등록   OCI Config File에 등록한 사용자 상세 페이지로 이동\n  왼쪽 아래 Resources에서 API Keys 선택\n  Add Public Key 클릭\n    oci setup config 실행 후 생성된 API Public Key 확인\n[opc@bastion-host ~]$ cat ~/.oci/oci_api_key_public.pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAycclV86OzQ+x6I13HEbe ... gCO1GdLyJNS08Zv1uiW6j1IYIszrmr6XK482Vf3r48u8ZkFgBBlsPjU03R9H1x52 dwIDAQAB -----END PUBLIC KEY-----   생성된 API Public Key 내용을 API Public Key 추가\n   OCI CLI를 위한 API Public Key 등록 완료   연결 확인   다시 리눅스 호스트로 돌아가 oci os ns get을 실행하여 연결 확인\n[opc@bastion-host ~]$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; }   kubectl CLI 설치 및 환경 구성 kubectl CLI 설치 공식 문서를 참고하여 kubectl OCI를 설치합니다.\n https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/  Linux 기준 예시\n  설치\ncurl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl   kubeconfig 파일 생성하기   생성한 OKE 클러스터 상세 페이지에서 Access Cluster를 클릭합니다.\n  Local Access 를 클릭합니다.\n   PRIVATE_ENDPOINT\nKubernetes API 접근도 Private IP를 통해서 할 때 사용합니다. PRIVATE_ENDPOINT, 즉 Private IP로 접근을 해야 하므로, bastion host 등 내부 IP로 접근이 가능한 서버에서 수행할 때 사용합니다.\n bastion host는 외부에서 SSH로 접근 가능하게 22 포트 오픈이 필요하며, 내부적으로는 OKE 클러스터의 Kubernetes API 및 Worker Nodes 들에 접근이 가능해야 합니다. kubeconfig 파일 생성 및 클러스터에 접속 확인  [opc@bastion-host ~]$ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.ap-seoul-1.aaaaaaaair2g7crtxvgchbrfkvf5dz6j7ncrjepinzm2infw6cjy33uzxhyq --file $HOME/.kube/config --region ap-seoul-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT New config written to the Kubeconfig file /home/opc/.kube/config [opc@bastion-host ~]$ kubectl cluster-info Kubernetes control plane is running at https://10.0.0.4:6443 CoreDNS is running at https://10.0.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.   PUBLIC_ENDPOINT\nKubernetes API 접근도 Public IP를 통해서 할 때 사용합니다. OKE 클러스터 생성시 Kubernetes API에 Public IP를 부여한 경우에 사용 가능한 방법입니다. 실제 kubeconfig 파일 생성하는 명령(oci ce cluster crate-kubeconfig ~~)은 마지막 옵션값인 PUBLIC_ENPOINT이외는 동일한 명령합니다.\n  ","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/3.access-cluster/2.local-access/","tags":["oke"],"title":"1.3.2 로컬 환경에서 클러스터 연결하기"},{"categories":null,"contents":"8.1 File Storage 구성 예시 서버간 공유 파일 시스템으로 File Storage를 통해 여러 개의 클라이언트 서버에서 파일을 공유할 수 있습니다. 필요로 하는 보안규칙을 이해하기 위해 아래 그림과 같이 기본 구성된 VCN의 Subnet과 별도의 Subnet 상에 File Storage를 구성하도록 해보겠습니다.\n  Security List\n 아래 그림에서는 VCN 상의 특정 Subnet에서 접근이 가능하도록 Source IP를 VCN의 CIDR로 지정하였습니다. 특정 Subnet에서만 접근하려고 하면 Security List의 Source IP의 CIDR로 제어하면 됩니다. 다른 서비스와는 달리 같은 Subnet상의 Compute Instance에서 접근하는 경우에도 반드시 Security List에 등록되어 있어야 합니다.    Export Option\n Security List외에 Export Option을 통해 Client 별로 접근 제어를 할 수 있습니다.     ","lastmod":"2020-01-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/1/","tags":["file storage"],"title":"8.1 File Storage 구성 예시"},{"categories":null,"contents":"14.3.1 OCI Request Signature Version 1 앞서 설명한 것처럼 OCI REST API를 호출하기 위해서는 요청 서명을 추가해야 합니다. draft-cavage-http-signatures-08 스펙에 기초하며 개략적인 절차는 다음과 같습니다.\n HTTPS (SSL protocol TLS 1.2) 요청 메시지를 만듭니다. 요청 메시지에 따른 서명대상 문자열을 만듭니다. 개인키와 RSA-SHA256을 사용해 서명대상 문자열을 서명(암호화)합니다. 요청 메시지 Authorization 헤더에 서명된 문자열 및 필요한 추가 정보를 추가합니다. OCI REST API 호출  자세한 절차는 공식 문서와 다음 블로그에서 잘 설명하고 있습니다.\n Oracle Cloud Infrastructure Documentation - Request Signatures Oracle Cloud Infrastructure (OCI) REST call walkthrough with curl  서명 샘플 또한 공식 문서에서는 여러가지 언어 및 명령행에서 실행할 수 있는 샘플을 제공하고 있습니다.\n Sample Code  ","lastmod":"2019-05-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/1/","tags":["rest api","request signature"],"title":"14.3.1 OCI Request Signature Version 1"},{"categories":null,"contents":"10.1 Load Balancer 구성 예시 인스턴스의 가용성을 보장하기 위해 OCI Load Balancer를 통해 여러 개의 대상 서버로 부하를 분산할 수 있습니다. 대상 서버도 가용성 보장을 위해 서로 다른 AD 또는 동일 AD 상의 서로 다른 Fault Domain에 일반적으로 구성합니다. 그리고 OCI Load Balancer 자체도 Fail Over를 위해 이중화되어 구성됩니다.\n ","lastmod":"2019-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/1/","tags":["load balancer"],"title":"10.1 Load Balancer 구성 예시"},{"categories":null,"contents":"6.1 Compartment Compartment는 자원들을 쉽게 관리할 수 있도록 하는 논리적인 개념으로 폴더 구조라고 생각하면 됩니다. Tenancy가 생성되면 최초로 Root Compartment 하나가 만들어져 있으며, 관리자가 Root Compartment 하위로 새로운 Compartment를 추가할 수 있습니다. 모든 OCI 자원들은 특정 Compartment에 속하게 되며 Compartment 단위로 사용자들의 접근 정책을 관리할 수 있습니다.\n최초 Tenancy가 만들어지면, Root Compartment가 만들어져 있으며, 모든 Cloud 자원들을 담을 수 있습니다. Root 폴더라고 생각하면 됩니다.\nTenancy 내에 Compartment를 구성하기 전에 고려할 사항이 많겠지만, 그중 아래 사항을 고려하여 구성합니다.\n 자원(예, 인스턴스, Block Storage Volume, VCN, Subnet)을 생성할 시점에, 자원을 담을 Compartment를 지정해야 함 자원이 만들어지면 다른 Compartment로 옮길 수 없음 Compartment를 삭제하려면 Compartment에 속한 모든 자원은 삭제 또는 Terminate 시켜야 함 사용자 그룹에 대해 전체 Tenancy에 대한 권한을 부여할 수 있으며, Compartment 단위로도 부여할 수 있음.  Compartment 구성 예시 OCI 문서 가이드에서는 크게 아래 두 가지 방법을 예로 들고 있습니다. Compartment 구성시 이를 참조하여 구성합니다.\n예시 #1, 그냥 하나만 사용하기 Tenancy를 소규모 조직이 사용하는 경우 최초 생성된 Root Compartment 하나만 사용합니다. 다만, 기능 테스트 등을 고려하여 오라클 문서에서는 Sandbox Compartment 하나는 적어도 생성한 다음에 일반 사용자그룹에 Sandbox에 대해서는 많은 권한을 부여하되, Root Compartment에 대해서는 엄격하게, 세부적인 권한을 부여하도록 하는 방법을 예로 들고 있습니다.\n예시 #2, 부서, 프로젝트 등을 고려한 Compartment 구성 Root Compartment, Sandbox Compartment 이외에 부서별, 프로젝트별 등을 고려해 Compartment를 구성하여 해당 Compartment 별로 세부적인 권한을 부여하도록 하는 방법을 예로 들고 있습니다.\nSandbox Compartment 만들기 다음 과정을 통해 Compartment를 만들 수 있습니다. 이전 장에서 이미 만든 경우는 생략합니다.\n OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Compartments 항목으로 이동합니다. Create Compartment 클릭 생성 정보 입력  Name: \u0026ldquo;Sandbox\u0026rdquo; 입력 Description: 설명 입력, 예, 이름과 동일하게 \u0026ldquo;Sandbox\u0026rdquo; Parent Compartment: 상위 Compartment 지정, Root Compartment 선택   Create Compartment 클릭  Production Compartment 만들기  테스트를 위해 여분으로 이름을 Production으로 하는 Compartment를 하나 더 만듭니다. 다음 정보로 Compartment 생성  Name: \u0026ldquo;Production\u0026rdquo; 입력 Description: 설명 입력, 예, 이름과 동일하게 \u0026ldquo;Production\u0026rdquo; Parent Compartment: 상위 Compartment 지정, Root Compartment 선택    ","lastmod":"2019-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/1/","tags":["compartment"],"title":"6.1 Compartment"},{"categories":null,"contents":"5.1 Linux 인스턴스에 Apache HTTP Server 설치   생성한 Instance에 SSH 명령을 통해 접속합니다. 아래 명령은 Oracle Enterprise Linux 기준입니다.\nssh –i \u0026lt;private_key\u0026gt; \u0026lt;username\u0026gt;@\u0026lt;public-ip-address\u0026gt;   Apache HTTP Server 설치\nsudo yum -y install httpd   OS 방화벽에서 Apache HTTP용 포트, 80 포트 개방\nsudo firewall-cmd --permanent --add-port=80/tcp  팁 방화벽 개방 포트\nApache 서버 디폴트 포트이외 포트, 다른 응용프로그램 사용시 실제 개방이 필요한 포트에 맞게 위 명령을 수정하여 해당 포트를 방화벽에서 개방합니다.     방화벽 변경정보 다시 반영\nsudo firewall-cmd --reload   Apache 시작\nsudo systemctl start httpd   테스트를 위해 서버의 Root Index Document 생성\nsudo su echo \u0026#39;Hello Apache\u0026#39; \u0026gt;/var/www/html/index.html   설치 예시 ubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i mysshkey opc@152.70.251.xx Last login: Mon Jan 10 08:38:45 2022 from 223.62.21.xx [opc@examplelinuxinstance ~]$ sudo yum -y install httpd Loaded plugins: langpacks, ulninfo ol7_MySQL80 | 3.0 kB 00:00:00 ol7_MySQL80_connectors_community | 2.9 kB 00:00:00 ol7_MySQL80_tools_community | 2.9 kB 00:00:00 ol7_UEKR6 | 3.0 kB 00:00:00 ol7_addons | 3.0 kB 00:00:00 ol7_ksplice | 3.0 kB 00:00:00 ol7_latest | 3.6 kB 00:00:00 ol7_oci_included | 2.9 kB 00:00:00 ol7_optional_latest | 3.0 kB 00:00:00 ol7_software_collections | 3.0 kB 00:00:00 (1/20): ol7_MySQL80_connectors_community/x86_64/updateinfo | 71 B 00:00:01 (2/20): ol7_MySQL80_tools_community/x86_64/primary_db | 13 kB 00:00:00 (3/20): ol7_UEKR6/x86_64/updateinfo | 389 kB 00:00:00 (4/20): ol7_MySQL80/x86_64/updateinfo | 71 B 00:00:01 (5/20): ol7_addons/x86_64/updateinfo | 131 kB 00:00:00 (6/20): ol7_addons/x86_64/primary_db | 209 kB 00:00:00 (7/20): ol7_MySQL80_tools_community/x86_64/updateinfo | 71 B 00:00:01 (8/20): ol7_ksplice/updateinfo | 7.9 kB 00:00:00 (9/20): ol7_latest/x86_64/group_gz | 136 kB 00:00:00 (10/20): ol7_ksplice/primary_db | 2.4 MB 00:00:00 (11/20): ol7_MySQL80_connectors_community/x86_64/primary_db | 27 kB 00:00:01 (12/20): ol7_oci_included/x86_64/primary_db | 754 kB 00:00:00 (13/20): ol7_latest/x86_64/updateinfo | 3.4 MB 00:00:00 (14/20): ol7_optional_latest/x86_64/updateinfo | 1.3 MB 00:00:00 (15/20): ol7_software_collections/x86_64/updateinfo | 8.9 kB 00:00:00 (16/20): ol7_software_collections/x86_64/primary_db | 5.8 MB 00:00:00 (17/20): ol7_MySQL80/x86_64/primary_db | 183 kB 00:00:02 (18/20): ol7_optional_latest/x86_64/primary_db | 5.6 MB 00:00:00 (19/20): ol7_UEKR6/x86_64/primary_db | 27 MB 00:00:01 (20/20): ol7_latest/x86_64/primary_db | 37 MB 00:00:01 Resolving Dependencies --\u0026gt; Running transaction check ---\u0026gt; Package httpd.x86_64 0:2.4.6-97.0.5.el7_9.2 will be installed --\u0026gt; Processing Dependency: httpd-tools = 2.4.6-97.0.5.el7_9.2 for package: httpd-2.4.6-97.0.5.el7_9.2.x86_64 --\u0026gt; Processing Dependency: /etc/mime.types for package: httpd-2.4.6-97.0.5.el7_9.2.x86_64 --\u0026gt; Processing Dependency: libapr-1.so.0()(64bit) for package: httpd-2.4.6-97.0.5.el7_9.2.x86_64 --\u0026gt; Processing Dependency: libaprutil-1.so.0()(64bit) for package: httpd-2.4.6-97.0.5.el7_9.2.x86_64 --\u0026gt; Running transaction check ---\u0026gt; Package apr.x86_64 0:1.4.8-7.el7 will be installed ---\u0026gt; Package apr-util.x86_64 0:1.5.2-6.0.1.el7 will be installed ---\u0026gt; Package httpd-tools.x86_64 0:2.4.6-97.0.5.el7_9.2 will be installed ---\u0026gt; Package mailcap.noarch 0:2.1.41-2.el7 will be installed --\u0026gt; Finished Dependency Resolution Dependencies Resolved ================================================================================================================= Package Arch Version Repository Size ================================================================================================================= Installing: httpd x86_64 2.4.6-97.0.5.el7_9.2 ol7_latest 1.2 M Installing for dependencies: apr x86_64 1.4.8-7.el7 ol7_latest 103 k apr-util x86_64 1.5.2-6.0.1.el7 ol7_latest 91 k httpd-tools x86_64 2.4.6-97.0.5.el7_9.2 ol7_latest 93 k mailcap noarch 2.1.41-2.el7 ol7_latest 30 k Transaction Summary ================================================================================================================= Install 1 Package (+4 Dependent packages) Total download size: 1.5 M Installed size: 4.3 M Downloading packages: (1/5): apr-1.4.8-7.el7.x86_64.rpm | 103 kB 00:00:00 (2/5): apr-util-1.5.2-6.0.1.el7.x86_64.rpm | 91 kB 00:00:00 (3/5): httpd-tools-2.4.6-97.0.5.el7_9.2.x86_64.rpm | 93 kB 00:00:00 (4/5): mailcap-2.1.41-2.el7.noarch.rpm | 30 kB 00:00:00 (5/5): httpd-2.4.6-97.0.5.el7_9.2.x86_64.rpm | 1.2 MB 00:00:00 ------------------------------------------------------------------------------------------------------------------ Total 1.5 MB/s | 1.5 MB 00:00:00 Running transaction check Running transaction test Transaction test succeeded Running transaction Installing : apr-1.4.8-7.el7.x86_64 1/5 Installing : apr-util-1.5.2-6.0.1.el7.x86_64 2/5 Installing : httpd-tools-2.4.6-97.0.5.el7_9.2.x86_64 3/5 Installing : mailcap-2.1.41-2.el7.noarch 4/5 Installing : httpd-2.4.6-97.0.5.el7_9.2.x86_64 5/5 Verifying : httpd-tools-2.4.6-97.0.5.el7_9.2.x86_64 1/5 Verifying : apr-1.4.8-7.el7.x86_64 2/5 Verifying : apr-util-1.5.2-6.0.1.el7.x86_64 3/5 Verifying : mailcap-2.1.41-2.el7.noarch 4/5 Verifying : httpd-2.4.6-97.0.5.el7_9.2.x86_64 5/5 Installed: httpd.x86_64 0:2.4.6-97.0.5.el7_9.2 Dependency Installed: apr.x86_64 0:1.4.8-7.el7 apr-util.x86_64 0:1.5.2-6.0.1.el7 httpd-tools.x86_64 0:2.4.6-97.0.5.el7_9.2 mailcap.noarch 0:2.1.41-2.el7 Complete! [opc@examplelinuxinstance ~]$ sudo firewall-cmd --permanent --add-port=80/tcp success [opc@examplelinuxinstance ~]$ sudo firewall-cmd --reload success [opc@examplelinuxinstance ~]$ sudo systemctl start httpd [opc@examplelinuxinstance ~]$ sudo su [root@examplelinuxinstance opc]# echo \u0026#39;Hello Apache\u0026#39; \u0026gt;/var/www/html/index.html [root@examplelinuxinstance opc]# exit exit [opc@examplelinuxinstance ~]$ curl http://127.0.0.1:80/index.html Hello Apache ","lastmod":"2019-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/1/","tags":["linux","apache"],"title":"5.1 Linux 인스턴스에 Apache HTTP Server 설치"},{"categories":null,"contents":"14.1.1.1 Windows에서 OCI CLI 설치하기   관리자 모드로 PowerShell 실행\n  PowerShell의 remote execution policy 구성을 위해 다음 명령 실행\nSet-ExecutionPolicy RemoteSigned  정책 변경을 위해 Y 응답    설치 스크립트 실행을 위해 다음 실행\npowershell -NoProfile -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.ps1\u0026#39;))\u0026#34;  Python 설치여부: Y 응답  설치 경로에 대한 확인 후 설정 CLI의 새 버전 업데이트 확인시 Y 응답 PATH에 CLI 추가 요청시 Y 응답      설치 후 현재 창에는 신규 등록한 PATH가 적용되지 않을 수 있으므로, 새로운 PowerShell을 열어 OCI CLI를 실행합니다.\n  설치 예시\nPS C:\\WINDOWS\\system32\u0026gt; Set-ExecutionPolicy RemoteSigned 실행 규칙 변경 실행 정책은 신뢰하지 않는 스크립트로부터 사용자를 보호합니다. 실행 정책을 변경하면 about_Execution_Policies 도움말 항목(https://go.microsoft.com/fwlink/?LinkID=135170)에 설명된 보안 위험에 노출될 수 있습니다. 실행 정책을 변경하시겠습니까? [Y] 예(Y) [A] 모두 예(A) [N] 아니요(N) [L] 모두 아니요(L) [S] 일시 중단(S) [?] 도움말 (기본값은 \u0026#34;N\u0026#34;): Y PS C:\\WINDOWS\\system32\u0026gt; powershell -NoProfile -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.ps1\u0026#39;))\u0026#34; ****************************************************************************** You have started the OCI CLI Installer in interactive mode. If you do not wish to run this in interactive mode, please include the -AcceptAllDefaults option. If you have the script locally and would like to know more about input options for this script, then you can run: help .\\install.ps1 If you would like to know more about input options for this script, refer to: https://github.com/oracle/oci-cli/blob/master/scripts/install/README.rst ****************************************************************************** 자세한 정보 표시: No valid Python installation found. Python is required to run the CLI. Install Python now? (Entering \u0026#34;n\u0026#34; will exit the installation script) [Y] Yes [N] No [?] 도움말 (기본값은 \u0026#34;Y\u0026#34;): Y 자세한 정보 표시: Downloading Python... 자세한 정보 표시: Download Complete! Installer executable written to: C:\\Users\\thekoguryo\\AppData\\Local\\Temp\\tmp18AF.tmp.exe 자세한 정보 표시: Installing Python to C:\\Users\\thekoguryo\\Python... 자세한 정보 표시: Successfully installed Python! 자세한 정보 표시: Downloading install script to C:\\Users\\thekoguryo\\AppData\\Local\\Temp\\tmpC089.tmp 자세한 정보 표시: C:\\Users\\thekoguryo\\Python False False 자세한 정보 표시: Using Python executable: C:\\Users\\thekoguryo\\Python\\python.exe to run install script... 자세한 정보 표시: Arguments to python script: \u0026#34;C:\\Users\\thekoguryo\\AppData\\Local\\Temp\\tmpC089.tmp\u0026#34; -- Verifying Python version. -- Python version 3.8.5 okay. ===\u0026gt; In what directory would you like to place the install? (leave blank to use \u0026#39;C:\\Users\\thekoguryo\\lib\\oracle-cli\u0026#39;): -- Creating directory \u0026#39;C:\\Users\\thekoguryo\\lib\\oracle-cli\u0026#39;. -- We will install at \u0026#39;C:\\Users\\thekoguryo\\lib\\oracle-cli\u0026#39;. ===\u0026gt; In what directory would you like to place the \u0026#39;oci.exe\u0026#39; executable? (leave blank to use \u0026#39;C:\\Users\\thekoguryo\\bin\u0026#39;): -- Creating directory \u0026#39;C:\\Users\\thekoguryo\\bin\u0026#39;. -- The executable will be in \u0026#39;C:\\Users\\thekoguryo\\bin\u0026#39;. ===\u0026gt; In what directory would you like to place the OCI scripts? (leave blank to use \u0026#39;C:\\Users\\thekoguryo\\bin\\oci-cli-scripts\u0026#39;): -- Creating directory \u0026#39;C:\\Users\\thekoguryo\\bin\\oci-cli-scripts\u0026#39;. -- The scripts will be in \u0026#39;C:\\Users\\thekoguryo\\bin\\oci-cli-scripts\u0026#39;. ===\u0026gt; Currently supported optional packages are: [\u0026#39;db (will install cx_Oracle)\u0026#39;] What optional CLI packages would you like to be installed (comma separated names; press enter if you don\u0026#39;t need any optional packages)?: -- The optional packages installed will be \u0026#39;\u0026#39;. -- Trying to use python3 venv. -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\Python\\\\python.exe\u0026#39;, \u0026#39;-m\u0026#39;, \u0026#39;venv\u0026#39;, \u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\u0026#39;] -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\\\\Scripts\\\\python.exe\u0026#39;, \u0026#39;-m\u0026#39;, \u0026#39;pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--upgrade\u0026#39;, \u0026#39;pip\u0026#39;] Collecting pip Downloading pip-21.3.1-py3-none-any.whl (1.7 MB) |████████████████████████████████| 1.7 MB 6.8 MB/s Installing collected packages: pip Attempting uninstall: pip Found existing installation: pip 20.1.1 Uninstalling pip-20.1.1: Successfully uninstalled pip-20.1.1 Successfully installed pip-21.3.1 -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\\\\Scripts\\\\pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;C:\\\\Users\\\\thekoguryo\\\\AppData\\\\Local\\\\Temp\\\\tmp7z8s7qbi\u0026#39;, \u0026#39;wheel\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting wheel Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB) Installing collected packages: wheel Successfully installed wheel-0.37.1 -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\\\\Scripts\\\\pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;C:\\\\Users\\\\thekoguryo\\\\AppData\\\\Local\\\\Temp\\\\tmp7z8s7qbi\u0026#39;, \u0026#39;oci_cli\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting oci_cli Downloading oci_cli-3.4.2-py3-none-any.whl (23.8 MB) |████████████████████████████████| 23.8 MB 6.8 MB/s Collecting cryptography\u0026lt;=3.4.7,\u0026gt;=3.2.1 Downloading cryptography-3.4.7-cp36-abi3-win_amd64.whl (1.6 MB) |████████████████████████████████| 1.6 MB ... Collecting pyOpenSSL==19.1.0 Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 955 kB/s Collecting click==7.1.2 Downloading click-7.1.2-py2.py3-none-any.whl (82 kB) |████████████████████████████████| 82 kB 5.8 MB/s Collecting six\u0026gt;=1.15.0 Downloading six-1.16.0-py2.py3-none-any.whl (11 kB) Collecting oci==2.54.0 Downloading oci-2.54.0-py2.py3-none-any.whl (12.2 MB) |████████████████████████████████| 12.2 MB 6.4 MB/s Collecting arrow\u0026gt;=1.0.0 Downloading arrow-1.2.1-py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB ... Collecting jmespath==0.10.0 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting pytz\u0026gt;=2016.10 Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB) |████████████████████████████████| 503 kB 6.8 MB/s Collecting certifi Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB) |████████████████████████████████| 149 kB ... Collecting PyYAML\u0026lt;6,\u0026gt;=5.4 Downloading PyYAML-5.4.1-cp38-cp38-win_amd64.whl (213 kB) |████████████████████████████████| 213 kB ... Collecting terminaltables==3.1.0 Downloading terminaltables-3.1.0.tar.gz (12 kB) Preparing metadata (setup.py) ... done Collecting python-dateutil\u0026lt;3.0.0,\u0026gt;=2.5.3 Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB) |████████████████████████████████| 247 kB ... Collecting circuitbreaker\u0026lt;2.0.0,\u0026gt;=1.3.1 Downloading circuitbreaker-1.3.2.tar.gz (7.9 kB) Preparing metadata (setup.py) ... done Collecting cffi\u0026gt;=1.12 Downloading cffi-1.15.0-cp38-cp38-win_amd64.whl (179 kB) |████████████████████████████████| 179 kB 6.4 MB/s Collecting pycparser Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB) |████████████████████████████████| 118 kB 6.8 MB/s Building wheels for collected packages: terminaltables, circuitbreaker Building wheel for terminaltables (setup.py) ... done Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15355 sha256=a28562e8abfb78420764123ed4224e44167c5bc969996d1745bc5c1116b2d8b2 Stored in directory: c:\\users\\thekoguryo\\appdata\\local\\temp\\tmp7z8s7qbi\\wheels\\08\\8f\\5f\\253d0105a55bd84ee61ef0d37dbf70421e61e0cd70cef7c5e1 Building wheel for circuitbreaker (setup.py) ... done Created wheel for circuitbreaker: filename=circuitbreaker-1.3.2-py3-none-any.whl size=6017 sha256=ac910cffc04f0ed6e90f2b65da6241bdaf9848fcf160dc03b1ec821db75600b8 Stored in directory: c:\\users\\thekoguryo\\appdata\\local\\temp\\tmp7z8s7qbi\\wheels\\97\\d2\\3d\\8ad7bff00d846a770cdc0ed208f0fae657c983e675d658c1d5 Successfully built terminaltables circuitbreaker Installing collected packages: pycparser, cffi, six, cryptography, pytz, python-dateutil, pyOpenSSL, circuitbreaker, certifi, terminaltables, PyYAML, oci, jmespath, click, arrow, oci-cli Successfully installed PyYAML-5.4.1 arrow-1.2.1 certifi-2021.10.8 cffi-1.15.0 circuitbreaker-1.3.2 click-7.1.2 cryptography-3.4.7 jmespath-0.10.0 oci-2.54.0 oci-cli-3.4.2 pyOpenSSL-19.1.0 pycparser-2.21 python-dateutil-2.8.2 pytz-2021.3 six-1.16.0 terminaltables-3.1.0 ===\u0026gt; Modify PATH to include the CLI and enable tab completion in PowerShell now? (Y/n): Y -- -- ** Close and re-open PowerShell to reload changes to your PATH ** -- In order to run the autocomplete script, you may also need to set your PowerShell execution policy to allow for running local scripts (as an Administrator run Set-ExecutionPolicy RemoteSigned in a PowerShell prompt) -- -- Installation successful. -- Run the CLI with C:\\Users\\thekoguryo\\bin\\oci.exe --help 자세한 정보 표시: Successfully installed OCI CLI! PS C:\\WINDOWS\\system32\u0026gt;   ","lastmod":"2018-12-24T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/1/","tags":["windows","CLI"],"title":"14.1.1.1 Windows에서 OCI CLI 설치하기"},{"categories":null,"contents":"1.6.2 File Storage 사용하기(CSI Driver 기반) 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)을 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 RWX 접근 모드로 사용하는 방법을 확인해 보겠습니다.\n 업데이트 Jan. 13, 2022일자 기준 업데이트 Support for provisioning Kubernetes Persistent Volume Claims (PVCs) on File Storage service로 OKE에서 File Storage Service에 대한 CSI Driver 공식적으로 출시되었으며, OKE 공식 문서에도 사용가이드가 포함되었습니다. 이전에 OCI 블로그를 통해 설명된 Flex Volume Driver가 아닌 공식지원하는 CSI Driver 기준으로 사용할 것을 권장합니다.   Files Storage 만들기 관련 문서를 참고하여 File Storage를 만듭니다.\n  https://docs.oracle.com/en-us/iaas/Content/File/home.htm\n  https://thekoguryo.github.io/oci/chapter08/\n    OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Storage \u0026gt; File Storage로 이동합니다\n  대상 Compartment를 확인합니다.\n  File Systems에서 Create File System을 클릭합니다.\n  기본 설정화면에서 간단히 아래 정보를 원하는 값이 맞게 알맞게 수정하고 생성합니다.\n File System Information:  Name   Mount Target Information:  New Mount Target Name Virtual Cloud Network Subnet       생성결과 확인\nFile Storage \u0026gt; Mount Target 에서 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\n Mount Target OCID: \u0026hellip;sc2mia IP Address: 예, 10.0.20.194 Export Path: 예) /OKE-FFS-Strorage     Security List 설정\nFile System 생성시 Mount Target의 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\n   File Storage 서비스를 이용하여 Persistent Volume 사용하기   Persistent Volume (PV) 만들기\n spec.csi.driver: 새로 나온 fss.csi.oraclecloud.com를 사용 spec.csi.volumeHandle: ::형식으로 아래와 같이 설정 spec.accessModes: FFS CSI Driver는 RWX - ReadWriteMany를 지원하므로 테스트를 위해 ReadWriteMany 접근 모드로 지정  apiVersion:v1kind:PersistentVolumemetadata:name:oke-fss-pvspec:capacity:storage:50GivolumeMode:FilesystemaccessModes:- ReadWriteManypersistentVolumeReclaimPolicy:Retaincsi:driver:fss.csi.oraclecloud.comvolumeHandle:ocid1.filesystem.oc1.ap_seoul_1.aaaaaaaaaaabgsxcpfxhsllqojxwiotboawwg2dvnzrwqzlpnywtcllbmqwtcaaa:10.0.20.194:/OKE-FFS-Storage  Persistent Volume Claime(PVC) 만들기\n storageClassName: \u0026ldquo;\u0026ldquo;로 설정 volumeName: 앞서 만든 PV의 이름 지정  apiVersion:v1kind:PersistentVolumeClaimmetadata:name:oke-fss-pvcspec:accessModes:- ReadWriteManystorageClassName:\u0026#34;\u0026#34;resources:requests:storage:50GivolumeName:oke-fss-pv  PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\nReadWriteMany 접근모드를 사용하므로 앞선 Block Volume을 PV 사용하는 예제와 달리 replica를 복수개로 지정할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-fss-pvcname:nginx-fss-pvcspec:replicas:3selector:matchLabels:app:nginx-fss-pvctemplate:metadata:labels:app:nginx-fss-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:oke-fss-pvc  실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oke-fss-pv 50Gi RWX Retain Bound default/oke-fss-pvc 15m NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 50Gi RWX 15m oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-9fb98454f-kgmbd 1/1 Running 0 16m 10.244.0.131 10.0.10.124 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-qxwg7 1/1 Running 0 16m 10.244.1.3 10.0.10.186 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-tltbx 1/1 Running 0 16m 10.244.0.4 10.0.10.157 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   파일 쓰기 테스트\n  아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-kgmbd -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.124\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-kgmbd -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.124 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-qxwg7 -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.124 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-tltbx -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 110.0.10.124     참고 문서 https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengcreatingpersistentvolumeclaim.htm#Provisioning_Persistent_Volume_Claims_on_the_FileStorageService\n","lastmod":"2022-02-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/6.persistent-volume/2.file-storage/","tags":["oke"],"title":"1.6.2 File Storage 사용하기"},{"categories":null,"contents":"14.3.2 Bash 샘플 클라이언트 (예전 스타일) Step 1. oci-curl 함수 준비 아래는 이전에 OCI 문서에서 제공하던 oci-curl bash 함수를 이용하는 방법을 사용합니다. 백업을 위해 남겨 놓으며, 현재 2022년 기준으로 oci raw-request와 같이 oci cli를 사용하는 것으로 변경되었습니다. 최신 내용은 아래 링크를 참조하세요.\n https://docs.cloud.oracle.com/iaas/Content/API/Concepts/signingrequests.htm#Bash  # Version: 1.0.2 # Usage: # oci-curl \u0026lt;host\u0026gt; \u0026lt;method\u0026gt; [file-to-send-as-body] \u0026lt;request-target\u0026gt; [extra-curl-args] # ex: # oci-curl iaas.us-ashburn-1.oraclecloud.com get \u0026#34;/20160918/instances?compartmentId=some-compartment-ocid\u0026#34; # oci-curl iaas.us-ashburn-1.oraclecloud.com post ./request.json \u0026#34;/20160918/vcns\u0026#34; function oci-curl { # TODO: update these values to your own local tenancyId=\u0026#34;ocid1.tenancy.oc1..aaaaaaaaba3pv6wkcr4jqae5f15p2b2m2yt2j6rx32uzr4h25vqstifsfdsq\u0026#34;; local authUserId=\u0026#34;ocid1.user.oc1..aaaaaaaat5nvwcna5j6aqzjcaty5eqbb6qt2jvpkanghtgdaqedqw3rynjq\u0026#34;; local keyFingerprint=\u0026#34;20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34\u0026#34;; local privateKeyPath=\u0026#34;/Users/someuser/.oci/oci_api_key.pem\u0026#34;; local alg=rsa-sha256 local sigVersion=\u0026#34;1\u0026#34; local now=\u0026#34;$(LC_ALL=C \\date -u \u0026#34;+%a, %d %h %Y %H:%M:%S GMT\u0026#34;)\u0026#34; local host=$1 local method=$2 local extra_args local keyId=\u0026#34;$tenancyId/$authUserId/$keyFingerprint\u0026#34; case $method in \u0026#34;get\u0026#34; | \u0026#34;GET\u0026#34;) local target=$3 extra_args=(\u0026#34;${@: 4}\u0026#34;) local curl_method=\u0026#34;GET\u0026#34;; local request_method=\u0026#34;get\u0026#34;; ;;\t\u0026#34;delete\u0026#34; | \u0026#34;DELETE\u0026#34;) local target=$3 extra_args=(\u0026#34;${@: 4}\u0026#34;) local curl_method=\u0026#34;DELETE\u0026#34;; local request_method=\u0026#34;delete\u0026#34;; ;;\t\u0026#34;head\u0026#34; | \u0026#34;HEAD\u0026#34;) local target=$3 extra_args=(\u0026#34;--head\u0026#34; \u0026#34;${@: 4}\u0026#34;) local curl_method=\u0026#34;HEAD\u0026#34;; local request_method=\u0026#34;head\u0026#34;; ;; \u0026#34;post\u0026#34; | \u0026#34;POST\u0026#34;) local body=$3 local target=$4 extra_args=(\u0026#34;${@: 5}\u0026#34;) local curl_method=\u0026#34;POST\u0026#34;; local request_method=\u0026#34;post\u0026#34;; local content_sha256=\u0026#34;$(openssl dgst -binary -sha256 \u0026lt; $body | openssl enc -e -base64)\u0026#34;; local content_type=\u0026#34;application/json\u0026#34;; local content_length=\u0026#34;$(wc -c \u0026lt; $body | xargs)\u0026#34;; ;;\t\u0026#34;put\u0026#34; | \u0026#34;PUT\u0026#34;) local body=$3 local target=$4 extra_args=(\u0026#34;${@: 5}\u0026#34;) local curl_method=\u0026#34;PUT\u0026#34; local request_method=\u0026#34;put\u0026#34; local content_sha256=\u0026#34;$(openssl dgst -binary -sha256 \u0026lt; $body | openssl enc -e -base64)\u0026#34;; local content_type=\u0026#34;application/json\u0026#34;; local content_length=\u0026#34;$(wc -c \u0026lt; $body | xargs)\u0026#34;; ;;\t*) echo \u0026#34;invalid method\u0026#34;; return;; esac # This line will url encode all special characters in the request target except \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;=\u0026#34;, and \u0026#34;\u0026amp;\u0026#34;, since those characters are used  # in the request target to indicate path and query string structure. If you need to encode any of \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;=\u0026#34;, or \u0026#34;\u0026amp;\u0026#34;, such as when # used as part of a path value or query string key or value, you will need to do that yourself in the request target you pass in. local escaped_target=\u0026#34;$(echo $( rawurlencode \u0026#34;$target\u0026#34; ))\u0026#34;\tlocal request_target=\u0026#34;(request-target): $request_method$escaped_target\u0026#34; local date_header=\u0026#34;date: $now\u0026#34; local host_header=\u0026#34;host: $host\u0026#34; local content_sha256_header=\u0026#34;x-content-sha256: $content_sha256\u0026#34; local content_type_header=\u0026#34;content-type: $content_type\u0026#34; local content_length_header=\u0026#34;content-length: $content_length\u0026#34; local signing_string=\u0026#34;$request_target\\n$date_header\\n$host_header\u0026#34; local headers=\u0026#34;(request-target) date host\u0026#34; local curl_header_args curl_header_args=(-H \u0026#34;$date_header\u0026#34;) local body_arg body_arg=() if [ \u0026#34;$curl_method\u0026#34; = \u0026#34;PUT\u0026#34; -o \u0026#34;$curl_method\u0026#34; = \u0026#34;POST\u0026#34; ]; then signing_string=\u0026#34;$signing_string\\n$content_sha256_header\\n$content_type_header\\n$content_length_header\u0026#34; headers=$headers\u0026#34; x-content-sha256 content-type content-length\u0026#34; curl_header_args=(\u0026#34;${curl_header_args[@]}\u0026#34; -H \u0026#34;$content_sha256_header\u0026#34; -H \u0026#34;$content_type_header\u0026#34; -H \u0026#34;$content_length_header\u0026#34;) body_arg=(--data-binary @${body}) fi local sig=$(printf \u0026#39;%b\u0026#39; \u0026#34;$signing_string\u0026#34; | \\ \topenssl dgst -sha256 -sign $privateKeyPath | \\ \topenssl enc -e -base64 | tr -d \u0026#39;\\n\u0026#39;) curl \u0026#34;${extra_args[@]}\u0026#34; \u0026#34;${body_arg[@]}\u0026#34; -X $curl_method -sS https://${host}${escaped_target} \u0026#34;${curl_header_args[@]}\u0026#34; \\ \t-H \u0026#34;Authorization: Signature version=\\\u0026#34;$sigVersion\\\u0026#34;,keyId=\\\u0026#34;$keyId\\\u0026#34;,algorithm=\\\u0026#34;$alg\\\u0026#34;,headers=\\\u0026#34;${headers}\\\u0026#34;,signature=\\\u0026#34;$sig\\\u0026#34;\u0026#34; }\t# url encode all special characters except \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;=\u0026#34;, and \u0026#34;\u0026amp;\u0026#34; function rawurlencode { local string=\u0026#34;${1}\u0026#34; local strlen=${#string} local encoded=\u0026#34;\u0026#34; local pos c o\tfor (( pos=0 ; pos\u0026lt;strlen ; pos++ )); do c=${string:$pos:1} case \u0026#34;$c\u0026#34; in [-_.~a-zA-Z0-9] | \u0026#34;/\u0026#34; | \u0026#34;?\u0026#34; | \u0026#34;=\u0026#34; | \u0026#34;\u0026amp;\u0026#34; ) o=\u0026#34;${c}\u0026#34; ;; * ) printf -v o \u0026#39;%%%02x\u0026#39; \u0026#34;\u0026#39;$c\u0026#34; esac encoded+=\u0026#34;${o}\u0026#34; done echo \u0026#34;${encoded}\u0026#34; } Step 2. 연결정보 설정   위 샘플 코드를 복사하여 oci-curl.sh 이름으로 저장합니다.\n  oci-curl.sh 내용중에 연결정보를 사용자에 맞게 업데이트합니다.\n  연결정보 예시\n 아래 정보를 찾는 방법은 14.2.3 Terraform OCI Provider 연결정보 구하기를 참고합니다.  local tenancyId=\u0026#34;ocid1.tenancy.oc1..aaaaaaaaba3pv6wkcr4jqae5f15p2b2m2yt2j6rx32uzr4h25vqstifsfdsq\u0026#34;; local authUserId=\u0026#34;ocid1.user.oc1..aaaaaaaat5nvwcna5j6aqzjcaty5eqbb6qt2jvpkanghtgdaqedqw3rynjq\u0026#34;; local keyFingerprint=\u0026#34;20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34\u0026#34;; local privateKeyPath=\u0026#34;/Users/someuser/.oci/oci_api_key.pem\u0026#34;;     아래 명령을 수행하면 oci-curl 함수를 현재 세션에서 실행할 수 있게 됩니다.\noracle@ubuntu:~/oci-curl$ . ./oci-curl.sh oracle@ubuntu:~/oci-curl$ oci-curl invalid method oracle@ubuntu:~/oci-curl$   Step 3. 사용자 조회 REST API 실행  ListUsers 설명   문서 링크: https://docs.cloud.oracle.com/iaas/api/#/en/identity/20160918/User/ListUsers Endpoint: identity.us-ashburn-1.oraclecloud.com GET /20160918/users/ Parameters  compartmentId: tenancy의 OCID    사용방법  oci-curl \u0026lt;host\u0026gt; \u0026lt;method\u0026gt; [file-to-send-as-body] \u0026lt;request-target\u0026gt; [extra-curl-args] 사용예시 - tenancy내 모든 사용자 조회 아래와 같이 사용자가 조회되는 것을 확인할 수 있습니다.  extra-curl-args로 -i를 입력하면 응답메시지 헤더를 확인할 수 있습니다.    oracle@ubuntu:~/oci-curl$ oci-curl identity.us-ashburn-1.oraclecloud.com GET \u0026#34;/20160918/users/?compartmentId=ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34; -i HTTP/1.1 200 OK Date: Sun, 19 May 2019 08:15:10 GMT Content-Type: application/json Content-Length: 3152 Connection: keep-alive opc-request-id: /2BF47D321833EFD084DBAAC718095658/E21191A3BE4584980FA4A533A6E50927 opc-next-page: AAAAAAAAAAJleUpyYVdRaU9pSTBPRGNpTENKbGJtTWlPaUpCTWpVMlIwTk5JaXdpWVd4bklqb2laR2x5SW4wLi53d0wzM0RiMnVMNGJld0FjLmVyQjNfU3RfbVpmNTRkQS1GRVlLaFk4N196S3dzakVRUVhicnZZeUowSVJxYWdpQnZSVWtEQW9ZWlRNX0hKQ1RzcEVFYTU1Qkt0cEFBdlJXdlRrdWlvNlBRRWpoUG5FNDhCSXpjZUd5UTlOOFBOdkVNRzFoLTEzRUJHbzJzSFJ0Q0hpTU8weFRMRHI4UjlhVHI4SnFjZVJyMXVkT3hlZWRVMmJ2Y1pZenlUM05aTXAzeG5HMDc4ZGpKTHBmbWViWFhtSHBLb1JrQ0JXZHl0VTlkNzRIYksyU19STlk5WWlKSGRRczhjenVzOEE2MGNJS2JDY1MyZ0FGVFhsTmI3UnNXS05ZaHlhTXZUcVZVX05JUTJETkstbVB1TUdjRFYwQU5GTkg1NzdISDRFbDY3R0w4TlAwOVBfeFprSFZrOWNFU0xfMXRHWDRUcUF5d0NoMVVrak5Nc2drb21VbkMzMWNQVThTdjBFQ1ZsS2ZjVGlmTGxCVkxqS0o1a1o1R3RoWUJVLXhJcGxHenJXNjZRUWFsbTlVM2VPVFMxbDFKZi1HS0F3ZGlDY0E1aU1ldWtxNE92d3JmT3Z6aXFGdktJUFZoeldpeHNQaF81ZVE5UzgxWlJmSURHU1dpNU85cUdXZ2I3WnA0bVhnOWtIaGFqY2YyMjJLdlNSTS0xSVFLcnlJazVDUzhranVZR2hJNE9Oa2JrUXIuWloyejcyZ05EUGVCODNZdFVJZEptZw== Cache-Control: no-cache, no-store, must-revalidate opc-limit: 25 Pragma: no-cache X-Content-Type-Options: nosniff [ { \u0026#34;capabilities\u0026#34; : { \u0026#34;canUseConsolePassword\u0026#34; : true, \u0026#34;canUseApiKeys\u0026#34; : true, \u0026#34;canUseAuthTokens\u0026#34; : true, \u0026#34;canUseSmtpCredentials\u0026#34; : true, \u0026#34;canUseCustomerSecretKeys\u0026#34; : true }, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;identityProviderId\u0026#34; : null, \u0026#34;externalIdentifier\u0026#34; : null, \u0026#34;timeModified\u0026#34; : \u0026#34;2019-05-13T04:56:33.114Z\u0026#34;, \u0026#34;isMfaActivated\u0026#34; : false, \u0026#34;id\u0026#34; : \u0026#34;ocid1.user.oc1..aaaaaaaa2um5iz27ms3cf43tp77k6tjjn4kbzjrilajem4xaiyl5vqeXXXXXX\u0026#34;, \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;oci.admin\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;OCI Admin\u0026#34;, \u0026#34;timeCreated\u0026#34; : \u0026#34;2019-05-13T04:55:06.156Z\u0026#34;, \u0026#34;freeformTags\u0026#34; : { }, \u0026#34;definedTags\u0026#34; : { }, \u0026#34;lifecycleState\u0026#34; : \u0026#34;ACTIVE\u0026#34; }, { \u0026#34;capabilities\u0026#34; : { \u0026#34;canUseConsolePassword\u0026#34; : true, \u0026#34;canUseApiKeys\u0026#34; : true, \u0026#34;canUseAuthTokens\u0026#34; : true, \u0026#34;canUseSmtpCredentials\u0026#34; : true, \u0026#34;canUseCustomerSecretKeys\u0026#34; : true }, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;identityProviderId\u0026#34; : null, \u0026#34;externalIdentifier\u0026#34; : null, \u0026#34;timeModified\u0026#34; : \u0026#34;2019-05-13T04:11:10.299Z\u0026#34;, \u0026#34;isMfaActivated\u0026#34; : false, \u0026#34;id\u0026#34; : \u0026#34;ocid1.user.oc1..aaaaaaaaeqzpkd5u7humc3xinp3ika4sjhnhqj5jbvfcvdqg4tdx4jqXXXXX\u0026#34;, \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;sandboxer\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;sandboxer\u0026#34;, \u0026#34;timeCreated\u0026#34; : \u0026#34;2019-05-13T04:09:32.205Z\u0026#34;, \u0026#34;freeformTags\u0026#34; : { }, \u0026#34;definedTags\u0026#34; : { }, \u0026#34;lifecycleState\u0026#34; : \u0026#34;ACTIVE\u0026#34; } ]oracle@ubuntu:~/oci-curl$ Step 4. 사용자 생성 REST API 실행  CreateUser 설명    문서 링크: https://docs.cloud.oracle.com/iaas/api/#/en/identity/20160918/User/CreateUser\n  Endpoint: identity.us-ashburn-1.oraclecloud.com\n  POST /20160918/users/\n  요청메시지 예시\n{ \u0026#34;compartmentId\u0026#34; : \u0026#34;tenancy OCID\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;사용자 이름\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;설명\u0026#34; }   사용방법  oci-curl \u0026lt;host\u0026gt; \u0026lt;method\u0026gt; [file-to-send-as-body] \u0026lt;request-target\u0026gt; [extra-curl-args] 사용예시 - 사용자 생성 아래와 같이 사용자가 만들어 지는 것을 확인할 수 있습니다.  oracle@ubuntu:~/oci-curl$ cat create_user_request.json { \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;KilDong OCI\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;kildong.oci@example.com\u0026#34; } oracle@ubuntu:~/oci-curl$ oci-curl identity.us-ashburn-1.oraclecloud.com POST ./create_user_request.json \u0026#34;/20160918/users/\u0026#34; -i HTTP/1.1 200 OK Date: Sun, 19 May 2019 08:33:39 GMT Content-Type: application/json Content-Length: 748 Connection: keep-alive opc-request-id: /3010DE4E4BFBF1963248FEC32FC1FFBA/FB514FBFDEAA1C6845BCAA66C2B4C31D Cache-Control: no-cache, no-store, must-revalidate ETag: 42e800af061123f725163d2b538d1f9560022422 Pragma: no-cache Location: http://identity.us-ashburn-1.oraclecloud.com/20160918/users/ocid1.user.oc1..aaaaaaaaniw34appawah7sicksca37hhzhq7pvfkmhwskf4gbkt3ctxXXXXXX X-Content-Type-Options: nosniff { \u0026#34;capabilities\u0026#34; : { \u0026#34;canUseConsolePassword\u0026#34; : true, \u0026#34;canUseApiKeys\u0026#34; : true, \u0026#34;canUseAuthTokens\u0026#34; : true, \u0026#34;canUseSmtpCredentials\u0026#34; : true, \u0026#34;canUseCustomerSecretKeys\u0026#34; : true }, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;identityProviderId\u0026#34; : null, \u0026#34;externalIdentifier\u0026#34; : null, \u0026#34;timeModified\u0026#34; : \u0026#34;2019-05-19T08:33:39.788Z\u0026#34;, \u0026#34;isMfaActivated\u0026#34; : false, \u0026#34;id\u0026#34; : \u0026#34;ocid1.user.oc1..aaaaaaaaniw34appawah7sicksca37hhzhq7pvfkmhwskf4gbkt3ctxXXXXX\u0026#34;, \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;kildong.oci@example.com\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;KilDong OCI\u0026#34;, \u0026#34;timeCreated\u0026#34; : \u0026#34;2019-05-19T08:33:39.788Z\u0026#34;, \u0026#34;freeformTags\u0026#34; : { }, \u0026#34;definedTags\u0026#34; : { }, \u0026#34;lifecycleState\u0026#34; : \u0026#34;ACTIVE\u0026#34; }oracle@ubuntu:~/oci-curl$ 생성결과 확인   ","lastmod":"2022-01-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/2/","tags":["rest api","bash"],"title":"14.3.2 Bash 샘플 클라이언트"},{"categories":null,"contents":"15.2 Resource Manager을 위한 Stack Template Stack Template   OCI 콘솔에서 내비게이션 메뉴의 Developer Services \u0026raquo; Resource Manager \u0026raquo; Stacks을 클릭합니다.\n  Create Stack 을 클릭합니다.\n  두번째 Template 옵션을 선택하고 Select Template을 클릭합니다.\n   제공하고 있는 템플릿을 볼 수 있습니다.\n  Quickstarts\n   Service\n   Architect\n   Private\n   지금인 비어 있지만, Private Template을 등록하는 작업을 통해 Tenancy 내에 공유할 템플릿을 활용할 수 있습니다.\n       Deploy to Oracle Cloud 앞으로 OCI 문서 및 OCI 예제 사이트에서 아래와 같은 버튼을 보게 될 것입니다.\n 그러면 OCI Console을 로그인후 자동으로 Stack 등록화면으로 이동하게 됩니다.\n Reference Solution Architecture OCI 상에 배포시 참조 모델을 위해 Reference 사이트를 제공하고 있습니다. 해당 사이트에서도 동일하게 링크을 통해 Reference 모델을 Stack Template 형태로 배포할 수 있게 지원하고 있습니다.\n  Oracle Cloud Infrastructure Architecture Center\n   ","lastmod":"2022-01-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter15/2/","tags":["resource manage","terraform"],"title":"15.2 Resource Manager을 위한 Stack Template"},{"categories":null,"contents":"14.1.2.2 CLI를 통한 파일 업로드  아래 페이지의 일부를 정리한 내용입니다. 전체 내용은 다음 링크를 참고하세요.  https://docs.cloud.oracle.com/iaas/Content/Object/Tasks/managingobjects.htm https://docs.oracle.com/en-us/iaas/tools/oci-cli/3.4.2/oci_cli_docs/cmdref/os/object/put.html      Object Storage Namespace을 조회합니다.\noci os ns get   Bucket상의 Object 목록 조회\noci os object list -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt;   실행예시\nPS D:\\\u0026gt; oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnzdbcmqu9s8\u0026#34; } PS D:\\\u0026gt; oci os object list -ns cnzdbcmqu9s8 -bn ExampleBucketForCLI { \u0026#34;prefixes\u0026#34;: [] }     Bucket에 Object 업로드\noci os object put -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; --file \u0026lt;file_location\u0026gt; --name \u0026lt;object_name\u0026gt; --no-multipart   Windows 실행예시\nPS D:\\ForObjectStorage\u0026gt; dir 디렉터리: D:\\ForObjectStorage Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 2022-01-12 오후 7:01 images -a---- 2022-01-12 오후 5:27 152 index.html PS D:\\ForObjectStorage\u0026gt; oci os object put -bn ExampleBucketForCLI --file index.html { \u0026#34;etag\u0026#34;: \u0026#34;4235d07c-1ef4-42db-b64a-4fb045de2fcd\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 02:42:23 GMT\u0026#34;, \u0026#34;opc-content-md5\u0026#34;: \u0026#34;B8Iehaa6j58u+qMiWnNldg==\u0026#34; } PS D:\\ForObjectStorage\u0026gt; oci os object list -ns cnzdbcmqu9s8 -bn ExampleBucketForCLI { \u0026#34;data\u0026#34;: [ { \u0026#34;archival-state\u0026#34;: null, \u0026#34;etag\u0026#34;: \u0026#34;4235d07c-1ef4-42db-b64a-4fb045de2fcd\u0026#34;, \u0026#34;md5\u0026#34;: \u0026#34;B8Iehaa6j58u+qMiWnNldg==\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;size\u0026#34;: 152, \u0026#34;storage-tier\u0026#34;: \u0026#34;Standard\u0026#34;, \u0026#34;time-created\u0026#34;: \u0026#34;2022-01-18T02:42:23.945000+00:00\u0026#34;, \u0026#34;time-modified\u0026#34;: \u0026#34;2022-01-18T02:42:23.945000+00:00\u0026#34; } ], \u0026#34;prefixes\u0026#34;: [] }     Bucket에 Object들 벌크 업로드\noci os object bulk-upload -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; --src-dir \u0026lt;source_directory_location\u0026gt; --no-multipart   Windows 실행예시\nPS D:\\ForObjectStorage\u0026gt; oci os object bulk-upload -bn ExampleBucketForCLI --src-dir . Uploaded images/icons8-oracle-96.png [####################################] 100% Uploaded index.html [####################################] 100% { \u0026#34;skipped-objects\u0026#34;: [], \u0026#34;upload-failures\u0026#34;: {}, \u0026#34;uploaded-objects\u0026#34;: { \u0026#34;images/icons8-oracle-96.png\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;388a319c-b5d0-4a51-bacf-9d9fe84bc621\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:05:04 GMT\u0026#34;, \u0026#34;opc-content-md5\u0026#34;: \u0026#34;HkJF80OO/DlmKI7usBCg4Q==\u0026#34; }, \u0026#34;index.html\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;883369a7-4701-4bd9-8351-3fbaada61fbe\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:05:05 GMT\u0026#34;, \u0026#34;opc-content-md5\u0026#34;: \u0026#34;B8Iehaa6j58u+qMiWnNldg==\u0026#34; } } }     결과 확인\n  업로드 소스 폴더\n   업로드 된 Objects\n폴더안의 파일은 그림처럼 업로드 되었습니다. 다만 cli로 업로드 된 파일은 application/octet-stream 타입임을 알 수 있습니다.\n      Multipart 업로드 큰 사이즈의 파일, 예를 들어 Custom Image 파일을 업로드할 때 업로드 시간이 많이 걸리게 됩니다. 이런경우 단일 파일을 여러 파일로 나누어 병렬로 업로드 하면 더 빠르게 업로드 할 수 있습니다. 또한 전송시 네트워크 에러 발생시 에러난 파트별도 재시도를 하여 보다 안정적으로 큰 파일을 업로드 할 수 있습니다.\noci os object put -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; --file \u0026lt;file_location\u0026gt; --name \u0026lt;object_name\u0026gt; --part-size \u0026lt;upload_part_size_in_MB\u0026gt; --parallel-upload-count \u0026lt;maximum_number_parallel_uploads\u0026gt;   Windows 실행예시\n2.28GB 파일을 100 MB 단위로 나누어서 동시 5개로 병렬로 업로드하는 예시로 3분 57초 걸림.\nPS D:\\ForObjectStorage\u0026gt; date 2022년 1월 18일 화요일 오후 12:25:22 PS D:\\ForObjectStorage\u0026gt; oci os object put -bn ExampleBucketForCLI --file big_file.iso --part-size 100 --parallel-upload-count 5 Upload ID: 3b38b958-3010-4d6d-5e84-55d189b787cc Split file into 24 parts for upload. Uploading object [####################################] 100% { \u0026#34;etag\u0026#34;: \u0026#34;2217a0e6-2ae3-4ae8-9507-03d2f36a38ec\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:29:18 GMT\u0026#34;, \u0026#34;opc-multipart-md5\u0026#34;: \u0026#34;mnHbvK3Eed2+o54P1P7VgA==-24\u0026#34; } PS D:\\ForObjectStorage\u0026gt; date 2022년 1월 18일 화요일 오후 12:29:19 PS D:\\ForObjectStorage\u0026gt;   결과확인\n   참고사항\nOCI CLI \u0026ndash;no-multipart 옵션 설명에서 처럼 128MiB이상이면, 옵션이 없어도 기본적으로 다중 분할하여 업로드 됩니다.\n By default, files above 128 MiB will be transferred in multiple parts, then combined.\n PS D:\\ForObjectStorage\u0026gt; oci os object put -bn ExampleBucketForCLI --file big_file.iso Upload ID: 75e4106e-bf1f-ffd0-5630-c78b68b6168b Split file into 19 parts for upload. Uploading object [####################################] 100% { \u0026#34;etag\u0026#34;: \u0026#34;cf5d681b-90a3-409c-b732-1e6c6028c046\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:37:46 GMT\u0026#34;, \u0026#34;opc-multipart-md5\u0026#34;: \u0026#34;eFqxsX7ROrZUQ0HrQYI/zw==-19\u0026#34; }   ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/2/2/","tags":["object storage","CLI","upload"],"title":"14.1.2.2 CLI를 통한 파일 업로드"},{"categories":null,"contents":"12.2 Monitoring Alarm 만들기 Compute 인스턴스에 설정한 모니터링 중에서 CPU 사용률이 지정한 수치에 이르면 메일을 발송하도록 알람을 생성해 보겠습니다.\n OCI 콘솔에서 내비게이션 메뉴의 Observability \u0026amp; Management \u0026raquo; Monitoring \u0026raquo; Alarm Definition에서 정의 자원 모니터링 메트릭의 옵션 메뉴에서 정의  위 두 가지 방법이 있지만, 본 문서에서는 선택한 모니터링 메트릭으로 Alarm의 메트릭이 자동으로 설정되어 사용이 쉬운 두 번째 방법으로 진행하겠습니다.\n메트릭에서 Alarm 만들기   앞서 설정한 Compute 인스턴스의 Resources \u0026raquo; Metrics 선택\n  CPU Utilization의 옵션에서 Create an Alarm on the Query 클릭   Alarm 정의 복잡한 Alarm 조건은 고급모드의 Query로 가능하며 아래는 기본모드로 UI에 선택하는 방식입니다.\n  Alarm 정의\n Alarm Name: 만들 Alarm 이름 입력 Alarm severity: 레벨을 선택, 일단은 기본값인 Critical 선택 Alarm body: Alarm 메시지로, 이후에 Email 전달할 메시지를 입력     Metric description\n Metric name: CpuUtilization을 선택합니다.     Metric dimensions 모니터링할 클라우드 자원의 범위를 지정합니다. 앞서 선택한 Compute 인스턴스가 자동으로 설정되어 있습니다.   Trigger rule\n알람을 설정할 조건을 지정합니다. 앞서 설정한 메트릭 기준(CPU 사용룔이 1분 평균)이 50% 이상일 때   Notification 정의   Notification 대상 선택\n알람이 기동할 때 통지할 Notification 서비스의 대상 Topic을 지정합니다. 이미 Topic이 있는 경우에 선택하면 되지만, 없는 경우 만들기 위해 Create a topic 클릭\n   대상 Topic 만들기\n   Destination으로 만든 Topic 선택\n   아래쪽 Save alarm 클릭\n  Notification - EMail 인증   앞서 Topic의 구독 프로토콜을 이메일로 정하면 대상 이메일로 인증 메일이 발송됩니다.\n  수신된 메일을 확인하고 인증 링크를 클릭합니다.\n   구독 확인 창\n   Notification - EMail Topic 상태 확인   앞서 생성한 CPU Alarm의 Notification 대상 클릭\n   앞서 만든 Topic 선택\n   이메일 구독이 Active 상태임을 확인\n   Alarm 발생 테스트   모니터링과 Alarm을 설정한 Compute 인스턴스에 SSH로 접속합니다.\n  stress 툴 설치\nsudo yum-config-manager --enable ol7_developer_EPEL sudo yum install -y stress   stress 수행\nsudo stress --cpu N --timeout N   실행 예시 사용할 Compute Instance의 CPU에 갯수에 맞춰 조정합니다.\n[opc@webserver1 ~]$ sudo stress --cpu 4 --timeout 60 stress: info: [12194] dispatching hogs: 4 cpu, 0 io, 0 vm, 0 hdd stress: info: [12194] successful run completed in 60s Alarm 발생 확인   만든 Alarm 아래쪽에 발생한 히스토리를 확인할 수 있습니다.\n   Alarm 메일 수신 확인\n   ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/2/","tags":["monitoring","alarm"],"title":"12.2 Monitoring Alarm 만들기"},{"categories":null,"contents":"12.3.1 Compute VM에 Grafana 설치 Compute Instance 만들기   Compute Instance 생성 화면으로 이동합니다.\n  이미지 소스를 Oracle Images 목록에 있는 Oracle Linux Cloud Developer Image 이미지로 선택합니다.Oracle Cloud Developer Image는 OCI CLI 등 개발툴이 이미 설치되어 있습니다.   Compute 인스턴스를 생성합니다.\n  생성된 Compute 인스턴스의 OCID를 복사해 둡니다.\n  OCI CLI - Instance Principal 설정 Compute 인스턴스에서 추가 인증없이 OCI API를 사용하기 위해서는 Instance Principal을 설정해야 합니다. 그 절차는 Compute 인스턴스를 Dynamic Group에 추가하고 해당 Group에 권한을 주는 방식입니다.\nDynamic Group 설정   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Dynamic Groups로 이동합니다.\n  Create Dynamic Group을 클릭합니다.\n  룰 작성 부분 오른쪽 Rule Builder를 클릭하여 툴을 통해 룰을 추가합니다.\n 매칭 기준: Instance OCID VALUE: 복사해둔 Compute 인스턴스의 OCID     NAME, DESCRIPTION을 입력하여 Dynamic Group을 생성합니다.\n Name: 예, MyInstancePrincipleDynamicGroup     Policy 설정   Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Policies로 이동합니다.\n  Create Policy을 클릭합니다.\n  다음 두 권한을 가지는 Policy를 생성합니다.\nallow dynamic-group \u0026lt;생성한 다이나믹 그룹\u0026gt; to read metrics in tenancy allow dynamic-group \u0026lt;생성한 다이나믹 그룹\u0026gt; to read compartments in tenancy   Policy 생성화면   OCI CLI 접속 확인   SSH로 앞서 생성한 Compute 인스턴스로 접속합니다.\n  다음명령으로 Instance Principal 방식으로 접속합니다. OCI CLI config 파일을 설정하지 않았는데 접속되는 것을 확인 할 수 있습니다.\noci os ns get --auth instance_principal   접속 예시\n[opc@mygrafana ~]$ oci os ns get --auth instance_principal { \u0026#34;data\u0026#34;: \u0026#34;thekoguryo\u0026#34; } [opc@mygrafana ~]$     Grafana 설치   Compute 인스턴스에 접속하여 다음 명령들을 순서대로 실행하여 Grafana를 설치합니다.\n 참고: https://grafana.com/grafana/download?edition=oss 2022년 1월 기준 8.3.3, 8.2.7이 설치후 접속 오류가 발생하여 8.1.8을 설치하여 진행하였습니다.  wget https://dl.grafana.com/oss/release/grafana-8.1.8-1.x86_64.rpm sudo yum install -y grafana-8.1.8-1.x86_64.rpm sudo systemctl start grafana-server sudo grafana-cli plugins install oci-metrics-datasource sudo firewall-cmd --permanent --add-port=3000/tcp sudo firewall-cmd --reload sudo systemctl restart grafana-server   실행예시\n[opc@grafana ~]$ wget https://dl.grafana.com/oss/release/grafana-8.1.8-1.x86_64.rpm --2022-01-17 04:40:23-- https://dl.grafana.com/oss/release/grafana-8.1.8-1.x86_64.rpm Resolving dl.grafana.com (dl.grafana.com)... 151.101.54.217, 2a04:4e42:d::729 Connecting to dl.grafana.com (dl.grafana.com)|151.101.54.217|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 56048174 (53M) [application/x-redhat-package-manager] Saving to: ‘grafana-8.1.8-1.x86_64.rpm’ grafana-8.1.8-1.x86_64.rpm 100%[===========================================================================\u0026gt;] 53.45M 15.9MB/s in 4.2s 2022-01-17 04:40:28 (12.6 MB/s) - ‘grafana-8.1.8-1.x86_64.rpm’ saved [56048174/56048174] [opc@grafana ~]$ sudo yum install grafana-8.1.8-1.x86_64.rpm Last metadata expiration check: 0:02:06 ago on Mon Jan 17 04:38:24 2022. Dependencies resolved. =========================================================================================================================================================== Package Architecture Version Repository Size =========================================================================================================================================================== Installing: grafana x86_64 8.1.8-1 @commandline 53 M Transaction Summary =========================================================================================================================================================== Install 1 Package Total size: 53 M Installed size: 178 M Downloading Packages: Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Preparing : 1/1 Installing : grafana-8.1.8-1.x86_64 1/1 Running scriptlet: grafana-8.1.8-1.x86_64 1/1 ### NOT starting on installation, please execute the following statements to configure grafana to start automatically using systemd sudo /bin/systemctl daemon-reload sudo /bin/systemctl enable grafana-server.service ### You can start grafana-server by executing sudo /bin/systemctl start grafana-server.service POSTTRANS: Running script Verifying : grafana-8.1.8-1.x86_64 1/1 Installed: grafana-8.1.8-1.x86_64 Complete! [opc@grafana ~]$ sudo systemctl start grafana-server [opc@grafana ~]$ sudo grafana-cli plugins install oci-metrics-datasource ✔ Downloaded oci-metrics-datasource v3.0.0 zip successfully Please restart Grafana after installing plugins. Refer to Grafana documentation for instructions if necessary. [opc@grafana ~]$ sudo firewall-cmd --permanent --add-port=3000/tcp success [opc@grafana ~]$ sudo firewall-cmd --reload success [opc@grafana ~]$ sudo systemctl restart grafana-server [opc@grafana ~]$     Grafana가 설치된 VM의 Security List의 Ingress에 3000번 포트를 개방합니다.\n  Grafana 로그인\n  Grafana가 설치된 Compute 인스턴스에 3000포트로 접속합니다. 기본 접속 계정은 admin/admin 입니다.\n참고 사이트  https://grafana.com/blog/2019/02/25/oracle-cloud-infrastructure-as-a-data-source-for-grafana/ https://blogs.oracle.com/cloudnative/data-source-grafana https://grafana.com/grafana/plugins/oci-metrics-datasource/ https://github.com/oracle/oci-grafana-metrics/blob/master/docs/linux.md  ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/3/1/","tags":["monitoring","grafana"],"title":"12.3.1 Compute VM에 Grafana 설치"},{"categories":null,"contents":"12.3.2 Grafana 구성 Data Source 등록   Grafana에 로그인 합니다.\n  왼쪽 메뉴 Configuration \u0026raquo; Data Sources로 이동하거나, 홈 대시보드에서 Add data source를 클릭합니다.   데이터 소스 중에서 Oracle Cloud Infrastructure를 선택   OCI 데이터 소스 설정\n Tenancy OCID: OCI Console에 로그인하여 Tenancy OCID를 확인후 입력 Default Region: 원하는 Region 선택 Environment: OCI Instance선택     Save \u0026amp; Test를 클릭하여 접속이 잘 되는지 확인합니다.\n  대시보드 만들기   왼쪽 메뉴 Dashboard \u0026raquo; Manage로 이동하거나, 홈 대시보드에서 New Dashboard를 클릭합니다.   빈 패널을 선택합니다.\n   왼쪽 메뉴에서 제일위의 Query 아이콘을 클릭합니다.\n  Query 항목에서 Data source를 앞서 추가한 Oracle Cloud Infrastructure Metrics를 선택하고 Region부터 Metric까지 순서대로 하나씩 설정합니다.   작성한 그래프를 저장합니다.\n  템플릿 기능 사용하기   오른쪽 위의 대시보드 설정 아이콘을 클릭합니다.\n   변수를 추가하기 위해 왼쪽에서 Variables를 선택하고 Add variable 클릭   새 변수를 추가합니다. Preview of values에서 보는 것 처럼 쿼리 결과를 변수로 조회해서 대시보드에서 사용할 수 있습니다.\n Name: region Query: regions()\n    아래 2개 변수를 추가합니다.\n   Name Query     region regions()   compartment compartments()      대쉬보드로 돌아 가면 위쪽에 선택항목으로 추가된 변수가 보입니다.\n   실제 조회 가능한 region과 compartment를 선택합니다. 그리고 대쉬보드 Setting에서 아래 변수도 추가합니다. region과 compartment를 선택하지 않으면, 아래 변수들은 연관 변수로 변수 추가시 최초 조회시 에러가 날 수 있습니다.\n   Name Query     namespace namespaces($region,$compartment)   resourcegroup resourcegroups($region, $compartment, $namespace)   metric metrics($region,$compartment, $namespace, $resourcegroup)      변수 추가 완료\n   대쉬보드를 저장합니다.\n  앞서 만든 그래프로 돌아갑니다. 처음 추가한 패널을 다시 Edit 합니다.\n  아래쪽 쿼리의 파라미터로 앞서 생성한 변수로 변경합니다. 예, Region: $region. 그러면 대시보드에서 위쪽에 보이는 드랍다운 리스트에서 값을 선택하면, 변수값이 변경되어 원하는 그래프로 조회됩니다. 드랍다운 리스트에서 metric은 모니터링되는 데이터가 없을 경우 리스트가 안 보일 수 있으니, 보이지 않더라도 오류가 아니니 오해하지 않기 바랍니다.   우측 상단 Apply를 누르고 대쉬보드를 저장합니다.\n  편집화면에서 대시보드 화면으로 이동합니다.\n  그림과 같이 대시보드에 조회할 수 있습니다.   참고 사이트  https://grafana.com/blog/2019/02/25/oracle-cloud-infrastructure-as-a-data-source-for-grafana/ https://blogs.oracle.com/cloudnative/data-source-grafana oci-grafana-metrics/using.md at master · oracle/oci-grafana-metrics (github.com)  ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/3/2/","tags":["monitoring","grafana"],"title":"12.3.2 Grafana 구성"},{"categories":null,"contents":"13.2 Autoscaling 구성 Autoscaling을 하기 위해서는 다음 사항이 필요합니다.\n Instance Pool: Autoscaling은 Instance Pool에서 현재 인스턴스 수를 설정된 메트릭에 따라 자동으로 변경하는 것으로 Instance Pool이 필요합니다. Monitoring 활성화: Compute 인스턴스는 모니터링하여 자원상태를 기반으로 하는 것으로 Monitoring이 활성화되어야 합니다. Service Limit: Autoscaling으로 새로운 인스턴스를 기동할 관련 자원이 충분해야 합니다.  Autoscaling 설정   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instance Pools 항목으로 이동합니다.\n  Autoscaling 할 Instance Pool의 상세화면에서 More Actions \u0026raquo; Create autoscaling configuration 클릭   Autoscaling Configuration\n  Name: 이름 입력, 예, autoscalingwebserver-autoscaling-config\n  INSTANCE POOL: 사용할 Instance Pool 선택, Instance Pool에서 Create Autoscaling Configuration로 이동했을 경우 자동으로 선택됨.\n     Autoscaling Policy\n Metric-based autoscaling: 메트릭 값에 따라 인스턴스의 갯수를 자동으로 조절합니다. 현재는 CPU, 메모리 사용량를 기반으로 설정 가능합니다. Schedule-based autoscaling: 설정한 스케줄에 따라 정의된 인스턴스 갯수로 변경하거나, 모든 인스턴스의 상태를 변경합니다.    메트릭 기반 오토스케일 설정\n  Autoscaling policy name: 이름 입력\n  Cooldown in seconds: Cooldown 시간, 기본 300초, Autoscaling은 Cooldown 기간동안 메트릭을 평가하여 Cooldown 시간이 지나면, 다시 Autoscaling, 즉 Instance Pool의 사이즈를 조정할 지 여부를 정합니다.\n  Performance metric: 기준 메트릭으로 현재는 CPU Utilization, Memory Utilization을 지원\n   Scale-out rule: 지정한 메트릭기반의 인스턴스를 늘리는 기준 설정\n  Scale-in rule: 지정한 메트릭기반의 인스턴스를 줄이는 기준 설정\n  Scaling limits: Scale In \u0026amp; Out시의 유지할 최소 인스턴스, 최대 인스턴스를 지정\n  테스트를 위해 1 인스턴스에서 CPU 부하 80% 초과 발생시 인스턴스 추가하고, CPU 부하 20% 미만시 다시 1개로 줄이는 정책으로 아래와 같이 설정합니다.\n     (참고) 스케줄 기반 오토스케일 설정\n Scale pool size: 설정한 스케줄에 따라 인스턴스 수를 지정할 수 있습니다. Change lifecycle state of all instances: 설정한 스케줄에 따라 인스턴스의 상태를 변경할 수 있습니다. 주말에 인스턴스 종료, 월요일 아침 시작 형태가 가능합니다. Another policy: 메트릭 기반과 달리 관리자가 지정한 스케줄에 따라 적용하는 것이기 때문에, 스케줄에 따른 여러 개의 규칙을 설정할 수 있습니다.     테스트 오토 스케일 생성완료\n   화면에서 보듯이 Edit를 클릭하여 생성 이후에도 규칙을 변경할 수 있습니다. 메트릭 기반 스케줄을 스케줄 기반 스케줄로 변경하는 것은 불가합니다.\n   ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter13/2/","tags":["autoscailing"],"title":"13.2 Autoscaling 구성"},{"categories":null,"contents":"8.2 FSS(File Storage Service)를 Linux VM에 마운트하여 사용하기  제약사항 File Storage는 Compute Instance와 같은 Subnet이라 하더라도, File Storage의 방화벽 개방을 위해서 반드시 Security List에 아래 규칙 설정이 필수 입니다.   Subnet 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; Virtual Cloud Networks 항목으로 이동합니다.\n  앞서 만든 VCN인 ExampleVCN 클릭\n  Create Subnets 클릭\n  생성정보 입력: 진한 글씨 항목만 입력하고 나머지는 기본값을 사용합니다.\n Name: 이름 입력, FSS Subnet Subnet Type: 기본값인 REGIONAL을 선택  REGIONAL: Region에 글로벌하게 있는 서브넷으로 다른 AD로 FailOver시에도 IP를 그대로 사용가능한 이점이 있음. AVAILIBILTY DOMAIN-SPECIFIC: AD내에 만들어지는 서브넷   CIDR Block: 10.0.2.0/24 Route Table: Default Route Table 선택 Subnet Access: Public Subnet 선택 DHCP Options: Default DHCP Options 선택    최하단으로 스크롤하여 Create Subnet 클릭\n  FSS Subnet 생성완료\n   File Storage Service용 Security List 만들기 ExampleVCN의 Public Subnet(10.0.0.0/24) 상의 VM에서 FFS Subnet(10.0.2.0/24) 상의 File Storage Service를 사용하도록 하기위해서는 관련 포트 개방이 필요합니다. 이를 위한 Security List를 만들겠습니다.\n  왼쪽 Resources \u0026raquo; Security Lists 클릭\n  Create Security List 클릭\n  생성정보 입력\n  Name: 이름 입력, FSS Security List\n  Allow Rules for Ingress: Public Subnet(10.0.0.0/24) 상의 VM 대상으로 열도록 아래 표 대로 입력\n   Stateless Source IP Protocol Source Port Range Destination Port Range     No 10.0.0.0/24 TCP All 2048-2050   No 10.0.0.0/24 TCP All 111   No 10.0.0.0/24 UDP All 2048   No 10.0.0.0/24 UDP All 111        생성완료\n   Subnet에 Security List 적용   앞서 만든 FSS Subnet의 상세 페이지로 이동합니다.\n  FSS Subnet의 Security List에 방금 만든 FSS Security를 추가합니다.\n   File System 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; File Storage \u0026raquo; File Systems 항목으로 이동합니다.\n  Create File System 클릭\n  생성정보 입력\n  기본값을 그대로 사용해도 되지만 편의상 이름을 변경합니다.\n  File System Information\n Name: 우측 Edit Details를 클릭하여 원하는 이름으로 변경, FSS-Storage 입력    Export Information : 기본값 사용\n  Mount Target Information\n New Mount Target Name: 우측 Edit Details를 클릭하여 원하는 이름으로 변경, FSS-Mount 입력 Virtual Cloud Network: ExampleVCN Subnet: FSS Subnet, 앞서 만든 File Storage를 위해 생성한 Subnet 선택       File System 마운트   생성된 File System의 아래쪽 Export의 우측 액션 메뉴를 클릭하고 Mount Commands 를 클릭합니다.   클라이언트에서 FSS를 Mount하기 위해 필요한 명령들을 확인합니다.\n이미지에서 OS를 선택합니다. 아래 그림은 Oracle Linux 기준입니다.\n   이전 실습에서 만든 ExampleVCN의 Public Subnet(10.0.0.0/24)상의 Compute Instance에 접속합니다.\n  앞서 OCI 콘솔에서 확인한 Mount Commands를 순서대로 실행합니다.\n  명령 예시\n# NFS 클라이언트 설치 sudo yum install nfs-utils # 마운트 폴더 생성 sudo mkdir -p /mnt/FSS-Storage # 마운트 sudo mount 10.0.2.105:/FSS-Storage /mnt/FSS-Storage   결과 확인 및 테스트\n# 마운트 결과 확인  df -h # 권한 변경 sudo chmod 777 /mnt/FSS-Storage # 파일 생성 확인 echo \u0026#39;Hello File Storage\u0026#39; \u0026gt; /mnt/FSS-Storage/hello1.txt ls -la /mnt/FSS-Storage/     결과 예시\nubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i mysshkey opc@152.67.220.47 Last login: Fri Jan 14 01:57:59 2022 from 223.62.21.154 [opc@examplelinuxinstance ~]$ # NFS 클라이언트 설치 [opc@examplelinuxinstance ~]$ sudo yum install nfs-utils Loaded plugins: langpacks, ulninfo Package 1:nfs-utils-1.3.0-0.68.0.1.el7.2.x86_64 already installed and latest version Nothing to do [opc@examplelinuxinstance ~]$ # 마운트 폴더 생성 [opc@examplelinuxinstance ~]$ sudo mkdir -p /mnt/FSS-Storage [opc@examplelinuxinstance ~]$ # 마운트 [opc@examplelinuxinstance ~]$ sudo mount 10.0.2.105:/FSS-Storage /mnt/FSS-Storage [opc@examplelinuxinstance ~]$ # 마운트 결과 확인 [opc@examplelinuxinstance ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 7.7G 0 7.7G 0% /dev tmpfs 7.7G 0 7.7G 0% /dev/shm tmpfs 7.7G 8.8M 7.7G 1% /run tmpfs 7.7G 0 7.7G 0% /sys/fs/cgroup /dev/sda3 39G 3.9G 35G 11% / /dev/sda1 200M 7.5M 193M 4% /boot/efi tmpfs 1.6G 0 1.6G 0% /run/user/0 tmpfs 1.6G 0 1.6G 0% /run/user/994 tmpfs 1.6G 0 1.6G 0% /run/user/1000 10.0.2.105:/FSS-Storage 8.0E 0 8.0E 0% /mnt/FSS-Storage [opc@examplelinuxinstance ~]$ # 파일 생성 확인 [opc@examplelinuxinstance ~]$ sudo chmod 777 /mnt/FSS-Storage [opc@examplelinuxinstance ~]# echo \u0026#39;Hello File Storage\u0026#39; \u0026gt; /mnt/FSS-Storage/hello1.txt [opc@examplelinuxinstance ~]# ls -la /mnt/FSS-Storage/ total 9 drwxr-xr-x. 2 root root 1 Jan 14 01:59 . drwxr-xr-x. 3 root root 25 Jan 14 01:59 .. drwxr-xr-x. 2 root root 0 Jan 14 01:59 .snapshot -rw-r--r--. 1 opc opc 19 Jan 14 01:59 hello1.txt   VM 재기동 후에도 자동으로 마운트 하려면 /etc/fstab를 업데이트합니다.\n# # /etc/fstab # Created by anaconda on Wed Dec 1 01:55:42 2021 ... 10.0.2.105:/FSS-Storage /mnt/FSS-Storage nfs defaults,nofail,nosuid,resvport 0 0   ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/2/","tags":["file storage","security list","subnet"],"title":"8.2 FSS를 Linux VM에 마운트하여 사용하기"},{"categories":null,"contents":"9.2 인스턴스에서 Custom Image 만들기  중요 이미지를 만드는 동안 인스턴스가 몇 분간 오프라인 상태가 됩니다. Custom Image를 만드는 동안 인스턴스를 STOP 시키는 것을 권장하며, 실행 중인 상태에서 진행하게 되면, 강제로 중지됩니다. 중지로 인해 데이터에 문제가 발생할 수 있습니다   Custom Image를 만들 대상 Instance에 Block Volume이 장착되어 있더라도, Custom Image를 만들게 되면, Boot Volume만 포함되어 있습니다.\nCustom Image 만들기   테스트 VM에 접속하여, Custom Image 만들기 전에 흔적을 만듭니다.\n[opc@examplelinuxinstance ~]$ echo \u0026#39;See you soon\u0026#39; \u0026gt; hello.txt [opc@examplelinuxinstance ~]$ cat hello.txt See you soon   Custom Image를 생성하기 전에 반드시 먼저 OS 레벨에서 종료합니다.\n[opc@examplelinuxinstance ~]$ sudo su [root@examplelinuxinstance opc]# shutdown now Connection to 140.238.29.108 closed by remote host. Connection to 140.238.29.108 closed.   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  Compute Instance 목록에서 대상 인스턴스를 클릭합니다.\n  OS 레벨에서 종료 여부를 다시 한번 확인한후 콘솔에서 Stop 명령으로 종료합니다.\n  Instance가 중지되면 Create custom image를 클릭합니다.\n   Custom Image를 저장할 Compartment를 선택하고, 이름을 입력 후 Create custom image를 클릭합니다.\n 이름: 예, ExampleLinuxCustomImage 아래 경고문에서 Custom Image 생성 전에 OS에서 종료 후, 인스턴스를 종료할 것을 권고하고 있습니다.     이미지 생성 중입니다.\n   인스턴스 목록 화면으로 돌아가 왼쪽 Custom Images메뉴로 가면 현재 Custom Image 목록을 확인할 수 있습니다.\n   Custom Image를 새 Instance 생성하기 방법 1   Custom Images 목록에서 원하는 이미지를 선택하고, 우측 액션 메뉴에서 Create Instance를 클릭하면, 해당 이미지를 기반으로 Instance 생성화면으로 이동합니다.\n   그 외 필요한 정보를 입력하고 인스턴스를 생성합니다.\n  방법 2  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다. Create Instance 클릭하여 인스턴스 생성을 시작합니다. Image and Shape에서 Change Image를 통해 이미지를 변경합니다. Image Sources에서 Custom Images를 선택합니다. Custom Image가 있는 Compartment를 선택하면 사용 가능한 Custom Images 목록이 보입니다. 사용할 Custom Image를 선택하고 Select Image을 클릭합니다.  그 외 필요한 정보를 입력하고 인스턴스를 생성합니다.  검증   Custom Image를 기준으로 인스턴스가 생성되었습니다.\n   생성된 인스턴스에 접속하여 이전에 작성한 파일이 있는 지 확인합니다.\nubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i mysshkey opc@129.154.219.239 Last login: Fri Jan 14 14:31:26 2022 from 220.117.236.6 [opc@examplelinuxinstance-fromcustomimage ~]$ ls hello.txt [opc@examplelinuxinstance-fromcustomimage ~]$ cat hello.txt See you soon   ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter09/2/","tags":["os image","custom image","image"],"title":"9.2 인스턴스에서 Custom Image 만들기"},{"categories":null,"contents":"7.2 Bucket에 파일 올리기 Object Storage는 단일 파일 기준 최대 50 GB까지 지원합니다. OCI Console의 메시지 기준으로는 2 GiB까지만 올릴 수 있습니다. 그 이상 더 큰 파일은 SDK 또는 API를 통해 올릴 수 있습니다.\nOCI Console에서 Bucket에 파일 올리기   Bucket 상세 정보를 보기 위해 이름을 클릭합니다.\n  Upload를 클릭합니다.\n   다이얼로그가 뜨면 파일을 드래그 앤 드랍하거나 브라우저 파일 업로그 기능을 통해 올릴 수 있습니다. select files를 클릭합니다.\n   원하는 파일을 선택합니다.\n   다이얼로그 화면에서 Upload를 클릭합니다.\n Object Name Prefix: Prefix를 입력하면 파일명 앞에 붙으며 아래와 같이 / 가 있으면 콘솔에서 폴더로 표현됩니다. Storage Tier: Bucket: 기본에서 필요하면 다른 값으로 변경합니다.     업로드 확인후 Close를 클릭합니다.\n  그림과 같이 파일이 업로드 되었고, Prefix에 /를 기준으로 폴더화 되었습니다. Object 우측 액션 메뉴를 통해 상세정보를 확인합니다.\n   상세정보 화면에서 URL 및 메타정보를 확인할 수 있습니다. 또한 [다운로드] 받을 수도 있습니다. URL 경로 포맷은 다음과 같습니다.\nhttps://objectstorage.\u0026lt;region_name\u0026gt;.oraclecloud.com/n/\u0026lt;object_storage_namespace\u0026gt;/b/\u0026lt;bucket\u0026gt;/o/\u0026lt;object_name\u0026gt;    생성된 Bucket은 기본적으로 Private Bucket으로 지금 바로 URL로 파일을 내려받지는 못합니다. 추가 설정 또는 인증을 통해 내려받을 수 있습니다.\n  ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/2/","tags":["object storage","bucket"],"title":"7.2 Bucket에 파일 올리기"},{"categories":null,"contents":"4.2 Reserved Public IP 할당하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  Instance 목록 중에서 고정 IP를 부여할 Instance의 이름을 클릭하여 상세정보로 이동합니다.\n  왼쪽 아래의 Resources \u0026raquo; Attached VNICs를 클릭합니다.\n연결된 가상 네트워크 인터페이스 카드(VNIC) 목록이 보이며, 최초 생성시 기본 생성된 Primary VNIC이 아래와 같이 하나 보입니다.\n   Primary VNIC의 이름을 클릭합니다.\n  아래 쪽에 보면 할당된 IP 주소가 보입니다. Public IP 주소를 보면 임시 주소인 Ephemeral Public IP이 부여된 걸 확인할 수 있습니다.\n   수정을 위해 우측 액션 메뉴에서 Edit를 클릭합니다.\n   \u0026ldquo;Not allowed until you unassign the existing public IP\u0026rdquo; 안내 문구처럼 예약한 Public IP를 할당하기 위해서는 현재 부여된 Ephemeral Public IP 할당 해지를 먼저 해야 합니다.위해 No public IP로 업데이트 합니다.\n   Public IP 주소가 할당 해지되었습니다. 수정을 위해 다시 우측 액션 메뉴에서 Edit를 클릭합니다.\n   Reserved public ip를 선택하고 앞서 생성한 Reserved Public IP를 목록에서 할당한 예약 IP를 선택하고 업데이트합니다.\n   Public IP 주소를 보면 지정한 Reserved Public IP로 부여된 걸 확인할 수 있습니다.\n   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; IP Management \u0026raquo; Reserved Public IPs 항목으로 이동합니다.\n  사용한 Reserved Public IP의 상태가 Assigned도 변경된 것을 확인할 수 있습니다.\n   ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter04/2/","tags":["reserved public ip"],"title":"4.2 Reserved Public IP 할당하기"},{"categories":null,"contents":"6.2 사용자 만들기  팁 OCI는 접속할 수 있는 사용자는 크게 IDCS 사용자와 OCI 사용자가 있습니다. OCI는 Identity Cloud Service와 기본적으로 연결설정이 되어 있어, IDCS의 사용자가 Federated 되어 로그인할 수도 있습니다. 그리고 IDCS는 등록되어 있지 않고, OCI에만 있는 자체 사용자도 있습니다. 여기서 내용은 OCI 자체 사용자만을 대상으로 합니다.   OCI 관리자 추가하기 Step 1. OCI 관리자 추가   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Users 항목으로 이동합니다.\n  사용자 추가를 위해 Create User 클릭\n  사용자 정보 입력\n  User Type: IAM User를 선택합니다.\n  Name: 사용자 이름 입력, Tenancy내에서 고유한 이름\n  Description: 설명 입력\n  Email: 이메일 입력, 암호 분실시 Forget Password 기능을 통해 암호 초기화를 위해 필요합니다.\n     Create 클릭\n  사용자 상세정보에서 아래쪽 Resources \u0026raquo; Groups 클릭\n  Add User to Group을 클릭하여 Administrators 그룹에 추가합니다.\n    사용자 상세정보에서 위쪽에 있는 Create/Reset Password를 클릭합니다.   다이얼로그가 뜨면 다시 Create/Reset Password를 한번 더 클릭합니다.\n  사용자를 위한 One Time Password가 만들어 졌습니다. IDCS 사용자는 IDCS Console에서 자동 이메일 발송기능을 제공하나 OCI 로컬 사용자에 대해서는 아직 OTP 메일 발송 기능을 제공하고 있지 않습니다. 패스워드를 사용자에게 이메일로 전달합니다.\n   Step 2. OCI 사용자 최초 로그인 하기   등록한 이메일로 검증 메일이 발송됩니다. 링크를 클릭하여 로그인합니다.\n   Oracle Cloud Infrastructure Direct Sign-In 으로 로그인합니다.\n   최초 로그인 후 패스워드를 변경합니다.\n   등록된 이메일이 활성화 되었습니다. 이제 로그인 화면의 Forget password에서 이메일을 입력하면 OTP를 메일로 수신할 수 있습니다.   OCI 일반 사용자 추가하기 Step 1. OCI 일반 사용자 추가   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; User 항목으로 이동합니다.\n  사용자 추가를 위해 Create User 클릭\n  사용자 정보 입력\n User Type: IAM User를 선택합니다. Name: 사용자 이름 입력, Tenancy내에서 고유한 이름 Description: 설명 입력 Email: 이메일 입력, 암호 분실시 Forget Password 기능을 통해 암호 초기화를 위해 필요합니다.     Create 클릭\n  관리자 생성과 동일하게 사용자 상세정보에서 위쪽에 있는 Create/Reset Password를 클릭하여 사용자를 위한 One Time Password를 생성합니다. IDCS 사용자는 IDCS Console에서 자동 이메일 발송기능을 제공하나 OCI 로컬 사용자에 대해서는 아직 OTP 메일 발송 기능을 제공하고 있지 않습니다. 패스워드를 사용자에게 이메일로 전달합니다.\n  Step 2. OCI 일반 사용자 최초 로그인 하기   관리자 사용자와 동일하게 이메일 검증과 패스워드는 관리자에게 전달받은 OTP를 이용해 로그인하는 과정을 수행합니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  왼쪽 아래 대상 Compartment 선택화면에서 앞서 생성한 root Compartment만 보이고 그외 Sandbox, Production 은 권한이 없어 보이지 않는 것을 알 수 있습니다.\n   root Compartment를 선택해도 추가 권한이 없어 Compute 인스턴스 조회조차 안되는 것을 알 수 있습니다.\n   ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/2/","tags":["oci user"],"title":"6.2 사용자 만들기"},{"categories":null,"contents":"3.2 가상 네트워크 환경을 위한 VCN 만들기 Virtual Cloud Network(VCN) 이란 Virtual Cloud Network(VCN)은 가상 네트워크 환경을 제공합니다. 사용자의 요구에 맞게 VCN 및 관련 자원을 구성하여 여러 가지 형태의 가상 네트워크 환경을 구성할 수 있습니다. Subnet은 VCN의 하위 요소로 단일 Availability Domain 단위로 만들어집니다. Subnet은 Route Table에 따라 트래픽은 대상 경로로 이동하며, 또한 Subnet은 Security List로 들어오고 나가는 트래픽을 제어할 수 있습니다.\nVCN 만들기  팁 VCN을 만들 때 두 가지 방법을 제공합니다. VCN만 만들기, VCN Wizard를 통해 VCN과 관련자원 함께 만들기를 제공합니다. 상세하게 구성하는 방법은 별도 문서를 참고하시고, 본 장에서는 VCN Wizard 통해 만들어진 자원을 바탕으로 빠르게 Compute Instance를 사용하고 그 개념을 이해하도록 합니다.     OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; Virtual Cloud Networks 항목으로 이동합니다.\n  왼쪽 아래 Compartment에서 Sandbox를 클릭합니다. 안 보일 경우 브라우저를 새로고침합니다.\n  Start VCN Wizard 클릭\n  Create VCN with Internet Connectivity을 선택하고 다시 한번 Start VCN Wizard 클릭\n 선택한 것은 그림에서 설명하는 것 처럼, Public, Private Subnet을 포함하고, 각각 Internat Gateway, NAT Gateway를 통해 인터넷과 통신합니다. 또한 Service Gateway를 OCI 서비스를 연동하기 위해 Oracle Services Network과 통신합니다.     생성 정보 입력\n  Basic Information\n Name: \u0026ldquo;ExampleVCN\u0026rdquo; 입력 Compartment: \u0026ldquo;Sandbox\u0026rdquo; Compartment 선택    Configure VCN and Subnets\n  아래 대역대로 기본 네트워크가 구성됩니다.\n  VCN CIDR Block: 10.0.0.0/16\n  Public Subnet CIDR Block: 10.0.0.0/24\n  Private Subnet CIDR Block: 10.0.1.0/24\n  DNS Resolution: 호스트 이름을 사용하는 기본값을 그대로 사용합니다.\n       다음으로 이동하여 값을 재확인 후 Create 클릭\n  아래와 같이 VCN 자원이 생성되는 것을 볼 수 있습니다.\n 앞서 설명한 내용외에 라우팅 테이블과 보안 리스트로 함께 만들어 집니다. 자세한 내용은 아래 View Virtual Cloud Network 버튼을 클릭하여, VCN으로 이동하여 확인할 수 있습니다.      팁 자동으로 생성되는 VCN 자원들은 OCI를 테스트해 보기 쉽게 하기 위해 설계된 것으로 실제 운영 환경에서 사용하기 위해서는 사용자 환경에 맞는 VCN 및 관련 네트워크 자원들을 구성하여 사용하기 바랍니다.   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/2/","tags":["virtual cloud network","VCN"],"title":"3.2 가상 네트워크 환경을 위한 VCN 만들기"},{"categories":null,"contents":"3.6.2 Block Volume을 Instance에 장착하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  앞서 생성한 대상 Instance의 이름을 클릭합니다.\n  Instance 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Attached block volumes을 클릭합니다.\n  Attached block volumes 버튼 클릭\n   다이얼 로그가 뜨면 다음을 선택합니다.\n  Volume: Compartment안에 있는 리스트에서 원하는 Block Volume 선택\n 앞서 만든 ExampleBlockVolume 선택 Device Path(옵션): 리스트에서 원하는 패스 선택 오라클 제공 Linux 계열 이미지를 사용하는 경우 장착될 경로를 지정할 수 있습니다. 인스턴스가 재시작하더라도 경로 변경되지 않도록 하기 위해 추가된 기능입니다.     Attachement type: ISCSI, Paravirtualized 방식 중에 선택 가능하나, OCI가 자동으로 선택되는 기본 옵션 그대로 사용\n  Access: 단일 VM에서 사용할 것이므로 기본 Read/write 선택\n Read/write: 단일 VM에 부착하여 읽기/쓰기 가능 Read/write - shareable: 다중 VM에서 사용가능하나, 사용할 VM에서 추가적인 모듈을 설치해야 함. Read only - shareable: 다중 VM에서 사용가능하나, 읽기만 가능. 추가 모듈 설치 필요없음       선택한 설정으로 Attach 합니다.\n  경고문구 확인\nBlock Volume이 VM에 부착되었지만, VM내부 OS 상의 작업이 필요합니다. 다음 장에서 이어 작업합니다.\n   완료되면 ATTACHED 상태로 되며, Instance에 장착이 완료되었습니다.\n   Access - Read/write - shareable 참고   공유 읽기/쓰기 Block Volume은 이후 다시 확인하기로 하고 콘솔에서 보이는 내용을 참고합니다.\n   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/2/","tags":["block volume"],"title":"3.6.2 Block Volume을 Instance에 장착하기"},{"categories":null,"contents":"3.7.2 백업으로 새 Volume 만들기 Block Volume 백업본을 이용해 새로운 Block Volume을 생성하는 복구방식입니다. 복구된 Block Volume은 새로운 Block Volume을 장착하는 것과 같은 방법으로 Compute Instance에 장착하게 사용하면 됩니다.\n백업으로 새 Block Volume 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Block Storage \u0026raquo; Block Volumes Backups 항목으로 이동합니다.\n  전체 백업 목록을 확인할 수 있습니다.\n  원하는 백업의 우측 액션 메뉴에서 Restore Block Volume을 클릭합니다.\n   생성정보 입력화면은 처음 Block Volume 생성할 때랑 동일합니다. 생성정보를 입력합니다.\n  Name: 원하는 이름 입력\n  Create in Compartment: 현재 사용중인 Compartment가 기본으로 보입니다. 원하는 Compartment을 선택합니다.\n  Availability Domain: Compute Instance가 속한 Availability Domain을 선택합니다. Volume과 Instance는 반드시 같은 AD여야 합니다.\n  Size: 이전 사이즈와 동일, 필요시 증설\n   Backup Policies: 선택 안함.\n  Cross Region Replication: Free Tier는 단일 Region 이므로 여기서는 OFF\n  Encryption: 지금은 OCI 제공 키 사용\n    Restore Block Volume 클릭\n  생성이 완료되면 AVAILBLE 상태로 표시됩니다.\n  복구된 Block Volume은 이제 다른 Block Volume과 동일한 Block Volume입니다. Block Volume을 장착하는 것과 같은 방법으로 장착해서 사용하면 됩니다.\n  ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/2/","tags":["block volume","backup"],"title":"3.7.2 백업으로 새 Volume 만들기"},{"categories":null,"contents":"3.8.2 백업으로 새 Boot Volume 만들기 Boot Volume 백업 복구 기능은 Block Volume 백업 복구 기능과 거의 같습니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Boot Volume 상세 페이지로 이동합니다.\n  Boot Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Boot Volume Backups를 클릭합니다.\n  원하는 백업의 우측 액션 메뉴에서 Restore Boot Volume을 클릭합니다.\n   생성정보 입력화면은 Block Volume 생성할 때랑 동일합니다. 생성정보를 입력합니다.\n  Name: 원하는 이름 입력\n  Create in Compartment: 현재 사용중인 Compartment가 기본으로 보입니다. 원하는 Compartment을 선택합니다.\n  Availability Domain: Compute Instance가 속한 Availability Domain을 선택합니다. Volume과 Instance는 반드시 같은 AD여야 합니다.\n  Size: 이전 사이즈와 동일, 필요시 증설\n   Backup Policies: 선택 안함.\n  Cross Region Replication: Free Tier는 단일 Region 이므로 여기서는 OFF\n  Encryption: 지금은 OCI 제공 키 사용\n    Restore Block Volume 클릭\n  생성이 완료되면 AVAILBLE 상태로 표시됩니다.\n  복구된 Boot Volume은 이제 다른 Boot Volume과 동일한 Boot Volume입니다. 인스턴스 생성시 사용할 수 있습니다.\n  ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/8/2/","tags":["boot volume","backup"],"title":"3.8.2 백업으로 새 Boot Volume 만들기"},{"categories":null,"contents":"2.2 OCI Console 접속하기 방법 #1. Welcome 이메일을 통한 접속   Free Tier Account 준비 완료 메일 수신되면 Oracle Cloud에 사인인을 클릭하여 OCI Cloud Console에 접속합니다.\n   로그인 화면에서 Free Tier 신청시 입력한 관리자 이메일과 암호를 입력하여 로그인합니다.\n   로그인 성공\n   방법 #2. 브라우저로 직접 접속   Oracle Cloud에 로그인을 위해 브라우저를 통해 https://cloud.oracle.com으로 접속합니다.\n   방법 #1: Oracle Identity Cloud Service 로그인\n  Oracle Identity Cloud Service로 선택합니다.\n   로그인 화면에서 Free Tier 신청시 입력한 관리자 이메일과 암호를 입력하여 로그인합니다.\n   로그인 완료\n 로그인 성공후 프로파일을 확인하면 oracleidentitycloudservice/ 까지가 계정 이름인것을 알 수 있습니다.       방법 #2: OCI Direct Sign In\n  아래쪽 OCI 직접 로그인 옵션으로 로그인합니다.\n   로그인 완료\n 로그인 성공후 프로파일을 확인하면 관리자 이름이 순수 계정 이름인것을 알 수 있습니다.       ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/2/","tags":["oci console"],"title":"2.2 OCI Console 접속하기"},{"categories":null,"contents":"6.2 Open Application Model 이해 및 애플리케이션 배포하기 참고\n https://verrazzano.io/latest/docs/guides/app-deployment/application-deployment-guide/  Open Application Model Open Application Model(OAM)은 컨테이너 또는 오케스트레이터 또는 인프라 보다는 개발자가 애플리케이션에 집중할 수 있도록 하고자 하는 모델입니다. 애플리케이션을 쿠버네티스상의 컨테이너로 배포하기 위해서 배포 외에도 모니터링, 트래픽 처리 등 추가 설정을 위해 여러가지 자원들을 배포해야 하고 그에 따른 YAML 파일들을 정의해야 합니다. 이 부분을 개발자는 애플리케이션에 초점을 맞추고, 운영, 관리 등을 위해 필요한 많은 부분은 템플릿 등을 통해 표준화하는 것을 지향하고 있습니다. Open Application Model은 현재 0.3 까지 나왔으며, Verrazzano에서는 Open Application Model 0.2.1을 기반으로 하고 있어 최신 버전과 약간의 문법 차이가 있습니다.\n주요 컨셉   https://verrazzano.io/latest/docs/concepts/verrazzanooam/\n   Component: 하나의 애플리케이션, 쉽게 컨테이너 앱라고 생각하면 될 것 같습니다. 이 컴포넌트에는 유형이 있을 수 있고, Verrazzano에서는 Helidon 프레임워크를 통해 개발한 앱이냐, WebLogic 앱이냐, Coherence 서버냐 등이 있을 것이고, 그 유형에 따라 배포시나 모니터링시의 특성이 있을 것입니다. 그래서 Component에서 어떤 Workload 타입인지를 지정하게 됩니다.\n  Workload: 어떤 타입인지 표시하는 것으로, WorkloadDefinition을 통해 정의할 수 있으며, VerrazzanoHelidonWorkload, VerrazzanoWebLogicWorkload 등 사전에 Verrazzano에서 정의한 유형 또는 Deployment, ConfigMap 등등 Kubernetes의 기본 자원형태을 그대로 사용 할 도 있습니다.\n  Application Configuration: 하나의 애플리케이션을 뜻하여, 여러 Component를 포함하게 됩니다. 하나의 컨테이너 앱에 관련된 설정을 가진 형태이거나, 여러 컨터이너를 Component로 가진 패키징된 앱의 형태일 수도 있습니다.\n  Trait: 하나의 Component에 운영, 모니터링 등의 필요에 의해 추가 설정하는 사항입니다. 사이트의 문서 예제에서 보면, 메트릭 수집 설정, Ingress 설정 등이 일반적인 예로 쓰입니다.\n  Scope: 여러 Component에 대한 한번에 설정하는 사항입니다.\n  Helidon 예제 애플리케이션 배포하기 Helidon 프레임워크를 사용해서 개발한 Java 애플리케이션을 배포하는 예제를 통해 Open Application Model을 사용해서 Verrazzano에서 배포하는 것을 확인해 봅니다. Java로 개발하여 컨테이너 이미지까지 빌드하는 방법은 어렵지 않으니, 그 이후에 Verrazzano 배포하는 부분을 확인해 봅니다.\n예제 애플리케이션: Helidon 샘플로, REST API로 /greet 로 요청하면, 환영 메시지가 오는 간단한 컨테이너 애플리케이션\nComponent 작성   Verrazzano에서 사용하는 Kubernetes CRD인 Component를 정의합니다.\n spec.workload: Component의 Workload를 정의합니다. spec.workload.kind: 일반유형이 아닌 Helidon 기반 컨테이너를 위해 Verrazzano가 사전에 정의한 VerrazzanoHelidonWorkload Workload Definition을 선택 spec.workload.spec: VerrazzanoHelidonWorkload의 스펙으로 Kubernetes Deployment 스펙을 그대로 사용  https://verrazzano.io/latest/docs/reference/api/oam/workloads/#verrazzanohelidonworkload    apiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:hello-helidon-componentnamespace:hello-helidonspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoHelidonWorkloadmetadata:name:hello-helidon-workloadlabels:app:hello-helidonspec:deploymentTemplate:metadata:name:hello-helidon-deploymentpodSpec:containers:- name:hello-helidon-containerimage:\u0026#34;ghcr.io/verrazzano/example-helidon-greet-app-v1:0.1.10-3-20201016220428-56fb4d4\u0026#34;ports:- containerPort:8080name:http  Application Configuration 작성   Verrazzano에서 사용하는 Kubernetes CRD인 Application Configuration을 정의합니다.\n spec.components.componentName: 방금 정의한 Component인 hello-helidon-component을 뜻합니다. spec.components 아래에 여러 Component가 추가될 수 있는 것을 알 수 있습니다. spec.components.traits.trait: 대상 Component에 운영, 관리상 추가할 부분을 정의합니다. 현재 IngressTrait, LoggingTrait, MetricsTrait를 제공하고 있습니다.  https://verrazzano.io/latest/docs/reference/api/oam/ingresstrait/    apiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:hello-helidon-appconfnamespace:hello-helidonannotations:version:v1.0.0description:\u0026#34;Hello Helidon application\u0026#34;spec:components:- componentName:hello-helidon-componenttraits:- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:MetricsTraitspec:scraper:verrazzano-system/vmi-system-prometheus-0- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitmetadata:name:hello-helidon-ingressspec:rules:- paths:- path:\u0026#34;/greet\u0026#34;pathType:Prefix  애플리케이션 배포   namespace 생성\nkubectl create namespace hello-helidon kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   Component 배포\nkubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-comp.yaml   Application Configuration 배포\nkubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-app.yaml   배포된 앱의 istio를 통해 등록된 ingress DNS 확인\nkubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39;   실행예시\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ \u0026gt; -n hello-helidon \\ \u0026gt; -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39; hello-helidon-appconf.hello-helidon.myenv.thekoguryo.ml   애플리케이션 테스트\n확인된 https://{ingress DNS 주소}/greet 주소로 정상 호출되는 것을 알 수 있습니다.\n   모니터링 Verrazzano 콘솔   Verrazzano Console에 로그인합니다.\n예, https://verrazzano.myenv.thekoguryo.ml\n 접속 유저: verrazzano 암호: 설치시 초기화한 암호    왼쪽 Resources 항목에서 Application과 Component 항목에서 배포된 앱 정보를 확인할 수 있습니다.\n   로그 모니터링(Elasticsearch / Kibana)   Verrazzano 콘솔에서 Kibana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  내비게이션 메뉴에서 Stack Management를 선택합니다.\n   Index Pattern \u0026gt; + Create index pattern 을 클릭합니다.\n  가능한 패턴에 예제앱을 위해 추가한 namespace를 포함하여 여러 소스가 보입니다.\n   표현식을 사용해도 되지만, 여기서는 예제 namespace 이름을 패턴 이름으로 입력합니다. 예, verrazzano-namespace-hello-helidon\n   Time field로 @timestamp 선택\n   추가완료\n   내비게이션 메뉴에서 Kibana \u0026gt; Discover 메뉴로 이동합니다.\n  생성한 인덱스 패턴에 대해 수집된 로그, 대상 namespace 상의 전체 수집된 로그가 보입니다.\n   필터 적용후 정확하게 로그를 확인할 수 있습니다.\n$ kubectl logs -n hello-helidon hello-helidon-deployment-c4d7859-nqt7l -c hello-helidon-container ... 2021.12.29 03:22:44 INFO io.helidon.microprofile.server.ServerCdiExtension Thread[main,5,main]: Server started on http://localhost:8080 (and all other host addresses) in 3075 milliseconds (since JVM startup). http://localhost:8080/greet 2021.12.29 03:22:44 INFO io.helidon.common.HelidonFeatures Thread[features-thread,5,main]: Helidon MP 2.3.2 features: [CDI, Config, Fault Tolerance, Health, JAX-RS, Metrics, Open API, REST Client, Security, Server, Tracing]    메트릭 모니터링(Prometheus / Grafana)   Verrazzano 콘솔에서 Grafana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  내비게이션 메뉴에서 Dashboard \u0026gt; Manage를 선택합니다.\n  기본 제공하고 있는 대쉬보드 중에 Helidon 대쉬보드를 선택합니다.\n   Helidon 대쉬보드에서 앞서 배포한 앱의 상태를 확인할 수 있습니다.\n   서비스 메쉬 모니터링(Kiali)   Verrazzano 콘솔에서 Kiali 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  왼쪽 메뉴에서 Graph를 클릭합니다.\n  대상 Namespace를 선택하면, 아래와 같이 상호 호출 관계를 볼 수 있습니다. 단일 서비스로 구성된 예제로 간단히 보입니다.\n   추가 참고사항   오픈소스 클라이언트 툴인 octant를 ApplicationConfiguration과 관계되 자원을 확인하였습니다.\n ApplicationConfiguration과 Component에서 직접 명시한 내용 자원에 추가하여 Deployment(하위 포함), Service, istio Gateway, Virtual Service가 배포되는 것을 확인할 수 있습니다.     ","lastmod":"2021-12-28T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/verrazzano/2.deploy-app/","tags":["oke","verrazzano"],"title":"6.2 Open Application Model 이해 및 애플리케이션 배포하기"},{"categories":null,"contents":"4.4.2 마이크로서비스 앱에 Service Mesh - Istio 적용하기 istioctl, Helm 또는 매뉴얼로 설치가 가능합니다. 여기서는 OKE 문서에서 예시로 설명하고 있는 istioctl 기준으로 설치하고 Istio 문서에 따라 서비스에 적용하는 것을 확인해보겠습니다.\n 공식 문서  https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengistio-intro-topic.htm https://istio.io/latest/docs/examples/microservices-istio/add-istio/    Istio 설치   Cloud Shell 또는 작업환경에 접속합니다.\n  Istio 다운로드\ncurl -L https://istio.io/downloadIstio | sh -   Istio 경로로 이동\ncd istio-1.12.1   PATH 환경 변수에 추가\nexport PATH=$PWD/bin:$PATH   사전검증 실행\nistioctl x precheck   설치\nistioctl install --set profile=demo   스케일\npod\u0026rsquo;s disruption budget 기본 1로 설정되어 있는 업그레이드를 고려하여 2개 이상으로 스케일합니다.\nkubectl scale --replicas=2 deployment -n istio-system istio-egressgateway kubectl scale --replicas=2 deployment -n istio-system istio-ingressgateway kubectl scale --replicas=2 deployment -n istio-system istiod   Istio Enable productpage 서비스 Istio Enable   Istio sidecar 추가하여 재배포\nistioctl kube-inject 명령으로 productpage 앱에만 sidecar를 추가합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | sed \u0026#39;s/replicas: 1/replicas: 3/g\u0026#39; | kubectl apply -l app=productpage,version=v1 -f -   결과 확인\nproduct Pod내에 istio-proxy 컨테이너가 추가된 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-79f774bdb9-29z48 1/1 Running 0 89m details-v1-79f774bdb9-d5lfq 1/1 Running 0 89m details-v1-79f774bdb9-sqp9p 1/1 Running 0 89m productpage-v1-c6885474-6g754 2/2 Running 0 16s productpage-v1-c6885474-r9l8l 2/2 Running 0 19s productpage-v1-c6885474-sz9mf 2/2 Running 0 14s ratings-v1-b6994bb9-6xlq7 1/1 Running 0 89m ratings-v1-b6994bb9-j2l78 1/1 Running 0 89m ratings-v1-b6994bb9-w748t 1/1 Running 0 89m reviews-v2-7bf8c9648f-4rh4z 1/1 Running 0 7m16s reviews-v2-7bf8c9648f-6vhlq 1/1 Running 0 7m18s reviews-v2-7bf8c9648f-8zjcw 1/1 Running 0 7m16s sleep-557747455f-jxk5z 1/1 Running 0 12m oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe pod productpage-v1-c6885474-6g75 Name: productpage-v1-c6885474-6g754 Namespace: default ... Containers: productpage: Container ID: cri-o://95f68b2fcb5e8168879e1a5703bf520a94729664cc410edbe0d3255113344149 Image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2 ... istio-proxy: Container ID: cri-o://74f2707caa99b8b12b3e293a129f74234c5ac1ab36729f70045de640eeb2b4b4 Image: docker.io/istio/proxyv2:1.12.1 ...   앞서와 동일하게 productpage 페이지를 접속합니다.\n http://{productpage external ip}:9080/productpage    istio-proxy 로그 확인\n웹페이지가 이전과 동일하게 접속이 되고 추가적으로 istio-proxy를 통해서 거쳐가는 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs -l app=productpage -c istio-proxy -f ... [2021-12-20T09:16:54.826Z] \u0026#34;GET /details/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 178 2 2 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u0026#34; \u0026#34;adeb5dbb-261c-9982-9d66-523cc3890e2b\u0026#34; \u0026#34;details:9080\u0026#34; \u0026#34;10.244.2.43:9080\u0026#34; outbound|9080||details.default.svc.cluster.local 10.244.2.149:41122 10.96.175.211:9080 10.244.2.149:55324 - default [2021-12-20T09:16:54.842Z] \u0026#34;GET /reviews/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 379 591 591 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u0026#34; \u0026#34;adeb5dbb-261c-9982-9d66-523cc3890e2b\u0026#34; \u0026#34;reviews:9080\u0026#34; \u0026#34;10.244.2.51:9080\u0026#34; outbound|9080||reviews.default.svc.cluster.local 10.244.2.149:45512 10.96.197.86:9080 10.244.2.149:35752 - default [2021-12-20T09:16:54.820Z] \u0026#34;GET /productpage HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 5183 616 615 \u0026#34;10.179.87.76\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\u0026#34; \u0026#34;adeb5dbb-261c-9982-9d66-523cc3890e2b\u0026#34; \u0026#34;146.56.142.95:9080\u0026#34; \u0026#34;10.244.2.149:9080\u0026#34; inbound|9080|| 127.0.0.6:41867 10.244.2.149:9080 10.179.87.76:0 - default   전체 서비스 Istio Enable   다음 명령을 통해 Review v2와 나머지 서비스들에도 Istio를 활성화합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app!=reviews -f - curl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v2 -f -   실행결과\nIstio-proxy가 추가 되어 Pod당 컨테이너가 2개로 보이면, bookinfo.yaml이 재적용되어 replica=1임을 참고합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-5c6d488c49-ltw8r 2/2 Running 0 69s productpage-v1-c6885474-6g754 2/2 Running 0 15h ratings-v1-7dfc8d4bbf-p8n2f 2/2 Running 0 68s reviews-v2-5ff75858c7-zhn57 2/2 Running 0 64s sleep-557747455f-jxk5z 1/1 Running 0 16h   참고\n아래와 같이 istio-injection 레이블을 달면 해당 네임스페이스에 배포되는 pod는 자동으로 istio sidecar가 추가됩니다.\nkubectl label namespace \u0026lt;NAMESPACE_NAME\u0026gt; istio-injection=enabled   Istio Ingress Gateway 설정 앞서 다른 장에서 Ingress Controller로 nginx ingress controller를 사용하였습니다. 마이크로 서비스들에 대한 컨트롤을 위해 서비스들에 대해서 Istio의 Ingress Gateway를 사용하도록 설정합니다.\n  참고문서\n https://istio.io/latest/docs/examples/microservices-istio/istio-ingress-gateway/    Gateway: 서비스 메쉬로 들어오고 나가는 트래픽에 대한 로드 밸런서 정보를 기술합니다.\n  VirtualService: 트래픽에 대한 라우팅 규칙을 지정합니다.\n    아래 내용으로 예제인 book-info에 대한 Istio Ingress Gateway를 생성합니다.\nkubectl apply -f - \u0026lt;\u0026lt;EOFapiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:bookinfo-gatewayspec:selector:istio:ingressgateway# use Istio default gateway implementationservers:- port:number:80name:httpprotocol:HTTPhosts:- \u0026#34;*\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:bookinfospec:hosts:- \u0026#34;*\u0026#34;gateways:- bookinfo-gatewayhttp:- match:- uri:exact:/productpage- uri:exact:/login- uri:exact:/logout- uri:prefix:/staticroute:- destination:host:productpageport:number:9080EOF  Istio Ingress Gateway를 통한 접속을 위해 IP를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.96.59.144 \u0026lt;none\u0026gt; 80/TCP,443/TCP 66m istio-ingressgateway LoadBalancer 10.96.150.51 146.56.186.52 15021:32250/TCP,80:32707/TCP,443:32572/TCP,31400:32532/TCP,15443:31522/TCP 66m istiod ClusterIP 10.96.188.42 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 66m   접속 확인\n 예시, http://146.56.186.52:80/productpage     Metric 모니터링   Prometheus \u0026amp; Grafana 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/grafana.yaml   설치된 Grafana 대쉬보드를 외부에서 접근\n 옵션 #1:  istioctl dashboard grafana 명령으로 로컬 프락시를 통해 접속할 수 있습니다. Cloud Shell는 Public IP 및 외부 접속을 허용하지 않아, 외부 접속이 필요하면 관련 설정이 필요합니다.   옵션 #2:  Istio Ingress Gateway를 통한 외부 접속을 설정할 수 있습니다. 설정을 통해 외부 접속 설정- https://istio.io/latest/docs/tasks/observability/gateways/      공식 문서를 참고하여 외부 접속을 설정합니다.\n https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http    외부 접속 설정 예시\n  설정을 위한 도메인 설정\n  Istio Ingress Gateway의 External IP를 확인하여, DNS 서버에 설정하거나, 클라이언트의 /etc/hosts 파일의 설정합니다. 사용할 도메인 주소를 입력합니다.\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.ml\u0026#34;     외부 접속 오픈\n  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:grafana-gatewaynamespace:istio-systemspec:selector:istio:ingressgatewayservers:- port:number:80name:http-grafanaprotocol:HTTPhosts:- \u0026#34;grafana.${INGRESS_DOMAIN}\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:grafana-vsnamespace:istio-systemspec:hosts:- \u0026#34;grafana.${INGRESS_DOMAIN}\u0026#34;gateways:- grafana-gatewayhttp:- route:- destination:host:grafanaport:number:3000---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:grafananamespace:istio-systemspec:host:grafanatrafficPolicy:tls:mode:DISABLE---EOF  테스트 데이타 발생\n productpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다.  테스트 앱 페이지 예시, http://146.56.186.52:80/productpage      Grafana 접속\n  예, http://grafana.istio.thekoguryo.ml\n  설정한 주소로 접속하면 Istio에서 제공하고 있는 기본 대쉬보드를 확인할 수 있습니다.\n    Grafana 대쉬보드를 보면 각 서비스의 처리 청보를 확인할 수 있습니다.     분산 추적(Distributed Tracing) 마이크로 서비스는 클라이언트의 요청을 분산된 서비스를 통해 처리한 후 클라이언트에게 응답합니다. 예시로 사용하고 있는 Bookinfo 앱도 productpage 서비스가 뒷단 서비스에서 취합한 정보를 클라이언트에게 제공하고 있습니다. 이러한 분산환경에서 클라이언트의 요청에 대한 서비스 들간의 호출 정보를 추적하기 위한 여러가지 툴들이 있습니다. Istio에서는 Envoy의 분산 추적에 대한 기능을 활용하여, Jaeger, Zipkin, LightStep 등의 툴을 활용할 수 있습니다. 여기서는 Zipkin을 통한 방법을 확인해 보겠습니다. 그외 툴들은 관련 페이지를 참고하세요.\n https://istio.io/latest/docs/tasks/observability/distributed-tracing/overview/  Zipkin   Zipkin 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/extras/zipkin.yaml   설치된 Zipkin 페이지를 외부에서 접근\n Grafana와 동일하게 옵션 #2: 외부 접속을 사용합니다. 공식 문서를 참고하여 외부 접속을 설정합니다.  https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http      외부 접속 설정 예시\n  설정을 위한 도메인 설정\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.ml\u0026#34;   외부 접속 오픈\n  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:tracing-gatewaynamespace:istio-systemspec:selector:istio:ingressgatewayservers:- port:number:80name:http-tracingprotocol:HTTPhosts:- \u0026#34;tracing.${INGRESS_DOMAIN}\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:tracing-vsnamespace:istio-systemspec:hosts:- \u0026#34;tracing.${INGRESS_DOMAIN}\u0026#34;gateways:- tracing-gatewayhttp:- route:- destination:host:tracingport:number:80---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:tracingnamespace:istio-systemspec:host:tracingtrafficPolicy:tls:mode:DISABLE---EOF  테스트 데이타 발생\n productpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다.  테스트 앱 페이지 예시, http://146.56.186.52:80/productpage      Zipkin 페이지 접속합니다.\n 예, http://tracing.istio.thekogury.ml    서비스 추적 테스트\n Find a trace 화면에서 빨간 플러스 기호를 클릭하여 검색할 서비스를 설정하고 쿼리를 실행합니다.     조회 결과를 확인하고 추적할 건에 대해서 오른쪽 SHOW를 클릭합니다.\n   해당 요청건에 대한 서비스간 호출 관계, 시간 및 각 서비스에 대한 태그 정보를 볼 수 있습니다.\n     Trace ID로 추적\n  각 요청건에 대해서 Span ID(Trace ID)를 통해 추적할 수 있습니다. 로그에서 확인할 수 있습니다.\n   화면 오른쪽 상단의 Trace ID 검색창을 통해서 바로 검색하면, 해당 요청건을 바로 확인할 수 있습니다.\n     서비스 메쉬 시각화 마이크로 서비스는 클라이언트의 요청을 분산된 서비스를 통해 처리한 후 클라이언트에게 응답합니다. 예시로 사용하고 있는 Bookinfo 앱도 productpage 서비스가 뒷단 서비스에서 취합한 정보를 클라이언트에게 제공하고 있습니다. 이러한 분산환경에서 클라이언트의 요청에 대한 서비스 들간의 호출 정보를 시각화를 Kiali를 통해 제공하고 있습니다.\n https://istio.io/latest/docs/tasks/observability/kiali/    Kiali 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.12/samples/addons/kiali.yaml   설치된 Kiali 페이지를 외부에서 접근\n Grafana와 동일하게 옵션 #2: 외부 접속을 사용합니다. 공식 문서를 참고하여 외부 접속을 설정합니다.  https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http      외부 접속 설정 예시\n  설정을 위한 도메인 설정\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.ml\u0026#34;   외부 접속 오픈\n  cat \u0026lt;\u0026lt;EOF | kubectl apply -f -apiVersion:networking.istio.io/v1alpha3kind:Gatewaymetadata:name:kiali-gatewaynamespace:istio-systemspec:selector:istio:ingressgatewayservers:- port:number:80name:http-kialiprotocol:HTTPhosts:- \u0026#34;kiali.${INGRESS_DOMAIN}\u0026#34;---apiVersion:networking.istio.io/v1alpha3kind:VirtualServicemetadata:name:kiali-vsnamespace:istio-systemspec:hosts:- \u0026#34;kiali.${INGRESS_DOMAIN}\u0026#34;gateways:- kiali-gatewayhttp:- route:- destination:host:kialiport:number:20001---apiVersion:networking.istio.io/v1alpha3kind:DestinationRulemetadata:name:kialinamespace:istio-systemspec:host:kialitrafficPolicy:tls:mode:DISABLE---EOF  테스트 데이타 발생\n productpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다.  테스트 앱 페이지 예시, http://146.56.186.52:80/productpage      Kiali 대쉬보드 접속합니다.\n 예, http://kiali.istio.thekoguryo.ml    내비게이션 메뉴에서 Graph를 클릭하면, 서비스 간의 호출 정보를 시각화해서 볼 수 있습니다.\n   Reviews 새 버전 배포 Reviews v3 배포   Reviews v3 버전을 배포합니다.\nIstio가 활성화 된 상태에서 버전 업데이트를 가정하여 v3를 배포합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.12/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v3 -f -   배포 결과 확인\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE ... reviews-v2-5ff75858c7-hwrgv 2/2 Running 0 5h15m reviews-v3-5484f6cbd6-685b6 2/2 Running 0 3m8s ...   productpage의 Istio Ingress Gateway를 통해 접속하여 테스트합니다.\n 테스트 앱 페이지 예시, http://146.56.186.52:80/productpage Reviews v2과 Reviews v3는 각각 Pod 하나로 분배규칙을 따라 설정하지 않았기 때문에, 한번씩 라우팅됩니다. Reviews v2(검은 별점)과 Reviews v3(빨간 별점)이 한번시 표시되는 것을 볼수 있습니다.     가중치 기반 라우팅 신규 버전의 완전한 서비스 전에 검증을 위해 가중치 기반의 일부 요청만 라우팅되도록 하는 시나리오를 확인 해 봅니다.\n  배포된 Reviews Pod의 버전 레이블을 확인합니다. 각각 version=v2, version=v2가 할당되어 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -L version NAME READY STATUS RESTARTS AGE VERSION details-v1-5c6d488c49-m5t7j 2/2 Running 0 5h28m v1 productpage-v1-c6885474-qkpx9 2/2 Running 0 5h28m v1 ratings-v1-7dfc8d4bbf-97p6z 2/2 Running 0 5h28m v1 reviews-v2-5ff75858c7-hwrgv 2/2 Running 0 5h28m v2 reviews-v3-5484f6cbd6-685b6 2/2 Running 0 15m v3 sleep-557747455f-jxk5z 1/1 Running 0 22h   레이블 기준으로 라우팅을 위해 Istio에 필요한 아래 설정을 배포합니다.\n DestinationRule: 서비스 엔드포인트에 대해서 labels로 필터링. 테스트 예제에서는 Service Type이 아닌, Pod에만 version label이 있음. Pod의 label로 하는 것 같음 VirtualService: 정의한 destination에 대해서 weight 기준으로 분배함. weight의 총합은 100이어야함. 100번 중에 90번, 10번 이렇게 정확한 분배가 아닌, 확률로 추측됨.  kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 weight: 90 - destination: host: reviews subset: v3 weight: 10 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 EOF   아래와 같이 반복 테스트합니다.\ncounter=1; \\ while [ $counter -lt 20 ]; \\ do curl -s http://146.56.186.52/productpage | grep color | head -1; counter=$(( $counter + 1 )); \\ done   테스트 결과\n20중에 18번 검은색 별표, 2번 빨간색 별표임을 알수 있습니다. 확률로 정확이 딱 안 맞을 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ counter=1; \\ \u0026gt; while [ $counter -lt 20 ]; \\ \u0026gt; do \u0026gt; curl -s http://146.56.186.52/productpage | grep color | head -1; \u0026gt; counter=$(( $counter + 1 )); \\ \u0026gt; done \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt;   Kiali에 호출 흐름 확인\n아래그림에서 보듯이 Reviews v2, v3에 설정한 가중치에 맞게 분배되는 것을 볼 수 있습니다.\n   ","lastmod":"2021-12-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/service-mesh/2.sampleapp-with-istio/","tags":["oss","service mesh","istio"],"title":"4.4.2 마이크로서비스 앱에 Service Mesh - Istio 적용하기"},{"categories":null,"contents":"4.1.2 NGINX Ingress Controller에서 host 기반 라우팅(feat. OCI DNS) Ingress Controller에서 도메인 네임을 기반하여 라우팅하기 위해 OCI DNS를 사용하는 방법을 확인합니다.\nOCI DNS 서비스 사용하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 freenom 사이트에서 발급받은 무료 Domain Name(thekoguryo.ml)을 사용하였습니다.\nOCI DNS 서비스 설정   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Networking \u0026gt; DNS Management \u0026gt; Zones로 이동합니다.\n  Create Zone 클릭\n   생성정보 입력\n사용한 Domain Name을 바탕으로 Zone을 생성합니다.\n  METHOD: Manual\n  ZONE NAME: 가지고 있는 Domain Name 입력\n  COMPARTMENT: 생성할 대상 Compartment\n  ZONE TYPE: Primary\n     Zone 내부에 NS 유형과 SOA 유형의 레코드가 생성되어 있습니다. NS는 네임 서버 레코드, SOA는 권한 시작 레코드입니다. Add Record를 클릭합니다.   추가할 레코드를 입력하고 제출합니다.\n  Record Type: A - IPv4 Address\n  NAME: *.ingress\n 와일드 카드 형식으로 ingress controller가 사용할 서브 Domain Name을 입력합니다.    TTL: 300, 우측 자물쇠는 클릭하여 잠금 해제 후 TTL 값 입력\n  RDATA MODE: Basic\n  ADDRESS: 매핑할 IP, 여기서는 앞서 만든 nginx ingress controller의 Load Balancer의 IP 입력\n     반영하기 위해 Publish Change 클릭\n  확인 창이 뜨면 한번 더 Publish Change 클릭\n   레코드 추가 및 반영 완료\n   레코드 중 NS 유형인 네임서버 주소를 모두 복사합니다.\n  Domain Name 제공 측에 설정 이제 Domain Name을 구입한 사이트에서 설정이 필요합니다. 아래 과정은 freenom 기준 설정입니다. 구입한 사이트에서 비슷한 방식으로 설정합니다.\n  freenom 사이트에 접속하여 My Domain에서 사용할 도메인 네임 우측의 Manage Domain 클릭   위쪽 Management Tools \u0026gt; Nameservers를 선택한 다음 **Use custom nameservers (enter below)**를 선택합니다.\n  앞서 OCI DNS Zone 에서 복사해둔 네임서버 주소를 차례대로 입력한 후 Change Nameservers 클릭   DNS 테스트   nslookup 툴로 등록한 DNS를 테스트 해봅니다. 잘 등록된 것을 알 수 있습니다.\nC:\\\u0026gt;nslookup *.ingress.thekoguryo.ml 서버: kns.kornet.net Address: 168.126.63.1 권한 없는 응답: 이름: *.ingress.thekoguryo.ml Address: 132.226.225.240   HOST 기반 라우팅 테스트 HOST 이름에 따라 라우팅 서비스를 달리하는 경우입니다.\n  테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(host-basic.yaml)을 작성합니다.\n blue.ingress.thekoguryo.ml 요청은 nginx-blue-svc 로 라우팅 green.ingress.thekoguryo.ml 요청은 nginx-green-svc로 라우팅  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-host-basicannotations:kubernetes.io/ingress.class:nginxspec:rules:- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  작성한 host-basic.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f host-basic.yaml ingress.networking.k8s.io/ingress-host-basic created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-host-basic \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80 6s   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n  blue.ingress.thekoguryo.ml 요청\n   green.ingress.thekoguryo.ml 요청\n   와일드 카드 주소로 DNS에 등록한 Ingress Controller의 Load Balancer를 거쳐 접속한 host의 FQDN에 따라 대상 서비스에 라우팅 되는 것을 확인할 수 있습니다.\n    ","lastmod":"2021-12-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/ingress-controller/2.nginx-ingress-host/","tags":["oss","ingress-controller"],"title":"4.1.2 NGINX Ingress Controller에서 host 기반 라우팅(feat. OCI DNS)"},{"categories":null,"contents":"3.2 Helm Chart Repostory로 사용하기 OCIR은OCI(Open Container Initiative) Registry로 Helm 3에서는 현재 OCI Registry를 실험적(experimental)인 기능으로 지원하고 있습니다.\n https://helm.sh/docs/topics/registries/  Helm CLI 환경 준비   helm cli를 사용할 Cloud Shell 또는 작업환경에 접속합니다.\n  helm cli 3.7 설치\nOCI Registry인 OCIR에 차트를 등록하기 위해서는 helm 3.7에서 제공하는 helm push 명령을 사용해야 원활히 됩니다. 문서 작성일 기준으로 현재 Cloud Shell에 기본 설치된 helm cli 버전은 3.5.4 여서 3.7을 추가 설치합니다.\nwget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz tar -zxvf helm-v3.7.1-linux-amd64.tar.gz linux-amd64/helm mv linux-amd64/helm ~/.local/bin/   .bashrc의 PATH에 등록\nPATH=$HOME/.local/bin:$HOME/bin:$PATH   OCI Support 활성화\nhelm cli에서 OCI Registry 지원은 실험적 기능으로 사용을 위해 다음 환경변수를 설정이 필요합니다.\nexport HELM_EXPERIMENTAL_OCI=1   Helm Chart 생성후 등록하기 샘플 차트 만들기 Helm Chart Template Guide 예제를 따라 만든 샘플 차트를\nOCIR 등록 해봅니다.\n  테스트를 위해 차트를 만듭니다.\noke_admin@cloudshell:helm (ap-seoul-1)$ helm create mychart Creating mychart   차트 작성\n생성된 차트는 nginx를 배포하는 샘플 차트입니다. 실제 차트 작성을 위해서는 앱에 맞게 수정하겠지만, 지금은 배포 테스트로 수정없이 그냥 사용합니다.\n  차트 패키징\nhelm package 명령으로 패키징합니다.\noke_admin@cloudshell:helm (ap-seoul-1)$ cd mychart oke_admin@cloudshell:mychart (ap-seoul-1)$ helm package . Successfully packaged chart and saved it to: /home/oke_admin/works/helm/mychart/mychart-0.1.0.tgz oke_admin@cloudshell:mychart (ap-seoul-1)$ ls charts Chart.yaml mychart-0.1.0.tgz templates values.yaml   OCIR 로그인 및 Helm Chart Push OCIR에 docker cli로 로그인 할때와 동일하게 사용자와 Auth Token을 사용해 로그인합니다. 이전 내용을 참고합니다.\n  앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 helm cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:mychart (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:mychart (ap-seoul-1)$ helm registry login -u cnrlxx3w0wgq/oke-admin ap-seoul-1.ocir.io Password: Login Succeeded   Helm Chart Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식 Push 하면 됩니다. 그러면 repo-prefix/ 을 포함하여 repository 가 생성됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-prefix\u0026gt;   mychart 예시  oke_admin@cloudshell:mychart (ap-seoul-1)$ helm push ./mychart-0.1.0.tgz oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm Pushed: ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart:0.1.0 Digest: sha256:842486615ee4398001092a9b2f931320add0ecd13885e039427ce6f7926b493b   OCIR 확인\nOCI 서비스 콘솔에서 OCIR을 보면 Push한 차트가 정상적으로 등록된 것을 알 수 있습니다. 예제는 편의상 root compartment로 push 하였습니다.\n 하위 compartment로 push하는 경우 사전에 repository를 만들어야 합니다. 예, helm/mychart repository를 push 전에 만들것     Helm Chart를 OKE 클러스터에 배포하기   Cloud Shell 또는 작업 환경에 접속합니다.\n  등록한 Chart로 배포합니다.\nhelm install mychart oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart --version 0.1.0   배포 예시\noke_admin@cloudshell:mychart (ap-seoul-1)$ helm install mychart oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart --version 0.1.0 --set service.type=LoadBalancer NAME: mychart LAST DEPLOYED: Fri Dec 3 15:17:51 2021 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running \u0026#39;kubectl get --namespace default svc -w mychart\u0026#39; export SERVICE_IP=$(kubectl get svc --namespace default mychart --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) echo http://$SERVICE_IP:80 oke_admin@cloudshell:mychart (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/mychart-76677cc888-rl4v6 1/1 Running 0 55s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 32m service/mychart LoadBalancer 10.96.198.136 146.56.152.14 80:30926/TCP 56s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mychart 1/1 1 1 56s NAME DESIRED CURRENT READY AGE replicaset.apps/mychart-76677cc888 1 1 1 55s   배포 앱 접속 확인\n   ","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/ocir/2.helm-chart/","tags":["container registry","helm chart"],"title":"3.2 Helm Chart Repostory로 사용하기"},{"categories":null,"contents":"2.1 DevOps 서비스를 이용한 Spring Boot 앱을 OKE에 배포 자동화하기 DevOps 서비스 사용을 위한 IAM Policy 설정 DevOps 서비스를 사용하기 위해서는 DevOps 자원들에 권한 설정이 필요합니다. 공식 문서를 참조하여 권한 설정을 위한 Dynamic Group 및 Group에 대한 Policy를 설정합니다.\n https://docs.oracle.com/en-us/iaas/Content/devops/using/devops_iampolicies.htm#build_policies  아래 Dynamic Group 및 Policy는 위 문서의 예제를 기준으로 작성한 내용으로 요구사항에 따라 일부 변경이 될 수 있습니다.\nDynamic Group 만들기 주어진 Compartment 내에서 DevOps 서비스를 사용할 수 있도록 Compartment에 대한 Dynamic Group을 먼저 생성합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\n  DevOps를 사용할 Compartment로 이동하여 OCID를 복사해 둡니다.\n  Identity \u0026gt; Dynamic Groups로 이동합니다.\n  Create Dynamic Group을 클릭합니다.\n  복사해둔 Compartment OCID를 이용해 필요한 Dynamic Group을 만듭니다.\n  CoderepoDynamicGroup\nALL {resource.type = \u0026#39;devopsrepository\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   ConnectionDynamicGroup\nALL {resource.type = \u0026#39;devopsconnection\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   BuildDynamicGroup\nALL {resource.type = \u0026#39;devopsbuildpipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   DeployDynamicGroup\nAll {resource.type = \u0026#39;devopsdeploypipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}     DevOps 서비스를 위한 Policy 설정하기   Identity \u0026gt; Policies로 이동합니다.\n  Create Policy을 클릭하여 새 Policy를 만듭니다.\n  Compartment 레벨로 다음 Policy를 만듭니다.\n Name: 예, DevOps-compartment-policy  Allow dynamic-group CoderepoDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage repos in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to read secret-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage generic-artifacts in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to use ons-topics in compartment \u0026lt;YourCompartmentName\u0026gt;   Root Compartment 레벨로 다음 Policy를 만듭니다.\n Name: 예, DevOps-root-policy  Allow dynamic-group ConnectionDynamicGroup to read secret-family in tenancy Allow dynamic-group DeployDynamicGroup to manage all-resources in tenancy Allow dynamic-group BuildDynamicGroup to inspect repos in tenancy Allow dynamic-group BuildDynamicGroup to use repos in tenancy OCIR에 Repository를 Push하기 전에 미리 생성하지 않으면 기본적으로 Root Compartment에 이미지가 Push됩니다. 이때 권한으로 에러가 발생하며, Root Compartment에도 허용하고자 하면 다음을 추가합니다.\nAllow dynamic-group BuildDynamicGroup to manage repos in tenancy   DevOps 서비스를 통한 CI/CD 배포 자동화 하기 DevOps 프로젝트 만들기 Notification Topic 만들기 DevOps 파이프 라인 실행이 발생하는 주요 이벤트를 알려주기 위한 용도로 Notification Topic 설정이 필요합니다. DevOps 프로젝트 생성시 필수 요구 사항이라 미리 만듭니다\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Application Integration \u0026gt; Notifications으로 이동합니다.\n  Create Topic을 클릭하여 Topic을 생성합니다.\n Name: 예, oke-labs-devops-topic     Notification을 위해 생성한 Topic 이벤트를 가져갈 Subscrition을 일단 생략합니다. 필요시 구성하시면 됩니다.\n  DevOps 프로젝트 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; DevOps로 이동합니다.\n  프로젝트 생성을 위해 Projects로 이동하여 Create DevOps project를 클릭합니다.\n  생성 정보를 입력하여 프로젝트를 만듭니다.\n Project name: 예, my-devops-project Notification Topic: 앞서 생성한 Topic 선택     프로젝트 생성완료\n   Enable Logging 프로젝트 생성 직후 Enable Logging 관련 정보가 보이는 것을 볼 수 있습니다. 설명문에서 보는 것 처럼 Logging을 활성화하지 않을 경우, 파이프라인 실행 화면에서 오른쪽에 보이는 실행 로그가 안보입니다. 그래서 Enable Logging은 필수입니다.\n   Project Overview에서 Enable Log을 클릭하거나 왼쪽 메뉴에서 Logs를 클릭합니다.\n   로그를 활성화 버튼을 토글합니다.\n   대상 Compartment에 이미 Log Group이 있는 경우 나열된 것 중에 선택이 가능합니다. 미리 생성된 Log Group이 없는 경우 아래와 같이 자동입력된 정보를 바탕으로 Enable Log 버튼 클릭시 새로 Log Group과 Log가 만들어 지게 됩니다. 필요시 설정을 수정하고 그렇치 않으면, Enable Log 버튼을 클릭합니다.\n   애플리케이션 배포 자동화 하기 Code Repository를 사용하여 애플리케이션 코드 관리하기 샘플로 Spring Boot Helloword 앱을 만들어 테스트하겠습니다.\n  코드 저장소 생성을 위해 왼쪽 메뉴에서 Code Repositories를 클릭합니다.\n  Create repository를 클릭하여 저장소를 만듭니다.\n Repository name: 예, spring-boot-hello-repo     생성된 코드 저장소 입니다. 일반적인 Git Repository입니다.\n   실제 개발 작업은 git 명령을 통해 개발 PC에서 진행하면 됩니다. 저장소 상세정보 위에 있는 Clone 버튼을 하면 Clone 명령어가 아래 그림처럼 뜨게 됩니다. 여기서는 Clone with HTTPS를 사용하겠습니다.\n   개발 PC에 복사한 주소를 사용해 git clone 명령어를 통해 복제합니다.\ngit clone \u0026lt;YourClonewithHTTPS URL\u0026gt;   이때 사용자 인증이 필요합니다. HTTPS기반 사용자 인증시 아래 유저명 형식과 AuthToken을 사용합니다.\n  인증 유저명\n Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-name\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-name\u0026gt;/\u0026lt;username\u0026gt; 이전 가이드들과 달리 tenancy-namespace가 아닌 tenacy-name인 것에 주의합니다.    AuthToken: 생성에 대한 내용은 이전 가이드들을 참고합니다.\n  Code Repository의 HTTPS 인증관련 문서\n https://docs.oracle.com/en-us/iaas/Content/devops/using/clone_repo.htm#https_auth    예시\n  $ git clone https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com/namespaces/cnrlxx3w0wgq/projects/my-devops-project/repositories/spring-boot-hello-repo Cloning into \u0026#39;spring-boot-hello-repo\u0026#39;... Username for \u0026#39;https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: thekoguryo/oke-developer Password for \u0026#39;https://oreozz/oke-admin@devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: remote: Counting objects: 2, done remote: Finding sources: 100% (2/2) remote: Getting sizes: 100% (1/1) Unpacking objects: 100% (2/2), done. remote: Total 2 (delta 0), reused 2 (delta 0)   현재 복제된 저장소는 비어 있습니다. 아래 가이드를 통해 spring-boot-hello 샘플 코드를 작성합니다.\n https://spring.io/guides/gs/spring-boot-docker/    작성된 코드를 git 명령어를 통해서 Code Repository에 저장합니다.\n  예시\ngit add . git commit -m \u0026#34;init\u0026#34; git push     코드 작성 및 반영 완료\n   Build Pipeline 만들기 CI/CD 중에 코드를 빌드하여 배포 산출물을 만드는 CI 과정에 해당되는 부분을 Build Pipeline을 통해 구성이 가능합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Build Pipelines로 이동합니다.\n  Create build pipeline을 클릭하여 파이프라인을 생성합니다.\n  Name: 예, spring-boot-hello-build-pipeline\n     생성된 파이프라인을 클릭합니다.\n  그림과 같이 Stage를 추가하여 파이프라인 흐름을 구성할 수 있습니다. Add Stage를 클릭합니다.\n   제공 Stage\n Managed Build: 빌드스펙에 정의된 내용에 따라 빌드 과정을 실행합니다. Delivery Artifacts: 빌드 산출물(예, 컨테이너 이미지)를 Artifact Repository에 저장합니다. Trigger Deployment: 빌드가 끝나고 Deployment Pipeline을 호출합니다. Wait: 일정시간 대기합니다.     Build Stage 만들기   빌드를 위해 먼저 Managed Build Stage를 추가합니다.\n  Managed Build Stage 설정\n Stage name: 예, build-stage Build Spec File Path: 빌드 스크립트 경로를 지정합니다. 기본적으로 소스 루트에 있는 build_spec.yaml을 파일을 사용합니다. Primary Code Repository: 빌드할 메인 소스가 있는 코드 저장소를 지정합니다.     Primary Code Repository 설정 화면\n 대상 소스 코드가 있는 저장소를 지정합니다.       설정된 Stage를 Add를 클릭하여 추가합니다.\n  아래 그림과 같이 build-stage가 추가되었습니다. Start Manual Run을 클릭하면 테스트를 해 볼수 있습니다.\n   테스트처럼 소스 코드상의 Build Spec의 정의가 필요합니다.\n   Build Spec은 다음 문서를 참조합니다.\n  https://docs.oracle.com/en-us/iaas/Content/devops/using/build_specs.htm\n  문서에 있는 Example 2 기준 예시\n steps: 실행할 스크립트를 정의하는 부분입니다. 예제이는 Build Source, Dockerizer 2개의 step이 정의되어 있고 각각 command에서 실행할 스크립트를 정의하고 있습니다. 정의된 순서대로 실행됩니다. env.exportedVariables: 전역으로 선언된 환경변수로 이전 step에서 값을 변경하면 그다음 step에도 적용됩니다. Deployment Pipeline을 호출시에도 전달됩니다. outputArtifacts: 빌드 산출물의 정의하는 부분으로, 이후 Delivery Artifact Stage를 통해 Artifact Storage에 저장할 때 여기서 정의된 이름을 통해 지정 가능합니다.  version:0.1component:buildtimeoutInSeconds:6000shell:bashenv:exportedVariables:- BuildServiceDemoVersionsteps:- type:Commandname:\u0026#34;Build Source\u0026#34;timeoutInSeconds:4000command:|echo $PATH mvn clean install- type:CommandtimeoutInSeconds:400name:\u0026#34;Dockerizer\u0026#34;command:|BuildServiceDemoVersion=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` echo $BuildServiceDemoVersion docker build -t build-service-demo .outputArtifacts:- name:build-service-demotype:DOCKER_IMAGElocation:build-service-demo- name:build-service-demo-kube-manifesttype:BINARYlocation:deployment/app.yml    Build Spec 정의\n  개발한 spring-boot-hello 소스 코드의 root 경로에 build_spec.yaml을 다음과 같이 정의하고 코드 저장소에 저장합니다.\n  build_spec.yaml\nversion:0.1component:buildtimeoutInSeconds:6000shell:bashenv:variables:appName:\u0026#34;spring-boot-hello\u0026#34;exportedVariables:- APP_NAME- OCIR_PATH- TAGsteps:- type:Commandname:\u0026#34;Init exportedVariables\u0026#34;timeoutInSeconds:4000command:|APP_NAME=$appName echo $APP_NAME- type:Commandname:\u0026#34;Build Source\u0026#34;timeoutInSeconds:4000command:|echo \u0026#34;build\u0026#34; mvn clean install- type:CommandtimeoutInSeconds:400name:\u0026#34;Build Source - Post\u0026#34;command:|echo \u0026#34;add dependency\u0026#34; mkdir -p target/dependency \u0026amp;\u0026amp; (cd target/dependency; jar -xf ../*.jar)- type:Commandname:\u0026#34;Define Image Tag - Commit ID\u0026#34;timeoutInSeconds:30command:|COMMIT_ID=`echo ${OCI_TRIGGER_COMMIT_HASH} | cut -c 1-7` BUILDRUN_HASH=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` [ -z \u0026#34;$COMMIT_ID\u0026#34; ] \u0026amp;\u0026amp; TAG=$BUILDRUN_HASH || TAG=$COMMIT_ID- type:Commandname:\u0026#34;Define OCIR Path\u0026#34;timeoutInSeconds:30command:|TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` REPO_NAME=$appName OCIR_PATH=$OCI_RESOURCE_PRINCIPAL_REGION.ocir.io/$TENANCY_NAMESPACE/$REPO_NAME- type:CommandtimeoutInSeconds:400name:\u0026#34;Containerize\u0026#34;command:|docker build -t new-generated-image . docker images- type:Commandname:\u0026#34;Check exportedVariables\u0026#34;timeoutInSeconds:30command:| [-z \u0026#34;$APP_NAME\u0026#34; ] \u0026amp;\u0026amp; APP_NAME=unknown [-z \u0026#34;$OCIR_PATH\u0026#34; ] \u0026amp;\u0026amp; OCIR_PATH=unknown [-z \u0026#34;$TAG\u0026#34; ] \u0026amp;\u0026amp; TAG=unknownecho \u0026#34;APP_NAME: \u0026#34; $APP_NAME echo \u0026#34;OCIR_PATH: \u0026#34; $OCIR_PATHecho \u0026#34;TAG: \u0026#34; $TAGoutputArtifacts:- name:output-imagetype:DOCKER_IMAGElocation:new-generated-image     Start Manual Run을 통해 다시 실행하면 아래와 같이 스크립트가 수행되는 것을 볼 수 있습니다.\n   ExportVariables 확인\n실행 결과 화면에서 오른쪽 위쪽 점3개를 클릭하여 상세 화면으로 이동하면 Build Output에서 실행결과로 나온 변수값을 볼 수 있습니다. 이 변수들은 이후 Stage 또는 연결되어 호출된 Deployment Pipeline으로 전달되어 사용할 수 있게 됩니다.\n   컨테이너 이미지 OCIR 등록 Stage 만들기   Build Pipeline 탭으로 이동합니다.\n  플러스 버튼을 클릭하여 build-stage 다음에 stage를 추가합니다.\n   Delivery Artifact Stage를 선택합니다.\n  stage 이름을 입력하고 Create Artifact를 선택합니다.\n   Container image 유형으로 Artifact 추가합니다.\n 이미지 경로: docker tag를 달때 사용하는 이미지 경로입니다. 직접 입력해도 되지만 여기서는 build-stage에서 넘어온 exportedVariable을 사용하여 ${OCIR_PATH}:${TAG} 과 같이 입력합니다.     같은 방식으로 하나 더 추가 합니다.\n Name: generated_image_with_latest Image Path: ${OCIR_PATH}:latest    Artifact 매핑\n  Associate Artifact에서 방금 추가한 2개의 Artifact에 실제 컨테이너 이미지 파일을 매핑해 줍니다. 앞서 build-stage에서 build_spec.yaml에서 정의한 outputArtifacts의 이름을 입력합니다.\noutputArtifacts:- name:output-imagetype:DOCKER_IMAGElocation:new-generated-image      이제 delivery stage까지 추가하였습니다.\n  파이프라인을 다시 실행해 봅니다. 실제 소스코드로 빌드된 컨테이너 이미지가 OCIR에 자동으로 등록됩니다.\n   Deploy Pipeline 만들기 CI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\n  Create pipeline을 클릭하여 파이프라인을 생성합니다.\n  Name: 예, spring-boot-hello-deployment-pipeline\n     생성된 파이프라인을 클릭합니다.\n  Add Stage를 클릭하여 Stage를 추가합니다.\n  제공 Stage\n  Deploy: OKE, Compute 인스턴스 배포, Oracle Function에 배포 기능을 제공합니다.\n  Control: 승인 대기, 트래픽 변경, 대기 등을 지원합니다.\n  Integration: 커스텀 로직 수행을 위한 Oracle Function 실행을 지원합니다.\n     Kubernetes에 배포할 manifest 파일 준비 Kubernetes에 배포할 Stage 유형을 사용하기 위해서는 사전에 배포할 manifest yaml 파일을 준비해야 합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Artifacts로 이동합니다.\n  Artifacts로 앞서 빌드 파이프라인 만들때 등록한 2개가 있는 것을 볼수 있습니다. 여기에 등록된 Artifact는 재사용이 가능합니다.\n   manifest 파일을 등록하기 위해 Add artifact를 클릭합니다.\n  4 가지 등록 유형을 제공합니다. 이중에 Kubernetes manifest를 선택합니다.\n   Kubernetes manifest 유형에는 Artifact Source로 2가지 유형을 제고합니다.\n Artifact Registry Repository: Container Registry로 OCIR을 제공하고 있듯시 Artifact Registry를 서비스로 제공하고 있습니다. 그곳에 있는 자원을 참조할 경우에 선택합니다. Inline: 인라인은 현재 DevOps 프로젝트에 있는 여기 Artifact에 직접 입력하는 것을 말합니다.    Artifact Source로 Inline 유형으로 다음과 같이 등록합니다.\n Name: 예, k8s_spring_boot_deploy_template     Value\n앞 서와 같이 build-stage에서 export한 변수값들을 사용할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:${APP_NAME}name:${APP_NAME}spec:replicas:1selector:matchLabels:app:${APP_NAME}template:metadata:labels:app:${APP_NAME}spec:containers:- name:${APP_NAME}image:${OCIR_PATH}:${TAG}---apiVersion:v1kind:Servicemetadata:name:${APP_NAME}-serviceannotations:service.beta.kubernetes.io/oci-load-balancer-shape:\u0026#34;10Mbps\u0026#34;spec:type:LoadBalancerports:- port:80protocol:TCPtargetPort:8080selector:app:${APP_NAME}    Kubernetes Enviroment 등록하기   my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Enviroments로 이동하여 배포할 OKE 환경을 등록합니다.\n  OKE 유형을 선택합니다.\n   배포할 클러스터를 선택합니다.\n   Kubernetes manifest 배포 Stage 만들기   등록한 Deployment Pipeline(spring-boot-hello-deployment-pipeline) 설정 페이지로 이동합니다.\n  Add Stage를 클릭하여 Apply manifest to your Kubernetes cluster Stage를 추가합니다.\n  배포할 환경 및 manifest 파일을 선택합니다\n   파이프라인 완성\n   Build Pipeline에서 Deployment Pipeline 호출하기 앞서 만든 Build Pipeline에서 컨테이너 이미지 까지 OCIR에 등록하고 나면, OKE에 배포할 Deployment Pipeline을 기동되어야 전체 빌드에서 배포까지가 완료됩니다. 이제 Deployment Pipeline을 등록하였으므로, Build Pipeline에서 호출할 수 있습니다.\n  앞서 만든 Build Pipelines(spring-boot-hello-build-pipeline)으로 이동합니다.\n  파이프라인 마지막에 Stage를 추가합니다.\n  Trigger Deployment 유형을 선택합니다.\n  설정한 Deployment Pipeline을 지정합니다.\n   전체 흐름이 완료되었습니다.\n   Trigger 설정하기 지금 까지는 테스트를 하기 위해 Build Pipeline에서 Start Manual Run을 통해 시작하였습니다. 실제로는 개발자가 코드를 코드 저장소에 반영이 될 때 자동으로 빌드, 배포 파이프라인이 동작할 필요가 있습니다. Trigger는 코드 저장소에 발생한 이벤트를 통해 빌드 파이프라인을 시작하게 하는 역할을 하게 됩니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Trigger로 이동합니다.\n  Create trigger을 클릭합니다.\n  Trigger를 설정합니다.\n Name: 예, spring-boot-hello-trigger Source Code Repository: OCI Code Repository, GitHub, GitLab 연동을 지원하며, 예제에서는 앞서 만든 OCI Code Repository상의 spring-boot-hello-repo를 선택합니다. Actions: 트리거링 되었을 때 호출하는 액션으로 작성한 빌드 파이프라인인 spring-boot-hello-build-pipeline을 선택합니다.     설정이 완료되었습니다.\n  테스트   Trigger에서 지정한 spring-boot-hello 소스 코드에 임의의 변경사항을 발생시키고 Code Repository에 반영합니다.\n  저는 Application.java에 있는 응답메시지를 \u0026ldquo;Hello OCI DevOps\u0026quot;로 변경하고 반영하셨습니다.\n  빌드 실행 내역을 보면, 그림과 같이 Trigger 된것은 Commit ID가 함께 보이며, Code Repository와 링크되어 있습니다.\n   Commit ID를 클릭하면 Code Repository상의 코드 변경 분을 확인할 수 있습니다.\n     빌드 파이프라인이 정상적으로 코드 빌드 부터 컨테이너 이미지 생성, 배포 파이프라인 호출까지 실행되었습니다.\n   배포 파이프라인도 정상 실행되었습니다.\n   OKE 클러스터를 조회해 보면 정상 배포 되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/spring-boot-hello-54848fcfd5-5jpxh 1/1 Running 0 5m39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16h service/spring-boot-hello-service LoadBalancer 10.96.186.158 146.56.186.172 80:32224/TCP 41m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spring-boot-hello 1/1 1 1 15h NAME DESIRED CURRENT READY AGE replicaset.apps/spring-boot-hello-54557d9c47 0 0 0 41m replicaset.apps/spring-boot-hello-54848fcfd5 1 1 1 5m39s   서비스 주소로 접속시 정상 동작을 확인할 수 있습니다.\n   ","lastmod":"2021-11-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/devops/1.deploy-app-on-oke-using-devops/","tags":["devops"],"title":"2.1 DevOps 서비스를 통해 Spring Boot 앱 배포 자동화하기"},{"categories":null,"contents":"1.6.2 File Storage 사용하기(Flex Volume Driver) 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)을 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 RWX 접근 모드로 사용하는 방법을 확인해 보겠습니다.\nFiles Storage 만들기 관련 문서를 참고하여 File Storage를 만듭니다.\n  https://docs.oracle.com/en-us/iaas/Content/File/home.htm\n  https://thekoguryo.github.io/oci/chapter08/\n    OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Storage \u0026gt; File Storage로 이동합니다\n  대상 Compartment를 확인합니다.\n  File Systems에서 Create File System을 클릭합니다.\n  기본 설정화면에서 간단히 아래 정보를 원하는 값이 맞게 알맞게 수정하고 생성합니다.\n File System Information:  Name   Mount Target Information:  New Mount Target Name Virtual Cloud Network Subnet       생성결과 확인\nFile Storage \u0026gt; Mount Target 에서 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\n Mount Target OCID: \u0026hellip;sc2mia IP Address: 예, 10.0.20.194 Export Path: 예) /OKE-FFS-Strorage     Security List 설정\nFile System 생성시 Mount Target의 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\n   File Storage 서비스를 이용하여 Persistent Volume 사용하기   Storage Class 만들기\n앞서 확인한 Mount Target OCID로 업데이트 후 적용\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:oci-fssprovisioner:oracle.com/oci-fssparameters:# Insert mount target from the FSS heremntTargetId:ocid1.mounttarget.oc1.ap_seoul_1.aaaaaa4np2szmmn5nfrw4llqojxwiotboawxgzlpovwc2mjnmfsc2mia  Persistent Volume (PV) 만들기\nMount Targe의 IP와 Export Path로 업데이트 후 적용\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumemetadata:name:oke-fss-pvspec:storageClassName:oci-fsscapacity:storage:100GiaccessModes:- ReadWriteManymountOptions:- nosuidnfs:# Replace this with the IP of your FSS file system in OCIserver:10.0.20.194# Replace this with the Path of your FSS file system in OCIpath:\u0026#34;/OKE-FFS-Storage\u0026#34;readOnly:false  Persistent Volume Claime(PVC) 만들기\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:oke-fss-pvcspec:storageClassName:oci-fssaccessModes:- ReadWriteManyresources:requests:storage:100GivolumeName:oke-fss-pv  PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\n앞선 예제와 달리 replica를 복수개로 지정할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-fss-pvcname:nginx-fss-pvcspec:replicas:3selector:matchLabels:app:nginx-fss-pvctemplate:metadata:labels:app:nginx-fss-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:oke-fss-pvc  실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oci-fss-storageclass.yaml storageclass.storage.k8s.io/oci-fss created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get sc,pv,pvc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/oci (default) oracle.com/oci Delete Immediate false 2d19h storageclass.storage.k8s.io/oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d19h storageclass.storage.k8s.io/oci-fss oracle.com/oci-fss Delete Immediate false 34s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 3h6m persistentvolume/oke-fss-pv 100Gi RWX Retain Bound default/oke-fss-pvc oci-fss 24s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/csi-bvs-pvc Bound csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO oci-bv 3h6m persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 100Gi RWX oci-fss 17s oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-9fb98454f-bc7hp 1/1 Running 0 24s 10.244.0.5 10.0.10.40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-bxw4x 1/1 Running 0 24s 10.244.1.18 10.0.10.15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-qm9tq 1/1 Running 0 24s 10.244.0.153 10.0.10.219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   파일 쓰기 테스트\n  아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.40\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bxw4x -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-qm9tq -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40     참고 문서 https://blogs.oracle.com/cloud-infrastructure/post/using-file-storage-service-with-container-engine-for-kubernetes\n","lastmod":"2021-11-10T00:00:00Z","permalink":"https://thekoguryo.github.io/archives/oracle-cloudnative/oke/6.persistent-volume/2.file-storage-by-flex-volume-driver/","tags":["oke"],"title":"1.6.2 File Storage 사용하기(Flex Volume Driver)"},{"categories":null,"contents":"1.2 OKE 클러스터 만들기 Quick 모드로 클러스터 만들기 처음 OKE 클러스터를 만드는 단계로 실환경에서는 별도의 OKE 사용자 및 VCN 등 커스텀한 환경을 사용하겠지만, OKE를 이해하기 위한 처음 단계로 Administrator 유저를 통해 Quick 모드로 설치합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n  List Scope에서 생성할 Compartment(예, oke-labs)를 선택합니다.\n  클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n  빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여생성된 OKE 클러스터를 통해 기분 구조를 이해하고자 합니다. 아래 Launch Workflow를 클릭합니다.\n   클러스터 생성 옵션   생성할 클러스터 이름을 입력합니다. 예, oke-cluster-1\n  설치될 Compartment를 선택합니다. 예, oke-labs\n  클러스터의 쿠버네티스 버전을 선택합니다. 예, v1.20.11\n  Kubernete API Endpoint\n Public API로 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kubernetes API를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Kubernetes API가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.    Kubernetes Worker Nodes\n Worker Nodes를 접속할 수 있는 방법을 선택합니다. 기본 선택된 Private Worker를 그대로 사용 생성되는 Worker Nodes를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Worker Nodes가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.     Shape Worker Node로 사용할 VM Shape를 지정합니다. 기본 선택된 Flex Shape에서 필요에 따라 OCPU, Memory를 조정합니다.\n   Number of nodes\nWorker Nodes 갯수를 지정합니다. 기본 값을 3개입니다.\n  Control Planes(Master Nodes)\nControl Plane은 OCI가 관리하는 영역으로 별도 크기 등을 지정하지 못하며, 기본적으로 다중화되어 있습니다.\n  Advanced Options\n Add an SSH Key  트러블 슈팅 등을 위해 Worker Node VM에 접근하기 위해서는 SSH Key 등록이 필요합니다. 사용할 SSH Key의 Public Key를 아래와 같이 등록합니다.       클러스터 생성 정보를 모두 입력하였습니다. 아래 Next를 클릭\n  앞서 입력한 값들을 리뷰한 후 Create Cluster를 클릭합니다.\n  클러스터 생성 및 관련 네트워크 자원\n아래 그림과 같이 Quick Create로 클러스터를 생성시 기본 네트워크 자원이 함께 생성되는 것을 볼수 있습니다.\n   클러스터 생성 확인   생성이 요청되면, 클러스터 생성, 노드 풀 생성, Worker Node 생성 및 구성 순으로 진행됩니다.\n  클러스터 상세정보에서 Resources \u0026gt; Node Pools를 보면 생성된 pool을 볼수 있습니다.\n   생성된 Node Pool인 pool1을 클릭하여 Node Pool 상세 정보로 이동합니다.\n  Node Pool 상세 정보에서 Resources \u0026gt; Nodes 정보를 보면 생성된 Worker Nodes를 확인할 수 있습니다. VM 생성후 쿠버네티스 구성 시간이 있어 Ready 상태가 될 때까지 약간의 시간이 걸립니다. 테스트 환경에서는 노드가 모두 Ready 될때 까지 5~6분 정도 걸렸습니다.\n   클러스터 및 네트워크 구성 확인 Quick Create \u0026amp; Public Endpoint \u0026amp; Private Workers Example Network Resource Configurations에 설명된 예시 처럼 Kubernetes API Endpoint, Worker Nodes, Service Load Balancer에 대해서 Private 또는 Public 서브넷을 조합하는 몇 가지 구성이 가능합니다. 여기서는 앞서 처럼 Quick Create 모드에서 Public Endpoint, Private Workers를 선택하였고, Service Load Balancer는 기본 생성시는 Public이며 Kubernetes에서 Load Balancer 생성시 선택할 수 있습니다.\n","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/2.install-quick-oke-cluster/","tags":["oke"],"title":"1.2 OKE 클러스터 만들기"},{"categories":null,"contents":"14.2.2 Terraform Configurations 작성하기 Terraform을 쓰려면 Terraform 설정 파일(.tf)에 HashiCorp Configuration Language(HCL) 형식으로 OCI 인프라 자원을 표현해야 합니다. JSON 형식도 지원하지만, 여기서는 HCL 포맷을 사용하겠습니다.\n아래 내용은 Terraform에서 가장 기초적인 일부 내용으로 전체 정보는 Terraform 및 Terraform OCI Provider 문서를 참조하기 바랍니다.\n Terraform OCI Provider 문서  OCI 콘솔에서 처음 VCN을 만든 것처럼 Terraform을 통해 VCN을 만드는 것으로 테스트 해 봅니다. 물론 콘솔에서 Wizard로 만드는 것과는 달리 정말 VCN만 만드는 예제입니다.\nOCI Provider 정의 Terraform 설정 파일에서 표현하는 자원들은 제공업체(Provider)에 따라 그 종류와 특정이 달라집니다. 다양한 업체들이 Terraform Provider로써 자신들의 자원을 정의하여 제공하고 있습니다. OCI로 Provider는 다음과 같이 설정하여 사용할 수 있습니다.\n  provider.tf\nvariable \u0026#34;tenancy_ocid\u0026#34; {} variable \u0026#34;user_ocid\u0026#34; {} variable \u0026#34;fingerprint\u0026#34; {} variable \u0026#34;private_key_path\u0026#34; {} variable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { tenancy_ocid = \u0026#34;${var.tenancy_ocid}\u0026#34; user_ocid = \u0026#34;${var.user_ocid}\u0026#34; fingerprint = \u0026#34;${var.fingerprint}\u0026#34; private_key_path = \u0026#34;${var.private_key_path}\u0026#34; region = \u0026#34;${var.region}\u0026#34; }   Variable 정의 Provider 정의에서 보듯이 Variable은 Terraform 모듈을 위해 필요한 파라미터 정의를 위해 사용합니다. variable \u0026quot;tenancy_ocid\u0026quot; {}와 같이 정의하고 ${var.tenancy_ocid} 처럼 필요한 곳에서 사용할 수 있습니다. 선언은 variable \u0026quot;변수명\u0026quot; {} 형식으로 선언하여 아래 예시와 같이 추가 설정을 할 수 있습니다.\n  type (옵션): 변수 타있을 정의합니다\n string, list, map 중 하나 표시하지 않으면 default에 사용된 값 형식에 기초로 함 표시하지 않고 default도 없는 경우는 string으로 가정    default (옵션): 디폴트 값 정의, 디폴트 값이 없으면, 호출할 때 환경변수 설정 등을 통해서 전달해야 합니다.\n  description (옵션): 변수 설명을 입력합니다.\n  예시\nvariable \u0026#34;tenancy_ocid\u0026#34; {} variable \u0026#34;user_ocid\u0026#34; {} variable \u0026#34;fingerprint\u0026#34; {} variable \u0026#34;private_key_path\u0026#34; {} variable \u0026#34;region\u0026#34; {} variable \u0026#34;AD\u0026#34; { default = \u0026#34;1\u0026#34; description = \u0026#34;Availability Domain\u0026#34; } variable \u0026#34;CPUCoreCount\u0026#34; { type = \u0026#34;string\u0026#34; default = \u0026#34;2\u0026#34; }   Variable 값 설정   Terraform 실행하기 전에 아래 처럼 TF_VAR_변수명 형식으로 환경변수에 지정하여 변수 값을 지정할 수 있습니다.\nexport TF_VAR_tenancy_ocid=\u0026#34;\u0026lt;tenancy OCID\u0026gt;\u0026#34;   terraform CLI 실행시 -var 옵션으로 할 수도 있습니다.\n  별도 변수 파일(terraform.tfvars) 생성해도 됩니다.\n  환경에 따라 변수를 달리하기 위해 파일명이 다른 경우는 terraform 실행시 -var-file 옵션으로 파일명 지정 가능합니다.\n  예시\n## terraform.tfvars file tenancy_ocid=\u0026#34;\u0026lt;tenancy OCID\u0026gt;\u0026#34;     Resource 설정 Resource는 생성할 OCI 자원을 나타냅니다. 아래와 같이 resource를 설정하고 terraform을 실행하면, OCI에 해당 자원이 실제로 생성됩니다. Terraform을 지원하는 OCI 자원 목록은 Terraform OCI Provider 문서에서 확인할 수 있습니다.\n  예시\nvariable \u0026#34;compartment_ocid\u0026#34; {} resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;example_vcn1\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; dns_label = \u0026#34;vcn1\u0026#34; compartment_id = \u0026#34;${var.compartment_ocid}\u0026#34; display_name = \u0026#34;vcn1\u0026#34; }   Data Source 설정 Data 소스는 조회할 OCI 자원을 나타냅니다. Resource와는 달리 조회 용도로만 사용됩니다. Terraform을 지원하는 OCI Data 소스는 Terraform OCI Provider 문서에서 확인할 수 있습니다.\n  예시, oci_identity_availability_domains\n Data Source: oci_identity_availability_domains 필수 파라미터를 이용해서 조회하는 예시  # Gets a list of Availability Domains data \u0026#34;oci_identity_availability_domains\u0026#34; \u0026#34;ADs\u0026#34; { #Required compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; }   Output 설정 Output 변수는 많은 데이터 중에서 쿼리를 통해 특정 값을 조회하는 용도로 terraform 실행시 로그로 출력되며, terraform output 변수명 명령으로 조회도 됩니다.\n  생성된 example_vcn1의 id 예시\noutput \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_virtual_network.example_vcn1.id}\u0026#34;] }   Terraform Configurations 예시   provider.tf\nvariable \u0026#34;tenancy_ocid\u0026#34; {} variable \u0026#34;user_ocid\u0026#34; {} variable \u0026#34;fingerprint\u0026#34; {} variable \u0026#34;private_key_path\u0026#34; {} variable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { tenancy_ocid = \u0026#34;${var.tenancy_ocid}\u0026#34; user_ocid = \u0026#34;${var.user_ocid}\u0026#34; fingerprint = \u0026#34;${var.fingerprint}\u0026#34; private_key_path = \u0026#34;${var.private_key_path}\u0026#34; region = \u0026#34;${var.region}\u0026#34; }   vcn.tf\nvariable \u0026#34;compartment_ocid\u0026#34; {} resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; dns_label = \u0026#34;vcn1\u0026#34; compartment_id = \u0026#34;${var.compartment_ocid}\u0026#34; display_name = \u0026#34;vcn1\u0026#34; } output \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_virtual_network.vcn1.id}\u0026#34;] }   ","lastmod":"2019-03-29T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/2/","tags":["terraform"],"title":"14.2.2 Terraform Configurations 작성하기"},{"categories":null,"contents":"10.2 VCN 구성 LoadBalancer 테스트용 VCN 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking » Virtual Cloud Networks 항목으로 이동합니다.\n  왼쪽 아래 Compartment에서 Sandbox를 클릭합니다.\n  Start VCN Wizard 클릭\n  Create VCN with Internet Connectivity을 선택하고 다시 한번 Start VCN Wizard 클릭\n 선택한 것은 그림에서 설명하는 것 처럼, Public, Private Subnet을 포함하고, 각각 Internat Gateway, NAT Gateway를 통해 인터넷과 통신합니다. 또한 Service Gateway를 OCI 서비스를 연동하기 위해 Oracle Services Network과 통신합니다.    생성 정보 입력\n Basic Information  Name: “LoadBalancerVCN” 입력 Compartment: “Sandbox” Compartment 선택   Configure VCN and Subnets  아래 대역대로 기본 네트워크가 구성됩니다. VCN CIDR Block: 10.0.0.0/16 Public Subnet CIDR Block: 10.0.0.0/24 Private Subnet CIDR Block: 10.0.1.0/24 DNS Resolution: 호스트 이름을 사용하는 기본값을 그대로 사용합니다.      다음으로 이동하여 값을 재확인 후 Create 클릭\n  아래와 같이 VCN 자원이 생성되는 것을 볼 수 있습니다.\n   ","lastmod":"2019-01-23T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/2/","tags":["load balancer","VCN"],"title":"10.2 VCN 구성"},{"categories":null,"contents":"14.1.1.2 리눅스계열에서 OCI CLI 설치하기   터미널 실행\n  설치 스크립트 실행을 위해 다음 실행\nbash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;  Python 설치여부: Y 응답  설치 경로에 대한 확인 후 설정 CLI의 새 버전 업데이트 확인시 Y 응답 PATH에 CLI 추가 요청시 Y 응답      설치 예시\noracle@ubuntu:~$ bash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 6226 100 6226 0 0 8910 0 --:--:-- --:--:-- --:--:-- 8919 Downloading Oracle Cloud Infrastructure CLI install script from https://raw.githubusercontent.com/oracle/oci-cli/6dc61e3b5fd2781c5afff2decb532c24969fa6bf/scripts/install/install.py to /tmp/oci_cli_install_tmp_vLIJ. ######################################################################## 100.0% Running install script. -- Verifying Python version. -- Python version 3.5.2 okay. -- Verifying native dependencies. -- Executing: \u0026#39;dpkg -s libssl-dev libffi-dev python3-dev build-essential\u0026#39; -- One or more of the following native dependencies are not currently installed and may be required. \u0026#34;sudo apt-get install -y libssl-dev libffi-dev python3-dev build-essential\u0026#34; ===\u0026gt; Missing native dependencies. Continue and install the following dependencies: libssl-dev, libffi-dev, python3-dev, build-essential? (Y/n): Y [sudo] password for oracle: Hit:1 http://kr.archive.ubuntu.com/ubuntu xenial InRelease Hit:2 http://kr.archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:3 http://kr.archive.ubuntu.com/ubuntu xenial-backports InRelease Hit:4 https://download.docker.com/linux/ubuntu xenial InRelease Hit:5 http://security.ubuntu.com/ubuntu xenial-security InRelease Hit:6 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done build-essential is already the newest version (12.1ubuntu2). libssl-dev is already the newest version (1.0.2g-1ubuntu4.14). libssl-dev set to manually installed. The following packages were automatically installed and are no longer required: linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic linux-headers-4.13.0-43 linux-headers-4.13.0-43-generic linux-headers-4.13.0-45 linux-headers-4.13.0-45-generic linux-image-4.13.0-36-generic linux-image-4.13.0-43-generic linux-image-4.13.0-45-generic linux-image-extra-4.13.0-36-generic linux-image-extra-4.13.0-43-generic linux-image-extra-4.13.0-45-generic Use \u0026#39;sudo apt autoremove\u0026#39; to remove them. The following additional packages will be installed: libexpat1-dev libpython3-dev libpython3.5-dev python3.5-dev The following NEW packages will be installed: libexpat1-dev libffi-dev libpython3-dev libpython3.5-dev python3-dev python3.5-dev 0 upgraded, 6 newly installed, 0 to remove and 204 not upgraded. Need to get 38.0 MB of archives. After this operation, 55.3 MB of additional disk space will be used. Get:1 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1-dev amd64 2.1.0-7ubuntu0.16.04.3 [115 kB] Get:2 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-dev amd64 3.5.2-2ubuntu0~16.04.5 [37.3 MB] Get:3 http://kr.archive.ubuntu.com/ubuntu xenial/main amd64 libpython3-dev amd64 3.5.1-3 [6,926 B] Get:4 http://kr.archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5-dev amd64 3.5.2-2ubuntu0~16.04.5 [413 kB] Get:5 http://kr.archive.ubuntu.com/ubuntu xenial/main amd64 python3-dev amd64 3.5.1-3 [1,186 B] Get:6 http://kr.archive.ubuntu.com/ubuntu xenial/main amd64 libffi-dev amd64 3.2.1-4 [161 kB] Fetched 38.0 MB in 4s (8,676 kB/s) Selecting previously unselected package libexpat1-dev:amd64. (Reading database ... 322016 files and directories currently installed.) Preparing to unpack .../libexpat1-dev_2.1.0-7ubuntu0.16.04.3_amd64.deb ... Unpacking libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.3) ... Selecting previously unselected package libpython3.5-dev:amd64. Preparing to unpack .../libpython3.5-dev_3.5.2-2ubuntu0~16.04.5_amd64.deb ... Unpacking libpython3.5-dev:amd64 (3.5.2-2ubuntu0~16.04.5) ... Selecting previously unselected package libpython3-dev:amd64. Preparing to unpack .../libpython3-dev_3.5.1-3_amd64.deb ... Unpacking libpython3-dev:amd64 (3.5.1-3) ... Selecting previously unselected package python3.5-dev. Preparing to unpack .../python3.5-dev_3.5.2-2ubuntu0~16.04.5_amd64.deb ... Unpacking python3.5-dev (3.5.2-2ubuntu0~16.04.5) ... Selecting previously unselected package python3-dev. Preparing to unpack .../python3-dev_3.5.1-3_amd64.deb ... Unpacking python3-dev (3.5.1-3) ... Selecting previously unselected package libffi-dev:amd64. Preparing to unpack .../libffi-dev_3.2.1-4_amd64.deb ... Unpacking libffi-dev:amd64 (3.2.1-4) ... Processing triggers for doc-base (0.10.7) ... Processing 2 added doc-base files... Processing triggers for man-db (2.7.5-1) ... Processing triggers for install-info (6.1.0.dfsg.1-5) ... Setting up libexpat1-dev:amd64 (2.1.0-7ubuntu0.16.04.3) ... Setting up libpython3.5-dev:amd64 (3.5.2-2ubuntu0~16.04.5) ... Setting up libpython3-dev:amd64 (3.5.1-3) ... Setting up python3.5-dev (3.5.2-2ubuntu0~16.04.5) ... Setting up python3-dev (3.5.1-3) ... Setting up libffi-dev:amd64 (3.2.1-4) ... ===\u0026gt; In what directory would you like to place the install? (leave blank to use \u0026#39;/home/oracle/lib/oracle-cli\u0026#39;): -- Creating directory \u0026#39;/home/oracle/lib/oracle-cli\u0026#39;. -- We will install at \u0026#39;/home/oracle/lib/oracle-cli\u0026#39;. ===\u0026gt; In what directory would you like to place the \u0026#39;oci\u0026#39; executable? (leave blank to use \u0026#39;/home/oracle/bin\u0026#39;): -- Creating directory \u0026#39;/home/oracle/bin\u0026#39;. -- The executable will be in \u0026#39;/home/oracle/bin\u0026#39;. ===\u0026gt; In what directory would you like to place the OCI scripts? (leave blank to use \u0026#39;/home/oracle/bin/oci-cli-scripts\u0026#39;): -- Creating directory \u0026#39;/home/oracle/bin/oci-cli-scripts\u0026#39;. -- The scripts will be in \u0026#39;/home/oracle/bin/oci-cli-scripts\u0026#39;. -- Downloading virtualenv package from https://github.com/pypa/virtualenv/archive/15.0.0.tar.gz. -- Downloaded virtualenv package to /tmp/tmpw9mlqtyq/15.0.0.tar.gz. -- Checksum of /tmp/tmpw9mlqtyq/15.0.0.tar.gz OK. -- Extracting \u0026#39;/tmp/tmpw9mlqtyq/15.0.0.tar.gz\u0026#39; to \u0026#39;/tmp/tmpw9mlqtyq\u0026#39;. -- Executing: [\u0026#39;/usr/bin/python3\u0026#39;, \u0026#39;virtualenv.py\u0026#39;, \u0026#39;--python\u0026#39;, \u0026#39;/usr/bin/python3\u0026#39;, \u0026#39;/home/oracle/lib/oracle-cli\u0026#39;] Already using interpreter /usr/bin/python3 Using base prefix \u0026#39;/usr\u0026#39; New python executable in /home/oracle/lib/oracle-cli/bin/python3 Also creating executable in /home/oracle/lib/oracle-cli/bin/python Installing setuptools, pip, wheel...done. -- Executing: [\u0026#39;/home/oracle/lib/oracle-cli/bin/pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;/tmp/tmpw9mlqtyq\u0026#39;, \u0026#39;oci_cli\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting oci_cli Downloading https://files.pythonhosted.org/packages/b0/05/edb582a14ad96757601e98f27ebf1a825c918bf745f197d177af2d6423a7/oci_cli-2.4.40-py2.py3-none-any.whl (2.3MB) 100% |████████████████████████████████| 2.3MB 2.7MB/s Collecting pytz==2016.10 (from oci_cli) Downloading https://files.pythonhosted.org/packages/f5/fa/4a9aefc206aa49a4b5e0a72f013df1f471b4714cdbe6d78f0134feeeecdb/pytz-2016.10-py2.py3-none-any.whl (483kB) 100% |████████████████████████████████| 491kB 11.3MB/s Collecting six==1.11.0 (from oci_cli) Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl Collecting certifi (from oci_cli) Downloading https://files.pythonhosted.org/packages/9f/e0/accfc1b56b57e9750eba272e24c4dddeac86852c2bebd1236674d7887e8a/certifi-2018.11.29-py2.py3-none-any.whl (154kB) 100% |████████████████████████████████| 163kB 14.7MB/s Collecting retrying==1.3.3 (from oci_cli) Downloading https://files.pythonhosted.org/packages/44/ef/beae4b4ef80902f22e3af073397f079c96969c69b2c7d52a57ea9ae61c9d/retrying-1.3.3.tar.gz Collecting httpsig-cffi==15.0.0 (from oci_cli) Downloading https://files.pythonhosted.org/packages/93/f5/c9a213c0f906654c933f1192148d8aded2022678ad6bce8803d3300501c6/httpsig_cffi-15.0.0-py2.py3-none-any.whl Collecting arrow==0.10.0 (from oci_cli) Downloading https://files.pythonhosted.org/packages/54/db/76459c4dd3561bbe682619a5c576ff30c42e37c2e01900ed30a501957150/arrow-0.10.0.tar.gz (86kB) 100% |████████████████████████████████| 92kB 14.3MB/s Collecting jmespath==0.9.3 (from oci_cli) Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl Collecting idna\u0026lt;2.7,\u0026gt;=2.5 (from oci_cli) Downloading https://files.pythonhosted.org/packages/27/cc/6dd9a3869f15c2edfab863b992838277279ce92663d334df9ecf5106f5c6/idna-2.6-py2.py3-none-any.whl (56kB) 100% |████████████████████████████████| 61kB 3.1MB/s Collecting python-dateutil==2.7.3 (from oci_cli) Downloading https://files.pythonhosted.org/packages/cf/f5/af2b09c957ace60dcfac112b669c45c8c97e32f94aa8b56da4c6d1682825/python_dateutil-2.7.3-py2.py3-none-any.whl (211kB) 100% |████████████████████████████████| 215kB 16.8MB/s Collecting pyOpenSSL==17.5.0 (from oci_cli) Downloading https://files.pythonhosted.org/packages/79/db/7c0cfe4aa8341a5fab4638952520d8db6ab85ff84505e12c00ea311c3516/pyOpenSSL-17.5.0-py2.py3-none-any.whl (53kB) 100% |████████████████████████████████| 61kB 11.9MB/s Collecting cryptography==2.1.4 (from oci_cli) Downloading https://files.pythonhosted.org/packages/15/4e/c0a0641dc0b4bba7c1dfcf66e30ef34effe7f7dc20a37459e9e052afc4cf/cryptography-2.1.4-cp35-cp35m-manylinux1_x86_64.whl (2.2MB) 100% |████████████████████████████████| 2.2MB 10.3MB/s Collecting terminaltables==3.1.0 (from oci_cli) Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz Collecting oci==2.1.3 (from oci_cli) Downloading https://files.pythonhosted.org/packages/1d/35/61606bc5c885c13bd1d742d76cf6e8eac6a2733b33d65bc41fd3b6c9ff1b/oci-2.1.3-py2.py3-none-any.whl (1.5MB) 100% |████████████████████████████████| 1.5MB 11.7MB/s Collecting click==6.7 (from oci_cli) Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB) 100% |████████████████████████████████| 71kB 4.9MB/s Collecting configparser==3.5.0 (from oci_cli) Downloading https://files.pythonhosted.org/packages/7c/69/c2ce7e91c89dc073eb1aa74c0621c3eefbffe8216b3f9af9d3885265c01c/configparser-3.5.0.tar.gz Collecting cx-Oracle==6.2.1 (from oci_cli) Downloading https://files.pythonhosted.org/packages/10/1c/fbd4e3d6f043f6c934d4a2711f543e755a2b82cfae7c382488b68a2d9db5/cx_Oracle-6.2.1-cp35-cp35m-manylinux1_x86_64.whl (585kB) 100% |████████████████████████████████| 593kB 16.0MB/s Collecting cffi\u0026gt;=1.7; platform_python_implementation != \u0026#34;PyPy\u0026#34; (from cryptography==2.1.4-\u0026gt;oci_cli) Downloading https://files.pythonhosted.org/packages/59/cc/0e1635b4951021ef35f5c92b32c865ae605fac2a19d724fb6ff99d745c81/cffi-1.11.5-cp35-cp35m-manylinux1_x86_64.whl (420kB) 100% |████████████████████████████████| 430kB 13.7MB/s Collecting asn1crypto\u0026gt;=0.21.0 (from cryptography==2.1.4-\u0026gt;oci_cli) Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB) 100% |████████████████████████████████| 102kB 3.0MB/s Collecting pycparser (from cffi\u0026gt;=1.7; platform_python_implementation != \u0026#34;PyPy\u0026#34;-\u0026gt;cryptography==2.1.4-\u0026gt;oci_cli) Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB) 100% |████████████████████████████████| 163kB 18.4MB/s Building wheels for collected packages: retrying, arrow, terminaltables, configparser, pycparser Running setup.py bdist_wheel for retrying ... done Stored in directory: /tmp/tmpw9mlqtyq/wheels/d7/a9/33/acc7b709e2a35caa7d4cae442f6fe6fbf2c43f80823d46460c Running setup.py bdist_wheel for arrow ... done Stored in directory: /tmp/tmpw9mlqtyq/wheels/ce/4f/95/64541c7466fd88ffe72fda5164f8323c91d695c9a77072c574 Running setup.py bdist_wheel for terminaltables ... done Stored in directory: /tmp/tmpw9mlqtyq/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e Running setup.py bdist_wheel for configparser ... done Stored in directory: /tmp/tmpw9mlqtyq/wheels/a3/61/79/424ef897a2f3b14684a7de5d89e8600b460b89663e6ce9d17c Running setup.py bdist_wheel for pycparser ... done Stored in directory: /tmp/tmpw9mlqtyq/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511 Successfully built retrying arrow terminaltables configparser pycparser Installing collected packages: pytz, six, certifi, retrying, idna, pycparser, cffi, asn1crypto, cryptography, httpsig-cffi, python-dateutil, arrow, jmespath, pyOpenSSL, terminaltables, configparser, oci, click, cx-Oracle, oci-cli Successfully installed arrow-0.10.0 asn1crypto-0.24.0 certifi-2018.11.29 cffi-1.11.5 click-6.7 configparser-3.5.0 cryptography-2.1.4 cx-Oracle-6.2.1 httpsig-cffi-15.0.0 idna-2.6 jmespath-0.9.3 oci-2.1.3 oci-cli-2.4.40 pyOpenSSL-17.5.0 pycparser-2.19 python-dateutil-2.7.3 pytz-2016.10 retrying-1.3.3 six-1.11.0 terminaltables-3.1.0 ===\u0026gt; Modify profile to update your $PATH and enable shell/tab completion now? (Y/n): Y ===\u0026gt; Enter a path to an rc file to update (leave blank to use \u0026#39;/home/oracle/.bashrc\u0026#39;): -- Backed up \u0026#39;/home/oracle/.bashrc\u0026#39; to \u0026#39;/home/oracle/.bashrc.backup\u0026#39; -- Tab completion set up complete. -- If tab completion is not activated, verify that \u0026#39;/home/oracle/.bashrc\u0026#39; is sourced by your shell. -- -- ** Run `exec -l $SHELL` to restart your shell. ** -- -- Installation successful. -- Run the CLI with /home/oracle/bin/oci --help oracle@ubuntu:~$   ","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/2/","tags":["linux","CLI"],"title":"14.1.1.2 리눅스계열에서 OCI CLI 설치하기"},{"categories":null,"contents":"5.2 방법 #1. Default Security List 업데이트 하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  원하는 Instance를 클릭하여 상세 페이지로 이동합니다.\n  왼쪽 아래의 Resources \u0026raquo; Attached VNICs를 클릭합니다.\n  그림과 같이 사용 중인 가상 네트워크 인터페이스 카드, VNIC(Virtual Network Interface Card)이 보입니다. 이 네트워크 카드가 속한 Subnet을 클릭합니다. 물론 VNIC 여러개 인 경우에는, 각각 속한 Subnet 설정에 영향을 받습니다.\n   해당 Subnet의 기본 Security List를 클릭합니다.\n   현재 설정된 방화벽 규칙을 확인할 수 있습니다. 앞서 설치된 Apache HTTP 서버가 사용하는 수신 포트의 개방하기 위해 Ingress Rule을 수정이 필요합니다.\n  추가를 위해 Add Ingress Rules을 클릭합니다.\n   아래 규칙을 추가합니다.\n  Source CIDR: 0.0.0.0/0, 모든 IP에서 오는 요청\n  IP Protocol: TCP\n  Destination Port Range: 80, 개방할 포트\n     아래와 같이 Ingress Rule이 추가되었습니다.\n   ","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/2/","tags":["security list"],"title":"5.2 Default Security List 업데이트 하기"},{"categories":null,"contents":"3.5.2 Windows에서 PuTTY로 접속하기 PuTTY Key Generator로 생성한 SSH Key는 Save private key를 통해 PuTTY Private Key (PPK) .ppk로 저장하여 PuTTY에서 사용합니다. ssh-keygen로 생성한 PEM(Privacy Enhanced Mail) 파일 형식의 키는 PEM -\u0026gt; PPK 변환을 통해 PuTTY에서 사용 가능합니다.\nPuTTY 설치 PuTTY가 설치되어 있지 않다면, http://www.putty.org/ 에서 putty를 내려 받아 설치합니다.\nWindows에서 Linux 인스턴스 접속하기   putty.exe 를 실행\n  Category에서 Session을 선택하고 다음을 입력\n  Host Name (or IP address):\n인스턴스의 Public IP입니다. OCI 콘솔에서 확인할 수 있습니다.\n  Port: 22\n  Connection type: SSH\n     Category 창에서 Window 하위에 Translation 선택\n  Remote character set 드랍 다운 리스트에서 UTF-8을 선택\nLinux 기반 인스턴스는 디폴트 로케일이 UTF-8입니다. 그래서 PuTTY에서도 동일하게 설정합니다.\n   Category 창에서 Connection 하위에 SSH 그리고 Auth 클릭\n  Browse를 클릭하고, 인스턴스 접속을 위한 개인키를 선택\n    Category 창에서 Session으로 이동하여 세션을 저장합니다.\n  Open을 클릭하여 세션을 시작합니다.\n접속하려는 인스턴스에 처음 접속한다면, 서버의 호스트 키가 레지스트리에 캐쉬되지 않았다는 알람 메시지가 뜨게 됩니다. Accept를 클릭합니다.\n    디폴트 사용자 로그인\n   디폴트 접속 사용자    OS Image Default User Name     Oracle Linux, CentOS opc   ubuntu ubuntu    ","lastmod":"2018-12-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/2/","tags":["windows","ssh","putty"],"title":"3.5.2 Windows에서 PuTTY로 접속하기"},{"categories":null,"contents":"1.2 Fault Domains Fault Domain은 Availability Domain 내에 하드웨어와 인프라스트럭쳐의 그룹입니다. 각 Availability Domain은 3개의 Fault Domain을 가지고 있습니다. Fault Domain은 Compute 인스턴스를 단일 Availability Domain 내에 하나의 물리적 하드웨어에 분배하지 않도록 합니다. 하나의 Fault Domain 내의 하드웨어 장애 또는 하드웨어 유지 보수 작업은 다른 Fault Domain에 영향을 끼치지 않습니다.\n새 Compute 인스턴스를 생성할 때, 사용자는 Fault Domain을 선택할 수 있습니다. 사용자가 선택하지 않으면, 시스템이 자동으로 지정합니다. 인스턴스의 Fault Domain을 변경하기 위해서는 인스턴스를 Terminate 시키고 인스턴스를 다시 생성하여야 하므로 새로 만들 때 잘 선택해야 합니다.\nFault Domains 사용하는 경우\n 예상하지 못한 하드웨어 실패에 대비하기 위해 계획된 하드웨어 유지 보수 작업에 대비하기 위해  ","lastmod":"2018-12-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/2/","tags":["fault domain"],"title":"1.2 Fault Domains"},{"categories":null,"contents":"14.1.1.3 Config File 설정   설정을 위해 필요한 정보 사전 확인\n  user OCID\n오른쪽 위 사용자 Profle에서 User Settings 클릭 후 사용자 OCID 확인\n    Tenancy OCID\n오른쪽 위 사용자 Profle에서 Tenancy 클릭 후 Tenancy OCID 확인\n    Region: 사용할 Region\n  API Signing Key: 여기서는 편의상 새로운 Private Key, Public Key를 생성하는 것으로 선택하겠습니다.\n    setup config 실행\noci setup config  실행 예시  [opc@bastion-host ~]$ oci setup config This command provides a walkthrough of creating a valid CLI config file. ... Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaazo6ilmezdaeozjcmsu6rcxnf5sjz2fau76kpdjvsbbakhqn35azq Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaamd5zq7ohrxkmcsai23pp634k6i2eymnznv4d6ur7n5n4qjr7vnta Enter a region by index or name(e.g. 1: ap-chiyoda-1, 2: ap-chuncheon-1, 3: ap-hyderabad-1, 4: ap-ibaraki-1, 5: ap-melbourne-1, 6: ap-mumbai-1, 7: ap-osaka-1, 8: ap-seoul-1, 9: ap-singapore-1, 10: ap-sydney-1, 11: ap-tokyo-1, 12: ca-montreal-1, 13: ca-toronto-1, 14: eu-amsterdam-1, 15: eu-frankfurt-1, 16: eu-marseille-1, 17: eu-zurich-1, 18: il-jerusalem-1, 19: me-dubai-1, 20: me-jeddah-1, 21: sa-santiago-1, 22: sa-saopaulo-1, 23: sa-vinhedo-1, 24: uk-cardiff-1, 25: uk-gov-cardiff-1, 26: uk-gov-london-1, 27: uk-london-1, 28: us-ashburn-1, 29: us-gov-ashburn-1, 30: us-gov-chicago-1, 31: us-gov-phoenix-1, 32: us-langley-1, 33: us-luke-1, 34: us-phoenix-1, 35: us-sanjose-1): 8 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: a0:e1:fe:79:22:22:f0:b5:6b:29:72:5f:5d:b6:22:32 Config written to /home/opc/.oci/config If you haven\u0026#39;t already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2   ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/3/","tags":["CLI","config"],"title":"14.1.1.3 Config File 설정"},{"categories":null,"contents":"14.2.3 Terraform OCI Provider 연결정보 구하기   Tenancy OCID\n오른쪽 위 사용자 Profle에서 Tenancy 클릭 후 Tenancy OCID 확인\n  user OCID\n오른쪽 위 사용자 Profle에서 User Settings 클릭 후 사용자 OCID 확인\n    region: 사용할 Region Identifier\n  API Key - private_key_path, fingerprint\n OCI CLI 설치시 사용한 API Key를 사용합니다. 또는 새로 생성합니다.  3.3 SSH 접속을 위한 Key Pair 만들기를 참고하여 사용할 SSH Key Pair 생성 오른쪽 위 사용자 Profle에서 User Settings 클릭 왼쪽 아래 API Keys 선택후 Add Key를 클릭하여 PEM 형식 Public Key 내용을 복사해서 등록 등록후 보이는 Fingerprint 복사      variable \u0026ldquo;compartment_ocid\u0026rdquo;\n OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security » Identity » Compartments 항목으로 이동합니다. 원하는 Compartment 클릭후 OCID 확인     Terraform 변수 파일 생성 예시   환경에 따라 변수를 달리하기 위해 파일명이 다른 경우는 terraform 실행시 -var-file 옵션으로 파일명 지정 가능합니다.\n  terraform.tfvars\n# OCI authentication tenancy_ocid = \u0026#34;ocid1.tenancy.oc1..~~~\u0026#34; # 1에서 확인한 Tenancy OCID 사용 user_ocid = \u0026#34;ocid1.user.oc1..~~~\u0026#34; # 2에서 확인한 User OCID 사용 compartment_ocid = \u0026#34;ocid1.compartment.oc1..~~~\u0026#34; # 5에서 확인한 대상 Compartment OCID 사용 private_key_path = \u0026#34;C:\\\\Users\\\\TheKoguryo\\\\.oci\\\\oci_api_key.pem\u0026#34; # 3에서 생성한 SSH Key중 Private Key의 위치 fingerprint = \u0026#34;7b:e7:~~~~\u0026#34; # 3에서 API Key로 등록한 Public Key의 Fingerprint region = \u0026#34;ap-seoul-1\u0026#34; # 4에서 확인한 대상 Region Name   ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/3/","tags":["terraform","oci provider"],"title":"14.2.3 Terraform OCI Provider 연결정보 구하기"},{"categories":null,"contents":"13.3 Autoscaling 테스트 Scale Out 발생 테스트   Instance Pool로 이동해서 현재 있는 Compute 인스턴스에 SSH로 접속합니다.\n  stress 툴 설치\nsudo yum-config-manager --enable ol7_developer_EPEL sudo yum install -y stress   stress 수행\nsudo stress --cpu N   실행 예시\n사용할 Compute Instance의 CPU에 갯수에 맞춰 조정하여 부하를 계속 줍니다. 오토스케일 설정시 정한 Cool Down을 값을 고려하여 그 시간 이상 부하를 줍니다.\n[opc@autoscalingwebserver ~]$ sudo stress --cpu 4 stress: info: [21438] dispatching hogs: 4 cpu, 0 io, 0 vm, 0 hdd   부하발생 중 모니터링   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instance Pools 항목으로 이동합니다.\n  Autoscaling 할 Instance Pool의 상세화면으로 이동합니다.\n  Resources \u0026raquo; Metrics 에서 인스턴스 풀에 대한 모니터링을 지원합니다.\n  Metric namespace는 oci_computeagent를 선택하면 인스턴스 풀에 속한 VM 들에 대한 메트릭 정보를 볼 수 있습니다. 아래와 같이 CPU 부하가 모니터링 되고 있습니다.\n   Scale Out 확인   지정한 Cooldown을 초과하여 부하가 계속발생하여 그림과 같이 Autoscaling이 발생합니다.\n   Instance Pool 상세 정보에서 서버 추가가 완료된 결과를 볼 수 있습니다.\n   Load Balancer의 Backend Set에서 신규 서버가 추가되는 것을 볼 수 있습니다.\n   서버 추가로 인해 Instance Pool의 평균 CPU 부하는 반으로 내려간 것을 볼 수 있습니다.\n   브라우저를 통해 LB의 Public IP로 접속합니다.\n   브라우저를 리프레쉬합니다.\n   Scale In 확인   1번 서버 SSH 접속 터미널에서 주던 부하를 중지합니다.\n[opc@autoscalingwebserver-264316 ~]$ sudo stress --cpu 4 stress: info: [7772] dispatching hogs: 4 cpu, 0 io, 0 vm, 0 hdd ^C [opc@autoscalingwebserver-264316 ~]$   모니터링에서 Instance Pool의 평균 CPU 부하 감소 확인   부하가 줄어들어 그림과 같이 Autoscaling이 발생합니다.   Load Balancer에서 먼저 제외하고 인스턴스를 종료하는 순으로 Scale In이 됩니다.   Scale In 완료   Load Balancer의 Backend Set에서도 삭제된 것이 확인됨.   ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter13/3/","tags":["autoscailing"],"title":"13.3 Autoscaling 테스트"},{"categories":null,"contents":"8.3 접근 제어를 위한 Export Option Export Option 설정을 통한 권한 제어   앞서 만든 File System의 상세화면으로 이동합니다. 아래쪽에 그림과 같이 Export 리스트에서 생성된 Export를 클릭합니다.\n   Export 상세화면 아래쪽에 그림과 같이 NFS Export Options이 보입니다.\n기본 생성된 옵션은 모든 클라이언트(0.0.0.0/0)에서 Read/Write로 접근 가능합니다.\n   Edit NFS Export Options을 클릭합니다.\n  기존 옵션 위에 테스트 VM이 있는 Public Subnet(10.0.0.0/24)는 읽기만 되게 옵션을 추가합니다.\n테스트 VM의 IP가 10.0.0.12라면 두 옵션의 Source에 모두 해당됩니다. 이런 경우 순서에 따라 먼저 매칭되는 것이 적용되어 실제로 첫번째 조건에 따라 Read Only 접근만 가능합니다.\n   설정완료\n   File System에 권한 테스트   앞서 테스트한 VM에 접속합니다.\n  기존 파일이 읽기가 되는 지 확인합니다.\nls -la /mnt/FSS-Storage/ cat /mnt/FSS-Storage/hello1.txt   실행결과\n[opc@examplelinuxinstance FSS-Storage]$ ls -la /mnt/FSS-Storage/ total 17 drwxr-xr-x. 2 root root 2 Jan 14 02:35 . drwxr-xr-x. 3 root root 25 Jan 14 01:59 .. drwxr-xr-x. 2 root root 0 Jan 14 05:34 .snapshot -rw-r--r--. 1 opc opc 19 Jan 14 02:18 hello1.txt [opc@examplelinuxinstance FSS-Storage]$ cat /mnt/FSS-Storage/hello1.txt Hello File Storage     새 파일 쓰기를 시도합니다.\necho \u0026#39;Hello File Storage\u0026#39; \u0026gt;/mnt/FSS-Storage/hello2.txt   실행결과\n아래와 같이 Public Subnet(10.0.0.0/24)에 해당되어 쓰기시 에러가 발생합니다.\n[opc@examplelinuxinstance FSS-Storage]$ hostname -I 10.0.0.12 [opc@examplelinuxinstance FSS-Storage]$ echo \u0026#39;Hello File Storage\u0026#39; \u0026gt;/mnt/FSS-Storage/hello2.txt -bash: /mnt/FSS-Storage/hello2.txt: Read-only file system     Export Option 순서 변경후 권한 재 확인   NFS Export Option 변경 화면 다시 이동합니다.\n  각 항목 오른쪽 액션메뉴의 Move Up, Down 메뉴를 통해 순서를 조정하여 업데이트 합니다.\n   변경 완료\n   테스트 VM으로 돌아가 다시 마운트한 파일 시스템에 쓰기를 시도합니다.\n[opc@examplelinuxinstance FSS-Storage]$ echo \u0026#39;Hello File Storage\u0026#39; \u0026gt;/mnt/FSS-Storage/hello2.txt [opc@examplelinuxinstance FSS-Storage]$ ls -la total 17 drwxrwxrwx. 2 root root 4 Jan 14 05:50 . drwxr-xr-x. 3 root root 25 Jan 14 01:59 .. drwxrwxrwx. 2 root root 0 Jan 14 05:50 .snapshot -rw-rw-r--. 1 opc opc 19 Jan 14 02:18 hello1.txt -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt   우선 매칭되는 0.0.0.0/0 Read/Write 권한에 따라 파일쓰기가 되는 것을 알 수 있습니다.\n  ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/3/","tags":["file storage","export option"],"title":"8.3 접근 제어를 위한 Export Option"},{"categories":null,"contents":"7.3 Object 권한 관리 - 사전인증(Pre-Authenticated Requests) Pre-Authenticated Requests는 Bucket 또는 Object에 대해서 인증 없이 사용자가 접근할 수 있도록 설정하는 기능입니다. 지정된 기간까지 인증없이 Bucket 또는 Object에 접근할 수 있는 임시 공유 링크로 생성됩니다. 이 공유 링크를 통해 임의의 사용자가 접근할 수 있게 됩니다.\nObject URL로 접근   Object Details 화면에서 Object의 URL 경로를 확인합니다.\n   확인한 URL 경로를 브라우저로 접근해 보면 다음과 같이 에러가 발생합니다.\n   Object에 Pre-Authenticated Requests 설정하기   Object의 우측 액션 메뉴에서 Create Pre-Authenticated Request를 클릭합니다.\n   Create Pre-Authenticated Request 설정화면입니다.\n  Target: Bucket, Object 단위 또는 Prefix 기준으로 대상을 선택할 수 있습니다. 여기서는 Object로 합니다.\n  Access Type: 읽기, 쓰기에 대해서 권한을 설정할 수 있습니다.\n  Expiration: 만료기간을 정할 수 있습니다. 기본값은 일주일입니다.\n설정후 아래 Create Pre-Authenticated Request를 클릭합니다.\n     생성된 PAR 공유 링크를 복사합니다.\n   브라우저로 PAR 공유 링크를 접근합니다. 그림과 같이 인증없이 잘 접근됩니다.\n   Pre-Authenticated Requests 삭제하기   Bucket 상세화면의 좌측 Resources 메뉴에서 Pre-Authenticated Requests를 선택합니다. 그러면 현재 생성된 PAR 리스트가 그림과 같이 보이게 됩니다. 삭제할 PAR의 우측 액션 메뉴를 선택하여 Delete를 클릭합니다.\n   확인 메시지를 확인후 OK를 클릭합니다.\n  PAR이 삭제된 후 앞서 접속한 PAR 공유 링크로 다시 접근합니다. 그림과 같이 이제 더 이상 접근되지 않습니다.\n   Bucket에 Pre-Authenticated Requests 설정하기   Bucket도 같은 방법으로 PAR 공유링크를 생성할 수 있습니다.\n  Enable Object Listing: API로 접근시 목록을 가져오는 기능을 추가적으로 활성화하는 것이 좋습니다.\n     Enable Object Listing을 활성화하면 Bucket PAR 공유링크를 접근하면, 아래와 같이 결과 리스트가 JSON 메시지로 보입니다. 개별 오브젝트에 대한 접근은 SDK 또는 API로 가능합니다. 그래서 일단 이 부분은 생략하도록 하겠습니다.\n   Prefix에 대한 사전 인증 링크도 동일한 방식으로 생성합니다.\n  ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/3/","tags":["object storage","PAR"],"title":"7.3 Object 권한 관리 - 사전인증"},{"categories":null,"contents":"3.7.3 백업을 다른 Region으로 복사하기 Block Volume의 백업은 단일 Region 내에서만 가능합니다. 장애 복구나 다른 용도로 다른 Region에서 필요한 경우에는 Region 간 복사 기능을 통해서 다른 Region으로 복사할 수 있습니다.\n 제약사항 Region간 백업 복사는 몇 가지 제약사항이 있습니다. 정확한 내용은 공식 문서를 참고하세요.\nLimitations for Copying Volume Backups Across Regions     OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Block Storage \u0026raquo; Block Volumes Backups 항목으로 이동합니다.\n  전체 백업 목록을 확인할 수 있습니다.\n  원하는 백업의 우측 액션 메뉴에서 Copy to Another Region을 클릭합니다.\n   복사 설정 화면\n  Name: 필요시 원하는 이름으로 변경\n  Destination Region: 복사할 대상 Region 선택\n Subscribe 중인 Region 중에서 대상을 선택할 수 있습니다.       복사 설정 요약 확인하고 다음으로 넘어갑니다.\n  우측 상단의 Region을 대상 Region으로 변경합니다. 그림과 같이 백업본이 복제된 것을 알 수 있습니다.\n   증분 백업으로 복사했지만, Backup Size를 통해 전체 백업이 복사된 것을 확인할 수 있습니다.\n  ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/3/","tags":["block volume","backup","copy"],"title":"3.7.3 백업을 다른 Region으로 복사하기"},{"categories":null,"contents":"3.3 SSH 접속을 위한 Key Pair 만들기 서버 인스턴스에 접근하기 위해 패스워드 대신 SSH Key Pair를 사용합니다. Key Pair는 개인키와 공개키로 구성되며, 개인키는 사용자가 사용자의 컴퓨터에 보관하며 공개키는 서버 인스턴스를 만들 때 등록해야 합니다.\nKey Pair를 만들기 위해서는 키 생성 도구가 필요하며 없는 경우 설치 후 사용합니다.\n 리눅스/유닉스: ssh-keygen 사용, 미 설치시 OpenSSH(http://www.openssh.com/portable.html) 설치 후 사용 Windows 10: ssh-keygen 사용 그외 Windows: PuTTY 사이트에서(http://www.putty.org/) PuTTY Key Generator(puttygen.exe) 설치 후 사용 또는 OCI에서 VM 생성시 자동생성된 것을 다운받아 사용  리눅스/유닉스, Windows 10 환경에서 SSH Key Pair 만들기   터미널 또는 Powershell 실행\n  ssh-keygen을 통해 Key Pair를 생성합니다.\n명령을 실행하면, 추가적으로 passphrase 값 입력을 요구합니다. 이것은 생성되는 개인 키 파일에 대한 암호로 설정하게 되면, 매번 ssh 접속시 추가적으로 입력을 요구하게 됩니다. 필요 없으면 값을 입력하지 않고 그냥 엔터키를 칩니다.\n$ ssh-keygen -t rsa -b 2048 -C \u0026#34;\u0026lt;comment\u0026gt;\u0026#34; -f \u0026lt;output_keyfile\u0026gt;   개인키와 공개키가 각각 \u0026lt;output_keyfile\u0026gt;, \u0026lt;output_keyfile\u0026gt;.pub 파일로 생성된 걸 확인할 수 있습니다.\nubuntu@NOTEBOOK-WORK:~$ ssh-keygen -t rsa -b 2048 -C \u0026#34;my ssh key\u0026#34; -f mysshkey Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in mysshkey Your public key has been saved in mysshkey.pub The key fingerprint is: SHA256:iWtp5RtNqW9prdInSQ0rE8PJjivXMdMeY3LuY5EUAkk my ssh key The key\u0026#39;s randomart image is: +---[RSA 2048]----+ | .Eo | | . . . | | o o . | | .*.o. | | .oS=o= | | .=O+@ . | | =o+#.B | | .oo o+% o | | o .*+= | +----[SHA256]-----+   윈도우즈 환경에서 SSH Key Pair 만들기   내려받아 설치한 puttygen.exe 실행\n  키 타입을 기본 RSA로 선택하고, 비트 수를 2048로 설정\n   Generate 클릭\n  무작위 데이터를 생성하기 위해 진행 바가 끝까지 갈 때 까지 가운데 빈 화면에서 마우스를 이리저리 움직입니다.\n   생성 완료\n   Key comment: 필요하면 수정\n  Key passphrase: 개인 키 파일에 대한 암호로 ssh 접속시 입력을 요구하게 됩니다. 필요 없으면 값을 입력하지 않고 그냥 엔터키를 칩니다.\n  Putty 전용 형식으로 개인 키 저장\nSave private key 클릭, 프롬프트가 뜨면 passphrase 없이 키 저장하도록 예(Y) 선택 후 파일 저장\n이때 저장되는 개인 키는 PuTTY Private Key (PPK) 형식으로 저장되어 PuTTY 에서만 사용 가능합니다.\n  PEM 형식으로 개인 키 저장\nSSH를 통해 Compute VM으로 접속시에는 PEM 형식 키를 일반적으로 사용됩니다.\n메뉴에서 Conversions \u0026raquo; Export OpenSSH Key 선택합니다. 프롬프트가 뜨면 passphrase 없이 키 저장하도록 예(Y) 선택 후 파일 저장\n  예시(mysshkey)  -----BEGIN RSA PRIVATE KEY-----MIIEpQIBAAKCAQEArKWHqta/NDy9DsuBpq4SSiS+p3VfUk96la8Q+/LgSJRU+RPI...HCSSOnUJxQR3xDOnrz4ywSh1bheoxUUjaHI1PtxTQtcNWHW7K2hPblE=-----END RSA PRIVATE KEY-----  공개 키 저장\n 표시된 공개키 문자를 모두 복사하여 개인키가 저장된 위치에 파일로 저장합니다. ssh-keygen에서 생성되는 포맷과 동일하게 공개키의 이름은 개인키 이름과 같게 하고, 확장자만 .pub로 변경하여 저장합니다.\nOCI에서는 OpenSSH 형식을 지원하나, \u0026ldquo;Save public key\u0026quot;은 OpenSSH 형식으로 저장되지 않으므로 사용하지 않습니다.\n 예시(mysshkey.pub)  ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCspYeq1r80PL0Oy4GmrhJKJL6ndV9ST3qVrxD78uBIlFT5E8gV+lbDma+aZN6QpYRTboccyngd5.........qeL5YOmSw2p7Uu5kqflg+45xP3cbm42R1zMLFs81a2+5vHy/nSV523el1 rsa-key-20220110  파일 경로와 파일명을 확인 후 공개키는 Compute Instance 생성시, 개인키는 SSH로 Instance 접속시 사용합니다.\n  ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/3/","tags":["ssh","key pair"],"title":"3.3 SSH 접속을 위한 Key Pair 만들기"},{"categories":null,"contents":"3.5.3 PEM \u0026lt;-\u0026gt; PPK 포맷 변경하기 PEM(Privacy Enhanced Mail) OCI 인스턴스에서 사용하는 Key Pair 포맷으로 앞선 문서 기준으로 ssh-keygen 명령을 통해 생성됨\nPPK(PuTTY Private Key) PuTTY에서 사용하는 키 포맷으로 PuTTY Key Generator를 통해 저장하면 기본적으로 생성되는 포맷\nPEM -\u0026gt; PPK 포맷 변환   PuTTY Key Generator(http://www.putty.org/) 설치 후 사용\n  PuTTY Key Generator를 실행합니다.\n  Conversion \u0026raquo; Import Key 명령으로 PEM 파일을 엽니다.\n   키 파일이 열리면, Save private key 클릭으로 PPK 형식으로 저장\n    PPK -\u0026gt; PEM 포맷 변환   PuTTY Key Generator를 실행합니다.\n  File \u0026gt; Load private key 명령으로 PPK 파일을 엽니다.\n   키 파일이 열리면 Export OpenSSH Key 클릭으로 PEM 형식으로 저장\n    ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/3/","tags":["pem","ppk","putty"],"title":"3.5.3 PEM \u003c-\u003e PPK 포맷 변경하기"},{"categories":null,"contents":"3.6.3 Block Volume에 연결하기 Block Volume 장착 후에 iSCSI 연결을 설정해야 합니다. iscsiadm 명령으로 실행되며 실행해야 할 명령은 장착된 Block Volume에서 제공하므로 복사 후 그대로 실행하면 됩니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  앞서 생성한 대상 Instance의 이름을 클릭합니다.\n  Instance 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Attached block volumes을 클릭합니다.\n  방금 장착한 Block Volume 옆에 있는 액션 아이콘(우측 점 3개)을 클릭한 다음 iSCSI commands and information을 클릭합니다.\n   iSCSI commands and information 다이얼로그가 뜹니다. 장착한 Volume에 대한 IP와 Port를 확인할 수 있고, 장착 및 장착해제에 사용될 명령이 보입니다. 명령에는 IP, Port가 이미 입력되어 있으므로 복사해서 그대로 사용하면 됩니다.\n연결하기 위한 Connect 항목에 있는 명령을 복사합니다.\n   Attach한 Instance에 접속한 SSH로 접속합니다.\n  Connect Commands의 명령을 복사해서 SSH 세션에서 순서대로 하나씩 실행합니다.\nubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i privateKey opc@146.56.171.xx Last login: Mon Jan 10 07:24:34 2022 from 223.62.21.xx [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -o new -T iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec -p 169.254.2.3:3260 sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec -p 169.254.2.3:3260 -lNew iSCSI node [tcp:[hw=,ip=,net_if=,iscsi_if=default] 169.254.2.3,3260,-1 iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec] added [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -o update -T iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec -n node.startup -v automatic [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec -p 169.254.2.3:3260 -l Logging in to [iface: default, target: iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec, portal: 169.254.2.3,3260] (multiple) Login to [iface: default, target: iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec, portal: 169.254.2.3,3260] successful.   장착을 위한 iSCSI 명령을 모두 수행이 끝나면, 이제 Linux에서 하드 디스크처럼 사용할 때처럼 포맷 및 마운트 작업을 수행하면 됩니다. 먼저, 장착 여부를 확인하기 위해 다음 명령을 수행합니다.\nsudo fdisk -l   실행결과\n 아래쪽에 보면 Disk /dev/sdb가 장착된 것을 확인 할 수 있습니다.  [opc@examplelinuxinstance ~]$ sudo fdisk -l WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sda: 50.0 GB, 50010783744 bytes, 97677312 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disk label type: gpt Disk identifier: BBBA97C2-4E47-43B0-9059-2702FDB720E7 # Start End Size Type Name 1 2048 411647 200M EFI System EFI System Partition 2 411648 17188863 8G Linux swap 3 17188864 97675263 38.4G Microsoft basic Disk /dev/sdb: 53.7 GB, 53687091200 bytes, 104857600 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes  아래쪽에 보면 Disk /dev/sdb가 장착된 것을 확인 할 수 있습니다.      ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/3/","tags":["block volume"],"title":"3.6.3 Block Volume에 연결하기"},{"categories":null,"contents":"3.8.3 Boot Volume으로 새 Instance 생성하기 아직 Instance에 사용 중이 아닌 Boot Volume을 이용하여 새 Compute Instance를 만들 수 있습니다.\nBoot Volume으로 새 Instance 생성하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  Instance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\n  사용할 이미지 선택\n  Image and Shape에서 Edit를 클릭하여 이미지 소스를 변경합니다. Change image를 클릭하여 이미지를 변경합니다.\n   Boot volumes을 소스로 선택하면, 아직 Attached 되지 않은 Boot volumes이 보이고, 앞서 복구한 Boot volume이 보입니다. 해당 이미지를 선택합니다.\n     추가 생성정보는 새 인스턴스 생성시와 동일하므로 원하는 값을 선택합니다.\n 기존 Boot Volume을 사용하는 것으로 이때는 기존 사이즈를 그대로 사용하는 차이가 있습니다.    인스턴스 생성이 완료되면 지정한 Boot Volume을 사용하는 것을 확인할 수 있습니다.   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/8/3/","tags":["boot volume"],"title":"3.8.3 Boot Volume으로 새 Instance 생성하기"},{"categories":null,"contents":"2.3 OCI Console 사용하기 OCI Console UI  서비스 내비게이션 왼쪽 위의 내비게이션 메뉴를 통해 서비스 및 자원에 대한 작업을 수행할 수 있습니다.\n Region 관리 현재 Region이 오른쪽 위에 표시되며, 여러 Region을 구독하여 사용하는 경우 해당 메뉴에서 다른 Region으로 전환할 수 있습니다. 또한 다른 Region에 구독하고 싶으면 하위 메뉴에 있는 Region 관리 메뉴를 통해 구독할 수 있습니다.\n  오른쪽 위 Region 선택 메뉴에서 Manage Regions를 클릭합니다.\n   가능한 Region 중에서 구독하려는 Region을 확인할 수 있습니다.\n   다만, Free Tier 계정은 하나의 Region만 사용할 수 있습니다.\n내비게이션 메뉴 \u0026gt; Governance \u0026amp; Administration \u0026gt; Limits, Quota and Usage를 클릭하고 Service를 Regions로 선택합니다.\n   ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/3/","tags":["oci console"],"title":"2.3 OCI Console 사용하기"},{"categories":null,"contents":"6.3 Multicluster Verrazzano 설치하기 참고문서\n https://verrazzano.io/latest/docs/applications/multicluster/  Verrazzano 설치 admin-cluster에 Verrazzano 설치 단일 클러스터 환경에 Verrazzano는 설치하는 것과 동일하게 설치합니다. dev 또는 prod 프로파일로 설치합니다. 다만, 설치 yaml 파일(예, install-oci.yaml)에 mananged-cluster와 구분하기 위해 환경 이름을 지정합니다.\n  예시\n...spec:environmentName:adminprofile:dev...  managed-cluster에 Verrazzano 설치 단일 클러스터 환경에 Verrazzano는 설치하는 것과 동일하게 설치합니다. managed-cluster 프로파일로 설치합니다. 다만, 설치 yaml 파일(예, install-oci.yaml)에 mananged-cluster와 구분하기 위해 환경 이름을 지정합니다.\n  예시\n...spec:environmentName:mc-1profile:managed-cluster...    설치 결과 확인\nkubectl get verrazzano my-verrazzano -o yaml   결과 예시\n managed-cluster 프로파일로 Managed Cluster를 위한 컴포넌트만 설치됩니다.  status: components: cert-manager: ... instance: consoleUrl: https://verrazzano.mc-1.thekoguryo.ml prometheusUrl: https://prometheus.vmi.system.mc-1.thekoguryo.ml state: Ready version: 1.1.0   Managed Cluster 등록하기 준비 단계 Admin Cluster 설정   Context 변경\n$ kubectl config use-context admin-cluster Switched to context \u0026#34;admin-cluster\u0026#34;. $ kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * admin-cluster cluster-cbrgetu5uwa user-cbrgetu5uwa managed-cluster-1 cluster-cpcnsgcmqsq user-cpcnsgcmqsq managed-cluster-2 cluster-cvoyziqrrzq user-cvoyziqrrzq   API Server 주소 확인\n$ kubectl cluster-info Kubernetes control plane is running at https://129.154.60.75:6443   확인한 주소로 ConfigMap 생성\nkubectl apply -f \u0026lt;\u0026lt;EOF - apiVersion: v1 kind: ConfigMap metadata: name: verrazzano-admin-cluster namespace: verrazzano-mc data: server: \u0026#34;https://129.154.60.75:6443\u0026#34; EOF   등록 단계 - LetsEncrypt staging 기준 Admin Verrazzano와 Managed Cluster간에 https로 통신을 합니다. 알려진 CA 인증서 외에, Self-Signed 인증서, Let\u0026rsquo;s Encrypt Staging을 사용하는 경우 Admin Cluster와 Managed Cluster 간의 원활한 통신을 위해 Managed Cluster의 CA 인증서를 Admin Cluster에 등록해 주는 절차가 필요합니다. 관련 절차는 아래 문서를 참조합니다.\n 필요시 관련 문서 참조 - https://verrazzano.io/latest/docs/setup/install/multicluster/#prerequisites  ※ Let\u0026rsquo;s Encrypt Staging의 경우 DST Root CA의 만료로 인해 관련 업데이트하지 않는 클라이언트 환경으로 인해 추가적인 작업이 더 필요합니다.\n Let\u0026rsquo;s Encrypt Staging  https://letsencrypt.org/docs/staging-environment/    Admin Cluster 설정   Context 변경\nkubectl config use-context admin-cluster   LetsEncrypt staging CA 등록\n# (STAGING) Artificial Apricot R3 curl -O https://letsencrypt.org/certs/staging/letsencrypt-stg-int-r3.pem # (STAGING) Pretend Pear X1 curl -O https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem MGD_CA_CERT=$(cat *.pem) echo $MGD_CA_CERT kubectl create secret generic \u0026#34;ca-secret-managed-cluster-1\u0026#34; \\  -n verrazzano-mc \\  --from-literal=cacrt=\u0026#34;$MGD_CA_CERT\u0026#34; \\  --dry-run=client \\  -o yaml \u0026gt; ca-secret-managed-cluster-1.yaml kubectl apply -f ca-secret-managed-cluster-1.yaml   Managed Cluster로 등록\n 등록 클러스터 이름이 managed-cluster-1 인 경우  kubectl apply -f \u0026lt;\u0026lt;EOF - apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: name: managed-cluster-1 namespace: verrazzano-mc spec: description: \u0026#34;VerrazzanoManagedCluster object\u0026#34; caSecret: ca-secret-managed-cluster-1 EOF   등록 대기\nkubectl wait --for=condition=Ready \\  vmc managed-cluster-1 -n verrazzano-mc   Managed Cluster용 등록 파일 생성\nkubectl get secret verrazzano-cluster-managed-cluster-1-manifest \\  -n verrazzano-mc \\  -o jsonpath={.data.yaml} | base64 --decode \u0026gt; managed-cluster-1-register.yaml   Managed Cluster 설정   Context 변경\nkubectl config use-context managed-cluster-1   등록\nkubectl apply -f managed-cluster-1-register.yaml   등록 결과\nManaged Cluster를 Admin Cluster에 등록하게 되면, Managed Cluster에서 주기적으로 Admin Cluster에 앞서 VerrazzanoManagedCluster 유형으로 등록한 자원(managed-cluster-1)의 상태를 주기적으로 업데이트 하게 됩니다.\n  ranch agent 오류 확인\n아래와 같이 x509: certificate signed by unknown authority 인증서 오류로 인해 POD가 기동하지 않습니다.\n[opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl get pod -n cattle-system NAME READY STATUS RESTARTS AGE cattle-cluster-agent-5dd4dd9594-v9tf8 0/1 CrashLoopBackOff 3 2m8s [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl logs -n cattle-system cattle-cluster-agent-5dd4dd9594-v9tf8 ... INFO: https://rancher.admin.thekoguryo.ml/ping is accessible ... time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate details from https://rancher.admin.thekoguryo.ml\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate #0 (https://rancher.admin.thekoguryo.ml)\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Subject: CN=*.admin.thekoguryo.ml\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Issuer: CN=(STAGING) Artificial Apricot R3,O=(STAGING) Let\u0026#39;s Encrypt,C=US\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;IsCA: false\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;DNS Names: [*.admin.thekoguryo.ml]\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;IPAddresses: \u0026lt;none\u0026gt;\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;NotBefore: 2022-01-04 04:52:39 +0000 UTC\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;NotAfter: 2022-04-04 04:52:38 +0000 UTC\u0026#34; .. time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate #1 (https://rancher.admin.thekoguryo.ml)\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Subject: CN=(STAGING) Artificial Apricot R3,O=(STAGING) Let\u0026#39;s Encrypt,C=US\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Issuer: CN=(STAGING) Pretend Pear X1,O=(STAGING) Internet Security Research Group,C=US\u0026#34; ... time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Certificate #2 (https://rancher.admin.thekoguryo.ml)\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Subject: CN=(STAGING) Pretend Pear X1,O=(STAGING) Internet Security Research Group,C=US\u0026#34; time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=info msg=\u0026#34;Issuer: CN=(STAGING) Doctored Durian Root CA X3,O=(STAGING) Internet Security Research Group,C=US\u0026#34; ... time=\u0026#34;2022-01-04T06:03:57Z\u0026#34; level=fatal msg=\u0026#34;Certificate chain is not complete, please check if all needed intermediate certificates are included in the server certificate (in the correct order) and if the cacerts setting in Rancher either contains the correct CA certificate (in the case of using self signed certificates) or is empty (in the case of using a certificate signed by a recognized CA). Certificate information is displayed above. error: Get \\\u0026#34;https://rancher.admin.thekoguryo.ml\\\u0026#34;: x509: certificate signed by unknown authority\u0026#34;   cattle-cluster-agent를 수정하여 반영합니다.\n  수정\nkubectl edit deploy cattle-cluster-agent -n cattle-system   추가 사항\ninitContainer를 통해 별도로 새 인증서를 다운로드 받아 공유 볼륨을 통해 cluster-register 컨테이너에 전달합니다.\napiVersion:apps/v1kind:Deployment...containers:- env:...name:cluster-register...volumeMounts:- mountPath:/cattle-credentialsname:cattle-credentialsreadOnly:true- mountPath:/etc/pki/tls/certs/name:certsinitContainers:- name:prepare-to-fix-ca-expireimage:ghcr.io/verrazzano/rancher-agent:v2.5.9-20211209021347-2e57ce2a4command:- /bin/sh- -c- |echo start curl -k https://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem -o /etc/pki/ca-trust/source/anchors/letsencrypt-stg-root-x1.pem update-ca-trust cp /etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem /certs/ca-bundle.crt cp /etc/pki/ca-trust/extracted/openssl/ca-bundle.trust.crt /certs/ca-bundle.trust.crt echo endvolumeMounts:- mountPath:/certs/name:certs ...volumes:- name:cattle-credentialssecret:defaultMode:320secretName:cattle-credentials-2b150a3- name:certsemptyDir:{}status...  결과 재 확인\n[opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl get pod -n cattle-system NAME READY STATUS RESTARTS AGE cattle-cluster-agent-6b785ffd86-vshsw 1/1 Running 0 42s   위 POD가 정상으로 기동해야 Admin Cluster의 Rancher에서 Managed Cluster가 Active 상태로 등록된 것을 확인할 수 있습니다.\n    등록 결과 확인   admin cluster로 context 변경\nkubectl config use-context admin-cluster   앞서 등록한 Managed Cluster 자원의 현재 상태 확인\n status.lastAgentConnectTime 값으로 마지막 연결된 시간을 통해 연결되었음을 확인합니다. managed-cluster-1의 apiUrl과 prometheusHost 주소도 등록된 것을 알 수 있습니다.  $ kubectl get vmc managed-cluster-1 -n verrazzano-mc -o yaml apiVersion: clusters.verrazzano.io/v1alpha1 kind: VerrazzanoManagedCluster metadata: ... spec: ... status: apiUrl: https://verrazzano.mc-1.thekoguryo.ml conditions: - lastTransitionTime: \u0026#34;2021-12-31T06:48:59Z\u0026#34; message: Ready status: \u0026#34;True\u0026#34; type: Ready lastAgentConnectTime: \u0026#34;2021-12-31T06:57:06Z\u0026#34; prometheusHost: prometheus.vmi.system.mc-1.thekoguryo.ml rancherRegistration: message: Registration of managed cluster completed successfully status: Completed state: Active   최종 확인 Promethus   Verrazzano Admin Cluster의 Prometheus로 접속합니다.\n 예, https://prometheus.vmi.system.admin.thekoguryo.ml/    샘플 쿼리 node_disk_io_time_seconds_total 로 하면 그림과 같이 Verrazzano Managed Cluster의 메트릭 정보도 함께 조회됨을 알 수 있습니다.\n   Kibana   Verrazzano Admin Cluster의 Kibana로 접속합니다.\n 예, https://kibana.vmi.system.admin.thekoguryo.ml/    아직 배포된 앱이 없다면, 모든 Verrazzano Cluster에 있는 namespace 기준(예, cert-manager)로 인덱스 패턴을 생성합니다.\n  선택 가능한 필드에서 cluster_name을 클릭합니다. 아래와 같이 Admin Cluster(local) 및 등록된 Managed Cluster에서도 로그를 가져오는 것을 알 수 있습니다.\n   Rancher   Verrazzano Admin Cluster의 Prometheus로 접속합니다.\n 예, https://rancher.admin.thekoguryo.ml/    아래와 같이 Managed Cluster가 정상적으로 등록된 것을 볼 수 있습니다.\n   Explorer를 클릭하여 클러스터로 화면에서도 잘 보이는 것을 알 수 있습니다.\n   ","lastmod":"2022-01-04T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/verrazzano/3.multicluster-verrazzano/","tags":["oke","verrazzano","multicluster"],"title":"6.3 Multicluster Verrazzano 설치하기"},{"categories":null,"contents":"4.1.3 NGINX Ingress Controller에서 TLS termination(feats. Let\u0026rsquo;s Encrypt) Ingress Controller에서 외부 수신을 SSL로 하기 위한 설정을 확인합니다.\nSelf-Signed 인증서 사용하기 테스트 목적으로 Self-Signed 인증서를 만들어 사용하는 방법을 확인해 봅니다. 실제 환경에서는 공인 인증기관에서 발급받은 인증서를 사용합니다. Self-Signed 인증서 발급 절차만 대체되어 TLS Secret 등록과정부터는 동일하게 수행됩니다.\n참고 문서\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm  인증서 만들기   Cloud Shell 또는 작업환경에서 다음 명령으로 인증서를 생성합니다. 공인 인증기관에서 발급받은 인증서 사용시 하지 않아도 됩니다.\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34;   TLS Secret을 만듭니다.\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt   실행결과\noke_admin@cloudshell:~ (ap-seoul-1)$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; Generating a 2048 bit RSA private key *************************************************************************************************************************************************************+++++ ****************************************************************************************************************************+++++ writing new private key to \u0026#39;tls.key\u0026#39; ----- oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret tls tls-secret --key tls.key --cert tls.crt secret/tls-secret created   TLS Ingress 자원 배포   테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(tls-termination.yaml)을 작성합니다.\n spec.tls.secretName으로 앞서 생성한 Self-Signed 인증서 이름을 사용합니다.  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-tls-terminationannotations:kubernetes.io/ingress.class:nginxspec:tls:- secretName:tls-secretrules:- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  기존 테스트 ingress는 삭제하고, 작성한 tls-termination.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f tls-termination.yaml ingress.networking.k8s.io/ingress-tls-termination created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80, 443 5s   TLS 적용 결과 검증   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서로 인한 경고 메시지가 뜹니다.\n   고급을 클릭하고 해당 페이지로 이동을 선택합니다.\n   브라우저 주소창 메뉴를 통해 인증서 정보를 확인합니다. Self-Signed 인증서로 루트 인증서가 신뢰할 수 없다는 경고를 확인할 수 있습니다.\n    Let\u0026rsquo;s Encrypt \u0026amp; Cert Manager 사용하기 Let\u0026rsquo;s Encrypt는 무료 인증서 발급 사이트로 TLS에 사용할 인증서를 발급 받을 수 있습니다. 대신 90일 동안만 유효하며 만료전에 갱신해야 합니다. Kubernetes에서는 Cert Manager를 통해 자동으로 갱신할 수 있습니다.\n https://letsencrypt.org/2015/11/09/why-90-days.html  설치 참고 문서\n https://cert-manager.io/docs/tutorials/acme/ingress/  Cert Manager 배포   Cloud Shell 또는 작업환경에서 Cert Manager를 배포합니다.\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml   설치 확인\ncert-manager namespace에 자원들이 정상 실행중인 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all -n cert-manager NAME READY STATUS RESTARTS AGE pod/cert-manager-55658cdf68-sk5nj 1/1 Running 0 18s pod/cert-manager-cainjector-967788869-b77w2 1/1 Running 0 18s pod/cert-manager-webhook-7b86bc6578-pnxtg 1/1 Running 0 18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager ClusterIP 10.96.100.11 \u0026lt;none\u0026gt; 9402/TCP 19s service/cert-manager-webhook ClusterIP 10.96.212.15 \u0026lt;none\u0026gt; 443/TCP 19s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 19s deployment.apps/cert-manager-cainjector 1/1 1 1 19s deployment.apps/cert-manager-webhook 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-55658cdf68 1 1 1 19s replicaset.apps/cert-manager-cainjector-967788869 1 1 1 19s replicaset.apps/cert-manager-webhook-7b86bc6578 1 1 1 18s   Let\u0026rsquo;s Encrypt Issuer 구성 본 예제에서는 Let\u0026rsquo;s Encrypt에서 제공하는 Staging Issuer, Production Issuer을 사용할 수 있습니다. 여기서는 테스트용도로 Staging Issuer를 사용하겠습니다.\n  Let\u0026rsquo;s Encrypt Staging Issuer 설정\nhttps://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml 파일에서 email 부분만 본인 것으로 수정하여 반영합니다.\n네임스페이스에만 사용되는 Issuer가 아닌 전체 쿠버네티스 클러스터에 사용하기 위해 ClusterIssuer 유형을 사용합니다.\napiVersion:cert-manager.io/v1kind:ClusterIssuermetadata:name:letsencrypt-stagingspec:acme:# The ACME server URLserver:https://acme-staging-v02.api.letsencrypt.org/directory# Email address used for ACME registrationemail:user@example.com# Name of a secret used to store the ACME account private keyprivateKeySecretRef:name:letsencrypt-staging# Enable the HTTP-01 challenge providersolvers:- http01:ingress:class:nginx  Let\u0026rsquo;s Encrypt Production Issuer\nProduction Issuer도 https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml 파일을 이용해 동일한 방식으로 설치할 수 있습니다. 다만 사용 limit로 인해 삭제, 생성을 반복할 경우 Rate Limit에 걸릴 수 있습니다.\n  설정 적용\noke_admin@cloudshell:$ (ap-seoul-1)$ kubectl create --edit -f https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml issuer.cert-manager.io/letsencrypt-staging created   TLS Ingress 자원 배포   테스트 앱은 이전 그대로 사용합니다.\n  ingress 설정 YAML(tls-termination-cert-manager.yaml)을 작성합니다.\n 문서 작성일 기준 (STAGING) Doctored Durian Root CA의 만료로 인해 staging issuer 사용시에도 유효하지 않은 인증서라고 나올 수 있습니다. cert manager issuer는 production issuer를 사용하겠습니다. 대신 production issuer는 생성을 반복할 경우 limit에 걸릴 수 있습니다. cert-manager.io/cluster-issuer: 방금 생성한 letsencrypt-staging 설정, issuer가 아닌 cluster-issuer를 사용합니다. spec.tls 하위에 tls 저장할 저장소 이름 및 발급받아 사용할 도메인 이름을 지정합니다  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-tls-termination-cert-managerannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:\u0026#34;letsencrypt-staging\u0026#34;spec:tls:- secretName:ingress-thekoguryo-ml-tlshosts:- green.ingress.thekoguryo.ml- blue.ingress.thekoguryo.mlrules:- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80  기존 테스트 ingress는 삭제하고, 작성한 tls-termination-cert-manager.yaml을 배포합니다.\noke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl apply -f tls-termination-cert-manager.yaml ingress.networking.k8s.io/ingress-tls-termination-cert-manager created oke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination-cert-manager \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80, 443 9s   인증서 발급 확인\n지정한 spec.tls.secretName으로 secret이 만들어지고, certificate 상태(READY)가 True가 되면 정상 발급되었습니다.\noke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-jbv7p kubernetes.io/service-account-token 3 3d19h ingress-thekoguryo-ml-tls kubernetes.io/tls 2 15m letsencrypt-staging Opaque 1 16m oke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get certificate NAME READY SECRET AGE ingress-thekoguryo-ml-tls True ingress-thekoguryo-ml-tls 16m   TLS 적용 결과 검증   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서와 달리 경고 없이 유효한 인증서로 표시됩니다.\n    DNS 대체 주소에 요청한 host가 모두 등록되어, blue 앱도 인증에러 없이 접속됩니다.\n   Let\u0026rsquo;s Encrypt Root CA 변경으로 인해 인증오류 해결   Staging Issuer를 사용할 경우, (STAGING) Doctored Durian Root CA X3 만료로 인해 웹브라우저 접속했을 때 인증 오류가 발생하는 경우가 있습니다. Production Issuer는 해당 문제가 발생하지 않습니다.\n   해당 에러가 발생하는 경우 변경된 새 Root CA를 let\u0026rsquo;s encrypt 사이트에서 다운 받아 브라우저에 등록해 줍니다.\n  파일 다운로드\nhttps://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem\n  브라우저에 다운받은 Root CA 추가(크롬 브라우저 기준)\n  크롬 브라우저 \u0026gt; 설정 \u0026gt; 개인정보 및 보안 \u0026gt; 인증서 관리 로 이동\n  인증서 가져오기 클릭\n   다운받은 파일 선택\n   인증서 설치\n   인증서 등록후 확인\n신뢰할 수 있는 루트 인증 기관에 방금 등록한 (STAGING) 인증서가 보임\n   인증서가 등록후 다시 앱의 웹페이지를 접속하면 인증오류가 발생하지 않고, 인증 경로가 아래와 같이 보이게 됩니다.\n       참고 링크\nhttps://github.com/vancluever/terraform-provider-acme/issues/161\n  ","lastmod":"2021-12-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oss/ingress-controller/3.nginx-ingress-tls-termination/","tags":["oss","ingress-controller"],"title":"4.1.3 NGINX Ingress Controller에서 TLS termination"},{"categories":null,"contents":"14.3.3 Insomnia - OCI REST API 호출하기 전문 REST 클라이언트 툴인 Insomnia를 사용하면 GUI를 통해 사용하기가 더 편리합니다. 아직 OCI Request Signature를 기본 지원하지 않아, Insomnia에서 OCI REST API를 호출하는 방법을 설명하고자 합니다.\nStep 0. insomnia 다운로드 공식 사이트에서 Free 앱을 다운로드 받습니다.\n https://insomnia.rest/download  Step 1. insomnia-plugin-oci-request-signature 플러그인 OCI Request Signature를 통한 호출을 하기 위해 Insomina에서 제공하는 플러그인 확장 기능을 통해 OCI용 플러그인을 만들었습니다. 이를 아래와 같이 설치해서 사용하면 됩니다.\n설치 가이드를 따라 플러그인을 설치합니다.\nStep 2. 환경변수 설정   Insomnia 환경변수를 다음 가이드에 따라 설정합니다.\n environment variables within Insomnia    필요 환경 변수\n isOracleCloud: \u0026ldquo;true\u0026quot;로 설정, 설정한 플러그인인 글로벌하게 적용되는 것이라, true일때만 세부 스크립트가 동작함 tenancyId: tenancy OCID authUserId: 사용자 OCID keyFingerprint: API Key의 핑거프린트 privateKeyPath: API Key로 등록한 Public Key에 매칭되는 Private Key의 경로     Step 3. 사용자 조회 REST API 실행 아래와 같이 환경변수만 선택하면, 추가작업 없이 REST API를 호출할 수 있습니다.\n API Endpoints  아래 Endpoint 중 IAM은 Home Region의 Endpoint를 사용합니다. Identity and Access Management Service API | Oracle Cloud Infrastructure API Reference and Endpoints   ListUsers  https://docs.oracle.com/en-us/iaas/api/#/en/identity/20160918/User/ListUsers     Step 4. 사용자 생성 REST API 실행 아래와 같이 환경변수만 선택하면, 추가작업 없이 REST API를 호출할 수 있습니다.\n CreateUser  https://docs.oracle.com/en-us/iaas/api/#/en/identity/20160918/User/CreateUser     ","lastmod":"2019-05-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/3/","tags":["rest api","insomnia"],"title":"14.3.3 Insomnia - OCI REST API 호출하기"},{"categories":null,"contents":"10.3 Backend 웹서버 만들기 첫 번째 Linux Instance 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute » Instances 항목으로 이동합니다.\n  Instance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\n  생성정보 입력\n  주요 항목만 필요한 값을 입력하고 나머지는 기본값을 그대로 사용합니다.\n  기본 정보\n Name: 이름 입력, 예를 들어 “Webserver1” Create in compartment: 위치할 Compartment를 선택, 앞서 생성한 Sandbox 선택    Image and Shape\n 기본값이 VM.Standard.E2.1.Micro 자원이 Limit로 인해 부족한지 확인후 부족한 경우 VM.Standard.E2.1로 변경합니다.    Networking\n앞서 만든 LoadBalancerVCN의 Public Subnet을 선택합니다.\n  Add SSH Keys\n이후 웹서버 설치 작업을 위해 VM에 접속시 사용할 SSH Key를 Public Key를 입력합니다.\n  생성 정보 입력후 Create 클릭\n    생성 완료\n   두 번째 Instance 만들기   동일한 방법으로 두 번째 인스턴스를 만듭니다.\n  생성정보 입력\n  일부 값만 다르게 하여 생성합니다.\n  기본 정보\n Name: \u0026ldquo;Webserver2\u0026rdquo;    Placement\n  가용성을 위해 첫번 째 인스턴스와 다른 Fault Domain을 선택하면 좋습니다.\n   Image and Shape\n 기본값이 VM.Standard.E2.1.Micro 자원이 Limit로 인해 부족한지 확인후 부족한 경우 VM.Standard.E2.1로 변경합니다.      생성완료\n   첫번째 Apache HTTP Server 설치 5.1 Linux 인스턴스에 Apache HTTP Server 설치와 설치과정은 거의 같습니다.\n  생성한 Instance에 SSH 명령을 통해 접속\n  Apache HTTP Server 설치\n# Apache HTTP Server 설치 sudo yum -y install httpd # OS 방화벽에서 Apache HTTP용 포트, 80 포트 개방 sudo firewall-cmd --permanent --add-port=80/tcp # 방화벽 변경정보 다시 반영 sudo firewall-cmd --reload # Apache 시작 sudo systemctl start httpd   테스트를 위해 서버의 Root Index Document 생성\nsudo su echo \u0026#39;Hello Apache on WebServer #1\u0026#39; \u0026gt;/var/www/html/index.html   설치후 테스트\n[opc@webserver1 ~]$ curl http://127.0.0.1 Hello Apache on WebServer #1   두번째 Apache HTTP Server 설치 두번째도 동일하게 Apache HTTP Server를 설치합니다.\n  ~ 2까지는 첫 번째와 동일\n  테스트를 위해 서버의 Root Index Document 생성\nsudo su echo \u0026#39;Hello Apache on WebServer #2\u0026#39; \u0026gt;/var/www/html/index.html   설치후 테스트\n[opc@webserver2 opc]# curl http://127.0.0.1 Hello Apache on WebServer #2   ","lastmod":"2019-01-23T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/3/","tags":["load balancer","backend"],"title":"10.3 Backend 웹서버 만들기"},{"categories":null,"contents":"6.3 그룹 만들기 Step 1. OCI 그룹 추가  관리자로 OCI 콘솔에 로그인합니다. OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Groups 항목으로 이동합니다. Cloud Account 생성후에 Administrators 그룹하나만 있는 것을 볼 수 있습니다. Create Group 클릭 그룹 정보 입력  Name: \u0026ldquo;SandboxGroup\u0026rdquo; 입력 Description: 설명 입력, 예, 이름과 동일하게 \u0026ldquo;SandboxGroup\u0026rdquo;   Create 클릭  Step 2. 그룹(Group)에 사용자 추가하기   생성된 그룹을 클릭합니다.\n  그룹에 사용자를 추가하기 위해 Add User to Group을 클릭   앞서 생성한 새로운 사용자를 추가할 사용자로 선택\n   그룹에 사용자가 추가된 모습   ","lastmod":"2019-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/3/","tags":["oci group"],"title":"6.3 그룹 만들기"},{"categories":null,"contents":"5.3 방법 #2. Custom Security List 추가하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; Virtual Private Networks 항목으로 이동합니다.\n  현재 사용중인 VCN(예, ExampleVCN)을 클릭하여 VCN의 상세 정보로 이동합니다.\n  왼쪽 아래 Resources \u0026gt; Security List 항목으로 이동합니다.\n  VCN 마법사로 생성한 Private Subnet용, Public Subnet용 Security외에 Custom Security List 추가를 위해 Create Security List를 클릭합니다.\n   생성창에서 Ingress 방화벽 규칙을 입력합니다.\n Security List Name: 새 규칙의 이름 입력, 예) Customer Security List Ingress Rule 설정 : Apache HTTP 서버가 사용하는 수신 포트 추가  Source CIDR: 0.0.0.0/0, 모든 IP에서 오는 요청 IP Protocol: TCP Destination Port Range: 80, 개방할 포트   Egress Rule 설정: 변경하지 않습니다.     하단으로 스크롤하여 Create Securit List를 클릭\n  새 Security List를 추가되었습니다.\n   Subnet에 새 Security List를 추가하기 위해 좌측 Resources 메뉴에서 Subnets으로 이동합니다.\n  Apache 서버가 설치된 인스턴스가 위치한 Public Subnet을 클릭합니다.\n   현재 Default Security를 변경하지 않고 새 Security List를 추가하기 위해 Add Security List를 클릭합니다.\n   앞서 만든 Custom Security List를 추가합니다.\n   이제 Public Subnet에 두 개 Security List가 모두 적용됩니다.\n   ","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/3/","tags":["security list"],"title":"5.3 Custom Security List 추가하기"},{"categories":null,"contents":"1.3 Compartment Compartment는 자원들을 쉽게 관리할 수 있도록 하는 개념으로 폴더 구조라고 생각하면 됩니다. Tenancy가 생성되면 최초로 Root Compartment 하나가 만들어져 있으며, 관리자가 Root Compartment 하위로 새로운 Compartment를 추가할 수 있습니다. 모든 OCI 자원들은 특정 Compartment에 속하게 되며 Compartment 단위로 사용자들의 접근 정책을 관리할 수 있습니다.\n ","lastmod":"2018-12-30T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/3/","tags":["compartment"],"title":"1.3 Compartment"},{"categories":null,"contents":"14.1.1.4 API Public Key 등록   OCI Config File에 등록한 사용자 상세 페이지로 이동\n  왼쪽 아래 Resources에서 API Keys 선택\n  Add API Key 클릭\n   oci setup config 실행 후 생성된 API Public Key 확인\n[opc@bastion-host ~]$ cat ~/.oci/oci_api_key_public.pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAycclV86OzQ+x6I13HEbe ... gCO1GdLyJNS08Zv1uiW6j1IYIszrmr6XK482Vf3r48u8ZkFgBBlsPjU03R9H1x52 dwIDAQAB -----END PUBLIC KEY-----   생성된 API Public Key 내용을 API Public Key 추가\n   OCI CLI를 위한 API Public Key 등록 완료   ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/4/","tags":["API Public Key","API Key"],"title":"14.1.1.4 API Public Key 등록"},{"categories":null,"contents":"8.4 백업을 위한 Snapshot 실행한 시간 기준으로 File System의 Snapshot을 만들 수 있습니다. 만들어진 Snapshot을 통해 파일 또는 전체를 복구할 수 있습니다.\nSnaphot #1   테스트한 VM에 접속합니다.\n  마운트된 File System에 파일 하나를 복사합니다.\n[opc@examplelinuxinstance ~]$ cd /mnt/FSS-Storage/ [opc@examplelinuxinstance FSS-Storage]$ wget https://github.com/oracle/oci-cli/releases/download/v3.4.2/oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip ... 2022-01-14 06:04:59 (6.24 MB/s) - ‘oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip’ saved [42528311/42528311] [opc@examplelinuxinstance FSS-Storage]$ ls -la total 43025 drwxrwxrwx. 2 root root 3 Jan 14 06:04 . drwxr-xr-x. 3 root root 25 Jan 14 01:59 .. drwxrwxrwx. 2 root root 0 Jan 14 06:05 .snapshot -rw-r--r--. 1 root root 19 Jan 14 02:18 hello1.txt -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:38 oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip   웹브라우저에서 OCI Console로 이동하여, 앞서 만든 File System의 상세화면으로 이동합니다.\n  왼쪽아래 Resources \u0026raquo; Snapshots로 이동합니다.\n  Snapshot을 만들기 위해 Create Snapshot 을 클릭합니다.\n   Snapshot 생성화면\n   생성확인\n   Compute Instance에서 확인\n마운트된 경로 바로 밑 .snapshot 폴더에 생성된 Snapshot과 파일들이 보입니다.\n[opc@examplelinuxinstance FSS-Storage]$ ls -la .snapshot/ total 2 drwxrwxrwx. 3 root root 1 Jan 14 06:16 . drwxrwxrwx. 2 root root 3 Jan 14 06:04 .. drwxrwxrwx. 2 root root 3 Jan 14 06:04 Snapshot-20220114-0614-39 [opc@examplelinuxinstance FSS-Storage]$ ls -la .snapshot/Snapshot-20220114-0614-39/ total 43025 drwxrwxrwx. 2 root root 3 Jan 14 06:04 . drwxrwxrwx. 3 root root 1 Jan 14 06:17 .. -rw-r--r--. 1 root root 19 Jan 14 02:18 hello1.txt -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:38 oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip   Snaphot #2   접속한 Compute Instance에서 마운트된 File System에 파일을 하나 더 복사합니다.\n[opc@examplelinuxinstance FSS-Storage]$ wget https://github.com/oracle/oci-cli/releases/download/v3.4.2/oci-cli-3.4.2-Oracle-Linux-8-Offline.zip ... 2022-01-14 06:18:36 (4.57 MB/s) - ‘oci-cli-3.4.2-Oracle-Linux-8-Offline.zip’ saved [42528311/42528311] [opc@examplelinuxinstance FSS-Storage]$ ls -la total 86033 drwxrwxrwx. 2 root root 4 Jan 14 06:18 . drwxr-xr-x. 3 root root 25 Jan 14 01:59 .. drwxrwxrwx. 3 root root 1 Jan 14 06:18 .snapshot -rw-r--r--. 1 root root 19 Jan 14 02:18 hello1.txt -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:38 oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:39 oci-cli-3.4.2-Oracle-Linux-8-Offline.zip   웹브라우저에서 OCI Console로 이동하여, Snapshot을 하나 더 만듭니다.\n  두 번째 Snapshot생성확인\n   Compute Instance에서 확인\n.snapshot 폴더에 생성된 Snapshot과 파일들이 보입니다.\n[opc@examplelinuxinstance FSS-Storage]$ ls -la .snapshot/ total 2 drwxrwxrwx. 4 root root 2 Jan 14 06:21 . drwxrwxrwx. 2 root root 4 Jan 14 06:18 .. drwxrwxrwx. 2 root root 3 Jan 14 06:04 Snapshot-20220114-0614-39 drwxrwxrwx. 2 root root 4 Jan 14 06:18 Snapshot-20220114-0620-02 [opc@examplelinuxinstance FSS-Storage]$ ls -ls .snapshot/Snapshot-20220114-0614-39/ total 43024 8 -rw-r--r--. 1 root root 19 Jan 14 02:18 hello1.txt 8 -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt 43008 -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:38 oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip [opc@examplelinuxinstance FSS-Storage]$ ls -ls .snapshot/Snapshot-20220114-0620-02/ total 86032 8 -rw-r--r--. 1 root root 19 Jan 14 02:18 hello1.txt 8 -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt 43008 -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:38 oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip 43008 -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:39 oci-cli-3.4.2-Oracle-Linux-8-Offline.zip   Snapshot은 읽기 전용 폴더로 Snapshot에 있는 파일 또는 특정 Snapshot을 전체 복사하는 방법을 통해 복구할 수 있습니다.\n[opc@examplelinuxinstance FSS-Storage]$ cd .snapshot/Snapshot-20220114-0620-02/ [opc@examplelinuxinstance Snapshot-20220114-0620-02]$ ls -la total 86033 drwxrwxrwx. 2 root root 4 Jan 14 06:18 . drwxrwxrwx. 4 root root 2 Jan 14 06:22 .. -rw-r--r--. 1 root root 19 Jan 14 02:18 hello1.txt -rw-rw-r--. 1 opc opc 19 Jan 14 05:50 hello2.txt -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:38 oci-cli-3.4.2-Oracle-Linux-7.9-Offline.zip -rw-rw-r--. 1 opc opc 42528311 Jan 13 16:39 oci-cli-3.4.2-Oracle-Linux-8-Offline.zip [opc@examplelinuxinstance Snapshot-20220114-0620-02]$ rm -rf oci-cli-3.4.2-Oracle-Linux-8-Offline.zip rm: cannot remove ‘oci-cli-3.4.2-Oracle-Linux-8-Offline.zip’: Read-only file system   ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/4/","tags":["file storage","snapshot"],"title":"8.4 백업을 위한 Snapshot"},{"categories":null,"contents":"9.3 Custom Image Export/Import Custom Image는 Import, Export 기능을 제공합니다. Custom Image는 해당 Region 내 Comparment에 속하게 되며, 다른 Compartment로 복사하거나, 다른 Region 내로 복사하기 위해서는 Object Storage로 Export 받은 다음, 원하는 곳에서 다시 Import 받으면 됩니다.\nObject Storage에 Bucket 생성하기 및 사전인증 부분에 대한 설명은 7. Object Storage 사용하기를 참조하시기 바랍니다.\nCustom Image Export   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Custom Images 항목으로 이동합니다.\n  Export를 원하는 이미지를 클릭합니다.\n  대상 이미지 우측의 액션 메뉴의 Export Image를 클릭합니다.   Export는 두 가지 방식으로 할 수 있으며 형식은 OCI 포맷으로 Export 됩니다.\n Object Storage Bucket: 로그인한 Tenancy내에 Object Storage Bucket 으로 Export합니다. 대상 Compartment와 Bucket을 선택합니다. Object Storage URL: URL을 통해 Export합니다. 다른 Tenancy로 Export하는 것이 가능하게 됩니다.    Export 정보를 입력하고 아래쪽 Export Image를 클릭합니다.\n  Bucket name: 대상 Object Storage Bucket 이름 선택, 없는 경우 사전에 만듭니다.\n  Image name: 예, ExampleLinuxCustomImage\n  Image format: .oci 포맷 선택\n     Custom Image의 상세화면으로 이동하여 아래쪽 Work requests를 보면 Export 시작, 종료 시간을 확인할 수 있습니다. 8분 정도 걸린 것을 알 수 있습니다.\n   대상 Object Storage Bucket에 가면 Export 된 것을 확인할 수 있습니다.\n그림에서 보듯 1.28GiB에 8분 정도 걸린 걸 알 수 있습니다.\n   Custom Image Import Object 사전인증(Pre-Authenticated Request) 설정 Object Storage URL에서 Import하기 위해서는 대상 Object가 별도 인증없이 접근할 수 있는 사전인증(Pre-Authenticated Request)이 설정되어 있어야 합니다. Public Bucket은 작성일자 기준으로 지원하지 않습니다.\n  Custome Image Object의 우측 액션 메뉴에서 Pre-Authenticated Request 생성 명령을 통해 Custom Image Object에 대한 읽기 권한으로 임시 사전 인증 링크를 생성합니다.\n   아래 생성된 링크를 복사합니다.\n     다른 Region 또는 다른 Cloud Account 에서 Import 하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Custom Images 항목으로 이동합니다.\n  오른쪽 위에서 임포트할 Region으로 변경하고 Import Image 클릭\n   임포트 입력정보\n  Name: 이미지 이름 입력\n  Operating system: Linux 선택\n  Object Storage URL: 앞서 복사한 이미지의 Object Storage URL 복사\n  Image type: OCI선택\n     정보 입력후 아래쪽의 Import image 클릭\n  임포트 중인 Custom Image의 상세화면으로 이동하여 아래쪽 Work requests를 보면 진행상태를 알수 있고, Import가 완료되면 시작, 종료 시간을 확인할 수 있습니다. 10분 정도 걸린 것을 알 수 있습니다.\n   임포트 완료되었습니다.\n   ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter09/3/","tags":["os image","custom image","image"],"title":"9.3 Custom Image Export/Import"},{"categories":null,"contents":"7.4 Object 권한 관리 - Public Bucket   생성된 Bucket은 기본적으로 Private 상태입니다. 인증없이는 접근할 수 없는 상태입니다. Bucket을 Public으로 변경하게 되면 별도 인증 없이 접근할 수 있습니다. Pre-Authenticated Requests 처럼 임시 공유 링크가 생기는 것이 아니라, Bucket에 대한 접근이 허용되어 Object의 URL 경로로 바로 접근할 수 있습니다.\n  Bucket의 상세 정보 화면으로 이동합니다. Visibility가 Private 임을 확인 할 수 있습니다. 변경을 위해 Edit Visibility를 클릭합니다.\n   Visiblity 설정화면입니다.\n  Visibility: PUBLIC으로 변경합니다.\n  Allow users to list objects from bucket: 기본적으로 선택되어 있습니다. 선택하면, API로 Object 목록 조회를 허용하게 됩니다.\n     Object의 URL 경로를 확인하기 위해 Object의 우측 액션메뉴를 통해 상세정보로 이동합니다.   Object 상세 정보에서 URL 경로를 확인합니다. URL 경로 포맷은 다음과 같습니다.\nhttps://objectstorage.\u0026lt;region_name\u0026gt;.oraclecloud.com/n/\u0026lt;object_storage_namespace\u0026gt;/b/\u0026lt;bucket\u0026gt;/o/\u0026lt;object_name\u0026gt;    확인된 Object URL을 브라우저로 접속합니다. 인증없이 바로 접속되는 것을 확인할 수 있습니다.   Object 상위 경로로 접근하면 아래와 같이 JSON 형식으로 Object의 목록이 조회됩니다.\nhttps://objectstorage.\u0026lt;region_name\u0026gt;.oraclecloud.com/n/\u0026lt;object_storage_namespace\u0026gt;/b/\u0026lt;bucket\u0026gt;/o/    ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/4/","tags":["object storage","bucket","public bucket"],"title":"7.4 Object 권한 관리 - Public Bucket"},{"categories":null,"contents":"3.7.4 정책 기반 Block Volume 백업하기 Volume Backup Policies   Bronze Policy\n 월단위 - 매월 첫날에 월간 증분 백업이 실행됩니다. 12개월 동안 유지됩니다. 매단위 - 매년 1월 1일에 전체 백업이 실행됩니다. 5년간 유지됩니다.    Silver Policy\n 주단위 - 매주 일요일에 주 단위 증분 백업이 실행됩니다. 4주 동안 보존됩니다. 월단위 - 매월 첫날에 월간 증분 백업이 실행됩니다. 12개월 동안 유지됩니다. 매단위 - 매년 1월 1일에 전체 백업이 실행됩니다. 5년간 유지됩니다.    Gold Policy\n 일단위 - 일일 증분 백업이 실행됩니다. 7일 동안 보존됩니다. 주단위 - 매주 일요일에 주 단위 증분 백업이 실행됩니다. 4주 동안 보존됩니다. 월단위 - 매월 첫날에 월간 증분 백업이 실행됩니다. 12개월 동안 유지됩니다. 매단위 - 매년 1월 1일에 전체 백업이 실행됩니다. 5년간 유지됩니다.    정책 기반 Block Volume 백업하기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Block Storage \u0026raquo; Block Volumes Backups 항목으로 이동합니다.\n  설정하려는 Block Volume의 우측 메뉴를 클릭하고, Edit를 클릭합니다.\n   백업 정책을 고르고 아래쪽 Save Changes를 클릭합니다.\n    Block Volume 리스트에서도 확인할 수 있으며, 추가 정보로 백업일정을 확인할 수 있습니다.\n   백업 정책에 따라 그림과 같이 스케줄에 따라 백업됩니다.\n그림은 Gold Policy로 인한 일단위 백업으로 7일간 보존을 위한 만료(Expiration) 시간 표시된 것을 볼 수 있습니다.\n   Block Volume 백업 정책 삭제하기   백업정책이 설정된 Block Volume의 우측 액션 메뉴에서 Edit를 클릭합니다.\n   기존 설정된 백업 정책 대신 None을 선택합니다.\n   경고 문구를 확인하고 Unassign을 다시한번 체크하고 저장합니다.\n   백업 정책 해제 완료\n   Block Volume 백업 정책 변경하기  백업 정책 변경은 Block Volume Edit에서 다른 정책으로 변경하면 됩니다.  ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/4/","tags":["block volume","backup","copy"],"title":"3.7.4 정책 기반 Block Volume 백업하기"},{"categories":null,"contents":"6.4 Policy 만들기 Step 1. 생성된 그룹을 위한 Policy 추가   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Policies 항목으로 이동합니다.\n  왼쪽 아래에 대상 Compartment를 Root Compartment로 지정합니다.\n  Create Policy 클릭\n  Policy 정보 입력\n  Name: \u0026ldquo;SandboxPolicy\u0026rdquo; 입력\n  Description: 설명 입력, 예, 이름과 동일하게 \u0026ldquo;SandboxPolicy\u0026rdquo;\n  Policy Statements:\n \u0026ldquo;Show manual editor\u0026quot;를 선택하여 직접입력합니다. SandboxGroup내 사용자는 Sandbox Compartment에 모든 권한을 부여하도록 다음과 같이 설정합니다.    Allow group SandboxGroup to manage all-resources in compartment Sandbox   Step 2. 적용된 Policy 확인을 위한 사용자 다시 로그인   기존 사용자에서 로그아웃합니다.\n  Policy가 적용되는 SandboxGroup 그룹에 속하는, 앞서 생성한 새 사용자(예, sandboxer)로 다시 로그인합니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  Root Compartment 내의 자원은 권한이 없어서 여전히 보이지 않습니다.   Compartment 선택 메뉴에 이전과 달리 Sandbox Compartment가 보이는 것을 알 수 있습니다. 권한이 없는 Production Compartment는 여전히 보이지 않습니다.\n   Sandbox Compartment를 선택하면 Sandbox 안에 있는 인스턴스가 보입니다. 물론 Sandbox 관리 권한이 있기 때문에 Sandbox 내 다른 모든 작업도 수행할 수 있습니다.   ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/4/","tags":["policy"],"title":"6.4 Policy 만들기"},{"categories":null,"contents":"3.4 Linux 인스턴스 생성하기  팁 인스턴스를 생성하기 전에 자원이 충분한지 사전에 확인하는 것이 좋습니다. Service Limit 조회를 통해 가용한 자원이 있는지, 특히 가용한 CPU가 있는 지를 인스턴스 생성 전에 확인합니다.     OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  Instance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\n  생성정보 입력\n  기본 정보\n Name: 이름 입력, 예를 들어 \u0026ldquo;ExampleLinuxInstance\u0026rdquo; Create in compartment: 위치할 Compartment를 선택, 앞서 생성한 Sandbox 선택    Placement\n서울, 춘천 리전은 AD가 하나 이기 때문에 Placement를 기본값을 그대로 사용하면 됩니다. 우측 Edit를 클릭하면 아래와 같이 상세 옵션을 볼 수 있으며, AD내 Fault Domain을 직접 지정할 수도 있습니다.\n   Image and Shape\n우측 Edit를 클릭하면 OS 이미지와 Shape 설정을 변경할 수 있습니다.\n   Image 변경\nOracle Linux, Ubuntu, CentOS, Windows 등 플랫폼에서 제공하는 기본 OS 이미지를 사용할 수 있고, Oracle, 파트너사 등에 제공하는 사전 제작된 패키지 이미지를 선택할 수 있습니다. 여기서는 기본 선택된 Oracle Linux를 그대로 사용합니다.\n   Shape 변경\nShape은 CPU, Memory에 대한 템플릿입니다. 아래 그림에서와 같이 VM, Baremetal 여부, CPU 제조사 선택후 가능한 Shape을 선택합니다. 무료 계정인지, 구매 고객인지, Region에 따라 가용한 Shape이 달라질 수 있습니다. 여기서는 기본 선택된 VM.Standard.E2.1.Micro를 선택합니다. Always Free-eligible 태그에서 보듯이 이 타입은 Always Free가 제공하는 인스턴스 내에서 항상 무료입니다.\n     Networking\n앞서 만든 ExampleVCN의 Public Subnet을 선택합니다. Public Subnet상의 VM에는 기본으로 Public IP가 부여됩니다. 필요시 Edit하여 수정합니다.\n   Add SSH Keys\n  VM에 SSH 접속시 사용한 SSH Key의 Public Key를 입력합니다. 이전 장에서 ssh-keygen 또는 PuTTY Key Generator로 만든 Public Key를 붙여 넣기 합니다.\n   Generate a key pair for me: SSH Key를 미리 생성하지 않은 경우 자동으로 생성된 Key를 사용할수도 있습니다. 이 옵션을 사용하는 경우 Private Key, Public Key를 다운받아 보관합니다.\n     Boot volume: 사이즈 변경 등이 가능하며, 지금은 별도 설정없이 기본값 그대로 사용합니다.\n  Advanced Options\n고급 옵션에서 아래 설정을 직접 선택할 수 있습니다. 지금은 별도 설정없이 기본값 그대로 사용합니다.\n Management: cloud-init을 통해 VM 생성시 커스텀 스크립트 추가 가능 Available configuration: Live migration 기본 옵션을 변경 가능 Oracle Cloud Agent: 기본 설치되는 Cloud Agent 변경 가능    생성 정보 입력후 Create 클릭\n    생성 완료\n인스턴스가 PROVISIONING 상태에서 몇 분이 지나고 생성이 완료되면 RUNNING 상태가 됩니다.\n   ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/4/","tags":["linux","compute","vm"],"title":"3.4 Linux 인스턴스 생성하기"},{"categories":null,"contents":"3.6.4 Block Volume 포맷하기   Compute Instance에 SSH로 접속한 후 Block Volume을 포맷합니다. 파일 시스템 타입은 원하는 다른 것으로 변경하여 포맷하셔도 됩니다.\nsudo mkfs -t \u0026lt;fs type\u0026gt; /dev/sdb   실행예시\n 아래와 같이 Block Volume을 조회한후 포맷하였습니다.  [opc@examplelinuxinstance ~]$ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT sdb sda ├─sda2 swap 7c02c8fa-405c-4583-bf26-72fcfff63ccf [SWAP] ├─sda3 xfs daec0eb0-26cf-43cb-bcb1-896cdc193242 / └─sda1 vfat 61E0-20B8 /boot/efi [opc@examplelinuxinstance ~]$ sudo mkfs -t xfs /dev/sdb meta-data=/dev/sdb isize=256 agcount=4, agsize=3276800 blks = sectsz=4096 attr=2, projid32bit=1 = crc=0 finobt=0, sparse=0, rmapbt=0 = reflink=0 data = bsize=4096 blocks=13107200, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=6400, version=2 = sectsz=4096 sunit=1 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 [opc@examplelinuxinstance ~]$ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT sdb xfs c15dcf4c-e7af-4150-b1f8-07953032464a sda ├─sda2 swap 7c02c8fa-405c-4583-bf26-72fcfff63ccf [SWAP] ├─sda3 xfs daec0eb0-26cf-43cb-bcb1-896cdc193242 / └─sda1 vfat 61E0-20B8 /boot/efi     ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/4/","tags":["block volume"],"title":"3.6.4 Block Volume 포맷하기"},{"categories":null,"contents":"2.4 최초 Cloud Account(Tenancy) 구조  Cloud Account 환경이 생성되면 OCI Native IAM에 의해서 사용자가 관리됩니다. OCI Classic의 유물로 기본적으로 Oracle Identity Cloud Service(IDCS)를 Identity Provider로 사용하게 Federtion되어 있습니다. OCI의 Administrator는 OCI Tenancy의 모든 권한을 가집니다. IDCS의 OCI_Administrator 그룹은 OCI의 Administrator 그룹에 매핑되어 있습니다. OCI Console에 SSO로 로그인하면 실제는 OCI_Administrator에 속한 사용자이지만, OCI 상에서는 Administrator 그룹으로 처리되어 관련 권한을 가지게 됩니다. IDCS 상에 유저를 생성하면, 예, neo@example.com 유저 생성되면 자동으로 동기화되어 OCI에 oracleidentitycloudservice/neo@example.com 란 유저가 생성됩니다. OCI 자원에 대한 구획인 Compartment는 최초 Root Compartment만 있으며 필요시 추가 하면 됩니다.   ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/4/","tags":["oci account","tenancy"],"title":"2.4 최초 Cloud Account(Tenancy) 구조"},{"categories":null,"contents":"6.4 Multicluster에 애플리케이션 배포하기 Helidon 예제 애플리케이션 배포하기 Helidon 프레임워크를 사용해서 개발한 Java 애플리케이션을, Open Application Model을 사용해서 Verrazzano에서 배포하는 것을 앞서 확인하였습니다. 단일 Kubernetes 클러스터에 대해서 배포하는 것을 확인했습니다.\n이번에는 멀티 클러스터 환경에서 배포하는 것을 확인해 보겠습니다. Verrazzano가 배포된 3개의 클러스터를 기준으로 확인해 봅니다.\n Admin Cluster  admin-cluster   Managed Cluster  managed-cluster-1 managed-cluster-2    애플리케이션 배포   namespace 생성\n  단일 클러스터 기준 (참고)\n앞서 예제에서 배포할 namespace를 생성하고 라벨을 사전에 설정해야 했습니다.\nkubectl create namespace hello-helidon kubectl label namespace hello-helidon verrazzano-managed=true istio-injection=enabled   멀티 클러스터를 위해 namespace를 생성(즉, Project를 생성)\n 애플리케이션이 배포될 멀티클러스터에 걸쳐있는 namespace를 관리하기 위해서 VerrazzanoProject 유형을 제공합니다. Admin Cluster에 다음 명령을 수행하면, Admin Cluster 및 대상 Managed Cluster에 해당 namespace가 생성되면 verrazzano에서 관리하게 됩니다. 아래 설정을 admin-cluster에 배포합니다.  apiVersion:clusters.verrazzano.io/v1alpha1kind:VerrazzanoProjectmetadata:name:hello-helidonnamespace:verrazzano-mcspec:template:namespaces:- metadata:name:hello-helidonplacement:clusters:- name:managed-cluster-1- name:managed-cluster-2   결과 확인\nVerrazzanoProject 만들면 아래와 같이 대상 클러스터들에 namespace가 만들어지고 자동으로 라벨이 부여됩니다.\n[opc@bastion-host ~ ( )]$ kubectl config use-context admin-cluster Switched to context \u0026#34;admin-cluster\u0026#34;. [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl get ns hello-helidon --show-labels NAME STATUS AGE LABELS hello-helidon Active 4m50s istio-injection=enabled,verrazzano-managed=true [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl config use-context managed-cluster-1 Switched to context \u0026#34;managed-cluster-1\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl get ns hello-helidon --show-labels NAME STATUS AGE LABELS hello-helidon Active 4m50s istio-injection=enabled,verrazzano-managed=true [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl config use-context managed-cluster-2 Switched to context \u0026#34;managed-cluster-2\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-2:default)]$ kubectl get ns hello-helidon --show-labels NAME STATUS AGE LABELS hello-helidon Active 5m3s istio-injection=enabled,verrazzano-managed=true     Component 배포\n단일 클러스터, 멀티 클러스터 차이가 없습니다. 설정 파일은 이전과 동일하고 대신 admin-cluster에 배포합니다.\nkubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-comp.yaml   Application Configuration 배포\n  멀티 클러스터 배포를 위한 YAML 예시\n멀티 클러스터 배포시에는 MultiClusterApplicationConfiguration 유형을 사용합니다. 기존 ApplicationConfiguration에서 정의된 부분이 MultiClusterApplicationConfiguration의 하위 spec으로 들어간 것을 알 수 있습니다. 또한 멀티클러스터 배포를 위한 placement 구문이 추가됩니다.\napiVersion: clusters.verrazzano.io/v1alpha1 kind: MultiClusterApplicationConfiguration metadata: name: hello-helidon-appconf namespace: hello-helidon spec: template: metadata: annotations: version: v1.0.0 description: \u0026#34;Hello Helidon application\u0026#34; spec: components: - componentName: hello-helidon-component traits: ... placement: clusters: - name: managed-cluster-1   멀티 클러스터 배포용 Application Configuration을 각자 placement에 맞게 수정하여 배포합니다.\nwget https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/multicluster/hello-helidon/mc-hello-helidon-app.yaml # placement 수정 kubectl apply -f mc-hello-helidon-app.yaml   필요하면 이전 단일클러스터 배포시 사용한 아래 파일과 직접 비교해봅니다.\nhttps://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-app.yaml       배포결과\nadmin-cluster에 Componet와 Application을 배포하면, 아래 결과에서 보듯이 placement로 지정한 클러스터에 배포되는 것을 알 수 있습니다.\n[opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl apply -f https://raw.githubusercontent.com/verrazzano/verrazzano/v1.1.0/examples/hello-helidon/hello-helidon-comp.yaml component.core.oam.dev/hello-helidon-component created [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl apply -f mc-hello-helidon-app.yaml multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf created [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon NAME AGE multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf 11m NAME WORKLOAD-KIND AGE component.core.oam.dev/hello-helidon-component VerrazzanoHelidonWorkload 17m [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl config use-context managed-cluster-1 Switched to context \u0026#34;managed-cluster-1\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon NAME AGE multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf 11m NAME WORKLOAD-KIND AGE component.core.oam.dev/hello-helidon-component VerrazzanoHelidonWorkload 11m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-helidon-deployment 1/1 1 1 11m [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl config use-context managed-cluster-2 Switched to context \u0026#34;managed-cluster-2\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-2:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon No resources found in hello-helidon namespace.   배포된 앱의 istio를 통해 등록된 ingress DNS 확인\nkubectl get gateway hello-helidon-hello-helidon-appconf-gw \\  -n hello-helidon \\  -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39;   실행예시\n$ kubectl get gateway hello-helidon-hello-helidon-appconf-gw \\ \u0026gt; -n hello-helidon \\ \u0026gt; -o jsonpath=\u0026#39;{.spec.servers[0].hosts[0]}\u0026#39; hello-helidon-appconf.hello-helidon.mc-1.thekoguryo.ml   애플리케이션 테스트\n확인된 https://{ingress DNS 주소}/greet 주소로 정상 호출되는 것을 알 수 있습니다.\n   모니터링 Verrazzano 콘솔   Verrazzano Console에 로그인합니다.\n예, https://verrazzano.myenv.thekoguryo.ml\n 접속 유저: verrazzano 암호: 설치시 초기화한 암호    왼쪽 Resources 항목에서 Application과 Component 항목에서 배포된 앱 정보를 확인할 수 있습니다.\n   로그 모니터링(Elasticsearch / Kibana)   Verrazzano 콘솔에서 Kibana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  단일 클러스터와 배포시와 동일한 방법으로 verrazzano-namespace-hello-helidon 네임스페이스로 인덱스 패턴을 생성합니다.\n  생성한 인덱스 패턴에 대해 수집된 로그, 대상 namespace 상의 전체 수집된 로그가 보입니다. admin-cluster는 등록된 Managed Cluster 모두의 로그를 수집합니다. 앞서 배포된 앱는 managed-cluster-1에만 배포했기 때문에 실제 수집된 로그도 동일함을 알수 있습니다.\n   메트릭 모니터링(Prometheus / Grafana)   Verrazzano 콘솔에서 Grafana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  기본 제공하고 있는 대쉬보드 중에 Helidon 대쉬보드를 선택합니다.\n  Helidon 대쉬보드에서 앞서 배포한 앱의 상태를 확인할 수 있습니다. 모든 클러스터에 수집된 메트릭을 볼 수 있으면, 실제 managed-cluster-1에 있기 때문에 수집된 데이터 기준으로 보이게 됩니다.\n   Helidon 예제 애플리케이션 배포 변경 애플리케이션 배포 위치 변경   placement 항목을 통해 클러스터 위치를 변경할 수 있습니다. 변경할 placement 파일을 생성합니다.\n# patch.yamlspec:placement:clusters:- name:managed-cluster-2  배포 위치 변경\nkubectl patch MultiClusterApplicationConfiguration hello-helidon-appconf \\  -n hello-helidon \\  --type merge \\  --patch \u0026#34;$(cat patch.yaml)\u0026#34;   배포결과 확인\n앞서 managed-cluster-1에 배포되어 있던 앱이 managed-cluster-2로 위치가 변경된 것을 알 수 있습니다.\n[opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl patch MultiClusterApplicationConfiguration hello-helidon-appconf \\ \u0026gt; -n hello-helidon \\ \u0026gt; --type merge \\ \u0026gt; --patch \u0026#34;$(cat patch.yaml)\u0026#34; multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf patched [opc@bastion-host ~ (⎈ |admin-cluster:default)]$ kubectl config use-context managed-cluster-1 Switched to context \u0026#34;managed-cluster-1\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon No resources found in hello-helidon namespace. [opc@bastion-host ~ (⎈ |managed-cluster-1:default)]$ kubectl config use-context managed-cluster-2 Switched to context \u0026#34;managed-cluster-2\u0026#34;. [opc@bastion-host ~ (⎈ |managed-cluster-2:default)]$ kubectl get multiclusterapplicationconfigurations,component,deployment -n hello-helidon NAME AGE multiclusterapplicationconfiguration.clusters.verrazzano.io/hello-helidon-appconf 3m25s NAME WORKLOAD-KIND AGE component.core.oam.dev/hello-helidon-component VerrazzanoHelidonWorkload 2m26s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-helidon-deployment 1/1 1 1 2m22s   ","lastmod":"2022-01-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/verrazzano/4.deploy-app-to-multicluster/","tags":["oke","verrazzano","multicluster"],"title":"6.4 Multicluster에 애플리케이션 배포하기"},{"categories":null,"contents":"1.4 앱 배포 및 Load Balancer 사용하기 Docker Hub 이미지 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest   배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-78b9999646-xgtjp 1/1 Running 0 17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 130m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-78b9999646 1 1 1 20s   Load Balancer 테스트 Load Balancer 서비스 사용하기   클라이언트 서비스를 위해 LoadBalancer Type으로 서비스를 생성합니다.\n  서비스 생성 결과를 확인하면 아래와 같이 LoadBalancer 타입으로 생성되어 Public IP가 할당 된 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc service/nginx-docker-hub-svc exposed oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 135m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 49s   서비스 주소인 Public IP로 접속하면, 연결되는 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ curl http://152.67.200.29 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   OCI Service Console에서 Load Balancer 확인하기   콘솔에서 Networking \u0026gt; Load Balancer 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  아래 그림과 같이 kubernetes 상에 생성된 서비스와 동일한 Public IP로 Load Balancer가 생성된 것을 알 수 있습니다.\n   Load Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n  상세화면에서 좌측 하단 Resources \u0026gt; Listeners로 이동합니다.\nkubernetes에서 Load Balancer 생성시 지정한 80 포트로 Listen 하고 있는 걸 알 수 있습니다.\n   Resources \u0026gt; Backend Set으로 이동합니다. 생성된 Backend Set을 클릭하여 상세화면으로 이동합니다.\n  좌측 하단 Resources \u0026gt; Backends로 이동합니다.\n세 개의 백엔드 노드의 30610 포트로 부하 분산하는 것을 볼 수 있습니다.\n   다시 터미널로 이동하여 서비스와 노드 정보를 조회합니다.\nkubectl get svc kubectl get nodes -o wide   조회결과\n조회 해보면 OCI Load Balancer 가 Worker Nodes 3개로, 각 노드의 Node Port인 30610으로 부하 분산 되는 것을 알 수 있습니다. 이처럼 kubernetes에서 Load Balancer Type 서비스를 생성하면, OCI Load Balancer와 연동되어 자동으로 자원이 생성됩니다.\n  oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 160m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 25m oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.11 Ready node 155m v1.20.11 10.0.10.11 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.44 Ready node 155m v1.20.11 10.0.10.44 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.73 Ready node 155m v1.20.11 10.0.10.73 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   OCI Service Console에서 Security List 확인하기   콘솔에서 Networking \u0026gt; Virtual Cloud Networks 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  OKE 클러스터가 사용하는 VCN으로 이동합니다.\n  Subnet을 보면 그림과 같이, 3개의 서브넷이 있습니다.\n oke-k8sApiEndpoint-subnet~~: Kubernetes API Endpoint를 위한 서브넷 oke-svclbsubnet-~~: Load Balancer가 사용하는 서브넷 oke-nodesubnet-~~: Worker Node 들이 사용하는 서브넷     Resources \u0026gt; Security Lists로 이동하면, 위 3개의 서브넷이 사용하는 3개의 Security List가 있습니다.\n   Load Balancer용 서브넷 확인\n먼저 oke-svclbsubnet-~~ 의 상세화면으로 이동합니다. Ingress/Egress Rule을 확인해 보면, 외부에서 80 포트로 수신하고, Worker Node들의 Node Port인 30610로 송신할 수 있도록 자동으로 추가된 것을 볼수 있습니다.\n    다시 VCN 상세 화면으로 이동하여, Worker Nodes용 서브넷을 확인합니다.\nOCI Load Balancer에서 Node Port: 30610으로 요청을 수신할 수 있도록 자동으로 규칙이 추가된 것을 볼수 있습니다.\n   위와 같이 OKE 클러스터에 kubernetes 명령으로 Load Balancer 서비스 타입을 생성하면, 그에 따라 OCI Load Balancer가 생성되고, 관련 Security List에도 등록되는 것을 알 수 있습니다.\n  ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/4.deploy-docker-hub-image-with-lb/","tags":["oke"],"title":"1.4 앱 배포 및 Load Balancer 사용하기"},{"categories":null,"contents":"14.3.4 Postman - OCI REST API 호출하기 가장 많이 사용하는 전문 REST 클라이언트 툴인 Postman을 사용하면 GUI를 통해 사용하기가 더 편리합니다. 아직 OCI Request Signature를 기본 지원하지 않아, Postman의 pre-script를 사용하여 OCI REST API를 호출하는 방법을 설명하고자 합니다.\nOCI Request Signature를 통한 호출을 하기 위해 Postman에서는 pre-script를 사용해야 합니다. Insomina처럼 플러그인 기능은 제공하지 않습니다. pre-script는 REST 요청단위로 설정할 수 있고, 폴더, Collection 단위로도 설정할 수 있습니다.\n폴더에 pre-script를 설정하면, 폴더하위의 모든 REST 요청전에 pre-script가 먼저 실행됩니다. 관리 편의를 위해 폴더의 pre-script를 사용하겠습니다.\nStep 0. Postman 다운로드 공식 사이트에서 앱을 다운로드 받습니다.\n https://www.postman.com/downloads/  Step 1. pre-script에서 쓸 RSA 라이브러리 로딩 ※ 외부 RSA 자바스크립트 라이브러리 로딩 pre-script에서 기본 포함된 Cryto 라이브러리는 OCI Request Signature가 사용하는 RSA 암호화를 지원하지 않습니다. 그래서 다음방식으로 우회하여 외부 암호화 라이브러리를 추가합니다.\n  GET 요청 만들기\n 대상 URL: http://kjur.github.io/jsrsasign/jsrsasign-latest-all-min.js    Test 탭에 다음 복사\npm.globals.set(\u0026#34;jsrsasign-js\u0026#34;, responseBody);   요청 실행\n  실행결과\n아래와 같이 실행되면 암호화모듈이 Postman 글로벌 변수(jsrsasign-js)에 저장됩니다.\n   Step 2. pre-script 설정   REST 요청을 포함할 폴더를 생성합니다.\n  폴더 이름을 우클릭 하고 Edit 클릭\n   폴더 설정에서 Pre-request Scripts 탭을 선택합니다.\n   다음 자바스크립트을 복사해서 붙여넣습니다.\nvar navigator = {}; //fake a navigator object for the lib var window = {}; //fake a window object for the lib eval(pm.globals.get(\u0026#34;jsrsasign-js\u0026#34;)); //import javascript jsrsasign  const isOracleCloud = pm.environment.get(\u0026#34;isOracleCloud\u0026#34;); if (isOracleCloud != \u0026#34;true\u0026#34;) return; const tenancyId = pm.environment.get(\u0026#39;tenancyId\u0026#39;); const authUserId = pm.environment.get(\u0026#39;authUserId\u0026#39;); const keyFingerprint = pm.environment.get(\u0026#39;keyFingerprint\u0026#39;); var privateKey = pm.environment.get(\u0026#34;privateKey\u0026#34;); var signAlgorithm = \u0026#34;RSA-SHA256\u0026#34;; var sigVersion = \u0026#34;1\u0026#34;; var now = new Date().toUTCString(); var host = getHost(request.url.trim()); var target = getTarget(request.url.trim()); var method = request.method; var keyId = tenancyId + \u0026#34;/\u0026#34; + authUserId + \u0026#34;/\u0026#34; + keyFingerprint; var headers = \u0026#34;(request-target) date host\u0026#34;; var request_target=\u0026#34;(request-target): \u0026#34; + method.toLowerCase() + \u0026#34; \u0026#34; + target; var date_header = \u0026#34;date: \u0026#34; + now; var host_header = \u0026#34;host: \u0026#34; + host; var signing_string = request_target + \u0026#34;\\n\u0026#34; + date_header + \u0026#34;\\n\u0026#34; + host_header; var methodsThatRequireExtraHeaders = [\u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;]; if(methodsThatRequireExtraHeaders.indexOf(method.toUpperCase()) !== -1) { var body = request.data; console.log(body); var signatureSign = CryptoJS.SHA256(body); var content_sha256 = signatureSign.toString(CryptoJS.enc.Base64); var content_type = \u0026#34;application/json\u0026#34;; var content_length = body.length; headers = headers + \u0026#34; x-content-sha256 content-type content-length\u0026#34;; var content_sha256_header = \u0026#34;x-content-sha256: \u0026#34; + content_sha256; var content_type_header = \u0026#34;content-type: \u0026#34; + content_type; var content_length_header = \u0026#34;content-length: \u0026#34; + content_length; signing_string = signing_string + \u0026#34;\\n\u0026#34; + content_sha256_header + \u0026#34;\\n\u0026#34; + content_type_header + \u0026#34;\\n\u0026#34; + content_length_header; pm.environment.set(\u0026#34;x-content-sha256_header\u0026#34;, content_sha256); } // RSA signature generation var signatureSign = new KJUR.crypto.Signature({\u0026#34;alg\u0026#34;: \u0026#34;SHA256withRSA\u0026#34;}); signatureSign.init(privateKey); signatureSign.updateString(signing_string); var signedSignatureHex = signatureSign.sign(); var signedSignature = hexToBase64(signedSignatureHex); const authorization = `Signature version=\u0026#34;${sigVersion}\u0026#34;, keyId=\u0026#34;${keyId}\u0026#34;, algorithm=\u0026#34;${signAlgorithm.toLowerCase()}\u0026#34;, headers=\u0026#34;${headers}\u0026#34;, signature=\u0026#34;${signedSignature}\u0026#34;`; pm.environment.set(\u0026#34;date_header\u0026#34;, now); pm.environment.set(\u0026#34;Authorization_header\u0026#34;, authorization); function getHost(url) { // https://identity.us-ashburn-1.oraclecloud.com/20160918/users/  var n1 = url.indexOf(\u0026#34;//\u0026#34;); var n2 = url.indexOf(\u0026#34;/\u0026#34;, n1 + 2); var start = n1 + 2; var length = n2 - start; var host = url.substr(start, length); return host; } function getTarget(url) { // https://identity.us-ashburn-1.oraclecloud.com/20160918/users/  url = url.replace(new RegExp(\u0026#39;^https?://[^/]+/\u0026#39;),\u0026#39;/\u0026#39;); // strip hostname  return url; } function hexToBase64(hexstring) { return btoa(hexstring.match(/\\w{2}/g).map(function(a) { return String.fromCharCode(parseInt(a, 16)); }).join(\u0026#34;\u0026#34;)); }   Step 3. 환경변수 설정  필요 환경 변수  isOracleCloud: \u0026ldquo;true\u0026quot;로 설정, 설정한 플러그인인 글로벌하게 적용되는 것이라, true일때만 세부 스크립트가 동작함 tenancyId: tenancy OCID authUserId: 사용자 OCID keyFingerprint: API Key의 핑거프린트 privateKey: API Key로 등록한 Public Key에 매칭되는 Private Key의 텍스트를 그대로 복사해 붙입니다.     Step 4. 사용자 조회 REST API 실행   REST 요청을 앞서 만든 폴더 밑으로 만듭니다.\n  오른쪽 위에 설정한 환경변수를 선택합니다.\n  REST 요청의 Header에 그림과 같이 date, Authorization을 추가합니다.\n date : {{date_header}} Authorization : {{Authorization_header}} date, Authorization의 실제 값은 pre-script가 실행되면서 실행시점에 설정됩니다. POST, PUT을 제외한 HTTP Operation은 위 두 개만 설정합니다.    실행결과   Step 5. 사용자 생성 REST API 실행   REST 요청을 앞서 만든 폴더 밑으로 만듭니다.\n  오른쪽 위에 설정한 환경변수를 선택합니다.\n  REST 요청의 Header에 그림과 같이 date, Authorization을 추가합니다.\n date : {{date_header}} Authorization : {{Authorization_header}} Content-Type : application/json x-content-sha256 : {{x-content-sha256_header}} Content-Type을 제외한 세 개의 실제 값은 pre-script가 실행되면서 실행시점에 설정됩니다. POST, PUT은 위 네 개를 설정합니다. 요청 메시지가 있어 요청메시지도 서명에 추가되기 때문입니다.    실행결과   POST, PUT으로 요청하는 경우 위 REST 요청을 복사해서 씁니다.\n  참고 문서 Postman의 pre-script에서 외부 RSA 자바스크립트 라이브러리를 사용하는 부분은 다음 링크를 참고하였습니다.\n https://github.com/postmanlabs/postman-app-support/issues/1607#issuecomment-401611119  ","lastmod":"2019-05-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/4/","tags":["rest api","postman"],"title":"14.3.4 Postman - OCI REST API 호출하기"},{"categories":null,"contents":"14.2.4 Terraform Configuration 실행하기   명령창실행\n  Configuration 파일이 있는 경로로 이동\n[opc@bastion-host example_vcn]$ ls -la total 12 drwxrwxr-x. 2 opc opc 63 Jan 18 09:50 . drwxrwxr-x. 3 opc opc 25 Jan 18 09:46 .. -rw-rw-r--. 1 opc opc 359 Jan 18 09:47 provider.tf -rw-rw-r--. 1 opc opc 479 Jan 18 09:50 terraform.tfvars -rw-rw-r--. 1 opc opc 284 Jan 18 09:47 vcn.tf   초기화 실행\n초기화를 수행하면 provider.tf에서 사용하는 OCI Provider를 자동으로 .terraform 폴더 밑으로 다운받습니다.\nterraform init   실행예시\n[opc@bastion-host example_vcn]$ terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/oci... - Installing hashicorp/oci v4.59.0... - Installed hashicorp/oci v4.59.0 (signed by HashiCorp) ... [opc@bastion-host example_vcn]$ ls -la total 20 drwxrwxr-x. 3 opc opc 4096 Jan 18 09:51 . drwxrwxr-x. 3 opc opc 25 Jan 18 09:46 .. drwxr-xr-x. 3 opc opc 23 Jan 18 09:51 .terraform -rw-r--r--. 1 opc opc 1002 Jan 18 09:51 .terraform.lock.hcl -rw-rw-r--. 1 opc opc 359 Jan 18 09:47 provider.tf -rw-rw-r--. 1 opc opc 479 Jan 18 09:50 terraform.tfvars -rw-rw-r--. 1 opc opc 284 Jan 18 09:47 vcn.tf [opc@bastion-host example_vcn]$ ls -la .terraform total 4 drwxr-xr-x. 3 opc opc 23 Jan 18 09:51 . drwxrwxr-x. 3 opc opc 4096 Jan 18 09:51 .. drwxr-xr-x. 3 opc opc 35 Jan 18 09:51 providers     실행 플랜 확인\n플랜 명령을 수행하면, 실제 OCI에 수행되는 계획을 확인할 수 있습니다.\nterraform plan   실행예시\n[opc@bastion-host example_vcn]$ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Note: You didn\u0026#39;t use the -out option to save this plan, so Terraform can\u0026#39;t guarantee to take exactly these actions if you run \u0026#34;terraform apply\u0026#34; now.     실행\n실행하면 실제 OCI에 Terraform 계획이 적용되어 관련된 OCI 자원이 생성됩니다. 또한 실행후 terraform output 명령을 통해 outout 변수를 조회할 수 있습니다.\nterraform apply   실행예시\n[opc@bastion-host example_vcn]$ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes oci_core_virtual_network.vcn1: Creating... oci_core_virtual_network.vcn1: Creation complete after 1s [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, ]     실행결과 확인\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking » Virtual Cloud Networks 항목으로 이동합니다.\n  Terraform Configuration시 지정한 Comparment를 선택합니다.\n  그림과 같이 생성된 VCN을 확인 할 수 있습니다.\n     State 관리\n  다시 terraform apply를 실행하면 아래와 앞서 이미 실행되어 자원이 생성되었기 때문에, 변경 적용할 것이 없다고 표시됩니다.\n[opc@bastion-host example_vcn]$ terraform apply oci_core_virtual_network.vcn1: Refreshing state... [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, ]   Terraform State 관리 파일\nterraform 설정을 실행하면, 실행후 아래처럼 terraform.tfstate 파일이 생성되고 생성된 자원에 대한 id와 관련 속성 등을 기록하여 자원 상태를 관리하게 됩니다. 그래서 다시 terraform apply를 통해 실행하더라도, tfstate에서 관리되는 id의 자원의 현재 OCI 자원의 현재 상태를 확인(Refresh)한 상태와 적용할 Plan을 비교하여 변경할 사항이 있는 지 확인후 반영됩니다.\n[opc@bastion-host example_vcn]$ ls -la total 24 drwxrwxr-x. 3 opc opc 4096 Jan 18 10:06 . drwxrwxr-x. 3 opc opc 25 Jan 18 09:46 .. drwxr-xr-x. 3 opc opc 23 Jan 18 09:51 .terraform -rw-r--r--. 1 opc opc 1002 Jan 18 09:51 .terraform.lock.hcl -rw-rw-r--. 1 opc opc 359 Jan 18 09:47 provider.tf -rw-rw-r--. 1 opc opc 2152 Jan 18 10:06 terraform.tfstate -rw-rw-r--. 1 opc opc 479 Jan 18 09:50 terraform.tfvars -rw-rw-r--. 1 opc opc 284 Jan 18 09:47 vcn.tf [opc@bastion-host example_vcn]$ cat terraform.tfstate { \u0026#34;version\u0026#34;: 4, \u0026#34;terraform_version\u0026#34;: \u0026#34;1.1.3\u0026#34;, \u0026#34;serial\u0026#34;: 2, \u0026#34;lineage\u0026#34;: \u0026#34;b9cd2a13-5ef9-f1e1-ffc9-ea5b2964955d\u0026#34;, \u0026#34;outputs\u0026#34;: { \u0026#34;vcn1_ocid\u0026#34;: { \u0026#34;value\u0026#34;: [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34; ], \u0026#34;type\u0026#34;: [ \u0026#34;tuple\u0026#34;, [ \u0026#34;string\u0026#34; ] ] } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;mode\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;oci_core_virtual_network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vcn1\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/oci\\\u0026#34;]\u0026#34;, \u0026#34;instances\u0026#34;: [ { \u0026#34;schema_version\u0026#34;: 0, \u0026#34;attributes\u0026#34;: { \u0026#34;cidr_block\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;cidr_blocks\u0026#34;: [ \u0026#34;10.0.0.0/16\u0026#34; ], \u0026#34;compartment_id\u0026#34;: \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34;, \u0026#34;default_dhcp_options_id\u0026#34;: \u0026#34;ocid1.dhcpoptions.oc1.ap-seoul-1.aaaaaaaaun4ti33kubad7cqexhe4npgwy773lldor7xxfbbu2lu5yz3q6uaa\u0026#34;, \u0026#34;default_route_table_id\u0026#34;: \u0026#34;ocid1.routetable.oc1.ap-seoul-1.aaaaaaaammsdhhl7czlkrxeji7eqot6tkiyfcvrhuqvf4hh3cra3rt3m2e2a\u0026#34;, \u0026#34;default_security_list_id\u0026#34;: \u0026#34;ocid1.securitylist.oc1.ap-seoul-1.aaaaaaaadmiqjfjkiw7k73lxwbilgci2xrzehxud3ezfujmt3gtjenocsoaa\u0026#34;, \u0026#34;defined_tags\u0026#34;: { \u0026#34;Oracle-Tags.CreatedBy\u0026#34;: \u0026#34;sandboxer\u0026#34;, \u0026#34;Oracle-Tags.CreatedOn\u0026#34;: \u0026#34;2022-01-18T10:00:51.322Z\u0026#34; }, \u0026#34;display_name\u0026#34;: \u0026#34;vcn1\u0026#34;, \u0026#34;dns_label\u0026#34;: \u0026#34;vcn1\u0026#34;, \u0026#34;freeform_tags\u0026#34;: {}, \u0026#34;id\u0026#34;: \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, \u0026#34;ipv6cidr_blocks\u0026#34;: [], \u0026#34;is_ipv6enabled\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;AVAILABLE\u0026#34;, \u0026#34;time_created\u0026#34;: \u0026#34;2022-01-18 10:00:51.389 +0000 UTC\u0026#34;, \u0026#34;timeouts\u0026#34;: null, \u0026#34;vcn_domain_name\u0026#34;: \u0026#34;vcn1.oraclevcn.com\u0026#34; }, \u0026#34;sensitive_attributes\u0026#34;: [], \u0026#34;private\u0026#34;: \u0026#34;eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH19\u0026#34; } ] } ] }     삭제\n앞서 Terraform으로 생성된 자원을 삭제하려면 다음 명령을 수행하면 됩니다.\nterraform destroy   실행예시\n[opc@bastion-host example_vcn]$ terraform destroy oci_core_virtual_network.vcn1: Refreshing state... [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be destroyed - resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { - cidr_block = \u0026#34;10.0.0.0/16\u0026#34; -\u0026gt; null - cidr_blocks = [ - \u0026#34;10.0.0.0/16\u0026#34;, ] -\u0026gt; null - compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34; -\u0026gt; null - default_dhcp_options_id = \u0026#34;ocid1.dhcpoptions.oc1.ap-seoul-1.aaaaaaaaun4ti33kubad7cqexhe4npgwy773lldor7xxfbbu2lu5yz3q6uaa\u0026#34; -\u0026gt; null - default_route_table_id = \u0026#34;ocid1.routetable.oc1.ap-seoul-1.aaaaaaaammsdhhl7czlkrxeji7eqot6tkiyfcvrhuqvf4hh3cra3rt3m2e2a\u0026#34; -\u0026gt; null - default_security_list_id = \u0026#34;ocid1.securitylist.oc1.ap-seoul-1.aaaaaaaadmiqjfjkiw7k73lxwbilgci2xrzehxud3ezfujmt3gtjenocsoaa\u0026#34; -\u0026gt; null - defined_tags = { - \u0026#34;Oracle-Tags.CreatedBy\u0026#34; = \u0026#34;sandboxer\u0026#34; - \u0026#34;Oracle-Tags.CreatedOn\u0026#34; = \u0026#34;2022-01-18T10:00:51.322Z\u0026#34; } -\u0026gt; null - display_name = \u0026#34;vcn1\u0026#34; -\u0026gt; null - dns_label = \u0026#34;vcn1\u0026#34; -\u0026gt; null - freeform_tags = {} -\u0026gt; null - id = \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34; -\u0026gt; null - ipv6cidr_blocks = [] -\u0026gt; null - is_ipv6enabled = false -\u0026gt; null - state = \u0026#34;AVAILABLE\u0026#34; -\u0026gt; null - time_created = \u0026#34;2022-01-18 10:00:51.389 +0000 UTC\u0026#34; -\u0026gt; null - vcn_domain_name = \u0026#34;vcn1.oraclevcn.com\u0026#34; -\u0026gt; null } Plan: 0 to add, 0 to change, 1 to destroy. Changes to Outputs: - vcn1_ocid = [ - \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, ] -\u0026gt; null Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: yes oci_core_virtual_network.vcn1: Destroying... [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] oci_core_virtual_network.vcn1: Destruction complete after 1s Destroy complete! Resources: 1 destroyed.   실행결과\n아래 그림과 같이 vcn1이 없어진 것을 볼 수 있습니다.\n     ","lastmod":"2019-04-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/4/","tags":["terraform"],"title":"14.2.4 Terraform Configuration 실행하기"},{"categories":null,"contents":"10.4 Load Balancer를 위한 Subnet 만들기 Subnet 만들기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking » Virtual Cloud Networks 항목으로 이동합니다.\n  앞서 만든 VCN인 LoadBalancerVCN 클릭\n  Create Subnets 클릭\n  생성정보 입력: 진한 글씨 항목만 입력하고 나머지는 기본값을 사용합니다.\n Name: 이름 입력, Public LB Subnet Subnet Type: 기본값인 REGIONAL을 선택  REGIONAL: Region에 글로벌하게 있는 서브넷으로 다른 AD로 FailOver시에도 IP를 그대로 사용가능한 이점이 있음. AVAILIBILTY DOMAIN-SPECIFIC: AD내에 만들어지는 서브넷   CIDR Block: 10.0.2.0/24 Route Table: Default Route Table 선택 Subnet Access: Public Subnet 선택 DHCP Options: Default DHCP Options 선택    최하단으로 스크롤하여 Create Subnet 클릭\n  Public LB Subnet 생성완료\n   LB용 Security List 만들기 이후 생성할 Load Balancer를 위한 Security List를 만듭니다. 기본적으로 VCN 생성시 자동으로 만든 Default Security List를 사용할 수 있지만, 별도로 규칙이 없는 비어 있는 Security List를 생성흡니다.\n 왼쪽 Resources » Security Lists 클릭 Create Security List 클릭 생성정보 입력  Name: 이름 입력, LB Security List    Subnet에 Security List 적용   앞서 만든 Public LB Subnet의 상세 페이지로 이동합니다.\n  Public LB Subnet의 Security List에 방금 만든 LB Security를 추가합니다.\n   ","lastmod":"2019-01-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/4/","tags":["load balancer","subnet"],"title":"10.4 Load Balancer를 위한 Subnet 만들기"},{"categories":null,"contents":"1.4 Virtual Cloud Network(VCN) OCI를 사용하게 되면 처음 해야 할 작업 중의 하나가 다른 클라우드 자원을 위해 Virtual Cloud Network(VCN)를 만드는 작업입니다. Oracle 데이터 센터에 서버 자원들이 사용할 가상의 네트워크 환경을 구성하는 것으로 네트워크 주소 할당, 방화벽, 라우팅 규칙, 게이트웨이 등을 하는 작업입니다.\n처음 VCN을 만들 때 자동 생성 방식으로 구성할 경우 생성되는 기본 구성은 다음과 같습니다.\n Virtual Cloud Network(VCN) OCI 상의 다른 클라우드 자원을 위한 가상 네트워크 환경입니다. 생성 시 VCN에서 사용할 IP 대역은 CIDR 형식으로 지정합니다. 예) 10.0.0.0/16 VCN과 함께 VCN에 속한 아래 컴포넌트를 구성하여 원하는 가상 네트워크 환경을 구성하게 됩니다. 아래 컴포넌트 이외에도 추가적인 컴포넌트들이 많이 있지만, 여기서는 가장 기본적인 컴포넌트에 대해서만 언급하도록 하겠습니다.\nInternet Gateway 명칭 그대로 인터넷과 연결하려면 통해야 하는 관문입니다. 이름을 부여하고 생성하기만 하면 됩니다.\nSubnet VCN상에 생성되는 서브 그룹으로 하나의 Availability Domain 내에 존재합니다. 사용할 IP 대역은 VCN의 대역에서 일부를 나누어 가집니다. 사용할 IP 대역은 CIDR 형식으로 지정합니다. 예) 10.0.0.0/24\nPublic Subnet과 Private Subnet이 있으며, Private Subnet에 있는 인스턴스는 Public IP를 가지지 못합니다. 반면 Public Subnet에 있는 인스턴스는 Public IP를 할당받게 됩니다.\nPrivate Subnet에 있는 인스턴스는 Public IP를 가지지 못하기 때문에 인터넷상의 외부 시스템에서 들어오지는 못합니다. 대신 추가로 Network Address Translate(NAT) GATEWAY를 설정하면 외부 인터넷으로 나가는 것은 가능합니다.\nRoute Table VCN을 위한 가상의 라우트 테이블로써 Subnet에서 외부 목적지로 가는 규칙을 정하는 Route Table입니다. 자동으로 생성한 디폴트 라우트 테이블은 그림처럼 모든 목적지(0.0.0.0/0)를 Internet Gateway로 라우트 되어 인터넷에 연결되게 설정되었습니다. 커스텀 라우팅 테이블을 구성하여 원하는 규칙을 설정할 수 있습니다.\nSecurity Lists VCN을 위한 가상의 방화벽으로 Subnet에서 들어오고 나가는 트래픽에 각각에 대한 ingress 규칙 및 egress 규칙을 설정할 수 있습니다. 그림에는 편의상 ingress rule만 표현하였으며, SSH 통신을 위해 22번 포트로 들어오는 모든 요청을 열어두었습니다. 그리고 생략되었지만, egress rule로 모든 요청이 나갈 수 있도록 설정되어 있습니다.\nCompute 인스턴스에 WebServer를 설치하거나 별도의 서비스를 위해 포트를 열어야 할 경우 이 Security Lists를 업데이트하면 됩니다. 물론 Security Lists는 VCN상에서 Subnet의 방화벽이며, Compute 인스턴스 자체에 있는 리눅스 방화벽 또는 윈도우 방화벽에 있을 경우에는 자체 방화벽에서 해당 포트도 당연히 열어 두어야 합니다.\n","lastmod":"2018-12-30T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/4/","tags":["virtual cloud network","VCN"],"title":"1.4 Virtual Cloud Network(VCN)"},{"categories":null,"contents":"5.4 Apache HTTP Server 접속 테스트   Apache HTTP Server가 구동하는 Compute Instance를 통해 접속을 테스트합니다. 그림처럼 잘 연결되는 것을 확인할 수 있습니다.\n   ","lastmod":null,"permalink":"https://thekoguryo.github.io/oci/chapter05/4/","tags":["apache"],"title":"5.4 Apache HTTP Server 접속 테스트"},{"categories":null,"contents":"7.5 Public Bucket을 통한 정적 웹사이트 호스팅 Public Bucket을 생성하면 URL 경로로 접근할 수 있다는 것을 이용하며 점을 이용하여 간단한 정적 웹사이트 호스팅을 해보도록 하겠습니다.Object Storage는 기본적으로 계층구조가 없는 일자 구조입니다. 폴더구조의 가진 웹페이지는 어떻게 처리할 수 있는지에 대해서 테스트 해 봅니다\n 제약사항 현재 버전 기준 Public Bucket은 파일 링크 공유 등으로 사용할 순 있지만, 일부 기능 제약으로 완전한 정적 웹사이트 호스팅을 위해 사용하기에는 제약사항이 있습니다. URL을 Index Document로 보내주는 기능이 현재는 지원하지 않습니다. 예를 들어, http://www.example.com 주소로 웹브라우저로 접속하면, 자동으로 http://www.example.com/index.html로 보내주는 기능이 현재는 없습니다. 루트 페이지뿐만 아니라, 서브 폴더에 대해서 해당 기능이 일반적으로 필요한데, 현재는 명시적으로 html 주소 전체를 브라우저에서 입력 또는 링크를 타고 갈 때 입력해야 합니다.   Public Bucket 생성 Bucket을 생성하고 Visibility를 PUBLIC으로 변경하여 Public Bucket을 하나 생성합니다.\n  Bucket 을 생성합니다.\n Name: 예) ExampleBucketForWeb    Edit Visibility를 클릭하여 Visibility를 Public으로 변경합니다.\n   간단한 웹페이지 테스트   간단한 웹페이지를 생성하고 index.html 파일명으로 저장합니다.\n\u0026lt;html\u0026gt; \u0026lt;header\u0026gt;\u0026lt;title\u0026gt;Hello !!!\u0026lt;/title\u0026gt;\u0026lt;/header\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello OCI Object Storage\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   index.html 파일을 Bucket에 올립니다.\n  업로드한 index.html의 상세페이지에서 URL 경로를 확인합니다.\n   확인된 URL 경로를 웹브라우저로 접속하면, 잘 동작합니다.\n   간단한 폴더구조의 웹페이지 테스트 Object Storage는 기본적으로 계층구조가 없는 일자 구조입니다. 그래서 Object 업로드 화면을 보면 폴더에 대한 항목이 없습니다. 하지만, 일반적으로 웹페이지는 폴더구조를 가지게 됩니다. 폴더 구조는 Object의 Name에 경로를 추가하는 방식으로 지원이 됩니다.\n  앞서 생성한 index.html 파일을 수정합니다.\n\u0026lt;html\u0026gt; \u0026lt;header\u0026gt;\u0026lt;title\u0026gt;Hello !!!\u0026lt;/title\u0026gt;\u0026lt;/header\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello OCI Object Storage\u0026lt;/h1\u0026gt; \u0026lt;img src=\u0026#34;images/icons8-oracle-96.png\u0026#34;/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   수정한 index.html 파일을 Bucket에 올립니다. Object Name이 같으면 기존 파일을 덮어쓰게 됩니다.\n  이미지 파일을 올립니다. 올릴 때 Object Name을 html 상에 있는 이미지 경로를 포함한 이름으로 변경합니다.\n   앞서 확인된 index.html의 URL 경로를 웹브라우저로 접속하면, 잘 동작합니다.\n   이미지 파일 Object의 URL 경로 확인해 봅니다.\nName 앞에 images/ 를 추가한 그대로 URL 경로가 설정된 것을 확인 할수 있습니다.\n   이미지 파일의 URL 경로를 웹브라우저로 접속하면, 잘 동작합니다.\n   많은 파일을 업로드할 때 일일이 Name을 경로를 포함한 이름으로 변경하는 것은 당연히 힘듭니다. CLI를 사용하거나, CloudBerry 같은 도구를 사용하면, 로컬 폴더를 그대로 올리면서 Object의 이름이 경로 형태로 자동으로 변경되니, 걱정할 필요는 없습니다.\n","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/5/","tags":["object storage","bucket","public bucket"],"title":"7.5 Public Bucket을 통한 정적 웹사이트 호스팅"},{"categories":null,"contents":"3.6.5 Block Volume 마운트하기 OCI 문서에 따르면 root volume 이외 volume이 둘 이상일 경우 장치 이름으로 마운트 할 경우 서버 재기동 후 장치 이름과 실제 장치의 대응 되는 순서가 달라 질 수도 있다고 합니다. 2019년 1월 10일 기준 서버 재기동 후 장치 이름과 실제 장치의 대응 되는 순서를 보장하기 위해 Consistent Device Path 기능이 출시 되었습니다. 지원되지 않는 이미지 및 인스턴스에서는 전과 동일하게 Volume의 UUID를 기준으로 마운트 방식을 사용합니다.\n Consistent Device Paths for Block Volumes  Services: Block Volume Release Date: Jan. 10, 2019 When you attach a block volume to an instance you can now optionally select a device path that will remain consistent between instance reboots. See Connecting to Volumes With Consistent Device Paths for more information. 지원 대상  Oracle-provided Platform 이미지로 만들어진 인스턴스 Linux 기반 이미지 2018년 11월 이후 이미지, 자세한 사항은 Image Release Notes 참고 2019년 1월 11일 이후 생성된 인스턴스      Device Path로 마운트 하기   OCI 콘솔에서 앞서 Attach된 Block Volume의 Device Path(/dev/oracleoci/oraclevdb)를 확인할 수 있습니다.\n   Compute Instance에 SSH로 접속한 후 Device Path를 확인합니다.\n[opc@examplelinuxinstance ~]$ ls -la /dev/oracleoci/ total 0 drwxr-xr-x. 2 root root 140 Jan 10 07:25 . drwxr-xr-x. 20 root root 3200 Jan 10 07:25 .. lrwxrwxrwx. 1 root root 6 Jan 10 05:41 oraclevda -\u0026gt; ../sda lrwxrwxrwx. 1 root root 7 Jan 10 05:41 oraclevda1 -\u0026gt; ../sda1 lrwxrwxrwx. 1 root root 7 Jan 10 05:41 oraclevda2 -\u0026gt; ../sda2 lrwxrwxrwx. 1 root root 7 Jan 10 05:41 oraclevda3 -\u0026gt; ../sda3 lrwxrwxrwx. 1 root root 6 Jan 10 07:35 oraclevdb -\u0026gt; ../sdb   마운트 할 디렉토리 생성\nsudo mkdir /mnt/vol1   /etc/fstab 업데이트\nsudo vi /etc/fstab 명령을 수행하여 확인한 Device Path를 바탕으로 다음 내용을 추가합니다.\nOCI 문서의 권고에 따라 인스턴스 재기동시 장착한 Volume의 장애로 인한 영향을 줄이기 위해 _netdev,nofail 옵션을 반드시 추가합니다.\n  Use the _netdev and nofail Options\nsudo vi /etc/fstab   업데이트 예시\n기존 내용의 제일 아래에 마운트 정보를 한 줄 추가합니다.\n# # /etc/fstab # Created by anaconda on Wed Dec 1 01:55:42 2021 ... ## https://docs.us-phoenix-1.oraclecloud.com/Content/Block/Tasks/connectingtoavolume.htm /dev/oracleoci/oraclevdb /mnt/vol1 xfs defaults,_netdev,nofail 0 2     마운트\nsudo mount -a   마운트 결과\n/dev/sdb가 /mnt/vol1에 마운트 된걸 알 수 있습니다.\n[opc@examplelinuxinstance ~]$ sudo mount -a [opc@examplelinuxinstance ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 310M 0 310M 0% /dev tmpfs 341M 0 341M 0% /dev/shm tmpfs 341M 5.1M 336M 2% /run tmpfs 341M 0 341M 0% /sys/fs/cgroup /dev/sda3 39G 3.1G 36G 8% / /dev/sda1 200M 7.5M 193M 4% /boot/efi tmpfs 69M 0 69M 0% /run/user/0 tmpfs 69M 0 69M 0% /run/user/1000 tmpfs 69M 0 69M 0% /run/user/994 /dev/sdb 50G 33M 50G 1% /mnt/vol1     인스턴스 재시작\n/etc/fstab에 추가하였기 때문에 인스턴스 재시작시에도 자동으로 마운트 됩니다. 아래와 같이 이상없이 마운트 되는지 확인해 봅니다. 재시작 후에도 /mnt/vol1으로 마운트 되어 있는 걸 알 수 있습니다.\n[opc@examplelinuxinstance ~]$ sudo reboot Connection to 146.56.171.40 closed by remote host. Connection to 146.56.171.40 closed. ubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i privateKey opc@146.56.171.xx Last login: Mon Jan 10 08:26:13 2022 from 223.62.21.xx [opc@examplelinuxinstance ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 310M 0 310M 0% /dev tmpfs 341M 0 341M 0% /dev/shm tmpfs 341M 5.0M 336M 2% /run tmpfs 341M 0 341M 0% /sys/fs/cgroup /dev/sda3 39G 3.1G 36G 8% / /dev/sda1 200M 7.5M 193M 4% /boot/efi /dev/sdb 50G 33M 50G 1% /mnt/vol1 tmpfs 69M 0 69M 0% /run/user/0 tmpfs 69M 0 69M 0% /run/user/1000   UUID로 마운트 하기 OCI 문서에 따르면 root volume 이외 volume이 둘 이상일 경우 장치 이름으로 마운트 할 경우 서버 재기동 후 장치 이름과 실제 장치의 대응 되는 순서가 달라 질 수도 있다고 합니다. 그래서 정확하게 하기 위해 Volume의 UUID를 기준으로 마운트합니다.\n관련 내용은 아래 문서를 참고합니다.\n Connecting to a Volume \u0026gt; Traditional fstab Options  ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/5/","tags":["block volume","mount"],"title":"3.6.5 Block Volume 마운트하기"},{"categories":null,"contents":"6.5 WebLogic Server를 Verrazzano를 통해 OKE에 배포하기 Verrazzano 설치시 WebLogic Operator가 함께 설치가 됩니다. 이번 내용은 아래 문서에 있는 내용을 따라 하며 정리한 내용으로, 기존 On-Premise에서 WebLogic Server 에 배포하여 사용하는 애플리케이션을 수정없이 WebLogic Server 를 통채로 Kubernetes 환경으로 이관을 원하는 경우 사용할 수 있는 방법에 대한 내용입니다.\n https://verrazzano.io/latest/docs/guides/lift-and-shift/lift-and-shift/  수행하는 절차는 아래와 같습니다.\n Step #1. WebLogic Server \u0026amp; MySQL DB를 사용하는 애플리케이션을 이관한다는 가정하에 동일한 환경 및 앱을 준비 Step #2. WebLogic Deploy Tooling을 통해 기 사용 WebLogic Server 분석후 메타데이터로 전환 Step #3. WebLogic Image Tool을 통해 메타데이터와 WebLogic 설치 파일을 이용해 컨테이너 이미지 생성 및 배포 파일 생성 Step #4. 생성한 컨테이너 이미지와 배포 파일을 Verrazzano에 배포하면 WebLogic Operator가 이를 이용하여 WebLogic Server 컨테이너 생성  사전 준비  Step #1. WebLogic Server \u0026amp; MySQL DB를 사용하는 애플리케이션을 이관한다는 가정하에 동일한 환경 및 앱을 준비  MySQL 준비  실제 On-Premise 환경에서는 베어메탈 또는 VM 등에 MySQL을 설치 사용하겠지만, 여기서는 데모용으로 도커 컨테이너를 사용합니다.    MySQL 이미지 다운로드\ndocker pull mysql:latest   도커 컨테이너 시작\n사용할 username, password로 변경\nexport MYSQL_USER=\u0026lt;your-mysql-username\u0026gt; export MYSQL_PASSWORD=\u0026lt;your-mysql-password\u0026gt; export MYSQL_ROOT_PASSWORD=\u0026lt;your-mysql-rootpassword\u0026gt; docker run --name tododb \\  -p 3306:3306 \\  -e MYSQL_USER=$MYSQL_USER \\  -e MYSQL_PASSWORD=$MYSQL_PASSWORD \\  -e MYSQL_DATABASE=tododb \\  -e MYSQL_ROOT_PASSWORD=$MYSQL_ROOT_PASSWORD \\  -d mysql:latest   패스워드 정책 변경\nMySQL 8.0 업그레이드후 기본 인증 방식이 변경되어 기존 클라이언트를 위해 이전 인증 방식으로 설정\n  MySQL CLI 클라이언트로 root로 접속, root 패스워드(your-mysql-rootpassword\u0026gt;) 입력\ndocker exec \\  -it tododb mysql \\  -uroot \\  -p   기본 인증 방식으로 변경\nALTER USER \u0026#39;\u0026lt;your-mysql-username\u0026gt;\u0026#39;@\u0026#39;%\u0026#39; identified with mysql_native_password by \u0026#39;\u0026lt;your-mysql-password\u0026gt;\u0026#39;;     WebLogic 서버 설치   Oracle WebLogic Server 12.2.1.4 요구 사항에 따라 64비트 jdk1.8.0_211 이상 다운로드 및 설치\n WebLogic Server Download 사이트에서 Oracle WebLogic Server 12.2.1.4 Generic 설치 파일 다운로드 Java Developement Kit 사이트에서 64비트 jdk1.8.0_211 이상 다운로드 및 설치    설치\nunzip fmw_12.2.1.4.0_wls_lite_Disk1_1of1.zip java -jar fmw_12.2.1.4.0_wls_lite_generic.jar   설치된 ORACLE_HOME 환경 변수 설정\nexport ORACLE_HOME=/home/opc/fmw/wls12c   WebLogic 도메인 생성   구성 마법사 실행\n$ORACLE_HOME/oracle_common/common/bin/config.sh   기본 설정값 기준으로 tododomain 도메인을 $ORACLE_HOME/user_projects/domains/tododomain 에 생성\nexport DOMAIN_HOME=$ORACLE_HOME/user_projects/domains/tododomain   WebLogic Server 실행\n$ORACLE_HOME/user_projects/domains/tododomain/bin/startWebLogic.sh   MySQL 도커 컨테이너용 DataSource 생성   WebLogic Admin Console, http://localhost:7001/console 에 접속\n  관리 콘솔 왼쪽 메뉴에서 서비스 \u0026gt; 데이터 소스 선택\n  JDBC 데이터 소스 페이지에서 **새로 만들기 ** \u0026gt; 일반 데이터 소스 선택\n  데이터 소스 생성 페이지에서 다음 정보 입력\nName:tododbJNDI Name:jdbc/ToDoDBDatabase Type:MySQ  다음 3번 클릭\n  데이터 소스 생성 페이지에서 다음 정보 입력\nDatabase Name:tododbHost name:localhostDatabase Port:3306Database User Name:\u0026lt;your-mysql-username\u0026gt;Password:\u0026lt;your-mysql-password\u0026gt;Confirm Password:\u0026lt;your-mysql-password\u0026gt;Click Next.  다음 페이지에서 구성 테스트 클릭하여 접속 테스트 시행\n  다음 페이지로 이동하여 AdminServer로 대상 선택후 완료 클릭\n  샘플 애플리케이션 배포   기존에 WAS에 배포하던 일반적인 자바 애플리케이션을 가정하여 샘플 예제를 사용하여 빌드합니다.\n  소스 코드 복사후 빌드\n git 클라이언트, maven 클라이언트 설치 필요  git clone https://github.com/verrazzano/examples.git cd examples/todo-list/ mvn clean package   빌드후 하위 target/todo.war 가 생김\n  WebLogic Admin Console, http://localhost:7001/console 에 접속\n  관리 콘솔 왼쪽 메뉴에서 배치 선택\n  설치를 클릭하여 todo.war가 있는 경우 지정\n  다음을 클릭하고 이후는 기본값을 사용하여 앱을 배포\n  데이터 초기화  http://localhost:7001/todo/rest/items/init 을 브라우저로 접속 MySQL 도커 컨테이너와 연결되어 데이터 초기화 성공하면 ToDos table initialized. 응답 메시지 확인됨  애플리케이션 접속   http://localhost:7001/todo/index.html 주소로 애플리케이션 접속하면, 정상적으로 데이터가 뜨는 것을 알 수 있습니다.\n   이관 작업 WebLogic Operator을 통해 아래 내용을 WDT Model을 통해 이관하는 방법입니다.\n 참고사항  Choose a domain home source type :: WebLogic Kubernetes Operator (oracle.github.io)    WDT 모델 생성  Step #2. WebLogic Deploy Tooling을 통해 기 사용 WebLogic Server 분석후 메타데이터로 전환   WebLogic Deploy Tooling (WDT)을 아래와 같은 명령으로 다운 받아 설치 합니다.  curl -OL https://github.com/oracle/weblogic-deploy-tooling/releases/latest/download/weblogic-deploy.zip unzip weblogic-deploy.zip cd weblogic-deploy export WDT_HOME=$(pwd)  WDT에서 제공하는 discoverDomain.sh를 통해 앞서 앱을 배포한 DOMAIN_HOME을 분석하여 결과물로 메타데이터 모델을 생성합니다.\n 실행전에 export JAVA_HOME으로 jdk home을 사전에 설정해야 합니다.  mkdir v8o $WDT_HOME/bin/discoverDomain.sh \\  -oracle_home $ORACLE_HOME \\  -domain_home $DOMAIN_HOME \\  -model_file ./v8o/wdt-model.yaml \\  -archive_file ./v8o/wdt-archive.zip \\  -target vz \\  -output_dir v8o   실행결과\n$ ls -la v8o/ total 32 drwxrwxr-x. 2 opc opc 4096 Jan 7 08:00 . drwxrwxr-x. 4 opc opc 67 Jan 7 08:00 .. -rwxr-x---. 1 opc opc 1944 Jan 7 08:00 create_k8s_secrets.sh -rw-r-----. 1 opc opc 3156 Jan 7 08:00 vz-application.yaml -rw-r-----. 1 opc opc 51 Jan 7 08:00 vz_variable.properties -rw-r-----. 1 opc opc 10671 Jan 7 08:00 wdt-archive.zip -rw-r-----. 1 opc opc 1179 Jan 7 08:00 wdt-model.yaml  create_k8s_secrets.sh - 웹로직 관리자 정보 등을 kubernes secret으로 만들기 위한 헬프 스크립트 vz-application.yaml - Verrazzano 애플리케이션으로 배포하기 위한 YAML이 자동으로 생성 vz_variable.properties - Verrazzano 애플리케이션으로 배포하기 위한 변수값 wdt-archive.zip - 앞서 배포된 ToDO List 애플리케이션을 포함한 WDT 아카이브 파일 wdt-model.yaml - WebLogic Server 도메인에 대한 WDT model    컨테이너 이미지 생성  Step #3. WebLogic Image Tool을 통해 메타데이터와 WebLogic 설치 파일을 이용해 컨테이너 이미지 생성 및 배포 파일 생성    WebLogic Image Tool을 아래와 같은 명령으로 다운 받아 설치 합니다.\ncurl -OL https://github.com/oracle/weblogic-image-tool/releases/latest/download/imagetool.zip unzip imagetool.zip cd imagetool export WIT_HOME=$(pwd)   WIT를 통해 컨테이너 이미지를 생성합니다. 툴을 사용하기 위해서는 앞서 다운 받은 WebLogic 설치 Zip 파일과 JDK 설치 파일이 필요합니다.\n 설치 파일이 위치한 경로를 맞게 수정하고 최종적으로 생성될 컨테이너 이미지의 태그를 원하는 값으로 수정합니다.  # The directory created previously to hold the generated scripts and models. cd v8o $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path ~/stage/jdk-8u301-linux-x64.tar.gz \\  --type jdk \\  --version 8u301 # The installer file name may be slightly different depending on # which version of the 12.2.1.4.0 installer that you downloaded, slim or generic. $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path ~/stage/fmw_12.2.1.4.0_wls_lite_Disk1_1of1.zip \\  --type wls \\  --version 12.2.1.4.0 $WIT_HOME/bin/imagetool.sh cache addInstaller \\  --path $WDT_HOME/../weblogic-deploy.zip \\  --type wdt \\  --version latest # Paths for the files in this command assume that you are running it from the # v8o directory created during the `discoverDomain` step. $WIT_HOME/bin/imagetool.sh create \\  --tag \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;/todo:1 \\  --version 12.2.1.4.0 \\  --jdkVersion 8u301 \\  --wdtModel ./wdt-model.yaml \\  --wdtArchive ./wdt-archive.zip \\  --wdtVariables ./vz_variable.properties \\  --resourceTemplates=./vz-application.yaml \\  --wdtModelOnly   생성된 컨테이너 이미지를 OCIR 레지스트리에 푸쉬합니다.\ndocker push \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;/todo:1   Verrazzano가 설치된 OKE에 배포하기  Step #4. 생성한 컨테이너 이미지와 배포 파일을 Verrazzano에 배포하면 WebLogic Operator가 이를 이용하여 WebLogic Server 컨테이너 생성  namespace 생성 및 라벨 달기   단일 클러스터 기준 다음과 같이 실행합니다.\nkubectl create namespace tododomain kubectl label namespace tododomain verrazzano-managed=true istio-injection=enabled   Secret 생성   WDT 실행으로 만들어지 헬프 스크립트(v8o/create_k8s_secrets.sh)로 secret을 만듭니다.\n  아래 username과 password를 원하는 값으로 변경합니다.\n weblogic 관리자 이름, 암호 ToDo 애플리케이션이 사용할 MySQL DB 유저 이름, 암호  여기서 만들어지는 secret으로 OKE에 배포될 MySQL도 바라보게 해야 합니다. 이후 배포되는 예제는 사전에 그렇게 설정되어 있습니다.    # Update \u0026lt;user\u0026gt; and \u0026lt;password\u0026gt; for weblogic-credentials create_paired_k8s_secret weblogic-credentials \u0026lt;user\u0026gt; \u0026lt;password\u0026gt; # Update \u0026lt;user\u0026gt; and \u0026lt;password\u0026gt; for jdbc-tododb create_paired_k8s_secret jdbc-tododb tododb \u0026lt;password\u0026gt; # Update \u0026lt;password\u0026gt; used to encrypt model and domain hashes # This secret is only required for model-in-image deployments create_k8s_secret runtime-encryption-secret \u0026lt;password\u0026gt;   스크립트를 실행합니다.\n 기본적으로 도메인 이름(tododomain)의 namespace에 생성됩니다. create_k8s_secrets.sh 설정 참고  $ ./create_k8s_secrets.sh secret/tododomain-weblogic-credentials created secret/tododomain-weblogic-credentials labeled secret/tododomain-jdbc-tododb created secret/tododomain-jdbc-tododb labeled secret/tododomain-runtime-encryption-secret created secret/tododomain-runtime-encryption-secret labeled $ kubectl get secret -n tododomain NAME TYPE DATA AGE default-token-snckl kubernetes.io/service-account-token 3 7m58s tododomain-jdbc-tododb Opaque 2 93s tododomain-runtime-encryption-secret Opaque 1 83s tododomain-weblogic-credentials Opaque 2 103s   OCIR에서 이미지를 가져오기 위한 imagePullSecret을 만듭니다. v8o/vz-application.yaml을 보면 tododomain-registry-credentials으로 사용하고 있습니다. 동일한 이름으로 secret을 생성합니다.\nkubectl create secret docker-registry tododomain-registry-credentials \\  --docker-server=\u0026lt;region-key\u0026gt;.ocir.io \\  --docker-email=your.name@example.com \\  --docker-username=\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; \\  --docker-password=\u0026#39;\u0026lt;AUTH_TOKEN\u0026gt;\u0026#39; \\  --namespace=tododomain   Verrazzano에 애플리케이션 배포 배포되는 예제 애플리케이션은 MySQL DB를 저장소로 사용하고 있습니다. OKE에 이관된 WebLogic Server 위에 애플리케이션도 구동을 위해서는 MySQL DB가 필요합니다. 문서 예제에서는 MySQL DB를 Verrazzano Component로 배포하고 있으며, 해당 Component를 사용하도록 v8o/vz-application.yaml에서 Data Source 주소를 변경하고 있습니다. 이 배포되는 MySQL DB Component가 배포시 앞서 만들 jdbc-tododb secret을 사용합니다.\nMySQL Component 배포   mysql-oam.yaml 파일을 다운로드 받습니다.\n  아래와 같이 다운로드 받은 YAML 파일을 배포합니다.\n[opc@bastion-host v8o (⎈ |managed-cluster-1:default)]$ kubectl apply -f mysql-oam.yaml component.core.oam.dev/todo-mysql-service created component.core.oam.dev/todo-mysql-deployment created component.core.oam.dev/todo-mysql-configmap created [opc@bastion-host v8o (⎈ |managed-cluster-1:default)]$ kubectl get component -n tododomain NAME WORKLOAD-KIND AGE todo-mysql-configmap ConfigMap 13s todo-mysql-deployment Deployment 13s todo-mysql-service Service 13s   Verrazzano 배포용 application configuration설정 업데이트   v8o/vz-application.yaml에 component 3개 추가\napiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:tododomain-appconf...spec:components:- componentName:tododomain-domain...- componentName:tododomain-configmap- componentName:todo-mysql-service- componentName:todo-mysql-deployment- componentName:todo-mysql-configmap   v8o/vz-application.yaml에 Data Source의 URL 업데이트\napiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:tododomain-configmap...spec:workload:...data:wdt_jdbc.yaml:|resources: JDBCSystemResource: \u0026#39;tododb\u0026#39;: JdbcResource: JDBCDriverParams: # This is the URL of the database used by the WebLogic Server application URL: \u0026#34;jdbc:mysql://mysql.tododomain.svc.cluster.local:3306/tododb\u0026#34;  애플리케이션(ToDo 애플리케이션 + MySQL) 배포   애플리케이션을 Verrazzano가 설치된 OKE에 배포합니다.\n$ kubectl apply -f vz-application.yaml applicationconfiguration.core.oam.dev/tododomain-appconf created component.core.oam.dev/tododomain-domain created component.core.oam.dev/tododomain-configmap created   tododomain-appconf 애플리케이션이 배포되고 VerrazzanoWebLogicWorkload 유형의 tododomain-domain이 배포됩니다.\n#v8o/vz-application.yaml 참고...---apiVersion:core.oam.dev/v1alpha2kind:Componentmetadata:name:tododomain-domainnamespace:tododomainspec:workload:apiVersion:oam.verrazzano.io/v1alpha1kind:VerrazzanoWebLogicWorkload...  배포가 완료되고 POD가 기동됩니다.\n$ kubectl get pod -n tododomain NAME READY STATUS RESTARTS AGE mysql-7bfb67d9f8-gn4qc 2/2 Running 0 5m35s tododomain-adminserver 4/4 Running 0 3m16s   istio상의 애플리케이션 주소를 확인합니다.\nkubectl get gateway tododomain-tododomain-appconf-gw \\  -n tododomain \\  -o jsonpath={.spec.servers[0].hosts[0]}; echo   결과 예시\n$ kubectl get gateway tododomain-tododomain-appconf-gw \\ \u0026gt; -n tododomain \\ \u0026gt; -o jsonpath={.spec.servers[0].hosts[0]}; echo tododomain-appconf.tododomain.thekoguryo.ml     /todo를 추가하여 애플리케이션에 접속할 수 있습니다.\nhttps://tododomain-appconf.tododomain.thekoguryo.ml/todo\n  On-Premise와 동일하게 데이터 초기화하여 MySQL DB Component와 연결되어 데이터가 정상 입력되는 것을 확인할 수 있습니다.\nhttps://tododomain-appconf.tododomain.thekoguryo.ml/todo/rest/items/init\n  WebLogic Admin Console 접속   기본 배포되면 애플리케이션만 istio ingress에 등록되어 있어, WebLogic Admin Console은 포트 포워딩을 통해 접속합니다.\nkubectl port-forward pods/tododomain-adminserver 7001:7001 -n tododomain   다만 포트 포워딩 에러가 나는 경우, 아래와 같이 ingress에 노출하여 외부에서 접속하게 추가할 수도 있습니다. 운영에서는 Public IP로 접근은 추천하지 않습니다.\n  vz-application.yaml을 아래와 같이 - path: \u0026ldquo;/console\u0026rdquo; 포함 2줄을 추가하고 다시 배포(kubectl apply -f vz-application.yaml)합니다.\n...apiVersion:core.oam.dev/v1alpha2kind:ApplicationConfigurationmetadata:name:tododomain-appconf...spec:components:- componentName:tododomain-domaintraits:- trait:...- trait:apiVersion:oam.verrazzano.io/v1alpha1kind:IngressTraitspec:rules:- paths:# application todo- path:\u0026#34;/todo\u0026#34;pathType:Prefix- path:\u0026#34;/console\u0026#34;pathType:Prefix...    이전 주소에서 /todo 대신 /console로 접속하고 v8o/create_k8s_secrets.sh 에서 입력한 WebLogic 관리자 이름, 암호를 사용합니다.\nhttps://tododomain-appconf.tododomain.thekoguryo.ml/console\n   모니터링 로그 모니터링(Elasticsearch / Kibana)   Verrazzano 콘솔에서 Kibana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  단일 클러스터와 배포시와 동일한 방법으로 verrazzano-namespace-hello-helidon 네임스페이스로 인덱스 패턴을 생성합니다.\n  kubectl로 조회한 컨테이너 로그가 Kibana에서 생성한 인덱스 패턴에 대해 수집된 로그, 대상 namespace 상의 전체 수집된 로그가 보입니다.\n  로그 조회\n$ kubectl logs tododomain-adminserver -n tododomain -c weblogic-server -f ... \u0026lt;Jan 7, 2022 9:21:58,416 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;WebLogicServer\u0026gt; \u0026lt;BEA-000331\u0026gt; \u0026lt;Started the WebLogic Server Administration Server \u0026#34;AdminServer\u0026#34; for domain \u0026#34;tododomain\u0026#34; running in development mode.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,416 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;Server\u0026gt; \u0026lt;BEA-002613\u0026gt; \u0026lt;Channel \u0026#34;Default\u0026#34; is now listening on 10.244.1.15:7001 for protocols iiop, t3, ldap, snmp, http.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,416 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;Server\u0026gt; \u0026lt;BEA-002613\u0026gt; \u0026lt;Channel \u0026#34;http-probe\u0026#34; is now listening on 127.0.0.1:8888 for protocols http.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,416 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;Server\u0026gt; \u0026lt;BEA-002613\u0026gt; \u0026lt;Channel \u0026#34;http-probe-ext\u0026#34; is now listening on 10.244.1.15:8888 for protocols http.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,417 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;Server\u0026gt; \u0026lt;BEA-002613\u0026gt; \u0026lt;Channel \u0026#34;Default\u0026#34; is now listening on 10.244.1.15:7001 for protocols iiop, t3, ldap, snmp, http.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,417 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;Server\u0026gt; \u0026lt;BEA-002613\u0026gt; \u0026lt;Channel \u0026#34;http-probe\u0026#34; is now listening on 127.0.0.1:8888 for protocols http.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,417 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;Server\u0026gt; \u0026lt;BEA-002613\u0026gt; \u0026lt;Channel \u0026#34;http-probe-ext\u0026#34; is now listening on 10.244.1.15:8888 for protocols http.\u0026gt; \u0026lt;Jan 7, 2022 9:21:58,428 AM UTC\u0026gt; \u0026lt;Notice\u0026gt; \u0026lt;WebLogicServer\u0026gt; \u0026lt;BEA-000360\u0026gt; \u0026lt;The server started in RUNNING mode.\u0026gt; Successfully started server AdminServer ... ...   Kibana\n     메트릭 모니터링(Prometheus / Grafana)   Verrazzano 콘솔에서 Grafana 링크를 클릭합니다. SSO 구성이 되어 추가 로그인은 필요없습니다.\n  기본 제공하고 있는 대쉬보드 중에 WebLogic Server 대쉬보드를 선택합니다.\n  기본 제공하는 WebLogic Server 대쉬보드를 통해 Verrazzano가 관리하는 모든 클러스터 상에 있는 WebLogic 기반 컨테이너의 상태를 모니터링 할 수 있습니다.\n   ","lastmod":"2022-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/verrazzano/5.weblogic-on-oke-using-verrazzano/","tags":["oke","verrazzano","weblogic"],"title":"6.5 WebLogic Server를 Verrazzano를 통해 OKE에 배포하기"},{"categories":null,"contents":"1.5 OCIR 이미지 사용하여 앱 배포하기 OCIR에 이미지 등록하기 Oracle Cloud Infrastructure Registry(OCIR)는 오라클이 제공하는 관리형 컨테이너 레지스트리로 Docker V2 API를 지원하며, Open Container Initiate 호환 컨테이너 레지스트리입니다. docker cli를 통해 이미지를 Push, Pull 해서 사용할 수 있으며, Kubernetes 클러스터에서도 사용할 수 있습니다.\nOCIR에 이미지를 사용하기 위해서는 먼저 등록 작업이 필요하며, 앞서 예제에서 사용한 nginx 이미지를 아래 절차에 따라 등록해 봅니다.\nOCIR Repository 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, oke-labs)를 선택합니다.\n  이미지를 Push하기 전에 먼저 OCIR에 repository를 생성이 필요합니다.\nCreate repositoy를 클릭하여 아래와 같이 nginx repository를 생성합니다. Push, Pull 모두 인증 테스트를 위해 Access 모드를 Private으로 선택합니다.\n   생성 완료\n   Repository 화면에서 Namespace를 복사해 둡니다. Region내에 Tenant를 나타내는 tenancy-namespace: cnrlxx3w0wgq로 이후 로그인시 필요합니다.\n  OCI Auth Token 만들기 docker cli로 docker hub에 이미지를 등록하거나, 가져올때 username/password로 docker login을 통해 로그인을 합니다. OCIR에도 마찬가지로 로그인이 필요하며, password 대신 보안을 위해 Auth Token을 사용합니다.\n  우측 상단 사용자의 Profile 아이콘을 클릭하여 User Settings으로 이동합니다.\n 아래 그림상의 유저는 OCI local 유저로 username이 oke-admin입니다. 유저명이 oracleidentitycloudservice/~~~로 시작하면 Oracle Identiry Cloud Service의 유저입니다.     왼쪽 아래 Resources \u0026gt; Auth Token으로 이동합니다.\n  Auth Token 생성을 위해 Generate Token을 클릭합니다.\n  설명을 입력하고 생성합니다. Auth Token은 생성시에만 볼수 있으므로 복사해 둡니다.\n    OCIR 로그인 및 이미지 Push   앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 docker cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:~ (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   이미지 Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식으로 태그를 한 후 Push 하면 됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt;:\u0026lt;tag\u0026gt;   nginx:latest 예시  docker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCIR 확인\nOCI 서비스 콘솔로 다시 돌아가서 대상 Compartment 기준으로 Push한 이미지가 정상적으로 등록된 것을 알 수 있습니다.\n   실수를 막기 위한 참고 사항 다음과 같은 상황에서는 docker push하면 어떻게 될까요?\n  Push 전에 OCIR Repository를 사전에 만들지 않은 경우\n  dev/nginx:latest(또는 bitnami/nginx:latest)와 같이 이미지 이름 앞에 추가 적인 경로가 있는 경우에 OCIR Repoistory를 dev 로만 만든 경우\n=\u0026gt; 사전에 OCIR Repository를 만들지 않으면, 기본 설정에 의해 root compartment 쪽에 push 됩니다.\n=\u0026gt; dev/nginx 까지마 Repository 이름으로 해야 합니다. 그렇게 만들지 않는 경우 동일하게 root compartment 쪽에 push 됩니다.\n  Container Registry 우측 상단에 Settings를 클릭하여 설정정보를 보면 아래와 같이 대상 repository가 없는 경우 root compartment에 private repository를 자동으로 새로 만들고 push 하는 것이 기본 값으로 체크되어 있습니다.\n OCIR 이미지로 OKE 클러스터에 배포 OCIR 이미지 배포 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   배포 결과 아래와 같이 private repository로 인증문제로 이미지를 가져오는 오류가 발생한 것을 알수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ocir-6c9d554866-nqgjg 0/1 ErrImagePull 0 10s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe pod nginx-ocir-6c9d554866-nqgjg Name: nginx-ocir-6c9d554866-nqgjg ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning Failed 11s (x2 over 23s) kubelet Failed to pull image \u0026#34;ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest\u0026#34;: rpc error: code = Unknown desc = Error reading manifest latest in ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx: denied: Anonymous users are only allowed read access on public repos Warning Failed 11s (x2 over 23s) kubelet Error: ErrImagePull   OCIR Private Repository 이미지 배포 테스트 - imagepullsecret Private Repository에서 이미지를 가져와서 사용하려면 인증을 위한 secret을 등록해서 사용해야 합니다. 아래 절차에 따라 secret을 만들어 사용합니다.\n  앞서 Auth Token을 사용하여 docker login을 하였습니다. 로그인 하면 사용자 홈 밑에 .docker/config.json에 인증정보가 저장됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   위 인증 정보를 통해 로그인 합니다.\nkubectl create secret generic ocir-secret \\  --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\  --type=kubernetes.io/dockerconfigjson   또는 docker login 정보 없이 직접 secret을 만들 수도 있습니다.\nkubectl create secret docker-registry \u0026lt;secret-name\u0026gt; --docker-server=\u0026lt;region-key\u0026gt;.ocir.io --docker-username=\u0026#39;\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;oci-username\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;oci-auth-token\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email-address\u0026gt;\u0026#39;   아래와 같이 imagepullsecret을 사용하여 다시 배포합니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-ocirname:nginx-ocirspec:replicas:1selector:matchLabels:app:nginx-ocirtemplate:metadata:labels:app:nginx-ocirspec:containers:- name:nginximage:ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latestimagePullSecrets:- name:ocir-secret  아래와 같이 정상 배포되는 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret generic ocir-secret \\ \u0026gt; --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\ \u0026gt; --type=kubernetes.io/dockerconfigjson secret/ocir-secret created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-2tvwr kubernetes.io/service-account-token 3 24h ocir-secret kubernetes.io/dockerconfigjson 1 13s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f nginx-ocir-deployment.yaml deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-798957d964-9rddt 1/1 Running 0 7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 8s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-798957d964 1 1 1 7s   OCIR Private Repository 이미지 배포 테스트 - default imagepullsecret 매번 imagepullsecret을 지정하는 것이 불편한 경우 기본으로 사용할 Container Repository에 대한 인증을 default로 저장하여 사용할 수도 있습니다.\n  namespace에 default serviceaccount가 있는데, 여기에 아래와 같이 imagepullsecret을 추가합니다.\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39;   그 결과 아래와 같이 default serviceaccount에 기본적으로 사용할 imagesecret이 추가되었습니다\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39; serviceaccount/default patched oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get sa default -o yaml apiVersion: v1 imagePullSecrets: - name: ocir-secret kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2021-11-08T13:51:17Z\u0026#34; name: default namespace: default resourceVersion: \u0026#34;277559\u0026#34; uid: f72718f9-135c-46d2-b0ac-a1ea1b990863 secrets: - name: default-token-2tvwr   앞서 배포한 yaml을 삭제하고 인증정보가 없어 처음 실패한 명령으로 다시 배포합니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   결과확인하면 default imagepullsecret을 사용하여 정상 배포됨을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-6c9d554866-vmjtb 1/1 Running 0 5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 5s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-6c9d554866 1 1 1 5s   ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/5.deploy-ocir-image/","tags":["oke"],"title":"1.5 OCIR 이미지 사용하기"},{"categories":null,"contents":"14.2.5 다중 환경을 위한 workspace 사용하기 OCI는 여러 Region을 제공하고 있습니다. 필요에 따라 동일 자원을 여러 Region에 한꺼번에 만들어야 하는 경우가 있습니다. 하지만 앞서 실습에서 잠깐 보았겠지만, terraform 설정이 실행되면 기본으로 해당 폴더에 terraform.tfstate로 상태를 관리하게되어 동일 Terraform 설정을 그대로 사용할 경우 상태파일이 충돌나서 원하는 데로 동작하지 않게 됩니다. Terraform 설정 폴더 자체를 통채로 복사해서 대상 Region 별로 사용해도 되지만, 관리가 번거롭게 됩니다. 그래서 Terraform에서는 workspace만 개념을 제공하고, 기본적으로 default workspace를 사용하게 됩니다.\nTerraform Configuration 파일 이전 실습에서 사용한 vcn 만들기 설정을 그대로 사용합니다.\nWorkspace 만들기   terraform workspace 구문\n# workspace 조회하기 terraform workspace list # 새로운 workspace 만들기 terraform workspace new {이름} # 해당 worksapce 삭제하기 terraform workspace delete {이름} # 현재 workspace를 전환하기 terraform workspace select {이름}     새로운 workspace를 만듭니다.\nterraform workspace new ap-seoul-1 terraform workspace new ap-chuncheon-1   예시\n[opc@bastion-host example_vcn]$ terraform workspace list * default [opc@bastion-host example_vcn]$ terraform workspace new ap-seoul-1 Created and switched to workspace \u0026#34;ap-seoul-1\u0026#34;! You\u0026#39;re now on a new, empty workspace. Workspaces isolate their state, so if you run \u0026#34;terraform plan\u0026#34; Terraform will not see any existing state for this configuration. [opc@bastion-host example_vcn]$ terraform workspace new ap-chuncheon-1 Created and switched to workspace \u0026#34;ap-chuncheon-1\u0026#34;! You\u0026#39;re now on a new, empty workspace. Workspaces isolate their state, so if you run \u0026#34;terraform plan\u0026#34; Terraform will not see any existing state for this configuration. [opc@bastion-host example_vcn]$ terraform workspace list default * ap-chuncheon-1 ap-seoul-1 [opc@bastion-host example_vcn]$     1번째 workspace: ap-seoul-1에서 terraform 실행하기   workspace를 먼저 전환하고 vcn을 만드는 terraform을 실행합니다.\n[opc@bastion-host example_vcn]$ terraform workspace select ap-seoul-1 Switched to workspace \u0026#34;ap-seoul-1\u0026#34;. [opc@bastion-host example_vcn]$ terraform apply -var=\u0026#34;region=ap-seoul-1\u0026#34; Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaa54ryitndueosfezrxvxvcuosutofi2d6f53rbgwz2dpqrgeci7lq\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] Do you want to perform these actions in workspace \u0026#34;ap-seoul-1\u0026#34;? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes oci_core_virtual_network.vcn1: Creating... oci_core_virtual_network.vcn1: Creation complete after 1s [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaavsea7yiamf4ktoajkby4sh45zx4e52cwscm6xljilibl5o3prpkq] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaavsea7yiamf4ktoajkby4sh45zx4e52cwscm6xljilibl5o3prpkq\u0026#34;, ] [opc@bastion-host example_vcn]$   결과 확인\nOCI 콘솔에서 서울 Region으로 변경하면 VCN이 생성된 것을 확인할 수 있습니다.\n   2번째 workspace: ap-chuncheon-1에서 terraform 실행하기   동일한 위치에서 workspace를 먼저 전환하고 vcn을 만드는 terraform을 동일하게 실행합니다.\n[opc@bastion-host example_vcn]$ terraform workspace select ap-chuncheon-1 Switched to workspace \u0026#34;ap-chuncheon-1\u0026#34;. [opc@bastion-host example_vcn]$ terraform apply -var=\u0026#34;region=ap-chuncheon-1\u0026#34; Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaa54ryitndueosfezrxvxvcuosutofi2d6f53rbgwz2dpqrgeci7lq\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] Do you want to perform these actions in workspace \u0026#34;ap-chuncheon-1\u0026#34;? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes oci_core_virtual_network.vcn1: Creating... oci_core_virtual_network.vcn1: Creation complete after 0s [id=ocid1.vcn.oc1.ap-chuncheon-1.amaaaaaavsea7yiamplordafqo25ulky46sjoz74r724wxnd5dyplya3ptoq] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-chuncheon-1.amaaaaaavsea7yiamplordafqo25ulky46sjoz74r724wxnd5dyplya3ptoq\u0026#34;, ] [opc@bastion-host example_vcn]$   결과 확인\nOCI 콘솔에서 춘천 Region으로 변경하면 VCN이 생성된 것을 확인할 수 있습니다.\n   Terraform State 확인\nworkspace를 사용하는 경우 아래와 같이 terraform.tfstate.d 폴더 밑에 workspace 별 폴더 밑에 terrafrom.tfstate 파일이 생성되어 workspace 별로 상태를 관리하는 것을 알 수 있습니다.\n[opc@bastion-host example_vcn]$ ls terraform.tfstate.d/ -la total 4 drwxr-xr-x. 4 opc opc 46 Jan 18 12:39 . drwxrwxr-x. 4 opc opc 4096 Jan 18 12:47 .. drwxr-xr-x. 2 opc opc 31 Jan 18 12:51 ap-chuncheon-1 drwxr-xr-x. 2 opc opc 31 Jan 18 12:47 ap-seoul-1 [opc@bastion-host example_vcn]$ ls terraform.tfstate.d/ap-seoul-1/ -la total 4 drwxr-xr-x. 2 opc opc 31 Jan 18 12:47 . drwxr-xr-x. 4 opc opc 46 Jan 18 12:39 .. -rw-rw-r--. 1 opc opc 2147 Jan 18 12:47 terraform.tfstate [opc@bastion-host example_vcn]$ ls terraform.tfstate.d/ap-chuncheon-1/ -la total 4 drwxr-xr-x. 2 opc opc 31 Jan 18 12:51 . drwxr-xr-x. 4 opc opc 46 Jan 18 12:39 .. -rw-rw-r--. 1 opc opc 2168 Jan 18 12:51 terraform.tfstate [opc@bastion-host example_vcn]$   ","lastmod":"2019-04-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/5/","tags":["terraform","workspace"],"title":"14.2.5 다중 환경을 위한 workspace 사용하기"},{"categories":null,"contents":"10.5 Load Balancer 만들기 Load Balancer 생성   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026raquo; Load Balancers 항목으로 이동합니다.\n  Create Load Balancer 클릭합니다.\n  Load Balancer 를 타입으로 선정합니다.\n Load Balancer Type: L7 로드밸런서로 HTTP Listener로 분배할때 사용합니다. Network Load Balancer Type: L4 로드밸런서로 일반 IP, Port로 분배할때 사용합니다.     기본 생성정보 입력\n  Name: Load Balancer 이름 입력, 예, ExampleLB\n  Choose visibility type: 여기서는 Public 선택\n Public: Public IP를 부여할지 여부 선택 Private: Private IP 만 가집니다. 내부용 로드밸런서인 경우 선택 용 로드밸런인지 설정합니다.    Assign a public IP address: Public IP로 외부 서비스를 하는 고정된 Public IP를 사용할지 여부를 지정합니다. 여기서는 테스트이므로 Ephemeral IP Address 선택\n Ephemeral IP Address: Load Balancer 생성시 부여된 Public IP를 사용하는 경우 Reserved IP Address: 사용자 예약한 Public IP를 사용하는 경우     Bandwidth: 여기서는 일단 기본값을 사용\n   Choose Networking: LB 테스트용으로 앞서 만든 VCN과 Subnet 선택\n VCN: LoadBalancerVCN Subnet: Public LB Subnet       Backend 설정\n  Load Balancing Policy: 분배규칙을 선택합니다. 여기서는 이후 테스트를 위해 라운드 로빈을 선택합니다.\n Weighted Round Robin: 가중치 기반 라운드 로빈 분배 방식 IP Hash: 클라이언트 IP 주소의 해쉬값을 이용해 분배 Least Connections: Backend의 연결 갯수를 기준으로 적은 쪽으로 분배     Add Backend\n로드밸런서가 부하분산할 대상을 추가하는 부분입니다. 앞서 미리 만들어 둔 WebServer 2개를 추가합니다.\n   Healch Check Policy: 현재 WebServer의 기본 포트(80)으로 헬스 체크할 것이므로 기본값을 그대로 사용합니다.\n  Use SSL: 지금은 Apache HTTP Server를 HTTP로만 서비스 중이므로 여기서는 체크안함\n   Advanced Option\n  Backend Set Name: 명명 규칙을 맞추려면 여기서 변경, 예, ExampleLB-BackendSet\n   Security List: 클라이언트-\u0026gt;LB간, LB-\u0026gt;Backend간 각각 서비스 포트를 개방하기 위한 Security List 업데이트 필요합니다. 기본적으로 자동으로 설정되며, 아래 고급옵션에서 자동 여부 및 대상 Security List를 변경할 수 있습니다. 추가로 만든 LB Security List를 선택합니다.\n   Session Persistence: 세션 쿠키값에 따른 Persistent를 사용할 경우 방법 설정, 이후 원활한 분배 테스트 확인을 위해서 여기서는 생략\n      Configure Listener\n Listener Name: 이름 입력, 예, ExampleLB-Listener Specify the type of traffic your listener handles: HTTP 선택  HTTP, HTTPS, HTTP/2가 선택옵션이나, HTTPS, HTTP/2 선택이 SSL 인증서를 추가설정이 필요합니다. 여기서는 테스트용으로 HTTP를 사용합니다.       Manage Logging\n 에러 로그와 액세스 로그를 OCI Log 서비스를 사용하도록 설정할 수 있습니다. 일단 여기서는 사용하지 않습니다.    Submit을 클릭하여 Load Balancer를 생성합니다.\n  Load Balancer가 생성되고 조금 지나면 Backend 서버의 헬스체크가 성공하고 Overall Health가 OK 상태가 되게 됩니다.\n   Load Balancer 생성 결과 확인   생성된 Loadb Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n  생성시 입력한 정보를 기준으로 Backend Sets과 Listeners가 만들어져 있습니다.\n  분배 규칙 및 Backend 서버 추가 등 업데이트를 지원합니다.\n 이름 길이는 제약이 있네요. 19자리가 최대인듯 합니다.     Listener 또한 생성된 것을 볼 수 있으며, 해당 설정은 업데이트를 지원합니다.\n   Security List 자동 업데이트 결과 확인   Load Balancer의 Security List인 LB Security List의 상세 화면으로 이동합니다.\n  Egress Rule이 1개 추가 되었습니다. Add Backend Set으로 추가된 WebServer 2대와 서비스 포트 80에 해당되는, 즉 해당 서브넷(10.0.0.0/24)의 80 포트로 개방된 것을 알 수 있습니다.\n   Backend인 WebServer 2가 포함된 서브넷의 Security List인 Default Security List에 Public LB Subnet(10.0.2.0/24)에서 WebServer 포트인 80으로 들어 올 수 있게 마지막에 규칙이 하나 추가되었습니다.\n   자동으로 추가된 위 2개의 규칙으로 Load Balancer -\u0026gt; WebServer간 80 포트 통신은 개방되었습니다. 하지만, 실제 클라이언트가 Load Balancer로 올 수 있게 LB Security List의 Ingress에는 80 포트가 개방되지 않았습니다.\n  다시 LB Security List의 상세 화면으로 돌아가 클라이언트 -\u0026gt; Load Balancer간의 개방을 위한 규칙을 설정합니다. LB Security List에 Ingress Rule을 아래와 같이 추가 합니다.\n   ","lastmod":"2019-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/5/","tags":["load balancer"],"title":"10.5 Load Balancer 만들기"},{"categories":null,"contents":"6.5 Policy 개념 잡기 IAM Policy IAM Policy는 OCI 자원에 대한 접근 정책입니다. 하나의 정책은 일반적으로 사용자 그룹에 특정 Compartment에 속한 특정 타입의 자원에 대한 권한을 허용하는 것을 정의하는 것으로 생각하면 하면 됩니다.\nPolicy 문법 모든 Policy는 다음과 같은 문법으로 되어 있습니다.\nAllow \u0026lt;subject\u0026gt; to \u0026lt;verb\u0026gt; \u0026lt;resource-type\u0026gt; in \u0026lt;location\u0026gt; where \u0026lt;conditions\u0026gt; 자세한 문법은 OCI Documentation을 보시기 바랍니다.https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm\nPolicy 일반적인 형식 자원에 대한 권한은 아래와 같은 Policy를 통해 정의되면, Tenancy 생성시 관리자에 대해서는 모든 권한이 부여되고, 별도 Policy를 적용하지 않은 일반 사용자, 그룹은 아무런 권한이 일절 없습니다. Policy로 명시적으로 허용한 권한만을 부여합니다.\n많이 사용하는 Policy는 아래와 같이 사용자 그룹이 compartment 내의 특정 자원 유형에 대해서 읽기, 쓰기, 전체 권한 등을 부여하는 형식으로 정의합니다.\nAllow group \u0026lt;group_name\u0026gt; to \u0026lt;verb\u0026gt; \u0026lt;resource-type\u0026gt; in compartment \u0026lt;compartment_name\u0026gt; 일부에 대해서는 compartment가 아닌 tenancy 전체에 대해 적용되는 Policy를 정의하기도 합니다.\nAllow group \u0026lt;group_name\u0026gt; to \u0026lt;verb\u0026gt; \u0026lt;resource-type\u0026gt; in tenancy Subjects 일반적으로 group이 오며, 특정 사용자는 지정할 수 없습니다. 모든 사용자(any-user)는 지정할 수 있습니다.\n참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm#one\nVerbs    Verb 설명 대상 사용자     inspect 사용자 정의 메타데이터, 보안 정보를 제외한 자원을 리스트(조회)할 수 있는 권한 조회가 필요한 제3자 사용자   read inspect에 추가하여 사용자 정의 메타데이터도 조회할 수 있는 권한 포함 조회가 필요한 내부 사용자   use read에 추가하여 자원을 사용할 수 있는 권한 포함. 자원 생성, 삭제에 대한 권한은 없음.일반적으로 업데이트 권한도 포함하지만, 생성과 동일한 역활을 하는 업데이트 권한은 포함하지 않음 일반 사용자   manage 자원에 대한 모든 권한 포함 관리자    참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Reference/policyreference.htm#Verbs\nResource-Type 사용할 자원을 지칭하여, 하나의 개별 자원 또는 동일한 자원 전체 등을 지칭할 수 있습니다.\n individual resource-type : 예, vcns, subnets, instances, volumes 등등 family resource-type : 예, virtual-network-family, instance-family, volume-family 등등 all-resources : Compartment 또는 Tenancy에 있는 모든 자원  참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Reference/policyreference.htm#Resource\nLocations 일반적으로 Comparment 또는 tenancy를 지정합니다.\n참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm#four\nConditions 추가적인 상세 조건을 지정할 수 있습니다.\n참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm#five\n계층적 Comparment Compartment가 여러 계층으로 이루어 진 경우, 부모 Compartment에 부여된 Policy는 자식 Compartment가 상속받게 됩니다.\nPolicy 예시 Tenancy 기준으로 특정 그룹에 사용자 관리 권한을 부여하여 Policy 예시\nAllow group HelpDesk to manage users in tenancy Compartment 기준으로 모든 자원을 관리하도록 관리자 그룹을 지정하는 Policy 예시\nAllow group A-Admins to manage all-resources in compartment Project-A Compartment 기준으로 특정 자원 패밀리에만 관리하도록 부분 관리자 그룹을 지정하는 Policy 예시\nAllow group A-Admins to manage instance-family in compartment Project-A Allow group A-Admins to manage volume-family in compartment Project-A Allow group A-Admins to use virtual-network-family in compartment Networks 사용할 만한 Policy 예시들 일반적으로 사용할 만한 Policy에 대해서 Common Policies로 문서에서 제공하고 있습니다. Policy 생성시 참고하시기 바랍니다.https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/commonpolicies.htm\n","lastmod":"2019-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/5/","tags":["policy"],"title":"6.5 Policy 개념 잡기"},{"categories":null,"contents":"1.5 Bare Metal, Virtual Machine OCI는 Bare Metal과 Virtual Machine 인스턴스를 모두 제공하고 있습니다.\n Bare Metal: 물리적인 서버를 단일 사용자에게 전용으로 사용할 수 있는 환경을 제공합니다. 물리적인 서버의 자원을 직접사용하고, 전용으로 사용하기 때문에 서버자원을 성능을 100% 활용하여 높은 성능을 제공합니다. Virtual Machine: 물리적인 서버 위의 가상화를 통해 컴퓨팅 환경을 제공합니다. Hypervisor에 의해 가상화된 레이어를 거치기 때문에 Bare Metal 보다는 상대적으로 낮은 성능을 제공합니다.  ","lastmod":"2019-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/5/","tags":["bare metal","virtual machine"],"title":"1.5 Bare Metal, Virtual Machine"},{"categories":null,"contents":"14.1.1.5 OCI CLI 설치 확인 oci os ns get 명령을 통해 Tenancy Name을 가져오는 지 확인하여 OCI CLI가 잘 동작하는 지 확인합니다.\n  다시 리눅스 호스트로 돌아가 oci os ns get을 실행하여 연결 확인\n[opc@bastion-host ~]$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; }   ","lastmod":"2018-12-24T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/5/","tags":["CLI"],"title":"14.1.1.5 OCI CLI 설치 확인"},{"categories":null,"contents":"7.6 CloudBerry Explorer를 통한 파일 업로드 Object Storage는 자체 API를 제공합니다. 또한 Amazon S3 호환 API를 제공합니다. OCI CLI를 통해 다량, 대용량 파일을 업로드가 가능합니다. 여기서는 사용 가능한 GUI 툴 중인 CloudBerry Explorer를 사용해 봅니다.\nCloudBerry Explorer를 통한 파일 업로드 Object Storage는 자체 API를 제공합니다. 또한 Amazon S3 호환 API를 제공하여, 기존 S3를 위한 클라이언트들을 그대로 사용할 수 있습니다. 대표적인 Cloud Berry Explorer 툴이 있으며, Freeware Edition 및 PRO 버전을 제공하고 있습니다.\n CloudBerry Explorer Freeware 다운로드 Explorer Free vs. Explorer PRO 기능 비교  Free 버전은 최대 5GB 파일 사이즈까지 지원합니다.    CloudBerry Explorer를 통한 파일 업로드 Step 1. Bucket 생성  Object Storage Bucket 을 생성합니다.  Name: 예) ExampleBucketForCloudBerry    Step 2. API 접근 사용자 생성 API 통한 접근이라 정확한 권한 체크를 위해 별도 유저, 그룹을 만들어 테스트 하였습니다\n  관리자로 OCI 콘솔에 로그인합니다.\n  Tool을 위해 그룹(ObjectStorageToolGroup) 및 API 접근 사용자(objectstoragetool)를 생성하고 해당 그룹에 사용자를 추가합니다.\n  Policy 설정\n 예, 이름: ObjectStorageToolPolicy  Allow group \u0026lt;group_name\u0026gt; to inspect buckets in compartment \u0026lt;compartment_name\u0026gt; Allow group \u0026lt;group_name\u0026gt; to manage objects in compartment \u0026lt;compartment_name\u0026gt;   Step 3. Customer Secret Key 생성 OCI Object Storage에 연결하기 위해서는 Customer Secret Key가 필요합니다. (초기에는 Amazon S3 Compatibility API key라고도 했습니다.)\n  해당 유저(objectstoragetool)로 로그인해서 생성하거나, 관리자 계정으로 해당 유저에 대해서 생성합니다.\n  OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026raquo; Identity \u0026raquo; Users 항목으로 이동합니다.\n  해당 유저(objectstoragetool)의 상세 페이지로 이동합니다.\n  왼쪽 아래 Resources \u0026raquo; Customer Secret Keys선택\n  Generate Secret Key 클릭\n   Key 이름을 입력하고 Generate Secret Key 클릭\n   생성된 키를 복사해 둡니다.\n키를 잊어버리면, 재확인 불가하며, 재생성하여야 합니다. 반드시 복사해 둡니다.\n   Access Key도 복사해 둡니다.\n   Step 4. CloudBerry Explorer 설치   CloudBerry Explorer Freeware를 내려받아 설치합니다.\n CloudBerry Explorer Freeware 다운로드 테스트 버전: 6.2.1.8    Tenancy 정보 확인\nOCI 콘솔 오른쪽 위의 사용자 프로필일 클릭하여 Tenancy 정보화면으로 이동합니다.\n   Tenancy 정보에서 Object Storage Namespace를 확인합니다.\n   CloudBerry Explorer Freeware를 실행합니다.\n  File \u0026gt; Add New Account 클릭\n   S3 Compatible 유형 선택\n   연결정보 설정\n  Display name : 원하는 이름을 입력\n  Service point : .compat.objectstorage.\u0026lt;리전 키\u0026gt;.oraclecloud.com 형식으로 입력\n 예, cnzdxxxxu9s8.compat.objectstorage.ap-seoul-1.oraclecloud.com    Access Key : 앞서 복사해둔 Customer Secret Key의 Access Key\n  Secret Key : Customer Secret Key 생성 후 복사해 둔 Secret Key\n  Signature version : \u0026ldquo;4\u0026rdquo; 선택\n r\u0026gt;\n    Test Connection을 클릭하여 연결 테스트\n   테스트가 완료되면, OK 클릭하여 Account 등록완료\n  연결 확인\n오른쪽 창의 Source을 생성한 대상 Account로 선택하면 아래와 같이 Bucket 리스트가 보입니다. 보이지 않는 경우 IAM Policy가 적용되었는지 다시 확인합니다.\n   Step 5. CloudBerry Explorer을 사용하여 파일 업로드   우측 화면에서 올릴 대상 Bucket을 클릭합니다.\n   좌측 화면의 로컬 디스크에서 올릴 파일을 선택하고, Copy 메뉴를 클릭합니다.\n   업로드 내용을 확인하고 Yes 클릭\n   업로드가 완료되었습니다.\n   Step 6. Object Storage 확인   OCI 콘솔로 돌아가 테스트 중인 Object Storage Bucket으로 이동합니다.\n  OCI Console에서 Bucket에 파일이 올라간 것을 확인할 수 있습니다.\n   ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/6/","tags":["object storage","cloudberry","upload"],"title":"7.6 CloudBerry Explorer를 통한 파일 업로드"},{"categories":null,"contents":"3.6.6 Block Volume 장착 해제하기 언마운트 하기   Compute Instance에 SSH로 접속합니다.\n  /etc/fstab 업데이트sudo vi /etc/fstab 명령을 수행하여 fstab 파일에서 언마운트할 Volume인 /mnt/vol1을 삭제합니다.\nsudo vi /etc/fstab   언마운트\nsudo umount /mnt/vol1   Volume에 연결 끊기   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026raquo; Instances 항목으로 이동합니다.\n  앞서 생성한 대상 Instance의 이름을 클릭합니다.\n  Instance 상세 페이지에서 왼쪽 아래의 Resources \u0026raquo; Attached block volumes을 클릭합니다.\n  방금 장착한 Block Volume 옆에 있는 액션 아이콘(우측 점 3개)을 클릭한 다음 iSCSI commands and information을 클릭합니다.\n   iSCSI Commands and Information 다이얼로그가 뜹니다.\n   Disconnect COMMANDS의 명령을 복사해서 Instance에 접속한 SSH 세션에서 순서대로 하나씩 실행합니다.\nubuntu@NOTEBOOK-WORK:~/.ssh$ ssh -i privateKey opc@146.56.171.xx Last login: Mon Jan 10 07:24:34 2022 from 223.62.21.xx [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec -p 169.254.2.3:3260 -u Logging out of session [sid: 1, target: iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec, portal: 169.254.2.3,3260] Logout of [sid: 1, target: iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec, portal: 169.254.2.3,3260] successful. [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -o delete -T iqn.2015-12.com.oracleiaas:2626c782-30e1-43c2-847a-2504080445ec -p 169.254.2.3:3260   장착해제 여부를 확인하기 위해 fdisk -l 명령으로 확인합니다.\n/dev/sdb 디스크가 없어진 것을 확인할 수 있습니다.\n[opc@examplelinuxinstance ~]$ sudo fdisk -l WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion. Disk /dev/sda: 50.0 GB, 50010783744 bytes, 97677312 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disk label type: gpt Disk identifier: BBBA97C2-4E47-43B0-9059-2702FDB720E7 # Start End Size Type Name 1 2048 411647 200M EFI System EFI System Partition 2 411648 17188863 8G Linux swap 3 17188864 97675263 38.4G Microsoft basic   Volume 장착 해제   OCI 콘솔에서 장착 해제할 Block Volume 옆에 있는 액션 아이콘을 클릭한 다음 Detach를 클릭합니다.\n   경고문을 확인한 후 Detach volume을 클릭합니다. 우리는 /etc/fstab 정리, unmount, iSCSI Disconnect command를 앞서 모두 실행했습니다.\n   DETACHING 상태로 보입니다. 완료되면 Attached Block Volumes 목록에서 사라지게 됩니다.\n  ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/6/","tags":["block volume"],"title":"3.6.6 Block Volume 장착 해제하기"},{"categories":null,"contents":"1.6 클라우드 서비스 제공업체(Cloud Service Provider) 간 제공 서비스 매핑 IaaS 기준으로 Oracle Cloud Infrastructure에서 제공하는 클라우드 서비스를 이해를 돕고자 타사 Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform의 서비스 이름과 매핑하면 Oracle Cloud 사이트 기준으로 다음과 같습니다.\n Cloud Service Mapping 공식 사이트   클라우드 서비스 매핑    Category Oracle Cloud Infrastructure AWS Azure Google     Regions Availability Domain (AD) Availability Zone (AZ) Availability Zone (AZ) Zones   Networking \u0026amp; Content Delivery Oracle Cloud Infrastructure Virtual Cloud Network (VCN) Amazon Virtual Private Cloud (VPC) Virtual Network Virtual Private Cloud   Networking \u0026amp; Content Delivery Oracle Cloud Infrastructure Load Balancing Elastic Load Balancing Load Balancer ; Application Gateway Cloud Load Balancing   Networking \u0026amp; Content Delivery Oracle Cloud Infrastructure FastConnect AWS Direct Connect Azure ExpressRoute Cloud Interconnect   Compute Oracle Cloud Infrastructure Compute - Virtual Machines Amazon EC2 Azure Virtual Machines Compute Engine   Compute Oracle Cloud Infrastructure Compute - Bare Metal Amazon EC2 - I3.metal     Compute Oracle Cloud Infrastructure Compute - GPU Amazon EC2 - P2, P3, G3 instances Azure N-Series Google GPU   Compute Oracle Cloud Infrastructure Container Engine for Kubernetes Amazon Elastic Compute Cloud Container Service Azure Kubernetes Service Kubernetes Engine   Compute Oracle Cloud Infrastructure Container Registry EC2 Container Registry Azure Container Registry Container Registry   Compute Oracle Functions AWS Lambda Azure Functions ; Azure Event Grid Cloud Functions   Storage Oracle Cloud Infrastructure Block Volumes Amazon Elastic Block Store (EBS) Azure Storage Disk Compute Engine Persistent Disks   Storage Oracle Cloud Infrastructure File Storage Amazon Elastic File System Azure Files ZFS / Avere ; Cloud Filestore (beta)   Storage Oracle Cloud Infrastructure Object Storage Amazon Simple Storage Service (S3) Azure Storage Cloud Storage ; Cloud Storage Nearline   Storage Oracle Cloud Infrastructure Archive Storage Amazon Glacier Azure Storage - Standard Cloud Storage Coldline   Storage Oracle Cloud Infrastructure Data Transfer Service Import/Export Disk ; Import/Export Snowball; Import/Export Snowmobile Import/Export ; Azure Data Box Transfer Appliance ; Transfer Service   Storage Oracle Cloud Infrastructure Storage Gateway Storage Gateway StorSimple ZFS / Avere   Security, Identity \u0026amp; Compliance Oracle Cloud Infrastructure Identity and Access Management AWS Identity and Access Management (IAM) ; AWS Organizations Azure Active Directory ; Azure Subscription and Service Management + Azure RBAC Cloud Identity \u0026amp; Access Management   Security, Identity \u0026amp; Compliance Oracle Cloud Infrastructure Key Management Service Key Management Service Key Vault Cloud Key Management Service   Security, Identity \u0026amp; Compliance Oracle Cloud Infrastructure Audit AWS CloudTrail Activity Logging Cloud Audit Logging   Database Oracle Database Cloud Service Amazon Relational Database Service (RDS) Azure SQL Database Cloud SQL (MySQL, Postgres); Cloud Spanner   Database Oracle Autonomous Transaction Processing Amazon Aurora Azure SQL Database; Azure Cosmos DB Cloud SQL ; Cloud Spanner   Database Oracle Autonomous Data Warehouse Amazon Redshift Azure SQL Data Warehouse BigQuery   Database MySQL Database Service Amazon RDS, Aurora Azure MySQL Google CloudSQL   Database MySQL with HeatWave Amazon RedShift, RDS, Aurora Azure Synapse Google BigQuery   Database Oracle NoSQL Amazon DynamoDB Azure Cosmos DB Cloud Datastore, Cloud Bigtable   Edge Oracle Cloud Infrastructure DNS Amazon Route53 Azure DNS ; Azure Traffic Manager Cloud DNS   Edge Oracle Cloud Infrastructure Email Delivery Amazon SES Marketplace - Email Partners   Edge Oracle Cloud Infrastructure Web Application Firewall Web Application Firewall Application Gateway Web Application Firewall Partners   Edge Oracle Cloud Infrastructure DDoS Protection AWS Shield Azure DDoS Protection Service Cloud Armor    ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/6/","tags":["service mapping"],"title":"1.6 클라우드 서비스 제공업체(CSP) 간 제공 서비스 매핑"},{"categories":null,"contents":"14.2.6 예시, list, count를 이용한 유저 만들기 자원을 만들다 보면 Array를 통해 동일한 자원을 여러개 만드는 경우가 있습니다. 실습환경을 구성하기위해 여러 유저를 만든다는 상황을 고려해 list, count를 통해 여러 유저를 한꺼번에 만드는 방법을 알아 보겠습니다.\nUser 생성을 위해 필요한 Policy Allow group \u0026lt;group_name\u0026gt; to manage users in tenancy User 만들기 OCI 유저는 oci_identity_user 구문을 사용하여 만들 수 있습니다.\n  예시\nresource \u0026#34;oci_identity_user\u0026#34; \u0026#34;test_user\u0026#34; { #Required compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;${var.user_description}\u0026#34; name = \u0026#34;${var.user_name}\u0026#34; #Optional defined_tags = {\u0026#34;Operations.CostCenter\u0026#34;= \u0026#34;42\u0026#34;} email = \u0026#34;${var.user_email}\u0026#34; freeform_tags = {\u0026#34;Department\u0026#34;= \u0026#34;Finance\u0026#34;} }   count를 사용해 여러 유저 만들기 기본적으로 하나의 resource 블럭은 하나의 자원에 해당됩니다. 여기에 count를 추가하게 되면 동일한 유형을 여러개 만들 수 있습니다. 일반적으로 루프 프로그래밍에서 처럼 count로 반복 횟수를 설정하고, count.index로 현재 루프값을 조회하여 사용합니다. 그래서 아래와 같이 count에 만들 자원 수를 지정하고, count.index를 통해 자원이름 등이 충돌하지 않게 사용할 수 있습니다.\n  testuser_01, testuser_02 처럼 원하는 count 수 만큼 유저를 만드는 예제\nresource \u0026#34;oci_identity_user\u0026#34; \u0026#34;test_user\u0026#34; { count = 2 #Required compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; name = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; }   users.tf 전체 내용 예시\n provider.tf, terraform.tfvars는 앞선 실습 파일을 그대로 사용  ### Create Users resource \u0026#34;oci_identity_user\u0026#34; \u0026#34;users_1\u0026#34; { count = 2 compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; name = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; } ### Set User Passwords ### This is one time password. resource \u0026#34;oci_identity_ui_password\u0026#34; \u0026#34;users_1_password\u0026#34; { count = 2 user_id = \u0026#34;${oci_identity_user.users_1.*.id[count.index]}\u0026#34; } ### Outputs output \u0026#34;my_users_1_password\u0026#34; { sensitive = false value = \u0026#34;${concat(oci_identity_user.users_1.*.name, oci_identity_ui_password.users_1_password.*.password)}\u0026#34; }   실행결과 output과 OCI 유저가 지정한 count 갯수 만큼 생성된 것을 볼 수 있습니다.\n[opc@bastion-host example_user_count]$ terraform apply ... Apply complete! Resources: 4 added, 0 changed, 0 destroyed. Outputs: my_users_1_password = [ \u0026#34;testuser_01\u0026#34;, \u0026#34;testuser_02\u0026#34;, \u0026#34;:\u0026lt;7:qa.Z[XqvKa19uL$u\u0026#34;, \u0026#34;s.;sl72P_.v]Eu]V{rxu\u0026#34;, ] [opc@bastion-host example_user_count]$    list, count를 사용해 여러 유저 만들기 앞선 예제와 달리 유저의 이름을 각각 전혀 다른 이름으로 만들고자 할때는 list 타입 배열변수와 count를 조합하여 다른 이름의 여러 유저를 만들 수 있습니다.\n  다른 이름의 여러 유저를 만드는 예제\nlist 타입의 변수를 만들고 \u0026quot;${element(LIST, INDEX)}\u0026quot;, **\u0026quot;${length(LIST)}\u0026quot;**를 사용하여, 특정 데이터 및 list 길이를 조회할 수 있습니다.\nvariable \u0026#34;user_names\u0026#34; { type = \u0026#34;list\u0026#34; default = [\u0026#34;oracle\u0026#34;, \u0026#34;neo\u0026#34;] } resource \u0026#34;oci_identity_user\u0026#34; \u0026#34;my_users_2\u0026#34; { count = \u0026#34;${length(var.user_names)}\u0026#34; compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;${element(var.user_names, count.index)}\u0026#34; name = \u0026#34;${element(var.user_names, count.index)}\u0026#34; }   users.tf 전체 내용 예시\n provider.tf, terraform.tfvars는 앞선 실습 파일을 그대로 사용  variable \u0026#34;user_names\u0026#34; { type = list default = [\u0026#34;oracle\u0026#34;, \u0026#34;neo\u0026#34;] } ### Create Users resource \u0026#34;oci_identity_user\u0026#34; \u0026#34;my_users_2\u0026#34; { count = \u0026#34;${length(var.user_names)}\u0026#34; compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;${element(var.user_names, count.index)}\u0026#34; name = \u0026#34;${element(var.user_names, count.index)}\u0026#34; } ### Set User Passwords ### This is one time password. resource \u0026#34;oci_identity_ui_password\u0026#34; \u0026#34;my_users_2_password\u0026#34; { count = \u0026#34;${length(var.user_names)}\u0026#34; user_id = \u0026#34;${oci_identity_user.my_users_2.*.id[count.index]}\u0026#34; } ### Outputs output \u0026#34;my_users_2_password\u0026#34; { sensitive = false value = \u0026#34;${concat(oci_identity_user.my_users_2.*.name, oci_identity_ui_password.my_users_2_password.*.password)}\u0026#34; }   실행결과 output과 OCI 유저가 리스트상의 이름대로 생성된 것을 볼 수 있습니다.\n[opc@bastion-host example_user_list]$ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create ... Apply complete! Resources: 4 added, 0 changed, 0 destroyed. Outputs: my_users_2_password = [ \u0026#34;oracle\u0026#34;, \u0026#34;neo\u0026#34;, \u0026#34;W$tC!bS6k\u0026amp;h;mH8R#}9u\u0026#34;, \u0026#34;(I;hZJ-uZq+.LkXGl5#Z\u0026#34;, ] [opc@bastion-host example_user_list]$    ","lastmod":"2019-04-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/6/","tags":["terraform","oci user"],"title":"14.2.6 예시, list, count를 이용한 유저 만들기"},{"categories":null,"contents":"10.6 Load Balancer 연결 확인   OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking » Load Balancers 항목으로 이동합니다.\n  앞서 생성한 Load Balancer의 Public IP를 확인합니다.\n   브라우저를 통해 LB의 Public IP로 접속합니다.\n   브라우저를 리프레쉬합니다.\n   LB의 분배규칙을 Round Robin으로 했기 때문에 순서대로 하나씩 가는 것을 알 수 있습니다.\n  ","lastmod":"2019-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/6/","tags":["load balancer"],"title":"10.6 Load Balancer 연결 확인"},{"categories":null,"contents":"7.7 수명주기를 통한 Archive 설정하기 Object Storage의 사용 사례의 하나로 디스크 이미지 백업, 파일 백업 등의 용도로 많이 사용합니다. 최근 백업을 사용할 가능성이 높지만, 그 외에 백업을 사용할 가능성을 낮아지지만, 보관 기간 등이 장기간 보관해야 하는 경우가 많습니다. 보관 기간 동안 Storage 비용을 조금이라도 줄이고 싶은 요구 사항이 있습니다.\nObject Storage의 Bucket에는 저장된 객체의 수명주기를 관리하는 기능이 있습니다. 객체 생성 후 일정한 기간이 지났을 때 삭제하거나, 좀 더 저렴한 Archive Storage로 이동시킬 수 있는 기능을 제공합니다.\n수명주기를 통한 Archive 설정하기 Step 1. Object Archive 권한 부여   OCI 전체 관리자로 OCI 콘솔에 로그인합니다.\n  대상 그룹(SandboxGroup)에 Object 관련 Policy 권한 추가\nObject Archive를 하기 위해서는 추가 권한이 필요합니다. Object 패밀리 관리 권한을 추가합니다.\nallow service objectstorage-\u0026lt;region_name\u0026gt; to manage object-family in compartment \u0026lt;compartment_name\u0026gt;   예시: 이름, ObjectStorageLifecyclePolicy\nallow service objectstorage-ap-seoul-1 to manage object-family in compartment Sandbox     Step 2. 수명 주기 설정하기   Sandbox 그룹 사용자로 OCI 콘솔에 로그인합니다.\n  수명 주기를 설정할 Bucket(예, ExampleBucket)의 상세 페이지로 이동합니다.\n  왼쪽 아래의 Resources \u0026raquo; Lifecycle Policy Rules를 클릭합니다.\n  새로운 규칙을 생성하기 위해 Create Rule을 클릭합니다.\n   수명 주기 규칙 생성\n  Name: 생성할 규칙의 이름을 입력합니다.\n  Lifecyle Action:\n Move to Archive, Move to Infrequent Access, Delete 중 원하는 것을 선택    Number of Days: 생성 후 며칠 지난 객체 적용할 지 기간 설정\n  Object Name Filters: Bucket 상의 대상 객체에 대한 조건을 지정하는 영역입니다. 현재는 아무 설정하지 않습니다.\n images/oracle.png 처럼 객체 이름에서 슬래쉬 기준 \u0026lsquo;images/\u0026lsquo;이 Prefix입니다. Prefix를 통해 대상 객체에 대한 조건을 지정할 수 있습니다.       설정 후 Create 클릭합니다.\n  수명 주기 규칙이 설정되었습니다.   Step 3. Archived 결과 확인하기 결과 내일 다시 확인\u0026hellip;\u0026hellip;\n  설정한 규칙이 적용되어 그림과 같이 Archived 상태가 된 것을 확인할 수 있습니다.   Archived 상태에서는 다운로드가 불가하여 Public Bucket에서 객체에 URL로 접근해도 다음과 같이 오류가 발생합니다.   Step 4. Restoring   Archived 상태에서 다시 다운로드 받기 위해서는 대상 객체를 Restore 하면 됩니다.   다운로드 가능한 시간은 미입력시 기본적으로 24시간동안이며, 변경코자 하면 1시간에서 240시간 범위내에서 지정할 수 있습니다. 복구후 지정한 시간동안(기본 24시간)만 다운로드 가능하고 그 이후는 다시 Archive 상태가 됩니다.   Restore하면 아래와 같이 상태가 표시되면, 완료될때 까지 시간이 좀 걸립니다. 한 번 밖에 테스트 해보진 않았지만, 걸린 시간은 다음과 같습니다.\n  요청 시각: Jan 14, 2022 12:48:19 AM, GMT\n  완료 시각: 05 08, 2019 11:20, GMT (추정)\n  걸리 시간: 약 3시간      복구가 완료되면 이제 다운로드 받을 수 있으며, Public Bucket인 경우 객체에 URL로 접근도 가능하게 됩니다.   오브젝트의 상세 정보를 보면, 설정한 시간 기준으로 다운로드 가능 남은 시간이 그림과 같이 보입니다.   ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/7/","tags":["object storage","lifecycle","archive"],"title":"7.7 수명주기를 통한 Archive 설정하기"},{"categories":null,"contents":"1.7 Kubernetes 지원 버전 및 업그레이드 Kubernetes 지원 버전 Kubernetes 버전은 x.y.z로 표현되며, 각각 x는 메이저, y는 마이너, z는 패치 버전을 뜻하며, 오픈소스 Kubernetes도 현재 버전과 그 이전 2개 버전까지를 지원하고 있습니다. OKE 또한 OKE가 지원하는 최신버전 기준, 그 이전 2개의 마이너 버전까지 지원하고 있습니다. 금일자 기준 지원하는 버전은 다음과 같습니다.\n 1.20.11 1.19.15 1.18.10  신규 버전 출시 후에 30일 동안만 그 이전 버전을 지원하고 있습니다. 예를 들어 OKE에서 1.20.11, 1.19.15가 2021년 10월 8일에 출시되어 기존 버전인 1.20.8, 1.19.12는 각각 30일후인 2021년 11월 7일까지만 지원합니다. 현재 지원 버전은 다음 링크를 참조합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm  버전 관리 단위 OKE 클러스터는 Control Plane과 Worker Node로 구성되며, Control Plane의 버전이 정해지면, Worker Node는 Control Plane과 같거나 그 이하 버전을 사용할 수 있습니다. 물론 호환되는 내에서 말입니다.\nWorker Node는 Node Pool 단위로 버전을 가질 수 있습니다. 여러 Node Pool을 만들어 각각 다른 버전을 가질 수 있습니다.\n버전 업그레이드 OKE 새 버전이 출시되면 버전 업그레이드는 다음 절차를 따릅니다.\n Control Plane 업그레이드  OCI 서비스 콘솔에서 OKE 클러스터 단위를 업그레이드하면 Control Plane만 업그레이드됨 오라클이 관리하는 영역으로 다운타임 없이 자동으로 업그레이드 됨 OKE 클러스터를 업그레이드 하면, 즉 Control Plane이 업그레이드하면 이전 버전으로 다시 다운그레이드 할 수 없음   Worker Node 업그레이드  OCI 서비스 콘솔에서 Node Pool 단위로 업그레이드 업그레이드 방식  in-place 업그레이드  대상이 되는 기존 Node Pool은 OCI 콘솔에서 버전 업그레이드, 실제 Node가 자동으로 업그레이드 되지 않음 kubectl drain 명령으로 특정 노드에 신규 컨테이너가 생기는 것을 방지함 OCI 서비스 콘솔에서 drain한 Node를 종료(Terminate) 시킴 Node 자가치유에 의해 신규 Node가 자동으로 생성되며, 생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전 기존 노드에 대해서 순서대로 모두 진행   out-of-place 업그레이드  신규 버전의 Node Pool 추가 생성 kubectl 명령으로 기존 Node Pool에 있는 Node 제거, Kubernetes에 의해 컨테이너가 모두 이동하면, Node 삭제하는 방식으로 하나씩 진행 기존 Node가 모두 제거되면, 기존 Node Pool 삭제        Node Pool 단위 버전 혼합 테스트를 위해 1.18.10, 1.19.12, 1.19.15, ,1.20.8, 1.20.11 버전이 사용 가능한 상태에서 1.19.12 버전의 OKE 클러스터를 기준으로 테스트를 진행합니다.\n 테스트 환경  OKE 클러스터 - oke-cluster-1 버전: 1.19.12 Control Plane 버전: 1.19.12 Node Pool - poo1 버전: 1.19.12      OKE 클러스터 상세화면으로 이동합니다.\n  왼쪽 아래 Resources \u0026gt; Node Pools 로 이동합니다.\n   현재 pool1이 1.19.12 버전이 있습니다. 추가 Node Pool 생성을 위해 Add Node Pool을 클릭합니다.\n  새 Node Pool 생성을 위한 정보를 입력합니다.\n Name: 새 Node Pool 이름 Version: 일반 버전 혼합을 확인하기 위해 1.18.10 버전은 선택합니다.  사용 가능한 버전을 보면, 현재 OKE 클러스터 버전 이하만 선택 가능한 걸 알 수 있습니다. 현재 OKE 클러스터 생성시 사용 가능한 1.19.15, ,1.20.8, 1.20.11 버전은 보이지 않습니다.      처음 생성시와 비슷하게 생성시 필요한 정보를 입력합니다. 다음은 예시입니다.  Shape: VM.Standard.E3.Flex Placement Configuration: Worker Node가 위치한 AD와 Node용 서브넷 지정 Advanced Options:  Add an SSH Key: Node에 SSH로 접근하기 위한 Publich Key        생성을 요청하면 실제 Node VM이 만들어지고, 준비되는 데 까지 앞서 설치시와 같이 약간의 시간이 걸립니다.\n  Node Pool을 추가 생성하면 그림과 같이 동일 OKE 클러스터에 두 가지 버전의 혼합을 지원하여, 앞서 Node Pool 추가시 본것 처럼 Pool 단위 VM 크기, 위치(AD, 서브넷)을 달리 할 수 있습니다.\n   kubectl로 노드를 조회해도 동일한 결과가 나옵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 23h v1.19.12 10.0.10.226 Ready node 23h v1.19.12 10.0.10.234 Ready node 6m53s v1.18.10 10.0.10.43 Ready node 6m30s v1.18.10 10.0.10.44 Ready node 23h v1.19.12 10.0.10.78 Ready node 6m41s v1.18.10   OKE 클러스터 버전 업그레이드 Control Plane 업그레이드 위와 같이 1.19.12 버전을 사용 중에 새로운 버전이 출시되었다고 가정합니다. 그러면 앞서 설명한 것과 같이 기술지원 정책에 따라 기존 버전은 30일간 지원하기 때문에, 그동안 버전 검증후 업그레이드가 필요합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm    업그레이드가 가능하면, OKE 클러스터 상세 화면에서 Upgrade Available 버튼이 활성화 됩니다.\n   Upgrade Available 버튼을 클릭하면 다음과 같이 안내 문구와 함께 업그레이드를 시작할 수 있습니다. 최신 버전인 v1.20.11을 선택하도록 하겠습니다.\n   버전을 선택하고 아래 Upgrade 버튼을 클릭하여 업그레이드를 시작합니다.\n  클러스터 상태가 UPDATING으로 표시되고 업그레이드가 진행됩니다. 오라클이 관리하는 Control Plane이 내부적으로 순차적으로 업그레이드가 발생합니다. 실제 애플리케이션이 동작하는 Worker Node는 서비스 중지 없이 업그레이드 됩니다.\n  테스트 시점에는 10~15분 후에 업그레이드 완료되었습니다.\n   Worker Node 업그레이드 - in-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. in-place 업그레이드 방식은 Node Pool 업그레이드 -\u0026gt; 기존 Node Drain \u0026gt; POD 이동 확인 -\u0026gt; 기존 Node VM 종료 순으로 업그레이드 합니다.\nNode Pool 업그레이드   업그레이드 하려는 Node Pool의 상세 페이지로 이동합니다.\n  수정을 위해 Edit를 클릭하면, 오른쪽에 수정 페이지가 뜹니다.\nVersion 항목에, 클러스터 버전과 Node Pool의 버전이 표시되며, 업그레이드 가능한 버전이 표시됩니다.\n   클러스터와 동일한 1.20.11로 선택하고 Save Change를 클릭하여 저장합니다.\n  Resources \u0026gt; Work Requests에 가서 보면, 2~3초 뒤에 Node Pool 업그레이드가 완료됩니다.\n   아직 실제 Worker Node가 업그레이드 된 것은 아닙니다.\nResources \u0026gt; Nodes에 가서 보면 기존 버전 그대로입니다.\n   Node Drain 시키기   kubectl 명령으로 Worker Node와 배포된 POD를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 24h v1.19.12 10.0.10.226 Ready node 24h v1.19.12 10.0.10.44 Ready node 24h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-b46nd 1/1 Running 0 72m 10.244.1.4 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 39m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-7tsjf 1/1 Running 0 39m 10.244.1.8 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 40m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets --delete-emptydir-data  명령으로 하나의 노드를 스케줄에서 제외시킵니다.\n emptydir을 사용하는 Pod가 있는 경우 drain시 에러가 발생합니다. emptydir을 임시데이터를 사용하는 용도임으로 \u0026ndash;delete-emptydir-data 옵션을 통해 drain 시킵니다. 테스트시는 elasticsearch가 설치된 상태에서 관련 이슈가 발생하였습니다.\n참고: https://pet2cattle.com/2021/08/cannot-delete-pods-with-local-storage\n oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl drain 10.0.10.44 --ignore-daemonsets --delete-emptydir-data node/10.0.10.44 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-7hc28, kube-system/kube-flannel-ds-b4dp6, kube-system/kube-proxy-f8qmp, kube-system/proxymux-client-6qs4x evicting pod default/nginx-fss-pvc-9fb98454f-7tsjf evicting pod default/nginx-bvs-pvc-7b574c9c5c-b46nd pod/nginx-bvs-pvc-7b574c9c5c-b46nd evicted pod/nginx-fss-pvc-9fb98454f-7tsjf evicted node/10.0.10.44 evicted   아래와 같이 44번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 25h v1.19.12 10.0.10.226 Ready node 25h v1.19.12 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-9smhh 1/1 Running 0 3m20s 10.244.0.138 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 45m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-8zkn2 1/1 Running 0 3m20s 10.244.0.6 10.0.10.185 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 46m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   종료할 Node의 노드 이름을 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -L displayName NAME STATUS ROLES AGE VERSION DISPLAYNAME 10.0.10.185 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-0 10.0.10.226 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-1 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-2   Node Pool 상세 화면에서 종료할 노드 이름을 클릭하여 Compute 인스턴스로 이동합니다.\n   Node에 해당되는 Compute 인스턴스를 종료합니다. boot volume도 같이 종료합니다.\n    Node 자가치유에 의해 신규 Node가 자동으로 생성됩니다. Work Requests를 보면 아래와 같이 재조정(NODEPOLL_RECONCILE) 작업이 발생되어 지정된 갯수에 맞게 노드가 다시 생성됩니다.\n   생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전으로 생성됩니다.\n  kubectl 명령으로 노드를 조회하면, 1.20.2 버전으로 신규 노드가 생성되었습니다. docker 런타임의 deprecate 예정으로 인해 참고로 1.20 부터는 컨테이너 런타임이 cri-o 변경되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.170 Ready node 7m25s v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.185 Ready node 25h v1.19.12 10.0.10.185 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11 10.0.10.226 Ready node 25h v1.19.12 10.0.10.226 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11   기존 노드에 대해서 순서대로 모두 진행합니다.\n  완료 결과\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.128 Ready node 6m10s v1.20.11 10.0.10.128 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.146 Ready node 35s v1.20.11 10.0.10.146 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.170 Ready node 23m v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   Worker Node 업그레이드 - out-of-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. out-of-place 업그레이드 방식은 업그레이드 버전의 Node Pool 신규 생성 -\u0026gt; 기존 Node Pool의 모든 노드 Drain -\u0026gt; 기존 Node Pool 삭제 순으로 업그레이드 합니다.\n새 버전의 Node Pool 만들기   OKE 클러스터 상세 페이지로 이동합니다.\n  Resources \u0026gt; Node Pools 로 이동합니다.\n  그림과 같이 기존 버전의 Node Pool이 있는 상태에서 신규 Node Pool 추가를 위해 Add Node Pool을 클릭합니다.\n   신규 Node Pool 정보를 입력하여 생성합니다.\n Name Version: 새 버전 선택 Shape: Node VM 유형 Number of nodes: 노드 수 Placement Configuration  Node가 위치할 AD, Subnet   Add an SSH key: Node VM에 SSH 접속시 사용할 키의 Private Key     추가 된 Node Pool을 OCI 서비스 콘솔 확인할 수 있습니다.\n  기존 Node Pool의 모든 노드 Drain   구동 중인 앱들이 기존 Node Pool에서 동작하고 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 18m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-2jbp8 1/1 Running 0 18m 10.244.0.5 10.0.10.242 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 19m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 18m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready node 71m v1.19.12 pool1 10.0.10.29 Ready node 71m v1.19.12 pool1 10.0.10.61 Ready node 71m v1.19.12 pool1 10.0.10.12 Ready node 4m25s v1.20.11 pool2 10.0.10.126 Ready node 4m48s v1.20.11 pool2 10.0.10.191 Ready node 4m42s v1.20.11 pool2   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets --delete-emptydir-data 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl drain 10.0.10.242 --ignore-daemonsets --delete-emptydir-data node/10.0.10.242 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-l62xw, kube-system/kube-flannel-ds-7dv8l, kube-system/kube-proxy-zv4ks, kube-system/proxymux-client-24nmv evicting pod kube-system/coredns-c5d4bf466-wv8fb evicting pod default/nginx-fss-pvc-9fb98454f-2jbp8 evicting pod kube-system/coredns-c5d4bf466-h5rrm pod/coredns-c5d4bf466-wv8fb evicted pod/nginx-fss-pvc-9fb98454f-2jbp8 evicted pod/coredns-c5d4bf466-h5rrm evicted node/10.0.10.242 evicted   아래와 같이 242번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 76m v1.19.12 pool1 10.0.10.29 Ready node 76m v1.19.12 pool1 10.0.10.61 Ready node 76m v1.19.12 pool1 10.0.10.12 Ready node 9m15s v1.20.11 pool2 10.0.10.126 Ready node 9m38s v1.20.11 pool2 10.0.10.191 Ready node 9m32s v1.20.11 pool2 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 22m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5k8wg 1/1 Running 0 57s 10.244.3.2 10.0.10.126 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 23m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 22m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   나머지 기존 Node Pool에 있는 Node들도 drain합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.29 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.61 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.12 Ready node 11m v1.20.11 pool2 10.0.10.126 Ready node 12m v1.20.11 pool2 10.0.10.191 Ready node 12m v1.20.11 pool2   기존 Node Pool 삭제   기존 Node Pool에 있는 모든 Node들이 drain되어 더이상 사용되지 않습니다.\n  OCI 서비스 콘솔에서 OKE 클러스터 상세페이지로 이동합니다.\n  Resources \u0026gt; Node Pools로 이동하여 기존 Node Pool을 삭제합니다.\n   업그레이드가 완료되었습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.12 Ready node 18m v1.20.11 pool2 10.0.10.126 Ready node 19m v1.20.11 pool2 10.0.10.191 Ready node 19m v1.20.11 pool2   ","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/7.supported-version-and-upgrade/","tags":["oke"],"title":"1.7 Kubernetes 버전 업그레이드"},{"categories":null,"contents":"14.2.7 OCI Terraform 사용 예제\u0026quot; 오라클에서 OCI Github에서 다양한 예제를 제공하고 있습니다. 다음 사이트를 참고하세요.\n terraform-provider-oci/docs/examples  ","lastmod":"2019-04-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/7/","tags":["terraform","example"],"title":"14.2.7 OCI Terraform 사용 예제"},{"categories":null,"contents":"3.6.7 Block Volume 삭제하기  주의사항 Block Volume을 Terminate 하면 완전히 삭제됩니다. 복구할 수 없으니 이점 유의합니다.     OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026raquo; Block Storage \u0026raquo; Block Volumes 항목으로 이동합니다.\n  Block Volume 옆에 있는 액션 아이콘을 클릭한 다음 Terminate를 클릭합니다.\n   재확인\nTerminate되면 복구할 수 없습니다. 그래도 삭제하려면 Terminate를 클릭합니다.\n   Terminate 완료\n   ","lastmod":null,"permalink":"https://thekoguryo.github.io/oci/chapter03/6/7/","tags":null,"title":"3.6.7 Block Volume 삭제하기"},{"categories":null,"contents":"1.8 애플리케이션 로그 모니터링 OKE 상에 배포되어 있는 애플리케이션 로그 모니터링을 OCI Logging 서비스를 통해 모니터링할 수 있습니다.\nOCI Logging 서비스 사용 권한 설정 Worker Node에 대한 Dynamic Group 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\n  OKE 클러스터가 있는 Compartment의 OCID를 확인합니다.\n   좌측 Dynamic Group 메뉴로 이동하여 아래 규칙을 가진 Dynamic Group을 만듭니다.\ninstance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39;    Dynamic Group에 대한 OCI Logging 서비스 권한 부여하기   좌측 Policy 메뉴로 이동하여 아래 규칙을 가진 Policy을 만듭니다. 방금 생성한 Dynamic Group에 대한 Policy를 만듭니다.\nallow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use log-content in compartment \u0026lt;compartment-name\u0026gt;    컨테이너를 위한 Custom Log 설정하기 Log Group 만들기 Log Group은 로그들을 관리하는 말 그대로 로그의 묶음 단위 입니다. 커스텀 로그를 만들기 위해 먼저 만듭니다.\n  좌측 상단 햄버거 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging \u0026gt; Log Groups로 이동합니다.\n  Create Log Group을 클릭하여 로그 그룹을 만듭니다.\n   Custom Log 만들기 Custom Log는 커스텀 애플리케이션에서 수집하는 로그에 매핑되는 것입니다. Custom Log를 정의하고, 이에 대한 로그 수집기를 정의합니다.\n  Resources \u0026gt; Logs 메뉴로 이동하여 Create custom log를 클릭합니다.\n  로그 이름과 보관 주기 등을 설정하여 custom log를 만듭니다.\n   이해를 돕고자 Agent는 별도로 설정합니다. 여기서는 일단 Add configuration later 선택\n   Agent Configuration 설정 Agent Configuration는 로그를 수집하는 agent를 설정하는 부분입니다.\n  Logging \u0026gt; Agent Configurations 메뉴로 이동하여 Create agent log를 클릭합니다.\n  Agent 이름 및 대상 Host Group을 앞서 만든 Dynamic Group으로 지정합니다.\n   Agent 설정 부분에서 로그가 위치한 경로 및 수집된 로그의 전달 위치를 지정합니다.\n  log input: /var/log/containers/*.log\n앞서 지정한 Dynamic Group상에 있는 VM, 여기서는 OKE 클러스터 Worker Node VM 상에 수집할 로그의 위치를 지정합니다.입력하고 엔터키를 꼭 칩니다.\n  log destination: 수집한 로그를 전달한 앞서 생성한 custom log 이름을 지정합니다.\n     참고: Worker Node VM상에 컨테이너 로그 위치   Worker Node VM에 SSH로 접속이 가능한 환경, 예, bastion host에서 Worker Node에 접속해 보면 컨테이너 로그 위치는 다음과 같습니다.\n[opc@bastion-host ~]$ ssh opc@10.0.10.175 Last login: Tue Nov 16 06:43:43 2021 from bastion-host.suba22926d1b.okecluster1.oraclevcn.com [opc@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 ~]$ sudo su [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 opc]# cd /var/log/containers/ [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# ls -la total 16 drwxr-xr-x. 2 root root 4096 Nov 15 14:05 . drwxr-xr-x. 13 root root 4096 Nov 15 13:55 .. lrwxrwxrwx. 1 root root 100 Nov 15 13:52 coredns-c5d4bf466-qdgrh_kube-system_coredns-a533d7375a4bd90b894d533e637bae6ce38e2e7d89cd0ff16e34bd120111c7e4.log -\u0026gt; /var/log/pods/kube-system_coredns-c5d4bf466-qdgrh_24b226a0-0fb3-4ede-b02f-17b177e6c248/coredns/0.log ... lrwxrwxrwx. 1 root root 101 Nov 15 14:05 nginx-bvs-pvc-7b574c9c5c-vdpdh_default_nginx-cc996c8fd2281d1bec7fdad75518c66a2ec6f03adc214f8bbd97c26967e8d0e7.log -\u0026gt; /var/log/pods/default_nginx-bvs-pvc-7b574c9c5c-vdpdh_aab712c6-81e1-434c-b051-f3e8fe50fa24/nginx/0.log lrwxrwxrwx. 1 root root 106 Nov 15 13:52 proxymux-client-cb86k_kube-system_proxymux-client-471c1c80fe2dc01c5e1413819af58d968796541cc744384f1b2daa566873d8ba.log -\u0026gt; /var/log/pods/kube-system_proxymux-client-cb86k_153b7b7a-a777-4be9-b971-657eb8ecdddd/proxymux-client/0.log   /var/log/containers/*.log는 위에서 보는 것처럼 링크라서 kubernetes namespace 기준으로 하고 싶다면, 로그 경로를 default namespace인 경우/var/log/pods/default_*/*/*.log 이렇게 해도 되겠습니다.\n  로깅 테스트   애플리케이션 로그 확인을 위해 이전 가이드에 샘플로 배포된 nginx 앱을 접속해 봅니다.\n   발생한 POD 로그는 다음과 같습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs nginx-bvs-pvc-7b574c9c5c-vdpdh -f ... 10.244.0.128 - - [16/Nov/2021:08:10:46 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   동일한 로그가 Worker Node VM 상에서도 로그를 조회해 보면 /var/log/containers/*.log 위치에 발생하는 것을 확인 할 수 있습니다.\n[root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# pwd /var/log/containers [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# tail -f nginx-bvs-pvc-7b574c9c5c-vdpdh_default_nginx-cc996c8fd2281d1bec7fdad75518c66a2ec6f03adc214f8bbd97c26967e8d0e7.log ... 2021-11-16T08:10:46.961696444+00:00 stdout F 10.244.0.128 - - [16/Nov/2021:08:10:46 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   OCI 서비스 콘솔에서 Logging 화면으로 다시 돌아갑니다.\n  Agent, Logs, Log Groups 각 화면에서 Resources \u0026gt; Explore Log에서 로그를 조회 할 수 있습니다. 검색을 위해서는 로그목록 오른쪽위에 있는 Explore with Log Search를 클릭합니다.\n   Custom filters 항목에서 POD 이름 또는 앞서 테스트 URL에 있는 customlogtest 같이 검색값으로 조회하면 됩니다. Custom filters에 값을 입력하고 엔터키를 꼭 칩니다.\n Log Agent를 통해 수집되는 주기가 있어 5분 내외가 걸릴 수 있습니다. 앞서 MuShop 앱 테스트 URL 한번만 접속한 경우 구간내에 Log Flush가 안되어 계속 기다려도 로그 조회가 안될 수 있으니, 테스트 URL 여러번 반복 접속합니다.     검색된 로그 데이터를 확인할 수 있습니다.\n   ","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke/8.monitor-application-log/","tags":["oke"],"title":"1.8 애플리케이션 로그 모니터링"},{"categories":null,"contents":"DevOps now supports deployment to private Kubernetes API endpoints DevOps 서비스가 OKE 쿠버네티스 클러스터의 API의 프라이빗 IP로 연결하는 것을 지원합니다.\n Oracle Cloud Infrastructure Documentation / Release Notes  OKE 클러스터는 프라이빗 IP가 기본적으로 할당되고, 생성시 선택에 따라 퍼블릭 IP가 추가적으로 할당됩니다. DevOps 서비스에서는 이제 Private Endpoint로 만든 Private Cluster에 배포하는 것을 지원합니다.\n  Quick Create 모드로 생성시\n Private Endpoint: API 엔드포인트에 프라이빗 IP만 할당됨 Public Endpoint: API 엔드포인트에 퍼블릭 IP 및 프라이빗 IP 모두 할당됨     Custom Create 모드로 생성시\n API 엔드포인트에 프라이빗 IP가 기본 할당됨. 옵션 선택시 퍼블릭 IP 할당됨.     DevOps 서비스에서 OKE Private Cluster에 배포하기 배포 환경 추가   DevOps 서비스에서 배포를 위해서는 환경(Environment)을 추가해야 합니다. 이때 OKE 유형을 택하면, 생성된 클러스터 중에서 아래 그림과 같이 이제 Private Endpoint를 가진 Private Cluster도 선택할 수 있습니다.\n   Private Cluster를 선택한 다음, 대상 Private Cluster의 Kubernetes API Endpoint로 접근 가능한 서브넷을 지정합니다.\n Quick Create 생성결과 기준, k8sApiEndpoint 서브넷을 선택하면 연결 오류가 발생합니다. nodesubnet을 반드시 선택합니다.\n    Security Rule 문서에는 DevOps가 속한 서비스 네트워크에서 들어올수 있는 Ingress Rule을 필요하면 추가하라고 하는데, Quic Create 생성된 클러스터 기준 테스트시 추가 작업없이 동작하였습니다.\nOKE 배포 환경 추가시 nodesubnet을 선택하면, nodesubnet은 당연히 API endpoint가 통신이 되도록 구성되어 있을 테니, 별다른 추가 설정없이 동작하였습니다.\n배포 결과  ","lastmod":"2022-03-29T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220329-devops-now-supports-deployment-to-private-kubernetes-api-endpoints/","tags":["DevOps","ci/cd"],"title":"DevOps now supports deployment to private Kubernetes API endpoints"},{"categories":null,"contents":"Support for Kubernetes version 1.22.5 OKE에서 쿠버네티스 지원 버전이 업데이트 되었습니다. 이제 1.22.5을 새롭게 지원합니다. 기존에 지원하던 1.21.5, 1.20.11은 계속 지원합니다.\n Oracle Cloud Infrastructure Documentation / Release Notes  업데이트 사항 1.22.x 지원으로 인해 1.19.15 버전은 2022년 4월 22일 까지 지원합니다.\n 지원이 종료하게 되면, 1.19.x버전으로 신규 클러스터 생성이 불가합니다. 지원이 종료하게 되면, 기존 클러스터에 1.19.x버전으로 새 Node Pool 추가가 불가합니다. 아래 그림은 현재 3월 22일 기준이며 1.22.5 출시로 인한 2022년 4월 22일 이후 변경이 발생하며, 지원 버전은 아래를 확인하세요.  Supported Versions of Kubernetes     업그레이드 가이드에 따라 종료일 이전에 업그레이드를 권고하고 있습니다.\n Upgrading the Kubernetes Version on Control Plane Nodes in a Cluster Upgrading the Kubernetes Version on Worker Nodes in a Cluster 1.7 Kubernetes 지원 버전 및 업그레이드  ","lastmod":"2022-03-21T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220321-support-for-kubernetes-version-1.22.5/","tags":["Container Engine for Kubernetes","oke"],"title":"Support for Kubernetes version 1.22.5"},{"categories":null,"contents":"Support for OCI Network Load Balancers OKE에서 Service Type을 Load Balanancer를 사용할때 이제는 OCI Network Load Balancer을 추가적으로 지원합니다.\n Oracle Cloud Infrastructure Documentation / Release Notes  릴리즈 노트를 기준으로 Network Load Balancer를 사용하면 다음과 같은 것이 가능합니다.\n  Load-balance traffic among Kubernetes pods with high throughput and low latency.\n=\u0026gt; OSI layer 3 and layer 4 (TCP/UDP/ICMP)기반 pass-through 방식으로 데이터를 열어보지 않고 분배하게 되어 보다 낮은 응답지연과 높은 처리량을 제공합니다.\n  Preserve source and destination IP addresses and ports.\n=\u0026gt; Source, Destination IP와 Port가 그대로 보존됩니다. Pod에서 Client IP를 알 수 있습니다.\n  Handle TCP and UDP traffic in the applications you deploy.\n=\u0026gt; TCP 뿐만 아니라, UDP도 지원합니다.\n  OCI Load Balancer와 OCI Network Load Balancer의 차이는 다음 블로그를 참고합니다.\n Comparing OCI Load Balancers: Quickly and Easily  Network Load Balancer 사용하기 OCI Load Balancer 사용하기   테스트를 위해 아래와 같이 nginx를 배포합니다.\n Service Type - Load Balancer에서 oci.oraclecloud.com/load-balancer-type를 지정하지 않으면, 기본값으로 OCI Load Balancer를 사용합니다.  apiVersion:apps/v1kind:Deploymentmetadata:name:my-nginxlabels:app:nginxspec:replicas:3selector:matchLabels:app:nginxtemplate:metadata:labels:app:nginxspec:containers:- name:nginximage:nginx:1.7.9ports:- containerPort:80---apiVersion:v1kind:Servicemetadata:name:my-nginx-lblabels:app:nginxannotations:oci.oraclecloud.com/load-balancer-type:\u0026#34;lb\u0026#34;spec:type:LoadBalancerports:- port:80selector:app:nginx  배포 결과 확인\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 26m my-nginx-lb LoadBalancer 10.96.197.58 129.154.54.161 80:32258/TCP 8m21s $ curl http://129.154.54.161 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ...   OCI Network Load Balancer 사용하기   앞서 배포된 Pod에 Network Load Balancer를 추가합니다.\n Service Type - Load Balancer에서 oci.oraclecloud.com/load-balancer-type=\u0026ldquo;nlb\u0026quot;로 지정하면 OCI Network Load Balancer를 사용합니다.  apiVersion:v1kind:Servicemetadata:name:my-nginx-nlblabels:app:nginxannotations:oci.oraclecloud.com/load-balancer-type:\u0026#34;nlb\u0026#34;spec:type:LoadBalancerports:- port:80selector:app:nginx  배포 결과 확인\n OCI Load Balancer 쓸때와는 다르게 curl로 접속이 안 됩니다.. OCI Network Load Balancer를 OKE에서 사용할 때는 Security Rule을 직접 등록을 해줘야 합니다.  $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 43m my-nginx-lb LoadBalancer 10.96.197.58 129.154.54.161 80:32258/TCP 24m my-nginx-nlb LoadBalancer 10.96.159.81 10.0.20.59,144.24.68.55 80:30350/TCP 31m $ curl http://138.2.117.137 ...   Security Rule 등록\n  Quick Create 모드로 생성한 클러스터를 기준으로 아래 규칙 추가가 필요합니다. (현재 my-nginx-nlb가 사용하는 Node Port는 30350 기준 설정입니다~\n  svclb subnet의 Security List 규칙 추가 설정\n  Ingress\n   Egress\nsvclb subnet -\u0026gt; node subnet:port(10.0.10.0/24:30350)으로 나갈 수 있게 추가\n     node subnet의 Security List 규칙 추가 설정\n  Ingress\nsvclb subnet -\u0026gt; node subnet:port(10.0.10.0/24:30350)로 들어올 수 있게 추가\n       Security Rule 추가후 다시 테스트하면 잘 동작합니다.\n$ curl http://144.24.68.55 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt;   컨테이너에서 Client IP 주소 가져오기   웹브라우저로 NLB를 통해 배포된 nginx를 접속합니다. 기존 nginx 컨테이너 로그에서 Client IP 확인하면 10.x.x.x의 OCI 내부 IP가 보이는 것을 볼 수 있습니다.\n$ kubectl logs -lapp=nginx -f ... 10.244.0.128 - - [21/Mar/2022:06:32:27 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   Source IP를 유지하는 기능을 활용하여, 컨테이너 상에서 접속한 클라이언트 IP를 알 수 있습니다. externalTrafficPolicy: Local을 아래와 같이 추가하면 됩니다.\n 해당 설정 적용을 위해 기존 Service를 삭제하고 다시 적용합니다.  apiVersion: v1 kind: Service metadata: name: my-nginx-nlb labels: app: nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;nlb\u0026#34; spec: loadBalancerIP: 144.24.68.55 type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 selector: app: nginx   Security Rule 등록\n  svclb subnet의 Security List 규칙 추가 설정\n 서비스 재배포로 인한 NodePort 변경시 변경 적용 또는 Kubernetes NodePort 대역 30000-32767을 사전에 적용    node subnet의 Security List 규칙 추가 설정\n  Ingress\nsvclb subnet -\u0026gt; node subnet:port 설정한 부분이, 이제 Source IP가 Load Balancer 내부 IP에서 인터넷상에서 오는 실제 Client IP가 그대로 오기 때문에, 이를 허용하기 위해 0.0.0.0/0으로 설정합니다.\n       테스트전 자신의 IP를 확인합니다.\n   웹브라우저로 NLB를 통해 배포된 nginx를 접속합니다.\n  기존 nginx 컨테이너 로그에서 Client IP 확인하면 아래와 같이 Client IP가 정상적으로 보이는 것을 볼 수 있습니다.\n$ kubectl logs -lapp=nginx -f ... 202.45.129.186 - - [21/Mar/2022:07:00:24 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   UDP로 서비스 하기   Load Balancer의 기본 프로토콜이 TCP이며, OCI Network Load Balancer를 사용하는 경우 UDP도 지원합니다.\n  앞선 설정에서 protocal: UDP만 다음과 같이 추가하면 됩니다.\napiVersion:v1kind:Servicemetadata:name:my-nginx-nlblabels:app:nginxannotations:oci.oraclecloud.com/load-balancer-type:\u0026#34;nlb\u0026#34;spec:type:LoadBalancerports:- port:80protocol:UDPselector:app:nginx  ","lastmod":"2022-03-15T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220315-support-for-oci-network-load-balancers/","tags":["Container Engine for Kubernetes","oke"],"title":"Support for OCI Network Load Balancers"},{"categories":null,"contents":"Container image scanning using the Console OCIR에서 컨테이너 이미지 스캐닝 기능을 지원하고 있습니다. 이에 추가하여 Vulnerability Scanning 콘솔에서도 할 수 있게 기능이 추가되었습니다.\n Oracle Cloud Infrastructure Documentation / Release Notes  Container Image Scan Recipe 만들기 IAM 권한 설정 compartment 단위 또는 전체 테넌시 단위로 권한을 지정할 수 있습니다. 여기서는 전체 테넌시에 대해 Vulnerability Scanning 서비스가 이미지 리파지토리 및 compartment에 접근할 수 있는 권한을 부여합니다.\n  전체 테넌시\n  이름 예, ocir-scanning-images-root-policy\nallow service vulnerability-scanning-service to read repos in tenancy allow service vulnerability-scanning-service to read compartments in tenancy     Container Image Scan Recipe 만들기   Oracle Cloud 콘솔에 로그입니다.\n  Identity \u0026amp; Security \u0026gt; Scanning \u0026gt; Scan Recipes 메뉴로 이동합니다.\n  Create를 클릭하여 이미지 스캔 레시피를 만듭니다.\n  이름 예, container-image-scan-recipe\n  Type: Container image\n     Container Image Target 생성   이미지 스캔 레시피 상세화면에서 Create Target을 클릭합니다.\n  컨테이너 이미지 타겟을 만듭니다.\n  이름 예, container-image-target\n  대상 Repository: 테넌시내 모든 Repository가 되도록 선택합니다.\n     스캐닝 리포트 확인   내비게이션 메뉴에서 Identity \u0026amp; Security \u0026gt; Scanning \u0026gt; Scanning Reports 메뉴로 이동합니다.\n  Container Image 탭으로 이동합니다.\n  스캐닝 된 이미지와 리포트를 볼 수 있습니다.\n   원하는 이미지의 이름을 클릭하면, 취약점 분석 결과를 볼 수 있습니다.\n   리스트된 취약점 중에 하나를 클릭합니다. 해당 취약점 기준으로 해당 취약점이 있는 호스트, 컨테이너 이미지 내역을 볼 수 있습니다.\n   CVE ID를 클릭하면, 취약점을 관리하는 원 데이터베이스에서 상세 정보를 확인할 수 있습니다.\n   취약점 리포트 확인   내비게이션 메뉴에서 Identity \u0026amp; Security \u0026gt; Scanning \u0026gt; Vulnerability Reports 메뉴로 이동합니다.\n  취약점 ID 기준으로 리포트를 볼수 있습니다. 각 CVE ID를 클릭하면, 스캐닝 리포트와 동일하게 해당 취약점에 대한 상세 내용을 확인할 수 있습니다.\n   ","lastmod":"2022-03-15T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220315-container-image-scanning-using-the-console/","tags":["Vulnerability Scanning","vss"],"title":"Container image scanning using the Console"},{"categories":null,"contents":"Support for function invocation logs Oracle Function에 대해서 Logging을 활성화 한 경우, Logging 화면에서 호출 수행시간과 에러시 에러 코드를 확인할 수 있게 되었습니다.\n Oracle Cloud Infrastructure Documentation / Release Notes  Function 배포   배포된 함수가 없는 경우 아래 문서를 참조하여 테스트할 함수를 사전에 배포합니다.\nFunctions QuickStart on Cloud Shell\n  Log 활성화하기   배포한 함수의 상세화면으로 이동합니다.\n  Enable Log를 토글하여 활성화합니다.\n   로그 생성\n생성한 Log Group이 없으면, 자동 생성하도록 설정합니다.\n   로그가 활성화되면, 생성된 로그의 이름을 클릭하여 이동합니다.\n 로그 테스트   Cloud Shell에서 배포된 함수를 호출합니다.\n$ fn invoke helloworld-app hello-java Hello, world! $ echo -n \u0026#39;John\u0026#39; | fn invoke helloworld-app hello-java Hello, John!   로그 확인\n아래와 같이 호출된 건에 대한 처리 시간을 확인할 수 있습니다.\n     ","lastmod":"2022-03-03T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220303-support-for-function-invocation-logs/","tags":["Functions","fn","serverless"],"title":"Support for function invocation logs"},{"categories":null,"contents":"Support for PVC block volume expansion OKE에서 Block Volume을 Persistent Volume으로 사용하는 경우, 할당 받은 이후에도 사용 중인 볼륨의 크리를 확장하는 기능을 지원합니다.\n Oracle Cloud Infrastructure Documentation / Release Notes  Block Volume Expansion 설정 확인 클러스터 생성후 기본 구성된StorageClass를 업데이트 전후로 확인한 내용입니다.\n  기능 업데이트 전\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 2d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d   업데이트 후\nALLOWVOLUMEEXPANSION 항목이 기존 false -\u0026gt; true로 변경되었습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 22h oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 22h   기본 구성된 oci-bv StorageClass를 사용하면 기본적으로 볼륨 사이즈 확장이 됩니다. 새롭게 Block Volume CSI Driver(provisioner: blockvolume.csi.oraclecloud.com)로 StorageClass를 생성한다고 하면 아래와 같이 allowVolumeExpansion: true 를 추가하면 됩니다.\n$ kubectl get sc oci-bv -o yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: oci-bv provisioner: blockvolume.csi.oraclecloud.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true   Block Volume Expansion 기능 확인   아래와 같이 PV 요청 yaml을 사용하여 50Gi 사이즈로 요청합니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:csi-bvs-pvcspec:storageClassName:\u0026#34;oci-bv\u0026#34;accessModes:- ReadWriteOnceresources:requests:storage:50Gi  테스트 앱 배포\n 요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱  apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-bvs-pvcname:nginx-bvs-pvcspec:replicas:1selector:matchLabels:app:nginx-bvs-pvctemplate:metadata:labels:app:nginx-bvs-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:csi-bvs-pvc  생성 결과\n 아래와 같이 정상적으로 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다.  winter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created winter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-f0f36095-34fa-4d57-8868-a2a102a044c7 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 15s winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-lst4d 1/1 Running 0 72s   PV를 사용하는 앱이 배포된 상태에서 볼륨 크기를 늘립니다. 50Gi -\u0026gt; 100Gi로 변경후 적용합니다.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-bvs-pvc spec: storageClassName: \u0026#34;oci-bv\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 100Gi   적용\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc configured winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-f0f36095-34fa-4d57-8868-a2a102a044c7 100Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 4m23s   볼륨 확장 결과확인\n 이벤트를 확인해 보면, 볼륨 확장이 성공했음을 알 수 있습니다. 컨테이너 재기동도 일어나지 않았습니다.  winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get events ... 6m50s Normal Created pod/nginx-bvs-pvc-7b574c9c5c-lst4d Created container nginx 6m50s Normal Started pod/nginx-bvs-pvc-7b574c9c5c-lst4d Started container nginx 3m53s Normal Resizing persistentvolumeclaim/csi-bvs-pvc External resizer is resizing volume csi-f0f36095-34fa-4d57-8868-a2a102a044c7 3m53s Warning ExternalExpanding persistentvolumeclaim/csi-bvs-pvc Ignoring the PVC: didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. 3m52s Normal FileSystemResizeRequired persistentvolumeclaim/csi-bvs-pvc Require file system resize of volume on node 3m14s Normal FileSystemResizeSuccessful persistentvolumeclaim/csi-bvs-pvc MountVolume.NodeExpandVolume succeeded for volume \u0026#34;csi-f0f36095-34fa-4d57-8868-a2a102a044c7\u0026#34; 3m14s Normal FileSystemResizeSuccessful pod/nginx-bvs-pvc-7b574c9c5c-lst4d MountVolume.NodeExpandVolume succeeded for volume \u0026#34;csi-f0f36095-34fa-4d57-8868-a2a102a044c7\u0026#34; winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-lst4d 1/1 Running 0 4m42s   컨테이너 내부 확인\n컨테이너 내부를 확인해 보면 PV가 마운트된 /usr/share/nginx/html의 사이즈가 99G인 것을 알 수 있습니다. 볼륨이 확장된 것을 알 수 있습니다.\n  winter@cloudshell:~ (ap-chuncheon-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-lst4d -- /bin/bash root@nginx-bvs-pvc-7b574c9c5c-lst4d:/# df -h Filesystem Size Used Avail Use% Mounted on overlay 39G 6.8G 32G 18% / tmpfs 64M 0 64M 0% /dev tmpfs 7.7G 0 7.7G 0% /sys/fs/cgroup shm 64M 0 64M 0% /dev/shm tmpfs 7.7G 28M 7.7G 1% /etc/hostname /dev/sda3 39G 6.8G 32G 18% /etc/hosts /dev/sdb 99G 60M 94G 1% /usr/share/nginx/html tmpfs 7.7G 12K 7.7G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 7.7G 0 7.7G 0% /proc/acpi tmpfs 7.7G 0 7.7G 0% /proc/scsi tmpfs 7.7G 0 7.7G 0% /sys/firmware   ","lastmod":"2022-02-28T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220228-support-for-pvc-block-volume-expansion/","tags":["Container Engine for Kubernetes","oke","pvc"],"title":"Support for PVC block volume expansion"},{"categories":null,"contents":"Support for 2 GB functions Oracle Function에 대해서 최대 2GB까지 메모리를 할당할 수 있게 되었습니다.\n Oracle Cloud Infrastructure Documentation / Release Notes   Function은 호출 건과 사용한 메모리에 따라 가격을 산정합니다. 메모리는 초당 사용한 GB 메모리 기준 400,000 까지 무료이며 그 이상은 비용을 청구합니다. 함수 당 2GB까지 설정할 수 있지만, 비용과 관련이 있으므로 적정량만 할당합니다.\n Cloud Function pricing   Function 메모리 설정하기   따로 지정하지 않는 경우 기본값으로 128MB 메모리를 할당합니다.\n  메모리 설정시 최대 2GB까지 지정할 수 있습니다. 128, 256, 512, 1024, 2048 중에 설정할 수 있습니다.\n  함수 소스 루트에 있는 func.yaml 파일에 memory: 2048과 같이 값을 추가합니다.\nschema_version:20180708name:hello-javaversion:0.0.4runtime:javabuild_image:fnproject/fn-java-fdk-build:jdk11-1.0.146run_image:fnproject/fn-java-fdk:jre11-1.0.146cmd:com.example.fn.HelloFunction::handleRequestmemory:2048  fn cli로 업데이트 합니다. fn update function --memory 1024 \u0026lt;app-name\u0026gt; \u0026lt;function-name\u0026gt;와 같이 실행합니다.\nfn update function --memory 2048 helloworld-app hello-java   Oracle Cloud 콘솔에서 함수 설정화면에서 변경합니다.\n     ","lastmod":"2022-02-11T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220211-support-for-2gb-functions/","tags":["Functions","fn","serverless"],"title":"Support for 2 GB functions"},{"categories":null,"contents":"참조할만한 사이트 OCI Documentation  Oracle OCI 공식 문서 사이트입니다.  Oracle Learning  오라클에서 OU에서 OCI를 학습할 수 있는 사이트로 동영상과 실습교재를 제공합니다. 프로모션 기간 중에는 무료로 사용할 수 있습니다.  Oracle LiveLabs  오라클에서 제공하는 OCI 실습 가이드를 제공하는 사이트입니다.  ","lastmod":null,"permalink":"https://thekoguryo.github.io/link/","tags":null,"title":""},{"categories":null,"contents":"aa\n","lastmod":null,"permalink":"https://thekoguryo.github.io/test/","tags":null,"title":""},{"categories":null,"contents":"","lastmod":null,"permalink":"https://thekoguryo.github.io/search/","tags":null,"title":"Search"}]