[{"categories":null,"contents":"17.1 Kafka Connect, Debezium로 PostgreSQL CDC 구성하기 Kafka에서 Debezium Connector를 통해 데이터 변경분 캡쳐(CDC)를 수행하고, JDBC Connector를 통해 대상 시스템에 동기화는 것을 구현하는 경우가 있습니다. 여기서는 Kafka를 대신하여 Kafka 호환 서비스인 OCI Streaming을 사용할 수 있는 지, 사용시 유의사항이 있는 지를 확인해 보고자 합니다. 그래서 여기서는 Kafka에 대한 자세한 설명보다는 OCI Streaming로 가능 여부 확인에 우선합니다.\nSource PostgreSQL -\u0026gt; Debezium Connector -\u0026gt; OCI Streaming -\u0026gt; JDBC Connector -\u0026gt; Target PostgreSQL 구성으로 CDC를 구성하도록 하겠습니다.\nKafka API 지원 OCI Streaming 서비스는 Apache Kafka 호환 서비스로 대부분의 Kafka API와 호환합니다. 지원 Kafka API는 다음과 같습니다. 아래 Kafka API를 지원하기 때문에, 위 Kafka Connect를 이용한 CDC 구성은 가능하다고 판단되어, 아래 테스트를 진행합니다.\nProducer (v0.10.0 and later) Consumer (v0.10.0 and later) Connect (v0.10.0.0 and later) Admin (v0.10.1.0 and later) Group Management (v0.10.0 and later) 업데이트 및 정확한 사항은 공식 문서 Using Streaming with Apache Kafka \u0026gt; Kafka API Support를 참조하세요.\n필요한 OCI Policy Compute 인스턴스, Container Instance, Streaming을 사용하기 위해 Policy를 사전에 구성합니다.\n# Compute Instance Allow group {domain-name}/{group_name} to manage instance-family in compartment {compartment-name} Allow group {domain-name}/{group_name} to use volume-family in compartment {compartment-name} Allow group {domain-name}/{group_name} to manage virtual-network-family in compartment {compartment-name} # Container Instance Allow group {domain-name}/{group_name} to manage compute-container-family in compartment {compartment-name} # OCI Streaming Allow group {domain-name}/{group_name} to manage stream-family in compartment {compartment-name} Source, Target PostgreSQL 데이터베이스 구성 먼저, Source, Target으로 사용할 PostgreSQL 데이터베이스 인스턴스를 만듭니다. 설치 편의상 debezium에서 제공하는 컨테이너 이미지를 사용하여, OCI Container Instance 서비스로 사용할 환경을 만듭니다.\n참고 - Source DB에 설정 요구사항 Debezium Setting up Postgres을 기준으로 설정값을 확인해 봅니다. 참고로, 작성일 2024년 5월기준, OCI Database with PostgreSQL 서비스는 아직 Debezium을 지원하지 않는다고 합니다. Source PostgresSQL 생성 정보\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Instances 로 이동합니다.\n아래 정보로 Container Instance를 생성합니다.\nName: postgresql-source Networking: 편의상 Public Subnet 사용 컨테이너 이미지 Registry hostname: docker.io Repository: debezium/postgres Tag: 16 환경 변수 POSTGRES_USER=postgres POSTGRES_PASSWORD=password123 Security List Ingress 규칙에 5432 포트를 개방합니다.\nTarget PostgresSQL 생성 정보\n아래 항목만 달리하여 Container Instance를 하나 더 만듭니다. Name: postgresql-target psql client 설치\nPostgreSQL에 접속툴로 psql을 설치합니다.\nMac 기준\nbrew install libpq echo \u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/libpq/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc # 확인 psql --version Oracle Linux 8\nsudo dnf install -y postgresql # 확인 psql --version Source DB에 CDC 대상 테이블 생성\n편의상 여기서는 별도 유저 생성없이 기본 관리자 유저를 사용합니다.\n데이터베이스 접속\npsql -h \u0026lt;Source-PostgreSQL-Public-IP\u0026gt; -p 5432 -d postgres -U postgres DATABASE 생성 및 선택\npostgres=# CREATE DATABASE sourcedb; CREATE DATABASE postgres=# \\c sourcedb psql (10.23, server 16.2 (Debian 16.2-1.pgdg110+2)) WARNING: psql major version 10, server major version 16. Some psql features might not work. You are now connected to database \u0026#34;sourcedb\u0026#34; as user \u0026#34;postgres\u0026#34;. sourcedb=# 샘플 테이블 생성\nCREATE TYPE gender AS ENUM(\u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;); CREATE TABLE employees ( emp_no INT NOT NULL, birth_date DATE NOT NULL, first_name VARCHAR(14) NOT NULL, last_name VARCHAR(16) NOT NULL, gender gender NULL, hire_date DATE NOT NULL, PRIMARY KEY (emp_no) ); Target DB 구성 설정\n편의상 여기서는 별도 유저 생성없이 기본 관리자 유저를 사용합니다.\n데이터베이스 접속\npsql -h \u0026lt;Target-PostgreSQL-Public-IP\u0026gt; -p 5432 -d postgres -U postgres DATABASE 생성 및 선택\nCREATE DATABASE targetdb; \\c targetdb 샘플 테이블 없음을 확인합니다.\nselect * from employees; OCI Streaming 서비스 구성 및 연결 준비 OCI 콘솔에 로그인합니다. 좌측 상단 햄버거 메뉴에서 Analytics \u0026amp; AI \u0026gt; Messaging \u0026gt; Streaming 으로 이동합니다. 메뉴에서 Stream Pools 을 클릭합니다. 아래 정보로 Stream Pool을 생성합니다. Stream Pool Name: cdc-stream-pool 고급 옵션 Auto create topics 선택 메뉴에서 Kafka Connect Configurations 을 클릭합니다. Kafka Connect를 사용하기 위해 필요한 Kafka Topic 3개를 만들 수 있습니다. 아래 이름으로 생성합니다. Kafka Connect Configuration Name: my-kafka-connect-conf 유저 Auth Token 준비 기 발급받은 Auth Token이 없는 경우 My profile \u0026gt; Auth tokens 에서 새 Auth Token을 발급하여 기록해 둡니다. Kafka Connect 설치 및 구성 Kafka 및 Connector 설치\nOCI 콘솔에서 Kafka Connect를 설치할 Compute 인스턴스를 하나 생성합니다.\nName: 예, kafka-connect OS: Oracle Linux 8 설치한 Compute 인스턴스에 SSH로 접속합니다.\nJava 17을 설치합니다.\nyum list jdk* sudo yum install -y jdk-17.x86_64 설치 확인\n$ java -version java version \u0026#34;17.0.5\u0026#34; 2022-10-18 LTS Java(TM) SE Runtime Environment (build 17.0.5+9-LTS-191) Java HotSpot(TM) 64-Bit Server VM (build 17.0.5+9-LTS-191, mixed mode, sharing) Kafka 최신 버전을 설치합니다.\ncd wget https://dlcdn.apache.org/kafka/3.7.0/kafka_2.13-3.7.0.tgz tar xf kafka_2.13-3.7.0.tgz mv kafka_2.13-3.7.0 kafka Source DB에 CDC를 위한 Debezium PostgreSQL Connector 2.x 버전을 설치합니다.\nmkdir -p ./kafka/plugins wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/2.6.1.Final/debezium-connector-postgres-2.6.1.Final-plugin.tar.gz tar zxvf debezium-connector-postgres-2.6.1.Final-plugin.tar.gz -C ./kafka/plugins/ Target DB 연결을 위한 JDBC Connector를 설치합니다.\nwget https://d1i4a15mxbxib1.cloudfront.net/api/plugins/confluentinc/kafka-connect-jdbc/versions/10.7.6/confluentinc-kafka-connect-jdbc-10.7.6.zip unzip confluentinc-kafka-connect-jdbc-10.7.6.zip -d ./kafka/plugins/ Connector 설치를 확인합니다.\n$ ls ./kafka/plugins/ confluentinc-kafka-connect-jdbc-10.7.6 debezium-connector-postgres Kafka Connect Properties 설정\nKafka Connect는 Standalone Mode와 Distributed Mode로 실행할 수 있습니다. 여기서는 Distributed Mode 기준으로 설정하겠습니다.\n설치된 기본 속성 파일을 참고하여, 새 설정파일을 생성합니다.\ncat ./kafka/config/connect-distributed.properties 새 설정파일을 만듭니다.\nvi connect-distributed.properties 다음과 같이 설정합니다.\n# connect-distributed.properties bootstrap.servers=cell-1.streaming.${REGION}.oci.oraclecloud.com:9092 group.id=debezium-connect-cluster key.converter=org.apache.kafka.connect.json.JsonConverter value.converter=org.apache.kafka.connect.json.JsonConverter key.converter.schemas.enable=true value.converter.schemas.enable=true offset.storage.topic=${CONNECT_HARNESS_OCID}-offset config.storage.topic=${CONNECT_HARNESS_OCID}-config status.storage.topic=${CONNECT_HARNESS_OCID}-status offset.flush.interval.ms=10000 security.protocol=SASL_SSL sasl.mechanism=PLAIN sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#34;${TENANCY_NAME}/${USER_NAME}/${STREAM_POOL_OCID}\u0026#34; password=\u0026#34;${AUTH_TOKEN}\u0026#34;; #producer.buffer.memory=4096 #producer.batch.size=2048 producer.sasl.mechanism=PLAIN producer.security.protocol=SASL_SSL producer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#34;${TENANCY_NAME}/${USER_NAME}/${STREAM_POOL_OCID}\u0026#34; password=\u0026#34;${AUTH_TOKEN}\u0026#34;; consumer.sasl.mechanism=PLAIN consumer.security.protocol=SASL_SSL consumer.sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=\u0026#34;${TENANCY_NAME}/${USER_NAME}/${STREAM_POOL_OCID}\u0026#34; password=\u0026#34;${AUTH_TOKEN}\u0026#34;; plugin.path=/home/opc/kafka/plugins 위 항목 중 일부 항목은 실제 값을 확인하여 업데이트합니다.\n앞서 만든 Stream Pool의 상세 정보에서 Resources \u0026gt; Kafka Connection Settings로 이동합니다.\nbootstrap.servers: Kafka Connection Settings의 Bootstrap Servers 값 사용 *.sasl.jaas.config: Kafka Connection Settings의 SASL Connection Strings 값에서 AUTH_TOKEN만 변경하여 사용 앞서 만든 Kafka Connect Configuration의 상세 정보로 이동합니다.\noffset, config, status.storage.topic: Kafka Connect Storage Topics 값 사용 group-id: 필요시 다른 고유한 값으로 변경\nplugin.path: Debezium, JDBC Connector 설치한 경로 지정\nKafka Connect 실행\n설정 파일을 사용하여 실행합니다.\n./kafka/bin/connect-distributed.sh connect-distributed.properties 실행 결과\n$ ./kafka/bin/connect-distributed.sh connect-distributed.properties ... [2024-05-02 09:53:47,998] INFO [Worker clientId=connect-10.0.0.220:8083, groupId=debezium-connect-cluster] Starting connectors and tasks using config offset -1 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1921) [2024-05-02 09:53:47,998] INFO [Worker clientId=connect-10.0.0.220:8083, groupId=debezium-connect-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1950) [2024-05-02 09:53:48,643] INFO [Worker clientId=connect-10.0.0.220:8083, groupId=debezium-connect-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2442) Source DB 연결을 위한 Debezium Connector 설정 Source DB 연결을 위한 설정 파일(connector-source-postgres.json)을 만듭니다.\n{ \u0026#34;name\u0026#34;: \u0026#34;source-postgres-employees\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.postgresql.PostgresConnector\u0026#34;, \u0026#34;database.hostname\u0026#34;: \u0026#34;xxx.xx.xx.xxx\u0026#34;, \u0026#34;database.port\u0026#34;: \u0026#34;5432\u0026#34;, \u0026#34;database.user\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;database.password\u0026#34;: \u0026#34;password123\u0026#34;, \u0026#34;database.dbname\u0026#34; : \u0026#34;sourcedb\u0026#34;, \u0026#34;topic.prefix\u0026#34;: \u0026#34;source-postgres\u0026#34;, \u0026#34;table.include.list\u0026#34;: \u0026#34;public.employees\u0026#34;, \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, \u0026#34;key.converter.schemas.enable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, \u0026#34;value.converter.schemas.enable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;time.precision.mode\u0026#34;: \u0026#34;connect\u0026#34; } } database.hostname: Source DB의 IP database.user: DB 접속 유저명 database.password: 유저 패스워드 database.dbname : 앞서 CREATE DATABASE 명령으로 만든 Source DB상의 DATABASE 이름 topic.prefix: OCI Streaming에 만들어질 Topic들의 Prefix 예, source-postgres employees 테이블의 변경분은 source-postgres.public.employees 이름으로 Topic이 만들어지게 됨 table.include.list: Source PostrgeSQL의 스키마.테이블명으로, 정규 표현시의 콤마리스트 time.precision.mode: connect: date/time/datetime 포맷 변환을 위해 필요, 하지 않을 경우, birth_date: \u0026lsquo;1953-09-02\u0026rsquo;가 Target에서는 -5965처럼될 수 있습니다. 설정 파일을 사용해 Source DB를 위한 Connector를 배포합니다.\ncurl --location --request POST \u0026#39;http://localhost:8083/connectors\u0026#39; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;@connector-source-postgres.json\u0026#39; 현재 배포된 Connector를 조회합니다.\ncurl localhost:8083/connectors | jq 필요시 삭제후 설정 파일 변경후 다시 배포합니다.\ncurl --location --request DELETE \u0026#39;http://localhost:8083/connectors/source-postgres-employees\u0026#39; Target DB 연결을 위한 JDBC Sink Connector 설정 Target DB 연결을 위한 설정 파일(connector-target-postgres.json)을 만듭니다.\n{ \u0026#34;name\u0026#34;: \u0026#34;target-postgres\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.jdbc.JdbcSinkConnector\u0026#34;, \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;connection.url\u0026#34;: \u0026#34;jdbc:postgresql://xxx.xx.xx.xx:5432/targetdb?currentSchema=public\u0026#34;, \u0026#34;connection.user\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;connection.password\u0026#34;: \u0026#34;password123\u0026#34;, \u0026#34;table.name.format\u0026#34;: \u0026#34;employees\u0026#34;, \u0026#34;topics\u0026#34;: \u0026#34;source-postgres.public.employees\u0026#34;, \u0026#34;auto.create\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;auto.evolve\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;delete.enabled\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;insert.mode\u0026#34;: \u0026#34;upsert\u0026#34;, \u0026#34;pk.mode\u0026#34;: \u0026#34;record_key\u0026#34;, \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, \u0026#34;key.converter.schemas.enable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, \u0026#34;value.converter.schemas.enable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;transforms\u0026#34;: \u0026#34;unwrap\u0026#34;, \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;none\u0026#34; } } connection.url: Target DB 접속을 위한 JDBC URL 입력 connection.user: DB 접속 유저명 connection.password: 유저 패스워드 table.name.format: Target DB에 만들 테이블 이름 형식 topics: 가져올 Topic의 이름 형식 auto.create: Target DB에 해당 테이블이 없을 경우, 자동으로 만들지 여부 지정 auto.evolve: Target DB에 해당 테이블과 스키마가 다를 경우, 자동으로 반영할 지 여부 지넝 delete.enabled: Target DB에 해당 테이블 삭제 가능 여부 지정 설정 파일을 사용해 Target DB를 위한 Connector를 배포합니다.\ncurl --location --request POST \u0026#39;http://localhost:8083/connectors\u0026#39; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;@connector-target-postgres.json\u0026#39; 현재 배포된 Connector를 조회합니다.\ncurl localhost:8083/connectors | jq 필요시 삭제후 설정 파일 변경후 다시 배포합니다.\ncurl --location --request DELETE \u0026#39;http://localhost:8083/connectors/target-postgres\u0026#39; OCI 콘솔에서 Streams 목록을 확인해 보면, 새로 Topic 생성된 것을 확인할 수 있습니다.\nCDC 테스트 Source DB에 데이터 변경분 발생\n편의상 여기서는 별도 유저 생성없이 기본 관리자 유저를 사용합니다.\n데이터베이스 접속\npsql -h \u0026lt;Soure-PostgreSQL-Public-IP\u0026gt; -p 5432 -d postgres -U postgres DATABASE 접속\n\\c sourcedb 새 데이터 삽입\nINSERT INTO employees VALUES (10001,\u0026#39;1953-09-02\u0026#39;,\u0026#39;Georgi\u0026#39;,\u0026#39;Facello\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;1986-06-26\u0026#39;); INSERT INTO employees VALUES (10002,\u0026#39;1964-06-02\u0026#39;,\u0026#39;Bezalel\u0026#39;,\u0026#39;Simmel\u0026#39;,\u0026#39;F\u0026#39;,\u0026#39;1985-11-21\u0026#39;); 데이터를 확인합니다.\nsourcedb=# select * from employees; emp_no | birth_date | first_name | last_name | gender | hire_date --------+------------+------------+-----------+--------+------------ 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 10002 | 1964-06-02 | Bezalel | Simmel | F | 1985-11-21 (2 rows) sourcedb=# Target DB에 데이터 확인\n편의상 여기서는 별도 유저 생성없이 기본 관리자 유저를 사용합니다.\n데이터베이스 접속\npsql -h \u0026lt;Target-PostgreSQL-Public-IP\u0026gt; -p 5432 -d postgres -U postgres DATABASE 접속\n\\c targetdb 현재 데이터 확인합니다.\ntargetdb=# select * from employees; emp_no | birth_date | first_name | last_name | gender | hire_date --------+------------+------------+-----------+--------+------------ 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 10002 | 1964-06-02 | Bezalel | Simmel | F | 1985-11-21 (2 rows) targetdb=# Target DB에 동일 테이블이 생성되고 데이터도 동기화되었습니다.\nKafka Connect 로그 확인\nKafka Connect의 실행로그를 확인하면, 아래와 같이 변경분이 확인하고, Target DB에 테이블이 없음을 확인하고 생성했다는 로그를 확인할 수 있습니다. 또한 2건의 변경분을 전달했다는 메시지도 확인할 수 있습니다.\nconnect-distributed.properties에서 OCI Streaming에서 사용하는 연결방식만 사용하면, 기존 Kafka들 대체하여, Kafka Connect, Connector 플러그인들을 활용하여 CDC를 수행할 수 있는 것을 확인하였습니다.\n","lastmod":"2024-05-02T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter17/oci-oss-cdc-postgresql-debezium/","tags":["oci streaming","kafka connect","cdc","debezium"],"title":"17.1 Kafka Connect, Debezium로 PostgreSQL CDC 구성하기"},{"categories":null,"contents":"1.2.5.1 Ampere (Arm) Node Pool 추가하기 오라클은 2021년 5월부터 Arm 기반의 Ampere A1 Compute Shape을 가상 머신에서 사용할 수 있습니다. Ampere는 ARM 기반 서버 성능 비교에서 보듯이 뛰어난 가성비를 제공합니다.\n여기서는 OKE에서 가성비를 이용하기 위해 Ampere (Arm) Node Pool을 추가로 구성하여 활용하는 방법을 알아봅니다.\n설정 순서 Shape: 추가 Node Pool을 생성할 때 Shape을 Ampere A1으로 선택합니다.\nTaint: 일반적으로 그동안 x86기반 이미지를 기본으로 사용해 왔고, 해당 이미지가 여전히 멀티 아키텍처를 지원하지 않을 수 있습니다. 기 배포 설정 파일을 사용하는 해당 이미지가 Arm Node Pool에 스케줄되는 것을 막을 필요가 있습니다. Arm 사용 노드에 taint 조건을 지정할 필요가 있습니다.\n예시\nkubectl taint nodes NAME arch=arm64:NoSchedule 매번 생성된 Node에 매뉴얼하게 위 명령을 수행할 수 없기 때문에, 자동으로 설정하기 위해서는 Node 생성시 cloud-init으로 설정할 필요가 있습니다. cloud-init을 통해 kubelet에 아래 옵션을 추가합니다.\n자세한 설명은 1.10.2 cloud-init으로 kubelet 옵션 변경하기을 참고하세요, \u0026ndash;register-with-taints string Register the node with the given list of taints (comma separated \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;:\u0026lt;effect\u0026gt;). No-op if --register-node is false. (DEPRECATED: This parameter should be set via the config file specified by the kubelet\u0026rsquo;s --config flag. See kubelet-config-file for more information.) Pod 배포 옵션 설정: arm 이미지를 지원하는 경우에, taint가 설정된 Node에 배포할 수 있도록 Pod 배포시 toleration 설정을 추가합니다.\nArm Node Pool 만들기 Node Pool 만들기\nOCI 콘솔에 로그인합니다.\n대상 OKE 클러스터로 이동합니다.\n클러스터 상세정보에서 Resources \u0026gt; Node Pools을 클릭합니다.\nAdd node pool을 클릭합니다.\n생성할 Node Pool의 기본 정보를 입력합니다.\nName: pool-arm\nNode type: Managed\nVersion: 예, v1.26.7\nNode Placement Configuration: Worker Node가 위치할 서브넷 지정\nShape and image: Ampere A1인 VM.Standard.A1.Flex를 선택합니다.\nNode count: 1\nPod communication: Worker Node가 위치할 서브넷 지정\nShow advanced options\nInitialization script을 사용하여 taint를 설정합니다. (kubelet-config-file은 잘 안되서, 그냥 \u0026ndash;kubelet-extra-args 사용)\n#!/bin/bash curl --fail -H \u0026#34;Authorization: Bearer Oracle\u0026#34; -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode \u0026gt;/var/run/oke-init.sh bash /var/run/oke-init.sh --kubelet-extra-args \u0026#34;--register-with-taints arch=arm64:NoSchedule\u0026#34; 나머지 항목은 요건에 맞게 설정합니다.\nAdd를 클릭하여 Node Pool을 추가합니다.\n생성된 Node Pool 확인하기\n생성된 Node는 기본적으로 CPU Architecture 유형이 레이블되어 있습니다. 아래와 같이 조회해 봅니다.\nkubernetes.io/arch에 amd64 또는 arm64인지 확인할 수 있습니다. $ kubectl get nodes -L kubernetes.io/arch,name --sort-by=\u0026#39;{.metadata.labels.name}\u0026#39; NAME STATUS ROLES AGE VERSION ARCH NAME 10.0.10.158 Ready node 18d v1.26.7 amd64 oke-cluster-1 10.0.10.42 Ready node 10d v1.26.7 amd64 oke-cluster-1 10.0.10.43 Ready node 10d v1.26.7 amd64 oke-cluster-1 10.0.10.248 Ready node 8h v1.26.7 arm64 pool-arm OKE 클러스터 상의 Node Pool 구성\ncloud-init으로 taint가 걸렸는지 확인해 봅니다.\n$ kubectl describe node 10.0.10.248 Name: 10.0.10.248 ... Taints: arch=arm64:NoSchedule Pod 배포해 보기 기본 애플리케이션 배포하기\n일반적으로 배포하던 방식으로 배포파일을 작성합니다.\n# nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 10 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 배포하고 배포된 Node를 확인합니다.\ntaint가 걸린 10.0.10.248 노드에는 실행중인 Pod가 없는 것을 볼 수 있습니다. $ kubectl apply -f nginx.yaml deployment.apps/nginx-deployment configured $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-6b7f675859-9b69w 1/1 Running 0 29s 10.0.10.115 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-d7fn4 1/1 Running 0 23s 10.0.10.173 10.0.10.158 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-gdhx9 1/1 Running 0 23s 10.0.10.80 10.0.10.158 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-gthnt 1/1 Running 0 29s 10.0.10.152 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-hmdr5 1/1 Running 0 29s 10.0.10.7 10.0.10.158 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-kxqpg 1/1 Running 0 29s 10.0.10.85 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-m4j96 1/1 Running 0 23s 10.0.10.77 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-pbcdc 1/1 Running 0 23s 10.0.10.238 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-wfz8g 1/1 Running 0 22s 10.0.10.71 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-6b7f675859-zmgwc 1/1 Running 0 29s 10.0.10.23 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; toleration을 추가하여 애플리케이션 배포하기\n앞선 배포 파일에 taint에 대응하는 toleration을 설정합니다.\ndocker hub상의 nginx 이미지는 multi architecture를 지원하여, arm 기반 노드도 지원합니다. # nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 10 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 tolerations: - key: \u0026#34;arch\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;arm64\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; 다시 배포하고 배포된 Node를 확인합니다.\ntaint가 걸린 10.0.10.248 노드에도 실행중인 것을 볼 수 있습니다. $ kubectl apply -f nginx.yaml deployment.apps/nginx-deployment configured $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-55cc658fc8-26rhq 1/1 Running 0 20s 10.0.10.147 10.0.10.158 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-4phgq 1/1 Running 0 14s 10.0.10.236 10.0.10.158 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-5q6fd 1/1 Running 0 20s 10.0.10.48 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-7x48p 1/1 Running 0 20s 10.0.10.208 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-8fwsv 1/1 Running 0 20s 10.0.10.29 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-9rldx 1/1 Running 0 20s 10.0.10.198 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-dbd2n 1/1 Running 0 12s 10.0.10.129 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-hm7pm 1/1 Running 0 10s 10.0.10.205 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-rl67q 1/1 Running 0 14s 10.0.10.131 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-55cc658fc8-zn9fn 1/1 Running 0 15s 10.0.10.235 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Arm Node Pool에만 애플리케이션 배포하기\n앞선 배포 파일에 추가 설정을 합니다.\ntaint와 toleration을 사용하면, taint가 설정된 Node에 배포할 수 있는지 없는 지 여부만 결정되어, taint가 없는 Node에도 배포가 됩니다.\nArm Node Pool에만 배포하기 위해서는 추가로 Node Affinity 또는 node selector를 같이 사용합니다. 여기서는 더 간단한 Node Selector를 사용하여, node의 label을 기준으로 배포될 노드를 선택합니다.\n# nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 10 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 tolerations: - key: \u0026#34;arch\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;arm64\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; nodeSelector: kubernetes.io/arch: arm64 다시 배포하고 배포된 Node를 확인합니다.\n모든 Pod가 arm 노드인 10.0.10.248 노드에서 실행중인 것을 볼 수 있습니다. $ kubectl apply -f nginx.yaml deployment.apps/nginx-deployment configured $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-799d665bbd-fr4ld 1/1 Running 0 45s 10.0.10.130 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-ftdhg 1/1 Running 0 53s 10.0.10.97 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-j2rms 1/1 Running 0 28s 10.0.10.108 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-jb56x 1/1 Running 0 53s 10.0.10.87 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-k7lt5 1/1 Running 0 42s 10.0.10.39 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-l5w8k 1/1 Running 0 44s 10.0.10.196 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-l6phv 1/1 Running 0 53s 10.0.10.68 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-q9v6p 1/1 Running 0 26s 10.0.10.209 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-sjr27 1/1 Running 0 53s 10.0.10.214 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-799d665bbd-vxqcb 1/1 Running 0 53s 10.0.10.106 10.0.10.248 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 참고문서 https://cablespaghetti.dev/2021/02/20/managing-multi-arch-kubernetes-clusters/\nhttps://medium.com/saas-infra/taints-and-tolerations-node-affinity-and-node-selector-explained-f329653c2bc6\n","lastmod":"2024-03-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/nodepool/1.arm-node-pool/","tags":["oke","arm","ampere","nodepool"],"title":"1.2.5.1 Ampere (Arm) Node Pool 추가하기"},{"categories":null,"contents":"1.3.1 Metrics Server 설치 Kubernetes Metrics Server는 클러스터 전체의 리소스 사용량 데이터 수집기입니다. Kubernetes Metrics Server는 서버는 각 Worker Node에서 실행되는 kubelet에서 리소스 지표를 수집하고 Kubernetes 지표 API를 통해 Kubernetes API 서버에 노출합니다.\nCPU 또는 메모리 기반의 Horizontal Pod Autoscaler와 Vertical Pod Autoscaler를 사용하기 위해서는 먼저 Metrics Server의 설치가 필요합니다.\n다음 명령으로 Metrics Server를 설치합니다.\n참고 - Kubernetes Metrics Server documentation kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.0/components.yaml 설치 결과를 확인합니다.\nkubectl get deployment metrics-server -n kube-system 노드 메트릭을 조회해 봅니다.\n$ kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% 10.0.10.158 206m 10% 2655Mi 17% 10.0.10.42 202m 10% 2090Mi 13% 10.0.10.43 92m 4% 2073Mi 13% Pod 메트릭을 조회해 봅니다.\n$ kubectl top pod NAMESPACE NAME CPU(cores) MEMORY(bytes) default my-nginx-6b7f675859-7fjph 0m 7Mi default my-nginx-6b7f675859-bq24p 0m 2Mi default my-nginx-6b7f675859-dl5hf 0m 2Mi ","lastmod":"2024-01-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/autoscaling/1.metric-server/","tags":["oke","metrics-server"],"title":"1.3.1 Metrics Server 설치"},{"categories":null,"contents":"1.2.3.1 Block Volume 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다. Block Volume을 Persistent Volume으로 사용되는 방법을 알아봅니다.\n기본 설치된 Persistent Volume을 위한 StorageClass 확인 OKE는 OCI IaaS를 기반으로 제공되는 서비스로 OCI Block Volume 서비스를 이용하여 Persistent Volume을 제공합니다. 현 버전 기준으로 FlexVolume 볼륨 플러그인과 CSI(Container Storage Interface) 볼륨 플러그인의 두 가지를 사용하고 있습니다.\n기본 StorageClass 확인\noci: OCI Block Volume 서비스를 위한 FlexVolume 플러그인 사용 oci-bv: OCI Block Volume 서비스를 위한 CSI 볼륨 플러그인 사용, OKE 디폴트 StorageClass $ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci oracle.com/oci Delete Immediate false 158m oci-bv (default) blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 158m CSI 볼륨 플러그인\nFlexVolume 플러그인 방식이 Kubernetes 1.2 버전때 부터 사용되어 더 오래되었지만, 업스트림 Kubernetes에서 이제는 CSI 볼륨 플러그인 방식을 사용하는 흐름입니다. OKE에서도 Release Note 2020년 8월 1일, Support for the Container Storage Interface (CSI) volume plug-in 에 나와 있는 것처럼 OKE에서도 신규 기능은 CSI 볼륨 플러그인에만 추가할 예정이며, FlexVolume 플러그인은 유지 보수만 할 계획입니다. 그래서 이하 설명에서는 CSI 볼륨 플러그인을 사용하는 oci-bv storageclass를 사용하겠습니다. FlexVolume 플러그인을 사용하는 oci storageclass에 대한 사항은 공식 문서를 참조바랍니다. OCI Block Volume용 CSI 볼륨 플러그인을 사용하여 Persistent Volume 만들어 사용하기 아래와 같이 PV 요청 yaml을 사용하여 요청합니다.\n# csi-bvs-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-bvs-pvc spec: storageClassName: \u0026#34;oci-bv\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 50Gi # Block Volume 최소 사이즈 테스트 앱 배포\n요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱 # nginx-deployment-bvs-pvc.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-bvs-pvc name: nginx-bvs-pvc spec: replicas: 1 selector: matchLabels: app: nginx-bvs-pvc template: metadata: labels: app: nginx-bvs-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: csi-bvs-pvc 생성 결과\n아래와 같이 정상적으로 PV 요청에 따라 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다. $ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created $ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-d4b81253-13ee-45e1-98f9-8b84b7970439 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 45s $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-5fd5849fd4-tvsx9 1/1 Running 0 75s Block Volume\nOCI 서비스 콘솔에서 Storage \u0026gt; Block Volumes 화면에서 보면 아래와 같이 PV용 Block Volume 만들어 졌으며, 구동된 Pod가 있는 Worker Node에 부착된 걸 알 수 있습니다.\n테스트\n아래와 같이 컨테이너 내부로 들어가 마운트 된 Persistent Volume내에 파일을 씁니다.\n$ kubectl exec -it nginx-bvs-pvc-5fd5849fd4-tvsx9 -- bash root@nginx-bvs-pvc-5fd5849fd4-tvsx9:/# echo \u0026#34;Hello PV\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt root@nginx-bvs-pvc-5fd5849fd4-tvsx9:/# cat /usr/share/nginx/html/hello_world.txt Hello PV root@nginx-bvs-pvc-5fd5849fd4-tvsx9:/# exit POD 삭제 후 재생성\nPersistent Volume이 유지되는 것을 확인하기 위해 기존 POD를 삭제하고 재생성되도록 합니다. 재생성되는 위치가 이전과 같은 Worker Node에 만들어 질 수도 있고, 그렇지 않을 수도 있습니다. Block Volume은 RWO 모드만 지원하여, 여러 Node에서 동시에 쓰기를 할 수 없습니다. 다른 Node에생성되는 경우, Multi-Attach error가 일시적으로 발생합니다. 기존 POD가 삭제되었다는 것을 인지하는 데 시간이 걸리며, POD가 생성후 Running 상태가 되기 까지 약간 시간이 걸립니다. $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-5fd5849fd4-tvsx9 1/1 Running 0 6m41s $ kubectl delete pod nginx-bvs-pvc-5fd5849fd4-tvsx9 pod \u0026#34;nginx-bvs-pvc-5fd5849fd4-tvsx9\u0026#34; deleted $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-5fd5849fd4-jst2z 0/1 ContainerCreating 0 1s $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-5fd5849fd4-jst2z 1/1 Running 0 6s 신규 POD에서 Persistent Volume 확인\n앞서 변경한 파일을 신규 POD에서 다시 조회해 보면 기존 내용이 남아 있는 걸 확인할 수 있습니다.\nkildong@cloudshell:~ (ap-chuncheon-1)$ kubectl exec -it nginx-bvs-pvc-5fd5849fd4-jst2z -- cat /usr/share/nginx/html/hello_world.txt Hello PV 참고사항\n앞선 테스트에서 처럼 ReadWriteOnce 접근 모드에서는 동시에 단일 Kubernetes Node에 있는 POD만 붙여서 사용할 수 있습니다. 다른 Node에 있는 POD에서 동일한 PV를 사용하려고 하면, 컨테이너 기동시 Multi-Attach 오류가 발생하며, 컨테이너가 기동이 되지 않습니다. 또한 Self-Healing으로 POD 재기동시 기존 POD는 종료되고, 새로운 POD가 다른 Worker Node에서 기동한 경우에, 기존 사용중인 POD가 없음을 인지하는 데 약간의 시간이 걸려 일시적으로 Multi-Attach 오류가 발생할 수 있습니다. ReadWriteMany 지원 여부 현재 버전 기준 CSI Driver for OCI Block Volume Service는 ReadWriteOnce만 지원합니다. 그래서 단일 Node에 멀티 Pod까지만 지원됩니다. 또한 위 작업 내용을 accessMode를 ReadWriteMany로 변경후 동일하게 수행하면 pod가 생성되지 않고 아래와 같이 에러가 나게 됩니다.\n$ kubectl get events --sort-by=.metadata.creationTimestamp LAST SEEN TYPE REASON OBJECT MESSAGE ... 1s Warning ProvisioningFailed persistentvolumeclaim/csi-bvs-pvc-2 failed to provision volume with StorageClass \u0026#34;oci-bv\u0026#34;: rpc error: code = InvalidArgument desc = invalid volume capabilities requested. Only SINGLE_NODE_WRITER is supported (\u0026#39;accessModes.ReadWriteOnce\u0026#39; on Kubernetes) ... ","lastmod":"2024-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/storage/block-volume/1.block-volume/","tags":["oke","block-volume"],"title":"1.2.3.1 Block Volume 사용하기"},{"categories":null,"contents":"4.1.1.1 OCI Cache with Redis 클러스터 만들기 OCI 콘솔에서 OCI Cache with Redis 서비스로 Redis 클러스터를 만들고, 클라이언트를 가정하여, redis-cli로 접속해 봅니다.\nRedis Cluster 만들기 OCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Database \u0026gt; Redis \u0026gt; Clusters로 이동합니다.\n클러스터 생성을 위해 Create cluster를 클릭합니다.\n기본 정보를 입력합니다.\nName: 클러스터 이름 입력, 예, my-redis-cluster Create in compartment: 위치할 컴파트먼트 선택 Redis software version: 현재는 7.0.5만 지원 다음으로 노드를 구성합니다.\nMemory (GB) per node: 노드당 할당할 메모리를 입력합니다. 2GB ~ 500GB를 지원하며, 여기서는 최소값을 선택합니다. 설정한 메모리에 따라 Bandwitdh (GBps)도 함께 변경됩니다. Node count: 노드 수를 입력합니다. 1개 ~ 5개 Primary (읽기/쓰기 가능) 하나에, 나머지는 Replica (읽기만 가능)로 생성됩니다. 다음으로 네트워크를 구성합니다.\n사용할 VCN과 서브넷을 선택합니다. 또는 새로 생성할 수 있습니다. 여기서는 기존 생성한 VCN의 Subnet을 선택합니다.\n생성된 Redis 클러스터는 현재 Private Endpoint만 제공하여, Public Subnet을 선택하셔도 인터넷에서 접속은 불가합니다.\n입력 정보를 리뷰한 후 클러스터를 생성합니다.\n참고로, 입력한 정보를 기준으로 클러스터 생성은 6분 50초 정도 걸렸습니다. 환경에 따라 달라 질 수 있습니다.\n생성된 클러스터\n현재 Private Endpoint만 제공하여, 인터넷에서 직접 접속은 불가합니다. 기본적으로는 같은 VCN 내에서만 접속이 가능합니다. Primary endpoint: 항상 클러스터의 Primary Node에 연결하는 Endpoint입니다. Replicas endpoint: 클러스터의 Replica 노드 중 하나로 연결하는 Endpoint입니다. Replica가 둘 이상인 경우, 연결시 이전과 다른 노드로 연결될 수 있습니다. Node endpoints: 각 노드로 직접 연결하는 Endpoint입니다. 각 노드로 연결이 잘 되는지 확인용으로 사용할 수 있습니다. Redis Cluster 연결하기 생성된 Redis Cluster의 Primary Endpoint로 접근이 가능한 위치에서는 클라이언트를 통해 직접 접속합니다. 아래 내용은 편의상 개발환경인 노트북 환경에서 Bastion 세션의 포트 포워딩을 통해 접속 테스트를 하는 과정입니다.\nredis-cli 설치 redis-cli를 설치하기 위해 Install Redis or Redis Stack 문서를 참조하여 사용할 환경에서 Redis를 설치합니다. 아래 내용은 macOS에 Redis를 설치하는 과정입니다.\n터미널에서 다음 명령을 실행합니다.\nbrew install redis 다음 명령으로 설치 확인합니다.\n$ redis-cli --version redis-cli 7.2.3 Security List에 규칙 등록 Redis 클러스터가 위치한 서브넷상의 Securit List에 Redis 서버 포트를 등록하여 개방할 필요가 있습니다.\nOCI 콘솔에 로그인합니다.\n클러스터가 속한 서브넷으로 이동합니다.\nSecurity List 목록에서 클러스터 생성시 만들어진 redis-security-list를 클릭합니다. 선택한 VCN내 없는 경우 생성됩니다.\nIngress Rules에 다음 규칙을 추가합니다.\nStateless: No, 체크안함\nService Type: CIDR\nSource CIDR: 클라이언트 대역대 입력, 예, 10.0.1.0/24\nIP Protocol: TCP\nSource Port Range: All\nDestination Port Range: 6379\nBastion Service 세션 만들기 redis-cli 또는 관련 클라이언트가 위치한 곳이 Redis 클러스터의 Private Endpoint를 접속 가능한 위치가 아닌 경우, 예를 들어, 인터넷상으로 접속하려는 경우 Bastion Server를 통한 경유가 필요합니다. 여기서는 Bastion Service를 사용하여, 경유해 보겠습니다.\nOCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Identity \u0026amp; Security \u0026gt; Bastion으로 이동합니다\n생성을 위해 Create bastion을 클릭합니다.\n아래와 같이 Bastion을 생성합니다.\nTarget VCN, Target Subnet: Redis 클러스터에 접속가능한 서브넷을 선택합니다. 여기서는 Redis 클러스터와 위치한 서브넷 선택 Enable FQDN Support and SOCKS5: Redis 클러스터의 Primary Endpoint 등이 FQDN으로 제공되므로 해당 사항 선택 CIDR block allowlist: Bastion을 접속할 클라이언트 IP의 CIDR입력 만든 Bastion 상세화면으로 이동합니다.\n생성이 완료되면, 사용할 세션을 만들기 위해 Create session을 클릭합니다.\n아래와 같이 세션을 생성합니다.\nConnect to the target host by using: Redis 클러스터의 Primary Endpoint의 FQDN을 사용하기 위해 Domain name을 선택 Domain Name: Redis 클러스터의 Primary Endpoint의 FQDN 입력 Port: 6379 Add SSH Key: 세션 연결시 사용할 SSH Key의 Public Key 입력 세션이 생성되면, 오른쪽 액션 메뉴를 클릭하여 View SSH Command를 확인합니다.\nredis-cli가 설치된 클라이언트에서 경로와 localPort를 6379로 하여 SSH command를 실행합니다. 오류없이 실행되는 것을 확인합니다. 실행 후 별다른 로그 없이 커서가 떠 있으면 됩니다.\nPermission denied (publickey) 오류가 발생하는 경우 세션 접속용 SSH command 실행시 접속 오류 - Permission denied (publickey) 참고하여 해결합니다. $ ssh -i ~/.ssh/id_rsa -N -L 6379:amaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxdgma-p.redis.ap-chuncheon-1.oci.oraclecloud.com:6379 -p 22 ocid1.bastionsession.oc1.ap-chuncheon-1.amaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxwpsq@host.bastion.ap-chuncheon-1.oci.oraclecloud.com Redis 클러스터의 Primary Endpoint에 대해서 로컬 포트로 포트포워딩이 되었습니다.\nredis-cli로 Redis 클러스터에 접속하기 redis-cli로 포트 포워딩된 localhost, 6379 포트로 접속합니다. 접속후 ping 명령을 수행하면, 응답이 오는 것을 확인할 수 있습니다.\n\u0026ndash;tls: OCI Cache with Redis로 만들어진 Redis 클러스터는 \u0026ndash;tls 옵션은 반드시 사용합니다. -h: 대상 Endpoint를 입력합니다. 여기서는 포트포워딩 되었기 때문에 localhost로 입력합니다. $ redis-cli --tls -h localhost localhost:6379\u0026gt; ping PONG localhost:6379\u0026gt; 테스트로 SET key value 순으로 아래와 같이 입력하고 GET key 명령으로 조회합니다.\nlocalhost:6379\u0026gt; SET hkey \u0026#34;Hello World!\u0026#34; OK localhost:6379\u0026gt; GET hkey \u0026#34;Hello World!\u0026#34; redis-cli로 OCI Cache with Redis 서비스로 생성한 Redis 클러스터에 연결을 확인하였습니다.\n","lastmod":"2023-11-16T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/databases/redis/cache-with-redis/1.cache-with-redis/","tags":["cache","redis"],"title":"4.1.1.1 OCI Cache with Redis 클러스터 만들기"},{"categories":null,"contents":"4.2.1 Oracle Autonomous Database Free Container Image 지난 2023년 9월, 개발자를 위해 Autonomous Database를 OCI가 아닌, 개인 데스크탑/랩탑에서 실행할 수 있는 Autonomous Database Free Container Image를 출시하였습니다. 로컬 PC에 docker CLI 호환 도구를 통해 실행할 수 있습니다.\nOCI Always Free를 통해 2개의 Autonomous Database 인스턴스를 제공하지만, 개발자가 많고, 개별 데스트탑에 설치하여 테스트하고자 하는 경우, 인터넷이 차단된 에어갭 환경에서 Autonomous Database를 개발자가 테스트하고자 하는 경우 등에 사용할 수 있습니다.\nadb-free GitHub OCI Blog - Introducing Autonomous Database Free Container Image Use the Oracle Autonomous Database Free Container Image Autonomous Transaction Processing과 Autonomous Data Warehouse 두 유형에 해당하는 MY_ATP, MY_ADW 두 인스턴스을 포함하고 있습니다. 로컬에 실행하는 것으로 Oracle Autonomous Database의 다음 기능을 지원합니다.\nOracle Rest Data Services (ORDS) APEX Database Actions Mongo API enabled (by default routed to MY_ATP) 각 데이터데이스의 스토리지 사이즈는 최대 20GB입니다.\n실행 요구 사항 4 CPU 8 GB 메모리 필요 Macbook M1 ARM64에서 실행 예시 컨테이너 실행전 요구사항 설치 Docker Desktop 라이선스 이슈로 공식문서에서 예시는 podman을 사용하고 있습니다. 예시에서는 Rancher Desktop이 설치된 상태를 기준으로 이후 필요한 과정입니다.\nRancher Desktop이 설치\n현재 Native ARM은 지원하지 않아, x86_64를 에뮬레이션 하기 위해 colima가 필요합니다. 아래와 같이 설치합니다.\nbrew install colima brew reinstall qemu docker context가 colima인지 확인합니다.\n$ docker context ls NAME DESCRIPTION DOCKER ENDPOINT ERROR colima * colima unix:///Users/thekoguryo/.colima/default/docker.sock default Current DOCKER_HOST based configuration unix:///var/run/docker.sock rancher-desktop Rancher Desktop moby context unix:///Users/thekoguryo/.rd/docker.sock 필요시 다음 명령으로 context를 전환합니다.\ndocker context use colima 요구 사항에 맞게 colima를 시작\n$ colima start --cpu 4 --memory 8 --arch x86_64 INFO[0000] starting colima INFO[0000] runtime: docker INFO[0000] preparing network ... context=vm INFO[0000] starting ... context=vm INFO[0232] provisioning ... context=docker INFO[0232] starting ... context=docker INFO[0239] done Autonomous Database Free Container 실행하기 Autonomous Database Free Container Image는 GitHub Container Registry 및 OCR(Oracle Container Registry)에서 제공합니다.\ndocker pull container-registry.oracle.com/database/adb-free:latest 또는 docker pull ghcr.io/oracle/adb-free:latest Autonomous Database Free Container 실행\ndocker run -d \\ -p 1521:1522 \\ -p 1522:1522 \\ -p 8443:8443 \\ -p 27017:27017 \\ --hostname localhost \\ --cap-add SYS_ADMIN \\ --device /dev/fuse \\ --name adb_container \\ container-registry.oracle.com/database/adb-free:latest 실행결과 확인\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES da5d47178568 container-registry.oracle.com/database/adb-free:latest \u0026#34;/bin/bash -c /u01/s…\u0026#34; 10 seconds ago Up 6 seconds (health: starting) 0.0.0.0:1522-\u0026gt;1522/tcp, :::1522-\u0026gt;1522/tcp, 0.0.0.0:8443-\u0026gt;8443/tcp, :::8443-\u0026gt;8443/tcp, 0.0.0.0:27017-\u0026gt;27017/tcp, :::27017-\u0026gt;27017/tcp, 0.0.0.0:1521-\u0026gt;1522/tcp, :::1521-\u0026gt;1522/tcp adb_container 데이터베이스가 오픈 되고 ORDS가 기동될 때까지 기다립니다.\n$ docker logs -f da5d47178568 LSNRCTL for Linux: Version 19.0.0.0.0 - Production on 19-OCT-2023 03:43:24 Copyright (c) 1991, 2023, Oracle. All rights reserved. ... SQL*Plus: Release 19.0.0.0.0 - Production on Thu Oct 19 03:43:25 2023 Version 19.20.0.1.0 Copyright (c) 1982, 2023, Oracle. All rights reserved. Connected to an idle instance. SQL\u0026gt; ORACLE instance started. Total System Global Area 1610612192 bytes Fixed Size\t9543136 bytes Variable Size\t385875968 bytes Database Buffers\t1207959552 bytes Redo Buffers\t7233536 bytes Database mounted. Database opened. SQL\u0026gt; Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production ... 2023-10-19T08:40:41.070Z INFO Oracle REST Data Services initialized Oracle REST Data Services version : 23.2.2.r2081534 Oracle REST Data Services server info: jetty/10.0.15 Oracle REST Data Services java info: Java HotSpot(TM) 64-Bit Server VM 11.0.19+9-LTS-224 APEX, Database Actions에 접속되는 지 확인합니다.\n컨테이너 기동시 \u0026ndash;hostname을 localhost가 아닌 다른 이름인 경우 해당 주소로 변경하여 접속합니다. Application MY_ATP MY_ADW APEX https://localhost:8443/ords/my_atp/ https://localhost:8443/ords/my_adw/ Database Actions https://localhost:8443/ords/my_atp/sql-developer https://localhost:8443/ords/my_adw/sql-developer APEX 접속 예시\nDatabase Actions\n설치후 Admin 암호는 만료된 상태로 최초 접속시 패스워드 변경이 필요합니다. MY_ATP, MY_ADW 별도로 각각 변경합니다.\nDatabase Expired ADMIN password MY_ATP Welcome_MY_ATP_1234 MY_ADW Welcome_MY_ADW_1234 변경 방법\ndocker exec \u0026lt;container_id\u0026gt; /u01/scripts/change_expired_password.sh \u0026lt;MY_Database\u0026gt; \u0026lt;user\u0026gt; \u0026lt;old_password\u0026gt; \u0026lt;new_password\u0026gt; 예시\n$ docker exec adb_container /u01/scripts/change_expired_password.sh MY_ATP admin Welcome_MY_ATP_1234 Welcome_MY_ATP_1235 SQL*Plus: Release 19.0.0.0.0 - Production on Thu Oct 19 04:08:43 2023 Version 19.20.0.1.0 Copyright (c) 1982, 2023, Oracle. All rights reserved. ERROR: ORA-28001: the password has expired Changing password for admin New password: Retype new password: Password changed Connected to: Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production Version 19.20.0.1.0 SQL\u0026gt; Disconnected from Oracle Database 19c Enterprise Edition Release 19.0.0.0.0 - Production Version 19.20.0.1.0 Database Actions에 접속하여 변경한 암호로 로그인해 봅니다.\nOCI Autonomouse Database에서 제공하는 Database Actions 상의 도구들을 확인할 수 있습니다.\n호스트에서 SQL Developer로 접속해 보기 개발자 데스크탑/랩탑에 사용할 클라이언트를 가정하여, SQL Developer로 아래와 같이 접속할 수 있습니다.\n접속을 위해 실행중인 adb_container 컨테이너에서 Wallet을 가져옵니다.\ndocker cp adb_container:/u01/app/oracle/wallets/tls_wallet . zip tls_wallet.zip ./tls_wallet/* SQL Developer를 실행합니다.\n새 데이터베이스 접속을 만듭니다.\n접속유형: 클라우드 전자 지갑 선택 구성 파일: 앞서 가져온 Wallet Zip 파일 선택 서비스: 접속할 서비스 명을 선택, MY_ATP, MY_ADW를 구분하여 원하는 서비스 선택 사용자 이름/비밀번호: admin, 변경한 암호 입력 테스트가 성공하면, 입력한 데이터 베이스 접속을 저장하고 접속합니다.\n유저 만들기 SQL Developer로 생성한 Connection으로 접속합니다.\n아래 명령으로 사용할 애플리케이션 유저를 만들 수 있습니다.\nCREATE USER APP_USER IDENTIFIED BY \u0026#34;\u0026lt;my_app_user_password\u0026gt;\u0026#34; QUOTA UNLIMITED ON DATA; -- ADD ROLES GRANT CONNECT TO APP_USER; GRANT CONSOLE_DEVELOPER TO APP_USER; GRANT DWROLE TO APP_USER; GRANT RESOURCE TO APP_USER; -- ENABLE REST BEGIN ORDS.ENABLE_SCHEMA( p_enabled =\u0026gt; TRUE, p_schema =\u0026gt; \u0026#39;APP_USER\u0026#39;, p_url_mapping_type =\u0026gt; \u0026#39;BASE_PATH\u0026#39;, p_url_mapping_pattern =\u0026gt; \u0026#39;app_user\u0026#39;, p_auto_rest_auth=\u0026gt; TRUE ); commit; END; / -- QUOTA ALTER USER APP_USER QUOTA UNLIMITED ON DATA; 실행 예시 ","lastmod":"2023-10-19T00:00:02Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/databases/oracle-database/1.adb-free/","tags":["container","free","database","developer"],"title":"4.2.1 Oracle Autonomous Database Free Container Image"},{"categories":null,"contents":"8.1 GraalVM 사용하기 GraalVM은 고급 JIT(Just-In-Time) 최적화 컴파일러를 추가하여 자바 애플리케이션의 성능 향상을 제공하는 자바 런타임입니다. Just-In-Time 방식으로 자바 바이트코드를 실행시점에 머신코드로 변환하여 실행할때, 개선된 최적화 기법을 사용합니다. 여기에 추가하여 AOT(Ahead-Of-Time) 방식도 지원하여, JVM 없이 바로 실행할수 있도록, 바이트 코드가 아닌, 실행가능한 Native Image 생성 기능을 제공합니다.\nAOT(Ahead-Of-Time) 방식은 Native Image을 생성하는 방식이라, 실행시점에 머신코드를 최적화하여 변경할 수 없으므로, Native Image 생성시점에, 분석한 코드를 기반으로 최적화합니다. 네이티브 이미지를 실행하므로 초기 기동이 빨라, 빠른 기동과 기동 즉시 실행되어야 하는 Serverless, 컨테이너 애플리케이션 등에 유용합니다.\n그리고 자바스크립트, 루비, R과 같은 다른 언어를 실행할 수 있는 Poliglot 환경을 제공하여 서로 다른 언어가 동일 환경에서 구동할 수 있는 기능을 제공합니다.\n향상된 JIT(Just-In-Time) 컴파일러를 통한 성능향상 코드 변경 없이 기존 JDK에 비해 자바 애플리케이션의 성능 개선을 가져올 수 있습니다. Accelerating Java performance Native Image를 통한 빠른 초기 기동 및 실행 지원 Native Image는 Serverless 함수, 컨테이너 등에서 빠른 기동을 제공하지만, 사용하는 개발 프레임워크에서 지원하여야 합니다. MicroNaut, Helidon, Quarkus 등에서 지원하며, Spring 프레임워크에서도 3.0부터 Spring Native를 통해 지원합니다. Spring Boot 3.0 Goes GA 여러 언어를 동일 환경에서 실행할 수 있는 기능 제공 GraalVM PM이 트위터(tweet by Thomas Würthinger)에서 JIT와 AOT간 트레이드 오프을 그림과 같이 설명하고 있습니다.\n참고\nOracle GraalVM이란? Oracle GraalVM for JDK GraalVM 에디션\n최근 GraalVM 라이선스 방식이 업데이트 이전에는 아래 표와 같이 Community 버전과 Enterprise 버전으로 나뉘었습니다. GraalVM Enterprise 에디션은 추가적인 기능을 통해 Community 버전에 비해 더 뛰어난 성능을 제공합니다. 하지만, GraalVM Enterprise 에디션은 운영에서 사용하기 위해서는 상업용 라이센스가 필요했습니다.\nCommunity 버전과 Enterprise 버전 비교\nGraalVM 무료 라이센스 발표\n2023년 6월 발표 GraalVM 무료 라이선스 인 GraalVM Free Terms and Conditions (GFTC) 발표와 함께 GraalVM Enterprise 버전은 이제 Oracle GraalVM for JDK 17, Oracle GraalVM for JDK 20로 변경되었습니다. GFTC 라이선스에 따라 상업적 용도를 포함한 모든 사용자가 무료로 사용할 수 있게 변경되었습니다. 분기별 보안 업데이트를 포함한 후속 릴리즈 또한 무료로 제공합니다.\n블로그 내용을 번역해 보면 다음과 같습니다.\n이전 Oracle GraalVM Enterprise을 이제 JDK 버전에 맞춰 Oracle GraalVM for JDK 17, Oracle GraalVM for JDK 20으로 출시하여, 이후 모든 분기별 보안 업데이트를 포함한 후속 릴리스를 무료로 제공합니다. 이러한 릴리스는 GraalVM 무료 이용 약관(GFTC) 라이선스에 따라 제공됩니다. 본 라이센스는 해당 조건에 따라 모든 사용자에게 무료 사용을 허용하며, 운영으로도 사용할 수 있습니다. 유료가 아니라면 재배포는 허용됩니다. 지정된 LTS 버전(예, Oracle GraalVM for JDK 17)의 경우, 후속 LTS 릴리스 후 1년까지 해당 버전을 제공합니다. 개발자와 조직은 무료 라이센스 발표 이전에 있었던, 클릭기반 라이선스 계약 동의 과정 없이, 쉽게 Oracle GraalVM을 다운로드하고, 사용, 공유 및 재배포할 수 있습니다. Oracle의 OpenJDK 빌드처럼동일한 조건에 따라 GPL 라이센스로 GraalVM Community Edition 릴리스를 계속 제공할 것입니다. 엔터프라이즈 환경에서 오라클 프리미어 서포트와 My Oracle Support를 통한 기술지원이 필요한 경우에는 여전히 Java SE 구독이 필요합니다.\nOCI에서 Java SE 및 GraalVM 사용에 대한 권한 Oracle Cloud Infrastructure 서비스는 Java SE 구독과 기술지원을 포함하고 있습니다. 또한 Java SE 구독에는 GraalVM 엔터프라이즈가 포함되어 있습니다. 따라서 OCI 서비스 상에서는 GraalVM을 추가비용없이 사용할 수 있습니다.\nOracle Java SE Subscription DataSheet GraalVM 기술지원관련\nGraalVM은 Java SE 구독에 포함되어 기술지원합니다. Experimental 기능은 운영환경에 사용하는 것을 권장하지 않으며, 기술지원하지 않습니다. 정확한 사항은 아래 링크를 참조하세요.\nOracle GraalVM for JDK 21 - Support \u0026amp; Experimental Components 새로운 Oracle GraalVM 컨테이너 이미지 발표 지난 9월 5일 오라클 블로그를 통해 New Oracle GraalVM Container Images를 발표하였습니다.\n커뮤니티 버전의 컨테이너 이미지는 GraalVM Community Edition Container Images에서 보듯이 여전히 사용할 수 있습니다. GraalVM Enterprise 버전 컨테이너 이미지는 Oracle Container Registry (OCR) 를 통해 제공하였지만, 라이선스 동의과정이 필요하여, OTN 사이트 가입후 Oracle Account로 로그인한 후 라이선스 동의 절차를 거치는 과정이 필요하였습니다. 또한 인증이 필요한 Private Repository 형태로 제공하였습니다.\nNew Oracle GraalVM Container Images를 발표 이후에는 동일하게 Oracle Container Registry (OCR) 를 통해 이미지를 제공하지만, GFTC 라이선스 하에, Public Repository로 형태로 제공합니다. OTN 사이트 가입이나, 추가 인증없이 바로 사용할 수 있습니다.\n지난 9월 Oracle GraalVM for JDK 21 출시와 함께 현재 Oracle GraalVM for JDK 17, Oracle GraalVM for JDK 21 두 가지 버전을 jdk, native-image로 제공합니다.\n아래와 같이 이미지를 Docker CLI로 가져올 수 있습니다.\ndocker pull container-registry.oracle.com/graalvm/jdk:17 docker pull container-registry.oracle.com/graalvm/jdk:21 native-image cli가 포함된 이미지는 -muslib를 태그에 추가해야 합니다.\ndocker pull container-registry.oracle.com/graalvm/native-image:17-muslib docker pull container-registry.oracle.com/graalvm/native-image:21-muslib 참고 Get Started with Oracle GraalVM Container Images ","lastmod":"2023-10-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/appdev/1.use-graal-vm/","tags":["appdev","graalvm"],"title":"8.1 GraalVM 사용하기"},{"categories":null,"contents":"1.10.1 Worker Node 디스크 사이즈 늘리기 용량이 큰 컨테이너 이미지들을 사용하거나, 이미지를의 합계 사이즈가 큰 경우, 실행을 위해 컨테이너 이미지를 Worker Node로 가져오다 오류가 날수 있습니다. 이를 방지하기 위해 Worker Node의 디스크 사이즈를 다음과정을 통해 늘립니다.\nWorker Node Boot Volume 사이즈 늘리기 Worker Node의 Boot Volume의 기본 사이즈는 50GB입니다. 클러스터 생성시 advance options 또는 Node Pool 설정화면에서 Boot volume 사이즈를 늘립니다.\nWorker Node에 SSH로 접속하여, 디스크 정보를 보면 다음과 같습니다.\nsda의 사이즈는 늘어났지만, 실제 파티션된 볼륨 사이즈는 변경되지 않아, 45.5G로 전체를 다 사용하지 못하고 있습니다. [opc@oke-c7fk4xxodaa-nk4a2kfionq-sfwfrelxbnq-0 ~]# sudo lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 100G 0 disk ├─sda1 8:1 0 100M 0 part /boot/efi ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 45.5G 0 part ├─ocivolume-root 252:0 0 35.5G 0 lvm / └─ocivolume-oled 252:1 0 10G 0 lvm /var/oled 루트 파티션을 확장할 수 있습니다. Extending the Root Partition on a Linux-Based Image을 참고하여 oci-growfs 명령으로 루트 파티션을 늘립니다.\nsudo /usr/libexec/oci-growfs -y 실행 예시 [opc@oke-c7fk4xxodaa-nk4a2kfionq-sfwfrelxbnq-0 ~]$ sudo /usr/libexec/oci-growfs -y Volume Group: ocivolume Volume Path: /dev/ocivolume/root Mountpoint Data --------------- mountpoint: / source: /dev/mapper/ocivolume-root filesystem type: xfs source size: 35.5G type: lvm size: 35.5G physical devices: [\u0026#39;/dev/sda3\u0026#39;] physical volumes: [\u0026#39;/dev/sda\u0026#39;, \u0026#39;/dev/sda\u0026#39;] partition number: [\u0026#39;3\u0026#39;] volume group name: ocivolume volume group path: /dev/ocivolume/root Partition dry run expansion \u0026#34;/dev/sda3\u0026#34; succeeded. CHANGE: partition=3 start=2304000 old: size=95371264 end=97675264 new: size=207411167 end=209715167 Expanding partition /dev/sda3: Confirm? Partition expand expansion \u0026#34;/dev/sda3\u0026#34; succeeded. ... 확장후 다시 조회하면, sda3의 사이즈가 98.9G로 늘어난 것을 볼 수 있습니다.\n[opc@oke-c7fk4xxodaa-nk4a2kfionq-sfwfrelxbnq-0 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 100G 0 disk ├─sda1 8:1 0 100M 0 part /boot/efi ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 98.9G 0 part ├─ocivolume-root 252:0 0 88.9G 0 lvm / └─ocivolume-oled 252:1 0 10G 0 lvm /var/oled cloud-init을 사용하여, Worker Node 생성시 자동으로 확장하기 클러스터 생성시 advance options 또는 Node Pool 설정화면에서 Boot volume 사이즈를 늘립니다.\nWorker Node 생성시 커스텀 cloud-init 스크립트를 실행할 수 있습니다.\n참조문서: Using Custom Cloud-init Initialization Scripts to Set Up Managed Nodes 문서상에 있는 디폴트 시작 스크립트에 oci-growfs 명령을 추가해 주면 됩니다.\n#!/bin/bash curl --fail -H \u0026#34;Authorization: Bearer Oracle\u0026#34; -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode \u0026gt;/var/run/oke-init.sh bash /var/run/oke-init.sh sudo /usr/libexec/oci-growfs -y 클러스터 생성시 advance options 또는 Node Pool 설정화면에서 Initialization Script에 복사합니다.\n추가된 내용으로 클러스터를 생성하거나, 기존 Node Pool인 경우 Worker Node를 재생성합니다.\n생성된 Worker Node에 SSH로 접속해 cloud-init-output 로그를 확인하면, 추가된 스크립트가 실행된 것을 볼 수 있습니다.\n$ cat /var/log/cloud-init-output.log ... Mon Jul 17 09:40:21 UTC 2023 Finished OKE bootstrap Volume Group: ocivolume Volume Path: /dev/ocivolume/root ... CHANGED: partition=3 start=2304000 old: size=95371264 end=97675264 new: size=207411167 end=209715167 Extending /dev/sda3 succeeded. Device /dev/sda3 extended successfully. Logical volume /dev/ocivolume/root extended successfully. Cloud-init v. 22.1-6.0.4.el8_7.2 running \u0026#39;modules:final\u0026#39; at Mon, 17 Jul 2023 09:39:26 +0000. Up 23.69 seconds. Cloud-init v. 22.1-6.0.4.el8_7.2 finished at Mon, 17 Jul 2023 09:40:23 +0000. Datasource DataSourceOracle. Up 80.32 seconds cloud-init이 실행되어 루트 볼륨이 늘어난 것을 확인할 수 있습니다.\n[opc@oke-c7fk4xxodaa-nk4a2kfionq-sfwfrelxbnq-0 ~]$ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sda 8:0 0 100G 0 disk ├─sda1 8:1 0 100M 0 part /boot/efi ├─sda2 8:2 0 1G 0 part /boot └─sda3 8:3 0 98.9G 0 part ├─ocivolume-root 252:0 0 88.9G 0 lvm / └─ocivolume-oled 252:1 0 10G 0 lvm /var/oled ","lastmod":"2023-07-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/customize/1.resize-worker-node/","tags":["oke"],"title":"1.10.1 Worker Node 디스크 사이즈 늘리기"},{"categories":null,"contents":"5.1 게임서비스를 위한 Agones 테스트 게임 서버를 쿠버네티스에서 서비스하기 많이 사용되는 Agones가 OKE에서 잘 동작하는 확인하기 위한 테스트입니다. Agones 공식문서를 참조하여 진행하였습니다.\nOKE Cluster 1.26에서 VCN-Native Pod Networking, Flannel 모두 동작함을 확인하였습니다.\nAgones 설치를 위한 OKE 클러스터 준비 OKE Cluster 지원 버전 확인\nAgones and Kubernetes Supported Versions을 참고하여 지원하는 쿠버네티스 버전을 확인하고 준비합니다. 여기서는 최신버전인 Agones 1.33을 OKE Cluster 1.26에 설치하겠습니다. Agones version Kubernetes version(s) 1.33 1.24, 1.25, 1.26 Agones 설치후 생성되는 GameServer에 할당되는 주소는 Worker Node의 IP가 할당됩니다. GameServer에 Public IP 할당이 필요한 경우, OKE Cluster 생성시 Worker Node를 Public Subnet에 배치합니다.\n참고\nHow are IP addresses allocated to GameServers?\nEach GameServer inherits the IP Address of the Node on which it resides. If it can find an ExternalIP address on the Node (which it should if it’s a publicly addressable Node), that it utilised, otherwise it falls back to using the InternalIP address.\nCreate Cluster - Quick Mode 시\n클라이언트가 GameServer에 접속할 수 있도록, Worker Nodes가 속한 서브넷에 UDP 프로토콜로 7000-8000 포트들을 개방합니다.\nA Kubernetes cluster with the UDP port range 7000-8000 open on each node. 참고 - AGONES \u0026gt; Create a Game Server \u0026gt; Prerequisites Agones를 OKE 클러스터에 설치하기 Helm Chart를 이용해 설치합니다.\nhelm repo add agones https://agones.dev/chart/stable helm repo update helm install my-release --namespace agones-system --create-namespace agones/agones --version 1.33.0 설치 상태를 확인합니다.\nkubectl get pods --namespace agones-system 확인 결과 예시\n$ kubectl get pods --namespace agones-system NAME READY STATUS RESTARTS AGE agones-allocator-56885bfc8f-czz99 1/1 Running 0 79s agones-allocator-56885bfc8f-dxm2h 1/1 Running 0 79s agones-allocator-56885bfc8f-t9k5v 1/1 Running 0 79s agones-controller-587bdb474-6k5m6 1/1 Running 0 78s agones-controller-587bdb474-xlpgl 1/1 Running 0 78s agones-extensions-7c785c6b8c-2z2tg 1/1 Running 0 78s agones-extensions-7c785c6b8c-qhdkw 1/1 Running 0 78s agones-ping-7486954d88-5xrzh 1/1 Running 0 78s agones-ping-7486954d88-mnjzf 1/1 Running 0 78s GameServer 설치 테스트로 배포할 GameServer의 YAML 파일입니다.\napiVersion: \u0026#34;agones.dev/v1\u0026#34; kind: GameServer metadata: generateName: \u0026#34;simple-game-server-\u0026#34; spec: ports: - name: default portPolicy: Dynamic containerPort: 7654 template: spec: containers: - name: simple-game-server image: us-docker.pkg.dev/agones-images/examples/simple-game-server:0.16 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;20m\u0026#34; limits: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;20m\u0026#34; 해당 YAML을 배포하여 테스트용 GameServer를 설치합니다.\nkubectl create -f https://raw.githubusercontent.com/googleforgames/agones/release-1.33.0/examples/simple-game-server/gameserver.yaml Worker Node의 Public IP 확인\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE 10.0.10.105 Ready node 8m34s v1.26.2 10.0.10.105 146.56.xx.xxx ... 10.0.10.233 Ready node 8m51s v1.26.2 10.0.10.233 144.24.xx.xx ... 10.0.10.29 Ready node 9m22s v1.26.2 10.0.10.29 158.180.xx.xxx ... 생성된 GameServer를 확인합니다.\nGameServer의 주소(ADDRESS)가 해당 Pod가 속한 NODE(10.0.10.29)의 EXTERNAL-IP인 것을 확인할 수 있습니다. 각 노드의 7000-8000 범위로 UDP로 오픈됩니다. 아래와 같이 GameServer Kind가 Pod에 대응됩니다. $ kubectl get gameservers NAME STATE ADDRESS PORT NODE AGE simple-game-server-pkj5d Ready 158.180.xx.xxx 7365 10.0.10.29 87s $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES simple-game-server-pkj5d 2/2 Running 0 7m22s 10.244.0.6 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; GameServer 연결확인을 위해 Pod 로그를 확인합니다.\nkubectl logs -f simple-game-server-pkj5d --all-containers GameServer에 nc 명령을 통해 메세지를 전송해 봅니다.\n-u 옵션을 통해 UDP로 게임서버 포트로 연결되는 것을 테스트합니다. Hello World ! 를 전달시 아래와 같이 서버에서 에코 응답이 오는 것을 볼 수 있습니다. $ nc -u 158.180.xx.xxx 7365 Hello World ! ACK: Hello World ! EXIT ACK: EXIT ^C GameServer Pod 로그에서도 수신된 메시지를 확인할 수 있습니다.\n$ kubectl logs -f simple-game-server-pkj5d --all-containers 2023/07/14 08:50:39 Creating SDK instance 2023/07/14 08:50:39 Starting Health Ping 2023/07/14 08:50:39 Marking this server as ready 2023/07/14 08:50:39 Starting UDP server, listening on port 7654 ... 2023/07/14 09:01:59 Health Ping 2023/07/14 09:02:01 Received packet from 129.213.xxx.xxx:37487: Hello World ! 2023/07/14 09:02:01 Received UDP: Hello World ! 2023/07/14 09:13:09 Health Ping 2023/07/14 09:13:11 Received packet from 129.213.xxx.xxx:43110: EXIT 2023/07/14 09:13:11 Received UDP: EXIT 2023/07/14 09:13:11 Received EXIT command. Exiting. 끝에 EXIT를 입력하면, GameServer가 종료되고, Pod도 종료됩니다.\n$ kubectl get gameserver No resources found in default namespace. Fleet 앞서 GameServer 실행시 EXIT가 되면, GameServer가 종료되었습니다. 안정적인 게임 서비스를 위해 Fleet를 사용하면, 특정 Pod에서 게임이 종료되어도, Fleet를 통해 지정한 GameServer를 유지하게 됩니다.\n테스트로 배포할 Fleet YAML 파일입니다. 2개의 GameServer를 만드는 예제입니다.\nFleet 형식에서 보듯 Deployment에 해당된다고 보면 됩니다. apiVersion: \u0026#34;agones.dev/v1\u0026#34; kind: Fleet metadata: name: simple-game-server spec: replicas: 2 template: spec: ports: - name: default containerPort: 7654 template: spec: containers: - name: simple-game-server image: us-docker.pkg.dev/agones-images/examples/simple-game-server:0.16 resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;20m\u0026#34; limits: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;20m\u0026#34; 해당 YAML을 배포하여 Fleet를 생성합니다.\nkubectl apply -f https://raw.githubusercontent.com/googleforgames/agones/release-1.33.0/examples/simple-game-server/fleet.yaml 생성결과를 확인합니다. Fleet에 의해 2개의 GameServer가 생성되었습니다.\nGameServer의 Deployment 역할 해주는 것이 Fleet으로 생각하면 됩니다. $ kubectl get fleet NAME SCHEDULING DESIRED CURRENT ALLOCATED READY AGE simple-game-server Packed 2 2 0 2 19s $ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-dbl9v Ready 158.180.xx.xxx 7392 10.0.10.29 32s simple-game-server-fvcsc-ldsg5 Ready 158.180.xx.xxx 7023 10.0.10.29 32s Scale Out 합니다.\nkubectl scale fleet simple-game-server --replicas=5 실행결과\n$ kubectl scale fleet simple-game-server --replicas=5 fleet.agones.dev/simple-game-server scaled $ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-4xp4r Ready 158.180.xx.xxx 7223 10.0.10.29 12s simple-game-server-fvcsc-7pfgk Ready 158.180.xx.xxx 7123 10.0.10.29 12s simple-game-server-fvcsc-dbl9v Ready 158.180.xx.xxx 7392 10.0.10.29 4m31s simple-game-server-fvcsc-ldsg5 Ready 158.180.xx.xxx 7023 10.0.10.29 4m31s simple-game-server-fvcsc-wnz4d Ready 158.180.xx.xxx 7234 10.0.10.29 12s GameServer를 할당합니다.\nkubectl create -f https://raw.githubusercontent.com/googleforgames/agones/release-1.33.0/examples/simple-game-server/gameserverallocation.yaml -o yaml 할당후 GameServer 상태를 보면, 하나의 GameServer의 상태가 Allocated로 변경되었습니다.\n$ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-4xp4r Allocated 158.180.xx.xxx 7223 10.0.10.29 4m58s simple-game-server-fvcsc-7pfgk Ready 158.180.xx.xxx 7123 10.0.10.29 4m58s simple-game-server-fvcsc-dbl9v Ready 158.180.xx.xxx 7392 10.0.10.29 9m17s simple-game-server-fvcsc-ldsg5 Ready 158.180.xx.xxx 7023 10.0.10.29 9m17s simple-game-server-fvcsc-wnz4d Ready 158.180.xx.xxx 7234 10.0.10.29 4m58s Scale In 합니다.\nkubectl scale fleet simple-game-server --replicas=0 할당된 서버는 삭제되지 않는 것을 볼 수 있습니다.\n$ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-4xp4r Allocated 158.180.xx.xxx 7223 10.0.10.29 5s 할당된 GameServer에 접속하여 앞서와 동일하게 테스트후 EXIT합니다.\n$ kubectl get gameservers | grep Allocated | awk \u0026#39;{print $3\u0026#34;:\u0026#34;$4 }\u0026#39; 158.180.xx.xxx:7223 $ nc -u 158.180.xx.xxx 7223 Hello World ! ACK: Hello World ! EXIT ACK: EXIT ^C 현재 GameServer를 다시 조회합니다. 할당된 GameServer의 서비스가 종료되었습니다.\n$ kubectl get gameserver No resources found in default namespace. Fleet Autoscaler 테스트로 배포할 Fleet Autoscaler YAML을 확인합니다.\napiVersion: \u0026#34;autoscaling.agones.dev/v1\u0026#34; kind: FleetAutoscaler metadata: name: simple-game-server-autoscaler spec: fleetName: simple-game-server policy: type: Buffer buffer: bufferSize: 2 minReplicas: 0 maxReplicas: 10 해당 YAML을 배포하여 Fleet Autoscaler를 배포합니다.\nkubectl apply -f https://raw.githubusercontent.com/googleforgames/agones/release-1.33.0/examples/simple-game-server/fleetautoscaler.yaml 현재 상태를 확인합니다. bufferSize: 2에 의해 GameServer 2개로 스케일되었습니다.\n$ kubectl describe fleetautoscaler simple-game-server-autoscaler ... Spec: Fleet Name: simple-game-server Policy: Buffer: Buffer Size: 2 Max Replicas: 10 Min Replicas: 0 Type: Buffer Sync: Fixed Interval: Seconds: 30 Type: FixedInterval Status: Able To Scale: true Current Replicas: 2 Desired Replicas: 2 Last Scale Time: 2023-07-14T09:52:19Z Scaling Limited: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal AutoScalingFleet 3m25s fleetautoscaler-controller Scaling fleet simple-game-server from 0 to 2 GameServer를 할당합니다.\nkubectl create -f https://raw.githubusercontent.com/googleforgames/agones/release-1.33.0/examples/simple-game-server/gameserverallocation.yaml -o yaml Fleet Autoscaler의 현재 상태를 확인합니다. 스케일되어 3개가 되었습니다. 할당된 것을 제외하고 여전히 2개가 Ready 상태입니다.\n$ kubectl describe fleetautoscaler simple-game-server-autoscaler ... Status: Able To Scale: true Current Replicas: 2 Desired Replicas: 3 Last Scale Time: 2023-07-14T09:57:49Z Scaling Limited: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Normal AutoScalingFleet 19s fleetautoscaler-controller Scaling fleet simple-game-server from 2 to 3 $ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-6btkj Ready 158.180.xx.xxx 7232 10.0.10.29 34s simple-game-server-fvcsc-7rps5 Allocated 158.180.xx.xxx 7178 10.0.10.29 94s simple-game-server-fvcsc-x8tvh Ready 158.180.xx.xxx 7045 10.0.10.29 94s 할당된 GameServer에 접속하여 사용하고 종료합니다.\n$ nc -u 158.180.xx.xxx 7178 Hello World ! ACK: Hello World ! EXIT ACK: EXIT ^C Fleet Autoscaler의 현재 상태를 확인합니다. 서비스된 GameServer가 종료되어 다시 스케일되어 2개가 되었습니다.\n$ kubectl describe fleetautoscaler simple-game-server-autoscaler ... Status: Able To Scale: true Current Replicas: 3 Desired Replicas: 2 Last Scale Time: 2023-07-14T10:03:19Z Scaling Limited: false Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Normal AutoScalingFleet 21s (x2 over 11m) fleetautoscaler-controller Scaling fleet simple-game-server from 3 to 2 $ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-x8tvh Ready 158.180.xx.xxx 7045 10.0.10.29 2m22s simple-game-server-fvcsc-zf2mq Ready 158.180.xx.xxx 7122 10.0.10.29 37s 추가 테스트 5개로 스케일해도 단일 Worker Node에 계속 GameServer가 만들어 지길래, 50개를 만들어 봤습니다.\n배포된 Fleet Autoscaler을 수정하여, bufferSize: 50, maxReplicas:100 으로 변경합니다.\nGameServer 확인\n$ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE simple-game-server-fvcsc-2b6hn Ready 158.180.xx.xxx 7877 10.0.10.29 67s simple-game-server-fvcsc-4mdrp Ready 146.56.xx.xxx 7783 10.0.10.105 67s simple-game-server-fvcsc-4xzrf Ready 146.56.xx.xxx 7252 10.0.10.105 67s simple-game-server-fvcsc-5sjkf Ready 158.180.xx.xxx 7369 10.0.10.29 67s simple-game-server-fvcsc-6p5qk Ready 158.180.xx.xxx 7185 10.0.10.29 67s simple-game-server-fvcsc-6pkxz Ready 158.180.xx.xxx 7339 10.0.10.29 67s ... simple-game-server-fvcsc-xnb2c Ready 146.56.xx.xxx 7468 10.0.10.105 67s simple-game-server-fvcsc-zf2mq Ready 158.180.xx.xxx 7122 10.0.10.29 5m41s 새로운 Worker Nodes에 생긴 GameServer를 테스트해도 잘 동작합니다.\n$ nc -u 146.56.xx.xxx 7468 Hello World ! ACK: Hello World ! EXIT ACK: EXIT ^C SuperTuxKart: 오픈 소스 아케이드 레이서 게임 테스트 실 게임 서버의 예로 Agones 예제로 나와 있는 카트 게임인 SuperTuxKart를 배포해서 테스트 해봅니다.\n테스트로 배포할 Fleet YAML 파일입니다.\napiVersion: \u0026#34;agones.dev/v1\u0026#34; kind: Fleet metadata: name: supertuxkart spec: replicas: 2 strategy: type: Recreate template: spec: ports: - name: default containerPort: 8080 health: initialDelaySeconds: 30 periodSeconds: 60 template: spec: containers: - name: supertuxkart image: us-docker.pkg.dev/agones-images/examples/supertuxkart-example:0.8 해당 YAML을 배포하여 Fleet를 생성합니다.\nkubectl apply -f https://raw.githubusercontent.com/googleforgames/agones/release-1.33.0/examples/supertuxkart/fleet.yaml 생성결과를 확인합니다. Fleet에 의해 2개의 GameServer가 생성되었습니다.\nGameServer의 Deployment 역할 해주는 것이 Fleet으로 생각하면 됩니다. 이후 사용할 첫번째 GameServer의 주소와 Port를 사용하겠습니다. $ kubectl get fleet NAME SCHEDULING DESIRED CURRENT ALLOCATED READY AGE supertuxkart Packed 2 2 0 0 10s $ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE supertuxkart-82ds7-p8zl7 Ready 138.2.xxx.xxx 7150 10.0.10.32 2m22s supertuxkart-82ds7-wwdtl Ready 138.2.xxx.xxx 7510 10.0.10.32 2m22s SuperTuxKart 다운로드 페이지에 가서 게임 클라이언트를 다운로드 받습니다.\n다운받은 클라이언트를 실행하고, Online 메뉴에서 Enter server address를 클릭합니다.\n접속할 GameServer 주소와 UDP 포트를 사용하여 접속합니다.\nGameServer에 접속되었습니다.\n접속한 GameServer 컨테이너의 로그를 확인합니다. 새로운 플레이어가 접속되었다는 로그를 확인할 수 있습니다.\n$ kubectl logs -f supertuxkart-82ds7-p8zl7 --all-containers ... Wed Aug 09 14:50:32 2023 [info ] STKHost: Host initialized. Wed Aug 09 14:50:32 2023 [info ] STKHost: Server port is 8080 Wed Aug 09 14:50:32 2023 [info ] ServerLobby: Resetting the server to its initial state. Wed Aug 09 14:50:32 2023 [info ] ProtocolManager: A 11ServerLobby protocol has been started. Wed Aug 09 14:50:32 2023 [info ] main: Creating a LAN server \u0026#39;agones stk server\u0026#39;. Wed Aug 09 14:50:32 2023 [info ] STKHost: Listening has been started. [wrapper] 2023/08/09 14:50:32 server ready Wed Aug 09 14:55:01 2023 [info ] STKHost: 220.117.xxx.x:62438 has just connected. There are now 1 peers. Wed Aug 09 14:55:01 2023 [info ] ServerLobby: Message of type 1 received. Wed Aug 09 14:55:01 2023 [info ] ServerLobby: New player TheKoguryo with online id 0 from 220.117.xxx.x:62438 with SuperTuxKart/1.4 (Macintosh). [wrapper] 2023/08/09 14:55:01 Player TheKoguryo joined 카트를 고르고 게임을 시작합니다. 잘 동작하는 것을 볼 수 있습니다.\n연속해서 레이스를 진행할 수도 있습니다. 레이스가 끝나면, 왼쪽 상단의 화살표를 클릭하여, 게임서버에서 나갑니다.\n게임서버에서 나가면, GameServer가 종료되고, 새로운 서버가 만들어 진 것을 확인할 수 있습니다.\n$ kubectl get gameserver NAME STATE ADDRESS PORT NODE AGE supertuxkart-82ds7-qwcr5 Ready 138.2.xxx.xxx 7599 10.0.10.32 9s supertuxkart-82ds7-wwdtl Ready 138.2.xxx.xxx 7510 10.0.10.32 22m ","lastmod":"2023-07-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/other-k8s-tools/agones/","tags":["oss","agones","game"],"title":"5.1 게임서비스를 위한 Agones 테스트"},{"categories":null,"contents":"11.1 Domain Name과 매핑하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 GoDaddy 사이트에서 구매한 도메인 네임(thekoguryo.xyz)을 사용하였습니다.\n앞선 과정에서 Load Balancer를 사용해 Apache 웹서버와 연동하였습니다. 가지고 있는 Domain Name을 Load Balancer의 IP와 매핑하도록 하겠습니다.\nOCI DNS 서비스 설정 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; DNS Management \u0026gt; Zones 항목으로 이동합니다.\nCreate zone 클릭\n생성정보 입력\nMethod: Manual\nZone type: Primary\nZone name: 가지고 있는 Domain Name 입력\nCreate 클릭\nResources \u0026gt; Records로 이동하면 생성된 항목을 볼 수 있습니다. Zone 내부에 NS 유형과 SOA 유형의 레코드가 생성되어 있습니다. NS는 네임 서버 레코드, SOA는 권한 시작 레코드입니다.\nManage records를 클릭하고, 다시 Add record를 클릭합니다.\n앞서 웹서버를 Load Balancer의 IP 대신 특정 주소(www.thekoguryo.xyz)로 접속이 되게 레코드를 추가해 봅니다.\nNAME: www\nType: A - IPv4 Address\nTTL: 3600, 기본값\nADDRESS: 매핑할 IP, 여기서는 앞서 만든 Load Balancer의 IP 입력\nAdd record 클릭\n변경분을 반영하기 위해 아래 Publish Change 클릭\n확인 창이 뜨면 한번 더 Confirm publish Change 클릭\n레코드 추가 및 반영 완료\n레코드 중 NS 유형인 네임서버 주소를 모두 복사합니다.\nDomain Name 제공 측에 설정 이제 Domain Name을 구입한 사이트에서 설정이 필요합니다. 아래 과정은 Godaddy 기준 설정입니다. 구입한 사이트에서 비슷한 방식으로 설정합니다.\nGodaddy 사이트에 접속하여 DNS 관리 화면에서 Nameservers 설정화면으로 이동합니다.\nChange Nameservers를 클릭합니다.\nOCI에서 복사해온 네임서버 주소를 모두 추가합니다.\n확인창이 뜨면 내용 확인후 다시 Continue를 클릭합니다.\n잠시 지난뒤 새로고침을 하면 등록한 정보로 네임서버 주소가 변경됩니다.\nDNS 반영 확인 등록하면, 실제 글로벌하게 반영되는 시간이 소요됩니다.\n반영되는 것을 확인하기 위해 DNS 전파 확인 사이트(예, whatsmydns.net)를 접속해 봅니다.\nNS 타입으로 검색해 보면, 일부에서만 바뀐 네임서버로 변경된 것을 알 수 있습니다.\n반영이 완료되면 A 타입으로 검색하면 해당 주소가 IP로 확인됩니다.\n테스트 브라우저를 통해 Domain Name으로 접속해 봅니다.\n그림과 같이 Domain Name을 통해 접속되는 것을 확인할 수 있습니다.\n","lastmod":"2023-06-30T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter11/1/","tags":["DNS","domain name"],"title":"11.1 Domain Name과 매핑하기"},{"categories":null,"contents":"1.2.2.1.1 OCI Native Ingress Controller 설치하기 OKE에서는 여러가지 오픈 소스 ingress controller를 그대로 사용할 수 있습니다. 그와 함께 Ingress Controller를 OCI 서비스로 제공하고 있습니다.\n참고 문서 OCI Documentation \u0026gt; Container Engine \u0026gt; Setting Up the OCI Native Ingress Controller on a Kubernetes Cluster OCI Blog \u0026gt; Introducing Oracle Cloud Native Ingress controller for Kubernetes 설치 요구 사항 설치하기 전에 먼저 최신 기준 설치 요구사항을 문서 Prerequisites for deploying the OCI Native Ingress Controller에서 확인합니다. 2024년 1월 기준 Flannel CNI를 사용하는 경우 Kubernetes 1.25, 1.26, 1.27 지원 OCI VCN-Native CNI를 사용하는 경우 OCI VCN-Native CNI 성능 향상을 위해 OCI VCN-Native CNI 2.0을 사용하여 관련 요구사항으로 설치합니다. Kubernetes 1.26 이상 Step 1. OKE Cluster 생성 다음 설정으로 OKE Cluster를 생성합니다. Kubernetes 1.26.2 OCI VCN-Native Pod Networking CNI Node Pool 이미지로 Oracle Linux 7 사용 Step 2. Load Balancer를 위한 Security Rules 추가 OCI VCN-Native Pod Networking CNI를 사용하기 때문에 이후 OCI Load Balancer에서 Pod로 분배를 위해서는 Load Balancer에서 Pod의 OCI Private IP로 통신을 위한 보안 규칙을 Security Rule에 추가해 주어야 합니다. 이 부분은 매뉴얼하게 추가하는 것이 필요합니다. 이후 테스트 앱 설정에서 추가하도록 하겠습니다.\nStep 3. 권한 부여를 위한 Dynamic Group 만들기 OCI Native Ingress Controller가 필요한 OCI 자원을 관리할 수 있도록 권한을 부여합니다. User Principal 또는 Instance Principal을 사용할 수 있습니다. 편의상 여기서는 Instance Principal을 사용합니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\nOKE 클러스터가 있는 Compartment의 OCID를 확인합니다.\n좌측 Dynamic Group 메뉴로 이동하여 아래 규칙을 가진 Dynamic Group을 만듭니다.\nName: 예, oke-native-ingress-controller-dyn-grp instance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39; Step 4. OCI Native Ingress Controller에 권한 부여하기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Policies로 이동합니다.\n아래 규칙을 가진 Policy를 만듭니다\nName: 예, oke-native-ingress-controller-policy subject-name: 앞서 만든 dynamic group 이름, 예, oke-native-ingress-controller-dyn-grp location: 대상 OKE Cluster가 위치한 compartment 이름 Allow dynamic-group \u0026lt;subject-name\u0026gt; to manage load-balancers in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to use virtual-network-family in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to manage cabundles in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to manage cabundle-associations in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to manage leaf-certificates in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to read leaf-certificate-bundles in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to manage certificate-associations in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to read certificate-authorities in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to manage certificate-authority-associations in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to read certificate-authority-bundles in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;subject-name\u0026gt; to read cluster-family in compartment \u0026lt;compartment-name\u0026gt; Step 5. cert-manager 설치하기 대상 OKE Cluster에 cert-managet를 설치합니다.\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/latest/download/cert-manager.yaml Step 6. Helm CLI 설치하기 Helm CLI 설치 문서를 참고하여 사용할 수 있도록 클라이언트 환경에 설치합니다.\nStep 7. OCI Native Ingress Controller 설치 OCI Native Ingress Controller 리포지토리를 가져옵니다.\ngit clone https://github.com/oracle/oci-native-ingress-controller oci-native-ingress-controller 폴더로 이동\nvi helm/oci-native-ingress-controller/values.yaml 파일을 수정합니다.\ncompartment_id: OCI Load Balancer와 OCI Certificate이 생성될 Compartment ID를 지정합니다.\ncompartment_id: \u0026#34;ocid1.compartment.oc1..aaaaaaaa______ddq\u0026#34; subnet_id: 생성될 OCI Load Balancer가 위치할 Subnet의 ID를 지정합니다.\nsubnet_id: \u0026#34;ocid1.subnet.oc1.iad.aaaaaaaa______dba\u0026#34; cluster_id: 설치될 OKE Cluster의 ID를 지정합니다.\ncluster_id: \u0026#34;ocid1.cluster.oc1.iad.aaaaaaaa______dba\u0026#34; authType: Instance Principal을 뜻하는 기본 instance 값을 그대로 사용합니다.\nauthType: instance Ingress Controller의 Pod의 가용성을 늘리고 싶은 경우 replica를 늘립니다.\nreplicaCount: 3 수정후 저장합니다.\nHelm CLI를 통해 설치합니다.\nhelm install oci-native-ingress-controller helm/oci-native-ingress-controller 설치 결과를 확인합니다.\nkubectl get pods -n native-ingress-controller-system -o wide 예시\n$ kubectl get pods -n native-ingress-controller-system -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES oci-native-ingress-controller-7666f768b6-452t6 1/1 Running 0 2m9s 10.0.40.26 10.0.10.213 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oci-native-ingress-controller-7666f768b6-722wk 1/1 Running 0 2m9s 10.0.40.57 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oci-native-ingress-controller-7666f768b6-ttr2c 1/1 Running 0 2m9s 10.0.40.81 10.0.10.156 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; OCI Native Ingress 자원 만들기 OCI Native Ingress Controller를 사용하기 위해서는 몇가지 쿠버네티스 자원을 만들어야 합니다.\nIngressClassParameters: OCI Native Ingress Controller를 통해 OCI Load Balancer를 만들기 위한 기본 정보를 입력합니다. IngressClass: 실제 OCI Load Balancer를 만드는데 사용됩니다. IngressClassParameters에 정의된 정보를 기반으로 OCI Load Balancer를 생성합니다. Ingress: OCI Load Balancer로 들어오는 외부 요청을 Pod Backend로 분배할 규칙을 정의합니다. 이전에 테스트한 Nginx Ingress Controller와 동일한 순서로 진행하기 위해서, 여기서는 IngressClass까지만 만들겠습니다.\nStep 8. IngressClassParameters 만들기 설정 가능 항목은 아래와 같습니다.\ncompartmentId: OCI Load Balancer가 위치할 Compartment가 Controller 설치시 지정한 것과 다른 경우 입력 subnetId: OCI Load Balancer가 위치할 서브넷이 Controller 설치시 지정한 것과 다른 경우 입력 loadBalancerName: 생성할 OCI Load Balancer 이름 isPrivate: 사용한 서브넷이 Private Subnet인지 아닌지 maxBandwidthMbps: 생성할 OCI Load Balancer Shape minBandwidthMbps: 생성할 OCI Load Balancer Shape apiVersion: \u0026#34;ingress.oraclecloud.com/v1beta1\u0026#34; kind: IngressClassParameters metadata: name: \u0026lt;icp-name\u0026gt; namespace: \u0026lt;ns-name\u0026gt; spec: compartmentId: \u0026#34;\u0026lt;compartment-ocid\u0026gt;\u0026#34; subnetId: \u0026#34;\u0026lt;subnet-ocid\u0026gt;\u0026#34; loadBalancerName: \u0026#34;\u0026lt;lb-name\u0026gt;\u0026#34; isPrivate: false maxBandwidthMbps: \u0026lt;max-bw\u0026gt; minBandwidthMbps: \u0026lt;min-bw\u0026gt; 아래와 배포 파일을 작성합니다.\n파일명: 예, native-ic-ingress-params.yaml OCI Native Ingress Controller 설치시 지정한 서브넷을 그대로 사용하는 경우, 아래와 같이 최소 필요한 정보로 배포합니다. OCI Load Balancer의 서브넷을 Helm 배포시 정할시, IngressClassParameters에 입력할지는 관리할 편한 쪽으로 할 필요가 있습니다. apiVersion: ingress.oraclecloud.com/v1beta1 kind: IngressClassParameters metadata: name: native-ic-ingress-params namespace: native-ingress-controller-system spec: loadBalancerName: \u0026#34;native-ingress-controller-lb\u0026#34; isPrivate: false minBandwidthMbps: 100 maxBandwidthMbps: 400 대상 클러스터에 배포합니다.\nkubectl apply -f native-ic-ingress-params.yaml Step 9. IngressClass 만들기 설정 가능 항목은 아래와 같습니다.\napiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: \u0026lt;ic-name\u0026gt; annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;\u0026lt;true|false\u0026gt;\u0026#34; spec: controller: oci.oraclecloud.com/native-ingress-controller parameters: scope: Namespace namespace: \u0026lt;ns-name\u0026gt; apiGroup: ingress.oraclecloud.com kind: ingressclassparameters name: \u0026lt;icp-name\u0026gt; 아래와 배포 파일을 작성합니다.\n파일명: 예, native-ic-ingress-class.yaml OCI Native Ingress Controller 사용하고, IngressClassParameters를 앞서 만든 자원으로 지정해 줍니다. apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: native-ic-ingress-class annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; spec: controller: oci.oraclecloud.com/native-ingress-controller parameters: scope: Namespace namespace: native-ingress-controller-system apiGroup: ingress.oraclecloud.com kind: ingressclassparameters name: native-ic-ingress-params 대상 클러스터에 배포합니다.\nkubectl apply -f native-ic-ingress-class.yaml 배포 결과를 확인합니다.\n생성된 IngressClass를 조회하면, Annotation에서 생성된 OCI Load Balancer의 ID를 확인할 수 있습니다. $ kubectl describe IngressClass native-ic-ingress-class Name: native-ic-ingress-class Labels: \u0026lt;none\u0026gt; Annotations: ingressclass.kubernetes.io/is-default-class: true oci-native-ingress.oraclecloud.com/id: ocid1.loadbalancer.oc1.ap-chuncheon-1.aaaaaaaa_______________________________________________gbfpa Controller: oci.oraclecloud.com/native-ingress-controller Parameters: APIGroup: ingress.oraclecloud.com Kind: ingressclassparameters Name: native-ic-ingress-params Events: \u0026lt;none\u0026gt; Load Balancer IP 확인 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Networking \u0026gt; Load Balancers \u0026gt; Load Balancer로 이동합니다.\nIngressClassParameters에서 지정한 이름으로 Load Balancer가 생성된 것을 확인할 수 있습니다.\n아직 라우팅할 Backend가 설정되어 있지 않기 때문에 Pending 상태입니다. 여기서 Load Balancer IP를 확인할 수 있습니다. 쿠버네티스에서 이후 Ingress 규칙을 생성하게 되면, kubectl 명령으로 확인할 수 있습니다. ","lastmod":"2024-01-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/oci-native-ingress/1.install-oci-ingress-controller/","tags":["ingress-controller","oci-ingress-controller"],"title":"1.2.2.1.1 OCI Native Ingress Controller 설치하기"},{"categories":null,"contents":"2.1 OKE Virtual Nodes 소개 OKE Virtual Nodes 쿠버네티스의 사용이 늘어나고 있습니다. 하지만, 관리 유저들은 다음 항목에 대한 관리상의 어려움이 많다고 합니다.\nInfrastructure right-sizing: 쿠버네티스는 대규모 애플리케이션을 관리하고, 스케일할 수 있도록 설계되어 있습니다. 스케일과 관련된 운영작업으로 CPU, 메모리 같은 인프라 자원에 적정량의 배분, 사이징하는 것에 대한 어려움이 발생합니다. Upgrades and maintenance: 쿠버네티스를 최신버전으로 유지하거나, 서포트 범위 내 버전을 유지하기 위해서도 주기적으로 업그레이드가 필요합니다. 또 그 업데이트 주기도 짧은 편입니다. Infrastructure security: 쿠버네티스 클러스터의 인프라가 노출되면, 해당 클러스터에 수행중인 모든 애플리케이션에 대해서도 위험이 발생합니다. 해당 인프라에 대한 보안이 강화되어야 하며, 접근은 통제되어야 하며, 지속적으로 보안 패치가 적용되어야 합니다. 이런 쿠버네티스 관리상의 어려움을 단순화하기 위해 OKE에서 Virtual Nodes을 출시하게 되었다고 합니다. OKE Virtual Nodes는 Serverless Kubernetes로써 유저가 Worker Nodes 인프라를 관리할 필요가 없습니다. Pod 레벨에서 탄력성을 제공하고, Pod 기준으로 비용이 발생합니다. 유저가 Worker Nodes를 스케일하거나, 업그레이드 하거나, 트러블 슈팅할 필요가 없습니다. 유저가 지속적인 보안 패치를 적용할 필요도 없으며, Virtual Nodes(Worker Nodes)에 접근 자체가 불가합니다. OKE 클러스터의 Control Plane, Data Plane을 OCI가 관리하고, 유저는 Kubernetes API를 통해 애플리케이션을 배포하고, 관리하면 됩니다.\n참고 문서 - OCI Blog \u0026gt; OKE virtual nodes deliver a serverless Kubernetes experience Virtual Nodes에서 지원하지 않는 기능 features and capabilities not supported when using virtual nodes에서 설명하고 있는 것처럼, Kuberbernetes 기능 중 지원하지 않거나, 지원 예정이나 아직 지원하지 않습니다. 또한 OKE Managed Nodes에서 지원하는 기능 중 일부에 대해서도 지원하지 않거나, 지원 예정이나 아직 지원하지 않으니, 해당 문서를 살펴보시기 바랍니다.\n그 중 일부 눈에 들어오는 내용을 보면 다음 기능에 대해서 지원하지 않거나, 지원 예정이나 아직 지원하지 않는 기능이 있습니다.\nLiveness and readiness probes can only use HTTP (not HTTPS) Kubernetes daemonsets 미지원 Persistent volume claims (PVCs) 미지원 Service mesh products 미지원 그외 추가 사항은 features and capabilities not supported when using virtual nodes 참조 ","lastmod":"2023-05-22T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-virtual-nodes/1.oke-virtual-nodes/","tags":["oke","enhanced cluster","virtual nodes"],"title":"2.1 OKE Virtual Nodes 소개"},{"categories":null,"contents":"3.1 OKE Self-Managed Nodes 소개 Self-Managed 노드는 OKE가 Work Node 인스턴스를 생성하는 것이 아니라, 유저가 Compute 서비스를 사용하여 직접 생성한 컴퓨트 인스턴스를 Worker Node로 사용하는 방법입니다. BYON(Bring Your Own Nodes) 라고도 합니다. Self-Managed 노드는 OKE 노드 풀로 관리되지 않습니다.\nSelf-Managed 노드에서는 유저가 Worker Node용 컴퓨트 인스턴스를 만들기 때문에, Managed 노드 또는 Virtual 노드에서 지원하는 Compute Shape이외의 Shape들을 사용할 수 있습니다. 추가적인 HPC, GPU 용 Shape을 사용할 수 있습니다. 또한 높은 대역폭, 낮은 응답속도, RDMA 네트워크 등을 사용하여, HPC, GPU 등의 작업에 활용할 수 있습니다.\nSelf-Managed 노드는 노드 풀로 관리되지 않기 때문에, 여러 Self-Managed 노드를 관리하려면, 필요하면 컴퓨트 인스턴스 풀을 사용하여 관리합니다.\n유의사항\nSelf-Managed Nodes를 추가한 후 클러스터를 삭제할 경우, Self-Managed에 해당하는 컴퓨트 인스턴스는 자동으로 삭제되지 않습니다. 삭제하는 것은 사용자의 몫입니다. Self-Managed Node의 쿠버네티스 버전의 업그레이드를 하려면, 업그레이드된 새로운 Self-Managed Node를 만들어 기존 노드를 대체해야 하며, 이것 또한 사용자의 몫입니다. Self-Managed Node에서 지원되지 않는 기능\n위 유의사항과 동일한 맥락에서 클라우드 콘솔에서 Node Pool로 관리하는 것이 아니기 때문에, 콘솔에서 노드 스케일을 하거나, 노드 사이클을 하는 것을 할 수 없습니다. 클러스터 오토 스케일 지원하지 않습니다. 그외 추가적으로 지원하지 않는 기능을 공식 문서를 참조합니다. Notable features and capabilities not supported when using self-managed nodes ","lastmod":"2023-05-22T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-self-managed-nodes/1.oke-self-managed-nodes/","tags":["oke","enhanced cluster","self-managed nodes"],"title":"3.1 OKE Self-Managed Nodes 소개"},{"categories":null,"contents":"1.9.1 OKE Enhanced Cluster 만들기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\nQuick 모드로 클러스터 만들때 조건 OKE 클러스터 생성 옵션에서 Quick Create 옵션을 선택합니다.\n#1. Virtual Nodes 선택한 경우\nNode type\n쿠버네티스 버전이 v1.25.4 이상인 경우 Node type 선택항목이 나타납니다.\nVirtual Nodes를 선택하면, 무조건 Enhanced Cluster가 됩니다.\n#2. 명시적으로 Basic cluster 선택한 경우\nEnhanced Cluster에서만 제공하는 기능을 선택하지 않은 경우 클러스터 생성 마지막 Review 화면의 제일 마지막에 Basic Cluster Confirmation 영역이 나타납니다. 여기에서 Create a Basic cluster을 체크하면 Basic Cluster가 됩니다.\n나머지 경우\n기본적으로 Enhanced Cluster 타입이 선택됩니다. Custom create 모드로 클러스터 만들때 조건 OKE 클러스터 생성 옵션에서 Custom create 옵션을 선택합니다.\n#1. cluster add-ons을 추가로 선택한 경우\nCreate cluster 화면에서 Advanced options을 펼치면, Configure cluster add-ons가 보입니다.\n필수 애드온으로 기본 설치되는 Core DNS, Kube-proxy 외에 추가 애드온을 선택하면, Enhanced Cluster가 됩니다.\n#2. Virtual Nodes 선택한 경우\nNode type\n쿠버네티스 버전이 v1.25.4 이상이고 Network type이 VCN-native pod networking 인 경우, Node type 선택항목이 나타납니다. Virtual Nodes를 선택하면, 무조건 Enhanced Cluster가 됩니다. #3. 명시적으로 Basic cluster 선택한 경우\nEnhanced Cluster에서만 제공하는 기능을 선택하지 않은 경우 클러스터 생성 마지막 Review 화면의 제일 마지막에 Basic Cluster Confirmation 영역이 나타납니다. 여기에서 Create a Basic cluster을 체크하면 Basic Cluster가 됩니다.\n나머지 경우\n기본적으로 Enhanced Cluster 타입이 선택됩니다. Quick 모드로 클러스터 만들기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\nList Scope에서 생성할 Compartment를 선택합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n빠른 클러스터 생성을 위해 기본선택된 Quick Create 옵션으로 클러스터를 만듭니다.\n생성할 클러스터 이름을 입력합니다. 예) oke-enh-cluster-1\n클러스터의 쿠버네티스 버전을 선택합니다. 예) v1.25.4\n나머지 항목은 앞선 Basic Cluster 만들 때와 동일하게 설정합니다.\nReview\nCluster\nEnhanced Cluster 인지 확인합니다.\n클러스터 생성 확인 생성이 요청되면, 클러스터 생성, 노드 풀 생성, Worker Node 생성 및 구성 순으로 진행됩니다.\n클러스터 상세정보에서 생성된 클러스터의 Cluster type을 확인할 수 있습니다.\n클러스터 상세정보에서 Resources \u0026gt; Node Pools를 보면 생성된 pool을 볼 수 있습니다.\n","lastmod":"2023-05-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/enhanced-cluster/1.create-enhanced-cluster/","tags":["oke","enhanced cluster"],"title":"1.9.1 OKE Enhanced Cluster 만들기"},{"categories":null,"contents":"Cloud Shell 문제해결 #가 포함된 텍스트 복사시 다음 줄에도 #가 추가되거나, 텍스트가 쉬프트 되는 현상 현상: #가 포함된 텍스트 여러 줄을 복사해서 Cloud Shell상의 VI Editor로 복사하는 경우, # 다음 줄에 #가 추가되어 복사되거나, #가 추가 되면서 텍스트가 오른쪽으로 쉬프트 되는 현상 발생\n복사하려는 텍스트\nversion: 0.1 component: build timeoutInSeconds: 6000 runAs: root shell: bash env: # these are local variables to the build config variables: defaultAppName: \u0026#34;app\u0026#34; # the value of a vaultVariable is the secret-id (in OCI ID format) stored in the OCI Vault service Cloud Shell 상의 VI Editor로 복사시\n해결책 #1:\n복사전 VI Editor에서 다음 실행\n:set paste 복사\n복사후 VI Editor에서 다음 실행\n:set nopaste 해결책 #2\nVI 설정 파일 수정\nvi ~/.vimrc 설정 파일에 다음 추가\nset paste 이후 VI Editor 에서 복사\n","lastmod":"2023-05-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/troubleshooting/cloud-shell/","tags":["cloud shell","vi","shift"],"title":"Cloud Shell 문제해결"},{"categories":null,"contents":"1.5.2.3.1 OpenSearch 사용 환경 구성하기 OpenSearch와 OpenDashboards는 2021년에 ElasticSearch와 Kibana에서 포크되어 별도 오픈소스 프로젝트로 운영되고 있습니다. OCI OpenSearch는 OCI에서 제공하는 관리형 서비스입니다.\nOCI Search 서비스 사용을 위한 Policy 설정하기 OCI Search 서비스 권한 부여하기\nAllow service opensearch to manage vnics in compartment \u0026lt;compartment_name\u0026gt; Allow service opensearch to use subnets in compartment \u0026lt;compartment_name\u0026gt; Allow service opensearch to use network-security-groups in compartment \u0026lt;compartment_name\u0026gt; Allow service opensearch to manage vcns in compartment \u0026lt;compartment_name\u0026gt; 오라클 클라우드 콘솔에서 OCI Search 서비스 관리 권한 부여하기\n사용자가 속한 그룹이 SearchOpenSearchAdmins인 경우 예시\nAllow group SearchOpenSearchAdmins to manage opensearch-family in compartment \u0026lt;compartment_name\u0026gt; OCI OpenSearch 클러스터 만들기 오라클 클라우드 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Databases \u0026gt; OpenSearch \u0026gt; Clusters 로 이동합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n생성할 compartment 위치 및 이름, 버전을 선택합니다.\n클러스터의 Primary Account의 유저, 패스워드를 입력합니다.\nusername으로 admin은 사용할 수 없습니다. 생성할 노드 구성을 선택합니다. 세 가지 구성이 가능하며, 여기서는 Development 구성을 선택합니다.\nDevelopment: 1 마스터 노드, 1 데이타 노드, 1 OpenSearch 대쉬보드 노드 Application Search, Log Analytics: 3 마스터 노드, 3 데이타 노드, 3 OpenSearch 대쉬보드 노드 구성을 선택후 노드 크기는 모두 수정할 수 있습니다. 또한 클러스터 생성이후에 크기를 변경할 수 있습니다. 클러스터가 사용할 서브넷을 설정합니다. 여기서는 OKE 클러스터 생성시 이미 만들어진 VCN과 서브넷을 편의상 선택하였습니다.\n선택할 구성 정보를 확인하고 클러스터를 생성합니다.\nDevelopment 구성기준으로 테스트 날짜기준, 생성시간은 20분 가량이 소요되었습니다.\n생성된 클러스터 정보\n클러스터 정보에서 보듯에 OpenSearch와 OpenSearch Dashboard 모두 Private IP와 Private FQDN만 있습니다. 2023년 4월중순 기준으로 아직 Public IP는 지원하지 않습니다. 클러스터의 OpenSearch API Endpoint, Private IP와 OpenSearch Dashboard의 API Endpoint 및 private IP를 확인합니다.\n기존 버전 사용자 참고사항\n2022년 9월 15일 부로 Endpoint 도메인이 oci.oracleiaas.com에서 oci.oraclecloud.com으로 변경되었습니다. 그리고 인증서도 Self-Signed 인증서에서 DigiCert 인증서로 변경되었습니다. 그래서 더 이상 Self-Signed 인증서를 API 호출시 클라이언트 추가해 주지 않아도 됩니다.\nOCI Documentation \u0026gt; Search Service with OpenSearch \u0026gt; Connection to a Cluster 의 Note 참고 2022년 9월 15일 전에 생성한 클러스터, Endpoint 도메인이 oci.oracleiaas.com로 끝나는 경우 연동절차는 이전 문서를 참고하세요.\n5.2.2 OpenSearch 기반 OCI Search 서비스를 사용한 로그 모니터링 - Self-Signed 인증 정보\n2023년 2월 기준으로 OCI Search with OpenSearch now supports role-based access control 기능 출시로 인해 권한 설정 부분이 추가 되었습니다. 그래서 OCI 콘솔에서 OpenSearch Cluster를 생성하게 되면, 기본적으로 ENFORCE 모드로 생성됩니다. ENFORCE 모드에서는 OpenSearch API 호출시에 사용자 인증이 필요합니다. 클러스터 연결 - JumpBox을 통한 Port Forwarding으로 연결하기 JumpBox VM 만들기\nOpenSearch Dashboard는 현재 Public IP를 지원하지 않습니다. OpenSearch Dashboard를 접근을 위해 Public IP를 보유한 JumpBox VM를 추가로 생성합니다.\n오라클 클라우드 콘솔, 좌측 상단 햄버거 메뉴에서 Compute \u0026gt; Instances로 이동합니다.\n새로 Compute VM를 만듭니다.\nName: 예) jumpbox\nNetworking: 생성한 OpenSearch 클러스터에 접근이 가능한 VCN, Public Subnet을 선택하고 Public IP를 할당합니다.\n예) 여기서는 생성한 OpenSearch 클러스터와 동일한 네트워크를 사용하였습니다.\n생성한 jumpbox VM에 접근할 수 있도록, jumpbox VM에서 OpenSearch에 접근 할 수 있도록 서브넷의 보안규칙을 추가합니다.\n아래 예시는 jumpbox 서브넷(10.0.30.0/24) 에서 OpenSearch가 위치가 서브넷인 10.0.20.0/24인 경우\njumpbox 서브넷(10.0.30.0/24)\nIngress Rules Stateless Source IP Protocol Source Port Range Destination Port Range Description No 0.0.0.0/0 TCP All 22 SSH Egress Rules Stateless Destination IP Protocol Source Port Range Destination Port Range Description No 10.0.20.0/24 TCP All 5601 OpenSearch Dashboard OpenSearch 서브넷(10.0.20.0/24)\nIngress Rules Stateless Source IP Protocol Source Port Range Destination Port Range Description No 10.0.30.0/24 TCP All 5601 OpenSearch Dashboard OpenSearch Dashboard 접속확인\nSSH 터널링을 통해 jumpbox VM을 통해 OpenSearch Dashboard를 접속합니다.\nOpenSearch Cluster정보 기준, OpenSearch Dashboard private ip가 10.0.20.22인 경우 ssh -L 5601:10.0.20.22:5601 opc@jumpbox SSH 터널링이 되어 있으므로, 로컬에서 브라우저로 OpenSearch Dashboard에 접속을 확인합니다.\n접속주소: https://localhost:5601\n설치시 입력한 사용자 정보로 로그인합니다.\n클러스터 연결 - Public Load Balancer로 연결하기\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Load Balancers 항목으로 이동합니다.\nCreate Load Balancer 클릭합니다.\nLoad Balancer 를 타입을 Load balancer로 선택합니다.\n기본 생성정보 입력\nName: Load Balancer 이름 입력, 예) lb-for-opensearch\nChoose visibility type: 여기서는 Public 선택\nPublic IP 종류와 Bandwidth는 원하는 값으로 선택합니다.\nChoose Networking: Load Balancer가 위치할 VCN과 Public Subnet 선택\nBackend 설정\nLoad Balancing Policy: 기본값을 사용합니다..\nAdd Backend: 지금은 Compute 인스턴스만 추가할 수 있으므로, 여기서는 추가하지 않습니다.\nHealth Check Policy: 생성된 OpenSearch DashBoard가 https에, 5601 포트를 사용하므로 그에 맞게 아래와 같이 설정합니다.\n나머지 항목은 기본값을 사용합니다.\nConfigure Listener\nListener Name: 이름 입력\nSpecify the type of traffic your listener handles: TCP 선택\n생성된 OpenSearch DashBoard가 https에, 5601을 사용합니다. 요청을 그대로 포워드 합니다. 요청을 그대로 포워드 하기 위해 TCP, 5601 포트로 설정합니다. Manage logging\n에러 로그와 액세스 로그를 OCI Log 서비스를 사용하도록 설정할 수 있습니다. 일단 여기서는 사용하지 않습니다. Submit을 클릭하여 Load Balancer를 생성합니다.\n생성이 완료되면, Resources \u0026gt; Backend sets을 클릭합니다.\n기본적으로 생성된 backend set을 클릭합니다.\nResources \u0026gt; Backends로 이동하여, Add backends를 클릭합니다.\nIP addresses 방식을 체크하고, 직접 IP를 입력합니다.\n생성된 OpenSearch 상세 화면에서 확인한 DashBoard의 IP, 즉 OpenSearch Dashboard private ip를 입력하여 추가합니다.\n추가된 backend을 통해 헬스 체크가 완료될 때까지 기다립니다.\n생성한 Load Balancer를 위해 보안규칙을 추가합니다.\n아래 예시는 Load Balancer 서브넷(10.0.30.0/24) 에서 OpenSearch가 위치가 서브넷인 10.0.20.0/24인 경우\nLoad Balancer 서브넷(10.0.30.0/24)\nIngress Rules Stateless Source IP Protocol Source Port Range Destination Port Range Description No 0.0.0.0/0 TCP All 5601 OpenSearch Dashboard Egress Rules Stateless Destination IP Protocol Source Port Range Destination Port Range Description No 10.0.20.0/24 TCP All 5601 OpenSearch Dashboard OpenSearch 서브넷(10.0.20.0/24)\nIngress Rules Stateless Source IP Protocol Source Port Range Destination Port Range Description No 10.0.30.0/24 TCP All 5601 OpenSearch Dashboard 설정한 Load Balancer의 Public IP를 확인하여, 아래 주소로 브라우저에서 접속합니다.\nhttps://{Load Balancer Public IP}:5601 접속 주소가 인증서에 등록된 주소가 아닌, IP로 접속해서 보안 경고가 발생하지만, 그대로 접속합니다.\n잘 접속되는 것을 볼 수 있습니다.\n보안 경고가 싫은 경우 클라이언트 hosts 파일에 아래와 같이 추가합니다.\n*.opendashboard.ap-chuncheon-1.oci.oraclecloud.com와 같이 와일드카드 인증서를 사용하고 있으므로, 등록할 호스트명은 원하는 것으로 설정해도 됩니다. # Linux/Mac의 경우 /etc/hosts 파일에 추가 # Windows의 경우 C:\\Windows\\System32\\drivers\\etc\\hosts 파일 추가 152.69.xxx.xxx opensearch-cluster-1.opendashboard.ap-chuncheon-1.oci.oraclecloud.com 등록한 전체 이름으로 다시 접속합니다. 보안 경고 없이 연결되는 것을 볼 수 있습니다. ","lastmod":"2023-04-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/oci-opensearch/1.configure-oci-opensearch/","tags":["oke","oci opensearch","opensearch"],"title":"1.5.2.3.1 OpenSearch 사용 환경 구성하기"},{"categories":null,"contents":"1.2.2.2.1 NGINX Ingress Controller 설치하기 OKE에서 Kubernetes에서 사용가능한 여러가지 오픈 소스 ingress controller를 사용할 수 있습니다. 본 문서에서는 그중에서 가장 많이 사용되며, OKE 문서에서 예제로 설명하고 있는 nginx-ingress-controller를 테스트 해보겠습니다.\n공식 문서\nhttps://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm https://kubernetes.github.io/ingress-nginx/deploy/#oracle-cloud-infrastructure 설치 버전\n2024년 1월 기준, OKE는 Kubernetes 1.25, 1.26, 1.27, 1.28 버전을 제공합니다.\n2024년 1월 기준, NGINX Ingress Controller의 지원 버전\nhttps://github.com/kubernetes/ingress-nginx#support-versions-table Ingress-NGINX version k8s supported version Alpine Version Nginx Version Helm Chart Version v1.9.5 1.28, 1.27, 1.26, 1.25 3.18.4 1.21.6 4.9.0* v1.9.4 1.28, 1.27, 1.26, 1.25 3.18.4 1.21.6 4.8.3 v1.9.3 1.28, 1.27, 1.26, 1.25 3.18.4 1.21.6 4.8.* v1.9.1 1.28, 1.27, 1.26, 1.25 3.18.4 1.21.6 4.8.* v1.9.0 1.28, 1.27, 1.26, 1.25 3.18.2 1.21.6 4.8.* v1.8.4 1.27, 1.26, 1.25, 1.24 3.18.2 1.21.6 4.7.* v1.8.2 1.27, 1.26, 1.25, 1.24 3.18.2 1.21.6 4.7.* v1.8.1 1.27, 1.26, 1.25, 1.24 3.18.2 1.21.6 4.7.* v1.8.0 1.27, 1.26, 1.25, 1.24 3.18.0 1.21.6 4.7.* v1.7.1 1.27, 1.26, 1.25, 1.24 3.17.2 1.21.6 4.6.* v1.7.0 1.26, 1.25, 1.24 3.17.2 1.21.6 4.6.* Kubernetes 1.26, 1.27, 1.28 모두를 지원하는 버전 중 가장 최신 버전인 v1.9.5로 설치하겠습니다.\nNGINX Ingress Controller 설치 kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\nnginx ingress controller 설치할 파일 deploy.yaml을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml ingress-nginx-controller의 Load Balancer Service 유형에 대한 설정을 annotation으로 추가합니다.\noci.oraclecloud.com/load-balancer-type: \u0026quot;lb\u0026quot; 을 추가하여 OCI Load Balancer를 사용하고 관련 설정을 추가합니다.\n... --- apiVersion: v1 kind: Service metadata: labels: ... name: ingress-nginx-controller namespace: ingress-nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape: \u0026#34;flexible\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-backend-protocol: \u0026#34;HTTP\u0026#34; spec: loadBalancerIP: 158.180.xx.xx # Reserved Public IP ... type: LoadBalancer ... 설정값 설명\n항목 값 설명 oci.oraclecloud.com/load-balancer-type \u0026quot;lb\u0026quot; Default: \u0026quot;lb\u0026quot; - OCI Load Balancer 사용 \u0026quot;nlb\u0026quot; - L4을 지원하는 OCI Network Load Balancer, 무료 서비스 service.beta.kubernetes.io/oci-load-balancer-shape \u0026quot;flexible\u0026quot; Default: \u0026quot;100Mbps\u0026quot;\nflexible 유형을 선택 service.beta.kubernetes.io/oci-load-balancer-shape-flex-min \u0026quot;10\u0026quot; flexible 유형인 경우 최소 용량 지정 service.beta.kubernetes.io/oci-load-balancer-shape-flex-max \u0026quot;10\u0026quot; flexible 유형인 경우 최대 용량 지정 service.beta.kubernetes.io/oci-load-balancer-backend-protocol \u0026quot;HTTP\u0026quot; Default: \u0026quot;TCP\u0026quot;생성되는 OCI Load Balancer의 Listener의 프로토콜을 지정WAF, WAA 지원을 위해 HTTP으로 지정 OCI Load Balancer의 설정가능한 annotations 전체 항목 loadBalancerIP: Reserved Public IP를 사전에 만들어 가지고 있는 경우, 해당 IP를 직접 입력하여, 원하는 IP를 부여합니다.\n다음 명령으로 NGINX Ingress Controller를 설치합니다.\nkubectl apply -f deploy.yaml 설치 확인\ningress-nginx namespace에 아래와 같이 설치된 것을 확인할 수 있습니다.\nkubectl get all -n ingress-nginx Ingress Controller 서비스의 로드밸런서 IP인 EXTERNAL-IP를 확인합니다.\n$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.96.150.50 158.180.xx.xx 80:30399/TCP,443:31348/TCP 58s ... 생성된 OCI Load Balancer 확인 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Networking \u0026gt; Load Balancers \u0026gt; Load Balancer로 이동합니다.\n동일한 Public IP로 생성된 Load Balancer를 확인할 수 있습니다.\n해당 Load Balancer를 클릭합니다.\nListener를 보면 그림과 같이 HTTP 프로토콜로 80, 443 포트로 수신하고 있습니다.\nLoad Balancer 자원 확인 Worker Node가 둘 이상인 경우, 배포가 성공한 경우에도 OCI 콘솔상에서 Load Balancer의 Health 상태가 Critical로 표시됩니다. 이와 관련된 질문이 많아 아래와 같이 설명합니다. 아래 내용은 버전 업이 되면서, Nginx Ingress Controller의 특정 설정이 변경되면서 발생한 내용입니다.\nLoad Balancer의 UID를 확인합니다.\n$ kubectl get svc ingress-nginx-controller -o jsonpath=\u0026#39;{.metadata.uid}{\u0026#34;\\n\u0026#34;}\u0026#39; -n ingress-nginx 570a9d86-6215-4264-9cd5-3219cbb22efc OCI 콘솔에 로그하여, Load Balancer 화면으로 이동합니다.\n목록중에서 확인한 UID가 이름으로 생성된 Load Balancer가 보일 것입니다. 고정 IP를 할당한 경우, 해당 IP로 찾아도 됩니다.\n해당 Load Balancer를 클릭하여 상세화면으로 이동합니다.\n왼쪽 아래 Resource \u0026gt; Backend sets 메뉴로 이동합니다.\nBackend sets의 상태 또한 Critical 인 걸 볼 수 있습니다.\nBackend set 중 하나를 클릭합니다. 여기서는 TCP-80을 클릭합니다.\n설정된 Backends를 확인합니다. 여기서 Backend는 Worker Node들입니다. 노드 하나쪽만 OK 상태이고 나머지는 Critical 상태인 걸 알 수 있습니다.\nIngress-nginx pod가 위치한 Worker Node를 조회해 봅니다.\n$ kubectl get pod -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-admission-create-qkqqs 0/1 Completed 0 21m 10.0.10.81 10.0.10.179 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-admission-patch-9t587 0/1 Completed 0 21m 10.0.10.230 10.0.10.179 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ingress-nginx-controller-5d974c544-hrpzn 1/1 Running 0 21m 10.0.10.100 10.0.10.179 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 실제 Pod가 위치한 Worker Node(10.0.10.179)만 Health가 OK 상태입니다. 배포한 NGINX Ingress Controller의 deploy.yaml 파일에 다음 설정의 영향입니다. 해당 설정은 Ingress Controller에 접속하는 클라이언트의 IP를 애플리케이션 Pod에서도 동일하게 클라이언트 IP로 유지하도록 합니다. 애플리케이션 Pod에서 로그 등에서 클라이언트를 추적이 필요한 경우 사용됩니다. 아래 Kubernetes 공식 문서의 관련 내용을 참고하세요.\nexternalTrafficPolicy: Local 출처 - Kubernetes 공식 문서 - 외부 로드 밸런서 생성하기 \u0026gt; Preserving the client source IP\n.spec.externalTrafficPolicy - 이 서비스가 외부 트래픽을 노드-로컬 또는 클러스터-전체 엔드포인트로 라우팅할지 여부를 나타낸다. 두 가지 옵션이 있다. Cluster (기본) 그리고 Local. Cluster 는 클라이언트 소스 IP를 가리고 다른 노드에 대한 두 번째 홉(hop)을 발생시킬 수 있지만, 전체적인 부하 분산에서 이점이 있다. Local 은 클라이언트 소스 IP를 보존하고 LoadBalancer와 NodePort 타입의 서비스에서 두 번째 홉(hop) 발생을 피할 수 있지만, 트래픽 분산이 불균형적인 잠재적인 위험이 있다.\n","lastmod":"2024-01-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/nginx-ingress/lb/1.install-nginx-ingress-controller/","tags":["oss","ingress-controller","nginx"],"title":"1.2.2.2.1 NGINX Ingress Controller 설치하기"},{"categories":null,"contents":"1.5.2.2.1 OCI Kubernetes Monitoring Solution을 사용하여 Logging Analytics로 모니터링 하기 OCI Logging Analytics, OCI Monitoring, OCI Management Agent 와 FluentD를 통해 Kubernetes를 모니터링할 수 있는 솔루션인 OCI Kubernetes Monitoring Solution을 오픈소스로 제공하고 있습니다.\nOCI Kubernetes Monitoring Solution 마켓플레이스를 통해 설치하거나, GitHub 리파지토리 소스를 통해 Resource Manager, Terraform, Helm 등으로 설치할 수 있습니다.\nOCI Kubernetes Monitoring Solution Monitor Kubernetes and OKE clusters with OCI Logging Analytics 문서에서 아키텍처를 보면, 로그 수집을 위해 컴포넌트로 FluentD Collector와 Logging Analytics FluentD Plugin이 설치되어 쿠버네티스 상의 로그를 수집합니다.\n마켓플레이스를 통해 설치 시작\nOCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Marketplace \u0026gt; All Applications으로 이동합니다.\nKubernetes Monitoring and Management으로 검색합니다.\nKubernetes Monitoring and Management을 선택합니다.\nOracle에서 제공하는 앱으로 무료로 제공하고 있습니다. OKE Cluster상의 서비스, 리눅스 시스템, 애플리케이션 컨테이너 로그 등을 OCI Logging Analytics로 수집하여 제공하는 기능을 제공한다고 설명하고 있습니다.\n설치할 Compartment를 선택하고, 약관에 동의하고 진행하면 됩니다.\n원 소스 저장소인 GitHub 리파지토리 보다 Marketplace는 버전이 낮을 수 있으므로 여기서는 GitHub을 통해 최신버전을 설치합니다.\nGitHub 에서 설치 시작\n오픈소스로 GitHub - oci-kubernetes-monitoring 저장소에서 소스 파일을 제공하고 있으면, README 파일에서 설명하는 것처럼 아래 버튼을 클릭하여 최신 소스로 설치를 시작할 수 있습니다.\nResource Manager Stack 설치\n마켓플레이스 또는 GitHub에서 설치를 시작하면 Resource Manager의 Stack 설치화면으로 이동됩니다.\n작성일 기준으로 V3.0.2 버전을 사용하였습니다.\n설치 기본 정보를 입력합니다.\nCreate in compartment: Resource Manager Stack이 설치될 위치입니다. 변수값을 입력합니다.\nOKE Cluster Information OKE Cluster가 위치한 Compartment와 대상 클러스터를 선택합니다. OCI Logging Analytics Information Logging Analytics 대쉬보드와 LogGroup의 위치하는 Compartment를 선택합니다. 만들 Logging Analytics LogGroup을 입력합니다. Policies and Dynamic Groups 모니터링할 OKE 클러스터에 대한 접근을 위해 자동으로 Dynamic Group과 Policy가 만들어집니다. 자동설치가 싫거나, 권한이 없는 경우, 사전에 별도로 권한에 설정합니다. 결과를 리뷰하고, Create를 클릭하여, 설치 및 적용합니다.\n설치가 완료할 때 까지 기다립니다. 실패한 경우, Logs를 확인하여 문제를 해결하고 재시도합니다.\n설치된 Dynamic Group 및 Policy 확인하면 아래와 같습니다.\nLog \u0026amp; Object Collection Pods가 있는 Worker Nodes들에 LogGroup에 로그를 업로드할 권한을 부여하고 있습니다\nDynamic Group: oci-kubernetes-monitoring-xxx\u0026hellip;\nMatch any rules defined below # Rule 1 ALL {instance.compartment.id = \u0026#39;ocid1.compartment.oc1..aaaaa_____32sa\u0026#39;} # Rule 2 ALL {resource.type=\u0026#39;managementagent\u0026#39;, resource.compartment.id=\u0026#39;ocid1.compartment.oc1..aaaaa_____32sa\u0026#39;} Policy: oci-kubernetes-monitoring-yyy\u0026hellip;\nOCI Logging Anaytics Comparment로 선택한 Compartment에 생성됨 Allow dynamic-group oci-kubernetes-monitoring-xxx... to {LOG_ANALYTICS_LOG_GROUP_UPLOAD_LOGS} in compartment oci-hol Allow dynamic-group oci-kubernetes-monitoring-xxx... to use METRICS in compartment oci-hol WHERE target.metrics.namespace = \u0026#39;mgmtagent_kubernetes_metrics\u0026#39; 왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging Analytics \u0026gt; Administration으로 이동합니다.\nResources \u0026gt; Log Groups에 보면 설치시 생성된 LogGroup를 확인할 수 있습니다.\nDashboard 메뉴를 클릭하면, Kubernetes 대쉬보드가 추가된 것을 확인할 수 있습니다.\n대상으로 지정된 OKE 클러스터 설치된 자원을 확인합니다.\nhelm chart로 설치된 것을 확인할 수 있습니다.\n$ helm list -A NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION oci-kubernetes-monitoring default 1 2023-09-07 08:32:45.717642732 +0000 UTC deployed oci-onm-3.0.2 3.0.0 oci-onm 네임스페이스에 관련 Pod가 설치되었습니다.\n$ kubectl get all -n oci-onm NAME READY STATUS RESTARTS AGE pod/oci-onm-logan-58dfdfd948-bp9jn 1/1 Running 0 20m pod/oci-onm-logan-947wz 1/1 Running 0 20m pod/oci-onm-logan-d2jdg 1/1 Running 0 20m pod/oci-onm-logan-x7bbp 1/1 Running 0 20m pod/oci-onm-mgmt-agent-0 1/1 Running 0 20m 설치된 fluentd가 사용하는 설정값이 있는 configmap입니다.\n$ kubectl get configmap -n kube-system NAME DATA AGE ... oci-la-fluentd-logs-configmap 2 10m oci-la-fluentd-objects-configmap 2 10m ... Fluentd 설정을 보면, flush_interval을 30초로 설정되어 있습니다. 최소값은 10초 입니다. Oracle Logging Analytics FluentD에서 플러그인 설명, Fluentd Flushing Parameters을 관련 파라미터를 참고바랍니다.\n$ kubectl get configmap oci-onm-logs -n oci-onm -o yaml ... \u0026lt;buffer\u0026gt; @type file path /var/log/oci_la_fluentd_outplugin/logs/buffer/ flush_thread_count \u0026#34;1\u0026#34; chunk_limit_size \u0026#34;2m\u0026#34; total_limit_size \u0026#34;5368709120\u0026#34; flush_interval \u0026#34;30\u0026#34; flush_thread_interval \u0026#34;0.5\u0026#34; flush_thread_burst_interval \u0026#34;0.05\u0026#34; retry_wait \u0026#34;2\u0026#34; retry_max_times \u0026#34;17\u0026#34; retry_exponential_backoff_base \u0026#34;2\u0026#34; retry_forever \u0026#34;true\u0026#34; disable_chunk_backup \u0026#34;true\u0026#34; \u0026lt;/buffer\u0026gt; .. Logging Analytics에서 로그 확인 Log Explorer로 로그 확인\nOCI 콘솔로 이동합니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging Analytics \u0026gt; Log Explorer으로 이동합니다.\n필터에서 Log Group Compartment를 원하는 대상으로 선택합니다.\n기본값으로 파이 차트 형식으로 보여 줍니다. 대상 클러스터에서 수집된 여러가지 소스에서 수집된 로그를 보여줍니다.\nWorker Node 로그 및 사전 정의한 Kubernetes 로그이외에 애플리케이션 로그를 확인하기 위해 Kubernetes Container Generic Logs를 드릴 다운합니다.\n클라스터상의 수집된 컨테이너 로그들을 볼수 있습니다.\n테스트를 위해 default 네임스페이스에 배포된 nginx 앱을 사용하겠습니다.\n필터링을 위해 Search Fields에 namespace로 검색합니다. 검색 결과 중에서 Namespace를 클릭하면 현재 검색된 로그들을 Namespace 단위로 카운트가 보입니다. 여기서 default namespace를 선택하고 적용합니다.\n검색 쿼리가 아래와 같이 변경되었습니다. 아래와 같이 직접 and Namespace = default를 입력하여도 됩니다.\n\u0026#39;Log Source\u0026#39; = \u0026#39;Kubernetes Container Generic Logs\u0026#39; and Namespace = default | timestats count as logrecords by \u0026#39;Log Source\u0026#39; | sort -logrecords 애플리케이션 로그 확인을 위해 이전 가이드에 샘플로 배포된 nginx 앱을 접속해 봅니다.\n발생한 POD 로그는 다음과 같습니다.\n$ kubectl logs -f nginx-docker-hub-67c59cc7d5-vxj2j ... 10.0.10.104 - - [07/Sep/2023:09:24:20 +0000] \u0026#34;GET /?logging-analytics-logtest HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\u0026#34; \u0026#34;-\u0026#34; ... Logging Analytics의 Log Explorer에서 동일한 로그들을 모두 확인할 수 있습니다. Fluentd Buffer 설정에 따라 30초 내외로 볼 수 있습니다.\n쿼리에 검색조건을 추가합니다.\nDashboard 확인\nLogging Analytics \u0026gt; Dashboard로 이동합니다.\nKubernetes 대쉬보드가 추가된 것을 확인할 수 있습니다.\nKubernetes Cluster Summary를 클릭합니다.\n배치된 위젯을 클릭하면, 해당 조건에 따라 로그를 쿼리하는 화면으로 이동합니다.\n다른 대쉬보드들로 비슷한 형식으로 제공합니다.\nLogging Analytics에서 모니터링 OCI Kubernetes Monitoring Solution 버전이 올라가면서 OCI Management Agent을 통한 메트릭 모니터링 기능도 추가되었습니다. 이전 수작업으로 Management Agent를 수작업으로 설치하던 것이 OCI Kubernetes Monitoring Solution 설치시 함께 설치됩니다.\n설치된 자원을 조회해 보면, mgmt-agent가 설치된 것을 알 수 있습니다.\n$ kubectl get all -n oci-onm NAME READY STATUS RESTARTS AGE pod/oci-onm-logan-58dfdfd948-bp9jn 1/1 Running 0 20m pod/oci-onm-logan-947wz 1/1 Running 0 20m pod/oci-onm-logan-d2jdg 1/1 Running 0 20m pod/oci-onm-logan-x7bbp 1/1 Running 0 20m pod/oci-onm-mgmt-agent-0 1/1 Running 0 20m Management Agent 및 메트릭 확인\nOCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Management Agents \u0026gt; Agent로 이동합니다.\nAgent가 등록된 것을 확인합니다.\n등록된 Agent를 클릭하면, 설치된 Agent의 현재 상태를 확인할 수 있습니다. 필요하면, 왼쪽 Time range에서 조회 시간을 1시간으로 변경합니다.\n내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Monitoring \u0026gt; Metrics Explorer로 이동합니다.\n화면 아래 Query 부분으로 이동합니다. Management Agent 설치로 인해 Metric namespace에 mgmtagent_kubernetes_metrics 가 추가되었습니다. 선택하면 Metric name에서 제공하는 메트릭들을 볼 수 있습니다.\n메트릭 이름까지 나열되면 OKE에 설치된 Agent Pod에서 OCI Metrics까지 연동을 성공한 것입니다.\n샘플 대쉬보드 등록하기\nAgent Helm 차트의 resources 폴더에 Logging Analytics용 샘플 대쉬보드를 제공합니다. 또는 직접 링크에서 mgmtagent_kubernetes_dashboard.json 파일을 다운로드 받습니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging Analytics \u0026gt; Dashboards로 이동합니다.\nImport Dashboards를 클릭하고 다운받은 대쉬보드 파일을 임포트합니다.\nManagement Agent와 동일한 Compartment에 임포트합니다.\n임포트된 Kubernetes Monitoring Sample Dashboard를 클릭합니다.\n그림과 같이 대상 OKE 클러스터의 메트릭을 통한 대쉬보드를 볼 수 있습니다.\n대쉬보드 수정하기 대쉬보드 오른쪽 위 Actions에서 Edit를 선택합니다.\n현재 대쉬보드상의 위젯을 편집하거나, 추가 할 수 있습니다.\n추가를 위해 Create Metric Widget를 클릭합니다.\nCompartment는 샘플대쉬보드에서 사용하는 기본값(이미 있는 Compartment 필터)을 사용합니다.\nSource에서 추가한 mgmtagent_kubernetes_metrics을 선택합니다.\n추가된 메트릭을 기준으로 차트를 만들 수 있습니다.\n","lastmod":"2023-09-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/logging-analytics/1.logging-analytics/","tags":["oke","oci logging analytics","logging"],"title":"1.5.2.2.1 Logging Analytics로 모니터링 하기"},{"categories":null,"contents":"6.1 Container Instances로 컨테이너 배포하기 OCI Container Instances 컨테이너 애플리케이션을 사용하기 위해 Container Orchestration으로 일반적으로 쿠버네티스를 많이 사용합니다. 또는 가상머신에 Docker 엔진을 직접 설치하여, 컨테이너를 간단하게 구동시킬 수도 있습니다. 이때 쿠버네티스 또는 Docker 엔진 등을 사용자가 관리해 주어야 합니다. Container Instances는 Container Orchestration을 사용자가 관리하지 않고, 클라우드 서비스 제공자가 관리합니다. 사용자는 간단하게 컨테이너를 배포해서 사용할 수 있고, 컨테이너가 직접 사용하는 자원에 대한 비용만 드는 장점이 있습니다.\n하나의 호스트를 가상화해서 VM간 독립된 환경을 제공하는 것과 동일하게, 하나의 호스트를 가상화해서 Container Instance를 위한 독립된 환경을 제공하여, Container Instance에서 Container Runtime 환경을 제공하고, OCI에서 관리하게 됩니다. 사용자는 Container Instance에서 실행할 컨테이너만 관리하면 됩니다. 상세한 내부 동작은 OCI First Principles video blog post을 참조하기 바랍니다.\nOCI Container Instances의 GA 블로그 내용에서 보듯이 독립적으로 배포해서 사용할 수 있는 오픈소스 패키지 솔루션을 배포해서 사용할 때 좋지 않을 까 생각듭니다.\n참조\nBlog - Announcing the availability of OCI Container Instances\nBlog - First principles: Inside OCI Container Instances\nTutorial - Manage container workloads on OCI using the Container Instances service\n워드프레스, MySQL 컨테이너로 배포하기 Container Instances 서비스를 통해 컨테이너 애플리케이션을 배포할 수 있습니다.\n현재 GA가 2022년 12월 버전기준 Persistent Volume을 지원하지 않습니다. Container Instance를 재시작하게 되면, 데이터가 저장되지 않아, 워드프레스 초기 설치부터 다시 진행하게 됩니다.\nPersistent Volume은 추후 지원예정이라고 합니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Instances 로 이동합니다.\n생성을 위해 Create container instance 버튼을 클릭합니다.\n기본 생성정보를 입력을 입력합니다.\nName: 예) wordpress\nCompartment: 사용할 Compartment 선택\nPlacement: 인스턴스가 위치할 Availability Domain와 Fault Domain을 선택할 수 있습니다. 단일 AD기준으로 기본값을 사용합니다.\nShape:\nContainer Instance를 위해 제공하는 Shape을 선택합니다. Flex 유형으로 원하는 OCPU와 메모리 크기를 선택할 수 있습니다. Container Instance내의 컨테이너들이 사용할 자원의 합으로 여기서는 워드프레스와 MySQL 두 컨테이너가 사용하게 됩니다. 2 OCPU, 8 GB 메모리를 선택합니다. Networking\n컨테이너 인스턴스가 위치할 VCN과 Subnet을 선택합니다. 여기서는 테스트를 위해 Public Subnet을 선택하고, Public IP를 할당합니다.\nShow advanced options에서 네트워크 고급 설정이 가능합니다. 컨테이너 인스턴스를 내부 네트워크에서 접근할 때 사용할 Private IP와 Private 도메인 이름을 설정할 수 있습니다. 해당 내용으로 내부 DNS Zone에 등록됩니다.\n화면 제일 아래의 Show advanced options을 클릭하여 추가적인 고급 설정을 지정할 수 있습니다. 컨테이너가 종료할때 대기하는 Graceful shutdown timeout과 Container Restart Policy를 설정할 수 있습니다.\nNext를 클릭합니다\nContainer Instance내에서 실행할 컨테이너 정보를 입력합니다.\n첫번째 컨테이너로 MySQL 컨테이너 정보를 입력합니다.\nName: 예) mysql\nImage: Select Image를 클릭합니다.\nOCIR과 외부 컨테이너 레지스트리를 지원합니다. 외부 레지스트리의 기본 서버는 Docker Hub입니다. 아래와 같이 이미지 이름과 태그를 이용해서 MySQL 이미지를 가져옵니다.\nEnvironmental variables: 사용하는 MySQL 컨테이너 이미지에서 제공하는 환경변수 값 중에서 데이터베이스 설치하기 위해 필요한 아래변수들을 추가합니다.\nKey Value MYSQL_DATABASE wordpress MYSQL_ROOT_PASSWORD wordpressonmysql MYSQL_USER wordpress MYSQL_PASSWORD wordpress Show advanced options을 클릭하여 추가적인 고급 설정을 지정할 수 있습니다.\nResources\nContainer Instance안에 컨테이너가 여러개가 있는 경우, 하나의 컨테이너가 자원을 일방적으로 사용하지 않도록, Throttling을 설정할 수 있습니다. 직접 OCPU, Memory를 지정하거나, 아래 그림과 같이 백분율을 설정할 수 있습니다. 여기서는 둘다 50%를 입력합니다.\nStartup Options\n컨테이너 시작시 사용할 값들을 설정할 수 있습니다. MySQL 8부터 기본 인증방식이 변경되어, 여기서는 편의상 기존 암호방식을 사용하기 위한 ENTRYPOINT 값을 --default-authentication-plugin=mysql_native_password을 추가합니다.\n+ Add container를 클릭하여 두번째 컨테이너를 추가합니다.\n두번째 컨테이너로 wordpress 컨테이너 정보를 입력합니다.\nName: 예) wordpress\nImage:\nSelect Image를 클릭하여 앞서와 동일하게 외부 레지스트리인 Docker Hub에서 가져옵니다. 이미지 이름만 wordpress로 입력합니다. 태그가 없는 경우 태그는 기본값으로 latest를 사용합니다.\nEnvironmental variables:\n사용하는 MySQL 컨테이너 이미지에서 제공하는 환경변수 값 중에서 데이터베이스 설치하기 위해 필요한 아래변수들을 추가합니다. Container Instances내에 여러 컨테이너가 있을 경우, CPU, Memory를 공유하며, 네트워크도 함께 공유합니다. 그래서 WORDPRESS_DB_HOST를 127.0.0.1로 해도 앞서 만든 MySQL 컨테이너로 접근이 가능합니다.\nKey Value WORDPRESS_DB_HOST 127.0.0.1 WORDPRESS_DB_NAME wordpress WORDPRESS_DB_USER wordpress WORDPRESS_DB_PASSWORD wordpress Show advanced options을 클릭하여 추가적인 고급 설정을 지정할 수 있습니다.\nResources\n두번째 컨테이너도 Throttling을 설정합니다. 아래 그림과 같이 나머지 50%로 백분율을 지정합니다.\nNext를 클릭합니다\n입력한 모든 정보를 확인하고 Create 버튼을 클릭하여 Container Instance를 생성합니다. Wordpress와 Wordpress가 사용할 MySQL까지 두개의 컨테이너가 하나의 Container Instance로 생성하게 됩니다.\n수초가 지나면 금방 Container Instance가 아래와 같이 생성됩니다. 생성된 Wordpress의 Public IP를 확인합니다.\n접속을 위해서는 Container Instance가 사용하는 Security List에 인터넷에서 80 포트로 접근할 수 Ingress에 미리 등록해야 합니다.\nStateless Source IP Protocol Source Port Range Destination Port Range Description No 0.0.0.0/0 TCP All 80 Wordpress Public IP로 접속하면 Wordpress 초기 설치화면을 볼 수 있습니다.\n모니터링 \u0026amp; 로그 확인하기 Container Instance 상세화면에서 왼쪽 아래 Resources \u0026gt; Metrics 에서 사용하고 있는 CPU, Memory 및 스토리지 메트릭 정보를 볼 수 있습니다.\nContainers 탭으로 이동하면, 배포된 컨테이너들을 확인할 수 있습니다. 자원 또한 전체 할당한 OCPU, Memory를 설정한대로 50% 씩 할당된 것을 볼 수 있습니다.\n첫번째 있는 wordpress 컨테이너를 클릭하여 상세 화면으로 이동합니다.\n컨테이너 내 메뉴에서 View logs 를 통해 컨테이너 로그를 확인할 수 있습니다.\n","lastmod":"2022-12-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/container-instances/1.use-container-instances/","tags":["container-instances"],"title":"6.1 Container Instances로 컨테이너 배포하기"},{"categories":null,"contents":"8.2 GraalVM 사용하여 Spring Boot 3 애플리케이션 개발하기 Spring Boot 예제를 바탕으로 앱 개발, 컨테이너 이미지 빌드, OKE에 배포하는 과정을 확인해 봅니다.\nSpring Boot 기반 마이크로 서비스 만들기 Spring Initializr를 사용하여 기본 프로젝트 소스파일을 기반으로 개발을 하게 됩니다.\nSpring Initializr을 통해 프로젝트 파일을 만듭니다.\n방법 1. Spring Initializr를 사용하여 기본 프로젝트 소스파일을 만듭니다.\n아래 그림과 같이 프로젝트 정보를 입력하고 Generate를 클릭하여 소스파일을 생성합니다.\nNative Image를 테스트하기 위해 Spring Boot 3 버전을 선택합니다.\n다운로드 받은 파일을 Cloud Shell에 업로드 합니다.\nCloud Shell에서 업로드된 파일을 unzip으로 압축해제 합니다.\n방법 2. Spring Initializr를 브라우저 대신 아래 명령을 통해 Cloud Shell에서 바로 기본 프로젝트 소스파일을 만듭니다.\ncurl https://start.spring.io/starter.tgz -d type=maven-project -d bootVersion=3.1.4 -d baseDir=rest-service -d name=rest-service -d artifactId=rest-service -d javaVersion=17 -d dependencies=web,actuator | tar -xzvf - rest-service 폴더로 이동합니다.\n요청에 대한 응답 메시지를 아래와 같은 JSON 메시지 응답하도록 코드를 구현할 예정입니다.\n{ \u0026#34;id\u0026#34;: 1, \u0026#34;content\u0026#34;: \u0026#34;Hello, World!\u0026#34; } 아래 코드를 복사하여 자바 클래스파일(src/main/java/com/example/restservice/Greeting.java)을 에 작성합니다.\npackage com.example.restservice; public class Greeting { private final long id; private final String content; public Greeting(long id, String content) { this.id = id; this.content = content; } public long getId() { return id; } public String getContent() { return content; } } /greeting URL로 요청을 보내면, 앞서 본 JSON 메시지 응답하도록 코드를 src/main/java/com/example/restservice/GreetingController.java 위치에 작성합니다.\npackage com.example.restservice; import java.util.concurrent.atomic.AtomicLong; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RequestParam; import org.springframework.web.bind.annotation.RestController; @RestController public class GreetingController { private static final String template = \u0026#34;Hello, %s!\u0026#34;; private final AtomicLong counter = new AtomicLong(); @GetMapping(\u0026#34;/greeting\u0026#34;) public Greeting greeting(@RequestParam(value = \u0026#34;name\u0026#34;, defaultValue = \u0026#34;World\u0026#34;) String name) { return new Greeting(counter.incrementAndGet(), String.format(template, name)); } } Kubernetes에서는 컨테이너 기동후 준비시간(readiness), 헬스체크를(liveness)를 Spring Boot에 활성화하기 src/main/resources/application.properties 파일에 다음 설정을 추가합니다.\nmanagement.health.probes.enabled=true JDK 버전 선택하기 CloudShell 상의 현재 JDK 버전을 확인하고, csruntimectl을 통해 생성한 Spring Boot 설정에 맞게 JDK 17로 변경합니다.\n$ csruntimectl java list graalvmeejdk-17 /usr/lib64/graalvm/graalvm22-ee-java17 * oraclejdk-11 /usr/java/jdk-11.0.17 oraclejdk-1.8 /usr/lib/jvm/jdk-1.8-oracle-x64 JDK 17로 변경합니다\n$ csruntimectl java set graalvmeejdk-17 The current managed java version is set to graalvmeejdk-17. 선택한 버전을 확인합니다.\n$ java -version java version \u0026#34;17.0.8\u0026#34; 2023-07-18 LTS Java(TM) SE Runtime Environment GraalVM EE 22.3.3 (build 17.0.8+9-LTS-jvmci-22.3-b21) Java HotSpot(TM) 64-Bit Server VM GraalVM EE 22.3.3 (build 17.0.8+9-LTS-jvmci-22.3-b21, mixed mode, sharing) 빌드후 JAR 파일로 실행하기 실행을 위해 코드를 빌드합니다.\n./mvnw clean package 빌드된 JAR 파일을 실행합니다.\njava -jar target/rest-service-0.0.1-SNAPSHOT.jar 아래와 같이 서비스가 빠르게 실행되고, 내장 Tomcat을 통해 8080 포트로 실행되는 것을 빠르게 실행되는 것을 알 수 있습니다.\n$ java -jar target/rest-service-0.0.1-SNAPSHOT.jar . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v3.1.4) 2023-09-25T14:19:48.177Z INFO 6819 --- [ main] c.e.restservice.RestServiceApplication : Starting RestServiceApplication v0.0.1-SNAPSHOT using Java 17.0.8 with PID 6819 (/home/winter/rest-service/target/rest-service-0.0.1-SNAPSHOT.jar started by winter in /home/winter/rest-service) 2023-09-25T14:19:48.182Z INFO 6819 --- [ main] c.e.restservice.RestServiceApplication : No active profile set, falling back to 1 default profile: \u0026#34;default\u0026#34; 2023-09-25T14:19:51.126Z INFO 6819 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2023-09-25T14:19:51.143Z INFO 6819 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2023-09-25T14:19:51.144Z INFO 6819 --- [ main] o.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/10.1.13] 2023-09-25T14:19:51.390Z INFO 6819 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2023-09-25T14:19:51.402Z INFO 6819 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 2971 ms 2023-09-25T14:19:52.488Z INFO 6819 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 1 endpoint(s) beneath base path \u0026#39;/actuator\u0026#39; 2023-09-25T14:19:52.637Z INFO 6819 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path \u0026#39;\u0026#39; 2023-09-25T14:19:52.660Z INFO 6819 --- [ main] c.e.restservice.RestServiceApplication : Started RestServiceApplication in 5.509 seconds (process running for 6.585) 테스트를 위해 브라우저 탭을 하나 더 열고 동일한 Oracle Cloud 계정으로 접속하여 Cloud Shell을 실행합니다.\n두 번째 Cloud Shell에서 서비스를 테스트합니다.\ncurl http://localhost:8080/greeting; echo {\u0026#34;id\u0026#34;:1,\u0026#34;content\u0026#34;:\u0026#34;Hello, World!\u0026#34;} 첫 번째 Cloud Shell에서 실행되는 앱을 중지합니다.\nNative Image로 실행하기 실행을 위해 코드를 빌드합니다.\n./mvnw -Pnative native:compile 빌드 예시\nCloudShell에서는 CPU 자원이 제한되어 Native Image를 만드는 데 시간이 더 걸릴 수 있습니다. ... ======================================================================================================================== GraalVM Native Image: Generating \u0026#39;/home/winter/rest-service/target/rest-service\u0026#39; (executable)... ======================================================================================================================== [1/7] Initializing... (16.1s @ 0.26GB) Version info: \u0026#39;GraalVM 22.3.3 Java 17 EE\u0026#39; Java version info: \u0026#39;17.0.8+9-LTS-jvmci-22.3-b21\u0026#39; ... [2/7] Performing analysis... [**********] (175.1s @ 1.76GB) 17,049 (92.22%) of 18,488 classes reachable 27,095 (67.72%) of 40,008 fields reachable 88,975 (64.53%) of 137,888 methods reachable 1,046 classes, 285 fields, and 5,506 methods registered for reflection 64 classes, 70 fields, and 55 methods registered for JNI access 4 native libraries: dl, pthread, rt, z [3/7] Building universe... (20.7s @ 2.65GB) [4/7] Parsing methods... [****] (19.2s @ 1.84GB) [5/7] Inlining methods... [***] (7.7s @ 1.96GB) [6/7] Compiling methods... [******************] (350.2s @ 2.01GB) [7/7] Creating image... (20.2s @ 1.61GB) ... ... ... ======================================================================================================================== Finished generating \u0026#39;/home/donghee_le/rest-service/target/rest-service\u0026#39; in 10m 22s. [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 10:45 min [INFO] Finished at: 2023-09-25T16:05:30Z [INFO] ------------------------------------------------------------------------ 빌드된 Native Image를 실행합니다.\n./target/rest-service 아래와 같이 서비스가 빠르게 실행되고, 내장 Tomcat을 통해 8080 포트로 실행되는 것을 빠르게 실행되는 것을 알 수 있습니다. JAR로 실행할 때는 시작될때 까지 앞선 로그에서 5.509 초가 걸렸으나, 아래 로그 기준 0.079초가 걸립니다.\n./target/rest-service . ____ _ __ _ _ /\\\\ / ___\u0026#39;_ __ _ _(_)_ __ __ _ \\ \\ \\ \\ ( ( )\\___ | \u0026#39;_ | \u0026#39;_| | \u0026#39;_ \\/ _` | \\ \\ \\ \\ \\\\/ ___)| |_)| | | | | || (_| | ) ) ) ) \u0026#39; |____| .__|_| |_|_| |_\\__, | / / / / =========|_|==============|___/=/_/_/_/ :: Spring Boot :: (v3.1.4) 2023-09-25T16:08:52.803Z INFO 21140 --- [ main] c.e.restservice.RestServiceApplication : Starting AOT-processed RestServiceApplication using Java 17.0.8 with PID 21140 (/home/donghee_le/rest-service/target/rest-service started by donghee_le in /home/donghee_le/rest-service) 2023-09-25T16:08:52.803Z INFO 21140 --- [ main] c.e.restservice.RestServiceApplication : No active profile set, falling back to 1 default profile: \u0026#34;default\u0026#34; 2023-09-25T16:08:52.818Z INFO 21140 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8080 (http) 2023-09-25T16:08:52.819Z INFO 21140 --- [ main] o.apache.catalina.core.StandardService : Starting service [Tomcat] 2023-09-25T16:08:52.819Z INFO 21140 --- [ main] o.apache.catalina.core.StandardEngine : Starting Servlet engine: [Apache Tomcat/10.1.13] 2023-09-25T16:08:52.826Z INFO 21140 --- [ main] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 2023-09-25T16:08:52.826Z INFO 21140 --- [ main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 23 ms 2023-09-25T16:08:52.863Z WARN 21140 --- [ main] i.m.c.i.binder.jvm.JvmGcMetrics : GC notifications will not be available because MemoryPoolMXBeans are not provided by the JVM 2023-09-25T16:08:52.867Z INFO 21140 --- [ main] o.s.b.a.e.web.EndpointLinksResolver : Exposing 1 endpoint(s) beneath base path \u0026#39;/actuator\u0026#39; 2023-09-25T16:08:52.873Z INFO 21140 --- [ main] o.s.b.w.embedded.tomcat.TomcatWebServer : Tomcat started on port(s): 8080 (http) with context path \u0026#39;\u0026#39; 2023-09-25T16:08:52.873Z INFO 21140 --- [ main] c.e.restservice.RestServiceApplication : Started RestServiceApplication in 0.079 seconds (process running for 0.084) 테스트를 위해 브라우저 탭을 하나 더 열고 동일한 Oracle Cloud 계정으로 접속하여 Cloud Shell을 실행합니다.\n두 번째 Cloud Shell에서 서비스를 테스트합니다. 결과는 동일합니다.\ncurl http://localhost:8080/greeting; echo {\u0026#34;id\u0026#34;:1,\u0026#34;content\u0026#34;:\u0026#34;Hello, World!\u0026#34;} 첫 번째 Cloud Shell에서 실행되는 앱을 중지합니다.\nContainer Image 만들기 쿠버네티스에서 실행하기 위해서는 구동할 서비스 애플리케이션을 컨테이너화 하여야 합니다. Docker 클라이언트를 통해 컨테이너 이미지를 만듭니다.\nGetting Started | Spring Boot Docker 예시에서 보면 Dockerfile 베이스 이미지로 eclipse-temurin:17-jdk-alpine을 사용합니다.\n# Dockerfile FROM eclipse-temurin:17-jdk-alpine VOLUME /tmp ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} app.jar ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app.jar\u0026#34;] 여기서는 Oracle Container Registry (OCR) 에서 제공하는 Oracle GraalVM Container Image을 베이스 이미지로 사용합니다. 프로젝트 폴더에 Dockerfile을 아래와 같이 만듭니다.\nDockerfile: JAR 파일 사용시\nFROM container-registry.oracle.com/graalvm/jdk:17 WORKDIR /app ARG JAR_FILE=target/*.jar COPY ${JAR_FILE} app.jar ENTRYPOINT [\u0026#34;java\u0026#34;,\u0026#34;-jar\u0026#34;,\u0026#34;/app/app.jar\u0026#34;] Dockerfile.native: 빌드된 Native Image 사용시\nFROM container-registry.oracle.com/os/oraclelinux:8-slim WORKDIR /app ARG APP_FILE=target/rest-service COPY ${APP_FILE} /app/native ENTRYPOINT [\u0026#34;/app/native\u0026#34;] 이미지를 빌드합니다.\ndocker build -t spring-boot-greeting:1.0 . docker build -t spring-boot-greeting:1.0-native . -f Dockerfile.native 현재 로컬(여기서는 Cloud Shell)에 빌드된 이미지를 조회해 봅니다. Native Image는 JAR 파일을 실행하기 위한 JDK 등의 파일들이 필요없기 때문에 이미지 사이즈가 상대적으로 작습니다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE spring-boot-greeting 1.0 5d0eaaec7e6d 13 seconds ago 658MB spring-boot-greeting 1.0-native 21acc4018289 About a minute ago 189MB container-registry.oracle.com/os/oraclelinux 8-slim 254a7c26275f 3 days ago 110MB container-registry.oracle.com/graalvm/jdk 17 ba31190ec464 3 weeks ago 637MB OCIR에 이미지 등록하기 OCIR에 컨테이너 이미지를 푸시하기 위해서는 다음과 같은 이미지 태그 네이밍 규칙을 따라야 합니다. 아래 정보를 확인합니다.\n\u0026lt;region-key or region-identifier\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt;:\u0026lt;tag\u0026gt;\nregion-key: 지금은 Region Key, Region Identifier 둘다 지원하므로, 서울은 icn, ap-seoul-1, 춘천은 yny, ap-chuncheon-1을 쓰면 됩니다. 전체 주소 정보는 OCIR Available Endpoint에서 확인하세요. tenancy-namespace: OCI 콘솔 Tenancy 상세 정보에서 Object Storage Namespace로 확인하거나, Cloud Shell에서 oci os ns get으로 확인합니다. repo-name: 이미지 이름, 경로가 있는 경우 경로를 포함한 이름 winter@cloudshell:rest-service (ap-chuncheon-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;axjowrxxxxxx\u0026#34; } winter@cloudshell:rest-service (ap-chuncheon-1)$ echo $OCI_REGION ap-chuncheon-1 OCIR 등록을 위해 기존 이미지에 추가로 태그를 답니다.\ndocker tag spring-boot-greeting:1.0 ap-chuncheon-1.ocir.io/${tenancy_namespace}/spring-boot-greeting:1.0 docker tag spring-boot-greeting:1.0-native ap-chuncheon-1.ocir.io/${tenancy_namespace}/spring-boot-greeting:1.0-native 동일한 이미지에 태그가 추가된 것을 알 수 있습니다.\n$ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ap-chuncheon-1.ocir.io/axjowrxxxxxx/spring-boot-greeting 1.0 5d0eaaec7e6d 3 minutes ago 658MB spring-boot-greeting 1.0 5d0eaaec7e6d 3 minutes ago 658MB ap-chuncheon-1.ocir.io/axjowrxxxxxx/spring-boot-greeting 1.0-native 21acc4018289 3 minutes ago 189MB spring-boot-greeting 1.0-native 21acc4018289 3 minutes ago 189MB container-registry.oracle.com/os/oraclelinux 8-slim 254a7c26275f 3 days ago 110MB container-registry.oracle.com/graalvm/jdk 17 ba31190ec464 3 weeks ago 637MB OCIR에 이미지를 Push 하기 위해서는 Docker CLI로 OCIR에 로그인이 필요합니다. Username 및 Password는 다음과 같습니다.\nUsername: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;user-name\u0026gt; 형식으로 user-name은 OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다. Password: 사용자의 Auth Token을 사용합니다. My Profile \u0026gt; Auth tokens \u0026gt; Generate token 을 통해 생성합니다. 아래와 같이 Docker CLI로 로그인합니다.\n# IDCS 유저인 경우 docker login ap-chuncheon-1.ocir.io -u ${tenancy_namespace}/oracleidentitycloudservice/~~~ # OCI Native 유저인 경우 docker login ap-chuncheon-1.ocir.io -u ${tenancy_namespace}/~~~ winter@cloudshell:~ (ap-chuncheon-1)$ docker login ap-chuncheon-1.ocir.io -u ${tenancy_namespace}/winter Password: WARNING! Your password will be stored unencrypted in /home/winter/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded OCIR를 위해 단 이미지 태그를 사용하여 이미지를 Push합니다.\ndocker push ap-chuncheon-1.ocir.io/${tenancy_namespace}/spring-boot-greeting:1.0 docker push ap-chuncheon-1.ocir.io/${tenancy_namespace}/spring-boot-greeting:1.0-native OKE에 마이크로 서비스 배포하기 OCIR에 이미지를 사용하여 OKE에 컨테이너를 기동하기 위해서는 OKE에서 OCIR 이미지에 접근하는 권한이 필요합니다. OCIR Private Repository로 등록했기 때문에 OKE에 접속을 위한 secret를 생성합니다. 이미 Cloud Shell에서 Docker CLI로 OCI에 로그인 했으므로 해당 정보를 이용하여 생성합니다.\nkubectl create secret generic ocir-secret \\ --from-file=.dockerconfigjson=$HOME/.docker/config.json \\ --type=kubernetes.io/dockerconfigjson 다음 YAML 파일을 이용해 OKE에 배포합니다. Load Balancer 사용도 함께 진행하기 위해 Service Type도 함께 배포합니다.\n배포 파일 생성합니다. 예, 파일명: spring-boot-greeting.yaml\nIMAGE_REGISTRY_PATH: 각자에 맞게 수정 필요, JAR 이미지 사용하는 경우 예시) ap-chuncheon-1.ocir.io/axjowrxxxxxx/spring-boot-greeting:1.0 Native Image를 사용하는 이미지 사용하는 경우 예시) ap-chuncheon-1.ocir.io/axjowrxxxxxx/spring-boot-greeting:1.0-native apiVersion: apps/v1 kind: Deployment metadata: labels: app: spring-boot-greeting name: spring-boot-greeting-deployment spec: replicas: 1 selector: matchLabels: app: spring-boot-greeting template: metadata: labels: app: spring-boot-greeting spec: containers: - name: spring-boot-greeting image: $IMAGE_REGISTRY_PATH imagePullSecrets: - name: ocir-secret --- apiVersion: v1 kind: Service metadata: name: spring-boot-greeting-service spec: selector: app: spring-boot-greeting ports: - protocol: TCP port: 80 targetPort: 8080 type: LoadBalancer 작성한 yaml 파일을 통해 개발한 Spring Boot 앱을 배포합니다.\nkubectl apply -f spring-boot-greeting.yaml kubectl get all 명령으로 배포된 자원을 확인합니다.\n$kubectl get all NAME READY STATUS RESTARTS AGE pod/spring-boot-greeting-deployment-84c4865b98-7rmrp 1/1 Running 0 34s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 10h service/spring-boot-greeting-service LoadBalancer 10.96.75.242 150.xxx.xxx.xxx 80:32418/TCP 35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spring-boot-greeting-deployment 1/1 1 1 35s NAME DESIRED CURRENT READY AGE replicaset.apps/spring-boot-greeting-deployment-84c4865b98 1 1 1 35s Pod가 정상적으로 기동하였습니다. LoadBalancer의 EXTERNAL-IP를 통해 서비스를 요청합니다.\ncurl http://150.xxx.xxx.xxx/greeting; echo {\u0026#34;id\u0026#34;:1,\u0026#34;content\u0026#34;:\u0026#34;Hello, World!\u0026#34;} JAR 파일 컨테이너 이미지, Native Image 컨테이너 이미지 둘 중 어느 것을 사용해도 결과는 당연히 동입합니다. 대신 Native Image를 사용하면, 이미지 사이즈도 작고, 기동 자체도 빨라 컨테이너 Pod 기동이 더 빠릅니다.\n","lastmod":"2023-09-26T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/appdev/2.develop-spring-boot-app/","tags":["appdev","graalvm","springboot3"],"title":"8.2 GraalVM 사용하여 Spring Boot 3 애플리케이션 개발하기"},{"categories":null,"contents":"1.1.1 OKE, OKE Managed Nodes 소개 Oracle Container Engine for Kubernetes는 OCI에서 제공하는 관리형 쿠버네티스 서비스 입니다. 줄여서 OKE라고 부며 CNCF 인증된 Kubernetes 버전을 사용하고 있습니다.\nQuick Start 버전으로 클러스터를 생성하게 되면 OCI 자원들을 사용하여 아래와 같은 구조를 가지는 쿠버네티스 클러스터가 생성됩니다.\nControl Plane Node:\n쿠버네티스는 Control Plane은 OCI가 관리하는 영역으로 사용자에게 직접 노출이 되지 않습니다. API Server 접속을 위한 정보만 VNIC을 통해 사용자의 VCN상의 API Subnet으로 노출하게 됩니다. 생성 옵션에 따라 Public, Private 모두 지원 Data Plane Nodes\n실제 사용자의 컨테이너가 구동되는 Worker Node 들을 VM 또는 베어메탈 서버를 사용합니다. 개발자가 직접 Node 에 접속할 수 있습니다. 생성 옵션에 따라 Public, Private 모두 지원 OCI 서비스 연계\n컨테이너 배포하고 서비스 하기 위해 필요한 Container Image Registry, Persistent Volume, Load Balancer, IAM, Audit 등 OCI 다른 서비스들과 연계된 기능을 제공합니다. OKE 클러스터 자체에 대한 비용은 무료이며, 클라스터가 사용하는 OCI 자원(예시, VM Node, 네트워크 사용량, 스토리지 사용량)에 대한 비용만 청구하는 방식입니다.\n","lastmod":"2022-01-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/basics/1.oke-managed-nodes/","tags":["oke"],"title":"1.1.1 OKE, OKE Managed Nodes 소개"},{"categories":null,"contents":"15.1 Resource Manager 사용하기 Resource Manager는 OCI 자원 Provisioning을 자동화하는 기능으로 Terraform Configuration을 등록해서 실행하는 서비스입니다. 클라이언트에 Terraform을 설치하는 것이 아닌, Resource Manager에서 Terraform Configuration 파일들을 등록해서 OCI 콘솔 또는 OCI CLI로 실행할 수 있습니다.\nTerraform CLI를 별도로 설치할 필요가 없고, Terraform 실행 State를 Resource Manager가 관리한다는 차이가 있습니다.\nStack\n스택은 Terraform Configuration 묶음을 등록하여 생성하는 Provisioning 단위입니다.\nJob\nJob은 스택으로 등록된 Terraform Configuration의 실행 작업이며, Terraform에서 경험한 것 처럼 Terraform Plan, Apply, Destroy이 실행되는 작업입니다.\nResource Manager 사용하기 Terraform Configuration 만들기 14.2.2 Terraform Configurations 작성하기에서 사용한 VCN 만들기 설정파일을 그대로 사용해 보겠습니다. 다만 차이는 oci provider에서 region만 필수 필드이고, 나머지 항목들은 필수가 아닙니다. 등록한 Resource Manager의 Tenancy에서 실행되고, 인증받은 유저가 실행하기 때문에 나머지 항목은 필수가 아닙니다.\nprovider.tf\nvariable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { region = \u0026#34;${var.region}\u0026#34; } vcn.tf\nvariable \u0026#34;compartment_ocid\u0026#34; {} resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; dns_label = \u0026#34;vcn1\u0026#34; compartment_id = \u0026#34;${var.compartment_ocid}\u0026#34; display_name = \u0026#34;vcn1\u0026#34; } output \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_virtual_network.vcn1.id}\u0026#34;] } 다음 변수들을 OCI 콘솔 정보를 통해 자동으로 계산됩니다. tenancy_ocid compartment_ocid region current_user_ocid https://docs.oracle.com/en-us/iaas/Content/ResourceManager/Concepts/terraformconfigresourcemanager.htm#configvar Terraform Configuration을 Resource Manager 용으로 변경하기\nOCI Console에서 Resource Manager를 사용하기 때문에 UI가 존재합니다. Terraform 설정값을 UI에서 설정할 수 있게 확장할 수 있는 스키마를 제공합니다.\nExtend Console Pages Using Schema Documents provider.tf\nvariable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { region = \u0026#34;${var.region}\u0026#34; } vcn.tf 변경분\nvariable \u0026#34;compartment_ocid\u0026#34; {} variable \u0026#34;vcn_name\u0026#34; {} variable \u0026#34;vcn_dns_label\u0026#34; {} variable \u0026#34;vcn_cidr_block\u0026#34; {} resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { cidr_block = var.vcn_cidr_block dns_label = var.vcn_dns_label compartment_id = var.compartment_ocid display_name = var.vcn_name } output \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_virtual_network.vcn1.id}\u0026#34;] } schema.yaml\nStack 설명구 추가, 변수 타이틀, 설명문 추가, 변수 UI 배열 구문 등을 아래와 같이 추가할 수 있습니다.\ntitle: \u0026#34;Resource Manager Example VCN by TheKoguryo\u0026#34; description: \u0026#34;Resource Manager로 VCN을 만드는 예제입니다.\u0026#34; schemaVersion: 1.0.0 version: \u0026#34;20220119\u0026#34; locale: \u0026#34;en\u0026#34; groupings: - title: \u0026#34;Basic Hidden\u0026#34; visible: false variables: - compartment_ocid - tenancy_ocid - region - title: \u0026#34;Basic Configuration\u0026#34; variables: - vcn_name - vcn_dns_label - title: \u0026#34;Configure VCN\u0026#34; variables: - vcn_cidr_block variables: vcn_name: type: string title: VCN Name required: true default: example_vcn vcn_dns_label: type: string title: VCN DNS Label description: \u0026#34;Only letters and numbers, starting with a letter. 15 characters max.\u0026#34; required: true default: examplevcn vcn_cidr_block: type: list(string) title: VCN IPv4 CIDR Block description: \u0026#34;Example: 10.0.0.0/16\u0026#34; required: true default: [\u0026#34;10.0.0.0/16\u0026#34;] OCI 콘솔에서 내비게이션 메뉴의 Developer Services \u0026gt; Resource Manager \u0026gt; Stacks을 클릭합니다.\nCreate Stack 을 클릭합니다.\nStack 생성 설정\n첫 번째 My Configuration으로 작성한 파일이 위치한 폴더를 Drop a folder로 드래그 앤 드랍합니다.\n설정 로딩이 되어 Stack의 기본 설명이 아래와 같이 보이게 됩니다.\n다음 페이지로 넘어갑니다.\n변수 값 입력\n작성한 schema.yaml을 바탕으로 그림과 같이 표현됩니다.\n값을 변경해 보고 리뷰후 Stack을 생성합니다.\nterraform plan 명령처럼 Plan 버튼을 클릭해봅니다.\nTerraform CLI에서 terraform plan을 수행했던 것과 동일한 결과가 로그로 확인됩니다.\n다시 Stack 상세화면으로 돌아가 Apply를 클릭하여 실행해 봅니다.\n생성이 성공하면 Resouces \u0026gt; Job Resources 에서 결과물로 생성된 자원을 확인할 수 있습니다.\n링크를 따라 가면 실제 VCN이 생성된 것을 확인할 수 있습니다.\nStack 상세 정보로 돌아가 Destroy 버튼을 클릭합니다.\n실행이 끝나고 VCN 리스트 화면으로 이동해 보면, 앞서 생성된 example_vcn이 삭제된 것을 알수 있습니다.\nStack의 지금까지 Plan, Apply, Destroy를 실행한 모든 작업 결과 내역은 Stack 상세 화면에서 볼 수 있습니다.\n","lastmod":"2022-01-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter15/1/","tags":["resource manage","terraform"],"title":"15.1 Resource Manager 사용하기"},{"categories":null,"contents":"14.1.2.1 Object Storage를 위한 IAM Policy 설정 Object Storage를 위한 Policy 설정 IAM Policy는 OCI 자원에 대한 접근 정책입니다. 하나의 정책은 일반적으로 사용자 그룹에게 특정 Compartment에 속한 특정 타입의 자원에 대한 권한을 허용하는 것을 정의하는 것으로 생각하면 하면 됩니다.\nObject Storage, Archive Storage, and Data Transfer 레퍼런스\nhttps://docs.cloud.oracle.com/iaas/Content/Identity/Reference/objectstoragepolicyreference.htm 예시\nObject 목록 조회\nAllow group \u0026lt;group_name\u0026gt; to inspect objects in compartment \u0026lt;compartment_name\u0026gt; Object 읽기\nAllow group \u0026lt;group_name\u0026gt; to read objects in compartment \u0026lt;compartment_name\u0026gt; Object 사용(읽기 \u0026amp; 업데이트)\nAllow group \u0026lt;group_name\u0026gt; to use objects in compartment \u0026lt;compartment_name\u0026gt; Object 관리(사용, 생성, 삭제, 복구)\nAllow group \u0026lt;group_name\u0026gt; to manage objects in compartment \u0026lt;compartment_name\u0026gt; Step 1. Bucket 생성 사용자가 쓸 Object Storage Bucket 을 해당 Compartment에 미리 생성합니다. Name: 예) ExampleBucketForCLI Step 2. API 접근 사용자에 권한 설정 관리자로 OCI 콘솔에 로그인합니다.\nPolicy 설정\noci cli로 연결되는 유저, 즉 oci setup config에서 설정한 유저가 속한 그룹에 다음 Policy가 필요합니다.\nAllow group \u0026lt;group_name\u0026gt; to inspect buckets in compartment \u0026lt;compartment_name\u0026gt; Allow group \u0026lt;group_name\u0026gt; to manage objects in compartment \u0026lt;compartment_name\u0026gt; 예시\n이름: ObjectStorageToolPolicy\n규칙\nAllow group ObjectStorageToolGroup to inspect buckets in compartment Sandbox Allow group ObjectStorageToolGroup to manage objects in compartment Sandbox ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/2/1/","tags":["object storage","policy"],"title":"14.1.2.1 Object Storage를 위한 IAM Policy 설정"},{"categories":null,"contents":"14.3.1 Terraform 설치하기 HashiCorp 다운로드 페이지에서 OS 맞는 설치파일을 다운받습니다. 압축을 풀면 terraform.exe 또는 terraform 파일 하나이므로 사용할 폴더에 복사하고 PATH에 등록하면 바로 사용할 수 있습니다.\nLinux 예시\nwget https://releases.hashicorp.com/terraform/1.4.6/terraform_1.4.6_linux_amd64.zip unzip terraform_1.4.6_linux_amd64.zip sudo mv terraform /usr/local/bin terraform -v ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/1/","tags":["terraform"],"title":"14.3.1 Terraform 설치하기"},{"categories":null,"contents":"13.1 Autoscaling을 위한 준비 Step #1: Autoscaling에 사용할 템플릿 Compute 인스턴스 생성 Autoscaling을 사용하려면 기준 이미지가 필요합니다. Oracle Linux에 Apache가 설치된 템플릿 인스턴스를 먼저 만들겠습니다. 설치 스크립트는 cloud-init을 사용하고, 추가 인스턴스임을 확인하기 쉽게 하기 위해 index.html에 생성된 호스트의 Private IP가 보이도록 합니다.\n왼쪽 상단의 Navigation Menu를 클릭하고 Compute으로 이동한 다음 Instances를 선택합니다.\n새 Compute Instance를 생성합니다.\nName: Web-Server-Template으로 입력합니다\n나머지는 이전과 동일하게 필요한 값들을 설정합니다.\n추가적으로 제일 아래쪽 Show advanced options 클릭\ncloud-init 을 사용하여 인스턴스 생성시 시작할 스크립트를 설정할 수 있습니다. 여기서는 bash 스크립트를 사용하여 Apache 설치 및 index.html 생성작업을 실행합니다.\n#!/bin/bash sudo yum -y install httpd sudo firewall-offline-cmd --add-port=80/tcp MY_IP=$(hostname -I) sudo bash -c \u0026#34;echo \u0026#39;Hello Apache on \u0026#39;$MY_IP \u0026gt;\u0026gt; /var/www/html/index.html\u0026#34; sudo systemctl start httpd sudo systemctl enable httpd sudo systemctl restart firewalld 아래쪽 Create를 클릭하여 인스턴스를 만듭니다.\ncloud-init 로그\n/var/log/cloud-init-output.log 파일에서 cloud-init의 실행로그를 확인할 수 있습니다. 패키지 설치 등으로 인해 인스턴스 생성직후 약간의 시간이 걸립니다.\ncloud-init 로그 예시\nCloud-init v. 22.1-6.0.4.el8_7.2 running \u0026#39;init-local\u0026#39; at Tue, 09 May 2023 10:44:10 +0000. Up 17.71 seconds. Cloud-init v. 22.1-6.0.4.el8_7.2 running \u0026#39;init\u0026#39; at Tue, 09 May 2023 10:44:12 +0000. Up 19.65 seconds. ... Installed: apr-1.6.3-12.el8.x86_64 apr-util-1.6.1-6.el8.x86_64 apr-util-bdb-1.6.1-6.el8.x86_64 apr-util-openssl-1.6.1-6.el8.x86_64 httpd-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 httpd-filesystem-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.noarch httpd-tools-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 mod_http2-1.15.7-5.module+el8.7.0+21029+de29ba63.4.x86_64 oracle-logos-httpd-84.5-1.0.1.el8.noarch Complete! success Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service. Cloud-init v. 22.1-6.0.4.el8_7.2 running \u0026#39;modules:final\u0026#39; at Tue, 09 May 2023 10:44:17 +0000. Up 24.13 seconds. Cloud-init v. 22.1-6.0.4.el8_7.2 finished at Tue, 09 May 2023 10:45:59 +0000. Datasource DataSourceOracle. Up 126.30 seconds 주의사항 cloud-init에서 앞서 Load Balancer 실습에서 사용한 firewall-cmd 구문을 그대로 사용하면 cloud-init시 다음 에러가 발생합니다. 그래서 대신 firewall-offline-cmd 구문을 사용합니다. Mar 5 05:50:46 instance-20190305-1447 cloud-init: ERROR:dbus.proxies:Introspect error on :1.3:/org/fedoraproject/FirewallD1: dbus.exceptions.DBusException: org.freedesktop.DBus.Error.NoReply: Did not receive a reply. Possible causes include: the remote application did not send a reply, the message bus security policy blocked the reply, the reply timeout expired, or the network connection was broken. Step #2: 생성한 인스턴스로 Instance Configuration 생성 Autoscaling시 만들어진 Instance의 구성정보를 만들기 위해 Instance Configuration이 필요합니다. 다음과 같은 순서로 만듭니다\n생성된 인스턴스의 상세 화면으로 이동합니다.\n추가 액션 메뉴 중 Create instance configuration 클릭\nInstance Configuration 이름 입력 후 Create Instance Configuration 클릭\n생성 완료\nStep #3: Instance Configuration을 이용해 찍어낼 Instance Pool 생성 Instance Pool은 Instance Configuration을 통해 만들어진 같은 구성기반의 인스턴스들의 모음입니다.\nInstance Pool에서 Load Balancer를 사용하기 위해서는 사전에 만들어야 합니다. 이전 장에서 만든 Load Balancer를 사용하거나, 없는 경우 미리 만듭니다.\n앞서 만든 Instance Configuration에서 Create instance pool을 클릭 Instance Pool 생성 기본 정보\nName: 예, web-server-instance-pool Create in compartment: 사용중인 Compartment Number of instances: 1 위치 정보\n생성될 VM 들이 위치할 도메인 및 VCN, Subnet을 선택합니다.\nLoad Balancer 설정\n인스턴스들간의 분배를 위해 Load Balacner가 필요하므로 Attach a load balancer을 체크합니다.\nLoad Balancer 장에서 만든 것을 그대로 사용합니다. 없으면 새로 만듭니다.\nLoad balancer: 사용할 Load Balancer 선택\nBackend set: Load Balancer의 분배 대상이 되어 Backend Set 선택\nPort: 수신할 포트 입력\nVNIC: Primary VNIC만 있으므로 선택\n다음 페이지에서 선택한 설정을 확인후 Create 합니다.\nInstance Pool이 만들어지고 인스턴스 1개가 만들어 집니다.\nResources \u0026gt; Load balancer 로 이동하여 연관된 Load Balancer을 클릭합니다.\nLB 상세정보에서 Backend Set으로 이동하여 Instance Pool이 사용하는 Backend Set 클릭\nBackend Set에 자동으로 추가된 것을 확인할 수 있습니다. 또한 Health가 OK인 상태로 생성된 VM에 있는 Apache 서버와 정상 통신 된 것을 알 수 있습니다.\n브라우저를 통해 LB의 Public IP로 접속합니다. 정상적으로 구성되었음을 알 수 있습니다.\n","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter13/1/","tags":["autoscaling","cloud-init"],"title":"13.1 Autoscaling을 위한 준비"},{"categories":null,"contents":"12.1 Compute 인스턴스 Monitoring 활성화 Monitoring 활성화 아래는 OracleCloudAgent가 이미 설치된 최신 Oracle Linux 이미지를 사용하였습니다.\n모니터링하려는 Compute 인스턴스의 상세 페이지로 이동합니다.\n왼쪽 아래 Resources \u0026gt; Metrics 선택\nCompute 인스턴스에 대한 메트릭을 확인할 수 있습니다. Boot Volumes, Block Volumes, VNICs Compute를 제외한 모니터링을 지원하는 다른 서비스들을 기본적으로 메트릭 정보가 취합됩니다. Compute에 장착된 자원에 대한 모니터링을 확인합니다.\nCompute 인스턴스의 왼쪽 아래 Resources \u0026gt; Boot Volume 선택후 장착된 Boot Volume을 클릭합니다. Compute 인스턴스 상세화면으로 돌아가서 장착된 VNIC과 Block Volume에 대해서도 동일하게 메트릭을 볼 수 있습니다.\n장착된 Block Volumes이 있는 경우 동일하게 모니터링됩니다.\n","lastmod":"2022-01-16T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/1/","tags":["monitoring"],"title":"12.1 Compute 인스턴스 Monitoring 활성화"},{"categories":null,"contents":"9.1 기본 제공하고 있는 OS Image Compute Instance 생성을 위한 OS Image 및 애플리케이션 Image를 제공하며, 사용자가 만든 Custom Image, Boot Volumes을 통해 Instance를 생성할 수 도 있습니다. 그림처럼 Compute Instance 생성시 오른쪽 이미지 선택 메뉴에서 고를 수 있는 현재 기본 제공하고 있는 OS Image 목록입니다. 제공 이미지 설치 OS 이미지 선택\nPlatform Images\nOracle Linux, Ubuntu, CentOS, Window 등 OS 이미지를 제공합니다.\nOS를 선택하면, 상세 OS 버전을 선택할 수 있습니다.\nMarketplace Images\nOracle과 파트너사에 제공하는 애플리케이션 설치를 위한 이미지를 제공합니다.\nMy images\n유저가 커스텀 이미지를 만들어 사용할 수 있습니다.\n","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter09/1/","tags":["os image","image"],"title":"9.1 기본 제공하고 있는 OS Image"},{"categories":null,"contents":"7.1 Bucket 생성하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Object Storage \u0026amp; Archive Storage \u0026gt; Buckets 항목으로 이동합니다.\nBucket을 생성할 Compartment를 선택합니다.\nCreate Bucket을 클릭합니다.\n생성 정보 입력\nBucket Name: 생성할 Bucket 이름을 입력합니다, 예) \u0026ldquo;ExampleBucket\u0026rdquo; Default Storage Tier: Standard: 표준 Object Storage로 빠르게 자주 사용할 데이터를 저장하기 위해 사용합니다. Archive: 가끔 사용하지만, 장기간 보관 해야 할 데이터를 저장하기 위해 사용합니다. STARNDARD 보다는 저렴하지만, 데이터 사용시 로딩 시간이 조금 더 걸립니다. 추가 옵션: 현재는 기본값 사용 Create을 클릭합니다.\n생성이 완료되면 상세 정보를 보기 위해 이름을 클릭합니다.\n생성된 Bucket 상세 정보\nBucket이 생성되었습니다. 이제 Object를 추가해서 사용할 수 있습니다.\n","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/1/","tags":["object storage","bucket"],"title":"7.1 Bucket 생성하기"},{"categories":null,"contents":"4.1 Reserved Public IP 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; IP Management \u0026gt; Reserved Public IPs 항목으로 이동합니다.\nReserve Public IP Address를 클릭합니다.\n생성 정보 입력\nName: 생성할 Reserved Public IP의 이름 입력\nCompartment: 위치할 Compartment 선택\nIP Address Source: 사용자가 가진 BYOIP가 있는 경우 선택할 수 있겠으나, 따로 없으면 Oracle로 선택\nReserve Public IP Address 클릭\nIP가 예약되었습니다. 반납(삭제)하지 않는 동안 원하는 자원에 부여하여 사용할 수 있습니다.\n","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter04/1/","tags":["reserved public ip"],"title":"4.1 Reserved Public IP 만들기"},{"categories":null,"contents":"3.1 작업 Compartment 만들기 Compartment는 자원들을 쉽게 관리할 수 있도록 하는 논리적인 개념으로 폴더 구조라고 생각하면 됩니다. Tenancy가 생성되면 최초로 Root Compartment 하나가 만들어져 있으며, 관리자가 Root Compartment 하위로 새로운 Compartment를 추가할 수 있습니다. 모든 OCI 자원들은 특정 Compartment에 속하게 되며 Compartment 단위로 사용자들의 접근 정책을 관리할 수 있습니다.\nCompartment, 사용자 관리, 접근 정책 관리 등은 Identity and Access Management에서 이후에 알아 보겠습니다.\n여기서는 Compute VM 생성작업을 위한 전 단계로 Compartment만 만들도록 하겠습니다.\nStep 1. Sandbox Compartment 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments 항목으로 이동합니다.\nCreate Compartment 클릭\n생성 정보 입력\nName: \u0026ldquo;oci-hol-xx\u0026rdquo; 입력 Description: 설명 입력, 예) 이름과 동일하게 \u0026ldquo;oci-hol-xx\u0026rdquo; Parent Compartment: 상위 Compartment 지정, Root Compartment 선택 Create Compartment 클릭\n만들어진 Compartment는 일종의 폴더로 이후 생성할 자원들을 관리할 공간이라고 생각하면 됩니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/1/","tags":["oci user"],"title":"3.1 작업 Compartment 만들기"},{"categories":null,"contents":"3.5.1 Linux, Mac 에서 접속하기 ssh-keygen로 생성한 PEM(Privacy Enhanced Mail) 파일 형식의 키는 Linux, Mac에서는 바로 사용 가능합니다. PuTTY Key Generator로 생성한 .ppk은 변환기를 통해 PEM 형식으로 변환하여 사용합니다.\n유닉스 스타일 시스템에서 Linux 인스턴스 접속하기 개인키 권한 변경\n$ chmod 400 \u0026lt;private_key\u0026gt; \u0026lt;private_key\u0026gt;: Linux 인스턴스에 등록된 공개키에 매칭되는 개인키 SSH 명령을 통해 접속\n$ ssh –i \u0026lt;private_key\u0026gt; \u0026lt;username\u0026gt;@\u0026lt;public-ip-address\u0026gt; \u0026lt;private_key\u0026gt;: Linux 인스턴스에 등록된 공개키에 매칭되는 개인키 \u0026lt;username\u0026gt;: 인스턴스의 디폴트 사용자입니다. \u0026lt;public-ip-address\u0026gt;: 인스턴스의 Public IP입니다. OCI 콘솔에서 확인할 수 있습니다. 디폴트 접속 사용자 OS Image Default User Name Oracle Linux, CentOS opc ubuntu ubuntu 접속 예시 - 유닉스 스타일 시스템 $ ssh -i privateKey opc@144.24.xxx.xxx FIPS mode initialized The authenticity of host \u0026#39;144.24.xxx.xxx (144.24.xxx.xxx)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:svS10DcrVps4neg/t9lYoutfow+V5a6UHU2VrC3UJ2Y. ECDSA key fingerprint is SHA1:2rtJUm8jt0QiYR7b+I3Ra6j4Yac. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;144.24.xxx.xxx\u0026#39; (ECDSA) to the list of known hosts. Activate the web console with: systemctl enable --now cockpit.socket [opc@examplelinuxinstance ~]$ ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/1/","tags":["linux","ssh"],"title":"3.5.1 Linux, Mac 에서 접속하기"},{"categories":null,"contents":"3.6.1 Block Volume 생성하기 Block Volume은 OCI Compute Instance와 함께 사용하는 네트워크 스토리지입니다. Block Volume을 생성하여 인스턴스에 장착하고 마운트하면 컴퓨터에 물리적인 하드 드라이브를 연결해서 사용하는 것처럼 사용할 수 있습니다. 한 번에 한 Instance에 장착될 수 있으면, 장착해제 후 다른 Instance에 장착할 수 있습니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Block Storage \u0026gt; Block Volumes 항목으로 이동합니다.\nCreate Block Volume을 클릭합니다.\n생성 정보 입력\nName: blockvolume-xx\nCompartment: 이전 단계에서 만든 compartment를 선택합니다.\nAvailability Domain: AD가 3중화 되어 있는 경우, 컴퓨트 인스턴스와 같은 AD로 선택합니다.\nVolume Size and Performance에서 Custom을 선택하여 사이즈를 변경합니다.\n노트: 사이즈는 50 GB에서 32 TB까지 설정이 가능하며, 범위내에서 1 GB 단위로 조절가능합니다. 기본값은 1024 GB(1TB)입니다. 여기서는 최소값을 선택합니다.\nSize: 50 GB 설정 볼륨 성능(IOPS/GB)을 조절할 수 있습니다. 여기서는 기본값은 Balanced를 그대로 사용합니다.\n노트: Block Volume 성능은 Volume Performance Units(VPUs) 개념을 사용합니다. GB 당 IOPS를 높이기 위해 VPU를 높이거나, 비용절감을 위해 낮출 수 있습니다. Lower Cost, Balanced, Higher Performance, Ultra High Performance 레벨이 있으며, 변경을 통해 성능을 조절할 수 있습니다. 자세한 사항은 공식 문서 Block Volume Performance Levels를 참조합니다.\nLower Cost: 스트리밍, 로그 처리 및 데이터 웨어하우스와 같은 대규모 순차 I/O가 있는 처리량 집약적 워크로드에 권장됩니다. 비용은 스토리지에 대한 비용뿐이며 추가 VPU 비용은 없습니다. 이 옵션은 블록 볼륨에만 사용할 수 있으며 부트 볼륨에는 사용할 수 없습니다.\nBalanced: 신규 및 기존 블록 및 부트 볼륨에 대한 기본 성능 수준이며 대부분의 워크로드에 대해 성능과 비용 절감을 고려한 기본 성능을 제공합니다. 이 옵션을 사용하면 매월 GB당 10개의 VPU를 구매하게 됩니다.\nHigher Performance: 보다 높은 I/O 요구 사항이 있는 워크로드에 권장됩니다. 이 옵션을 사용하면 매월 GB당 20개의 VPU를 구매하게 됩니다.\nUltra High Performance: I/O 요구 사항이 가장 높고 가능한 최고의 성능이 필요한 워크로드에 권장됩니다. 이 옵션을 사용하면 GB당 월 30 ~ 120 VPU 중에서 구매할 수 있습니다.\n노트: Block Volume에 대해서 자동 튜닝 기능을 제공합니다.\nTarget Volume Performance 화면에서 Performance Based Auto-tune 활성화하면 VPU의 최소, 최대값을 지정할 수 있으며, Block Volume의 성능에 병목이 발생하는 경우 해당 범위 내에서 증가되며, 사용이 줄면 자동으로 VPU를 줄입니다. Detached Volume Auto-tune을 활성화하면, 컴퓨트 인스턴스에서 탈착되어 더이상 사용하지 않는 경우, 24시간 후에 자동으로 Lower Cost로 VPU에 대한 비용을 줄일 수 있습니다. 백업 정책은 여기서는 따로 설정하기 않습니다.\n다른 값들은 기본값으로 두고 Create Block Volume을 클릭하여 볼륨을 생성합니다. AVAILABLE 상태가 될때까지 기다립니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/1/","tags":["block volume"],"title":"3.6.1 Block Volume 생성하기"},{"categories":null,"contents":"3.7.1 Block Volume Full 백업하기 이전 가이드에서 Block Volume을 삭제한 경우, 다시 Block Volume을 만듭니다.\n기본 Block Volume 백업하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Block Storage \u0026gt; Block Volumes 항목으로 이동합니다.\n백업하려는 Block Volume의 이름을 클릭합니다.\nBlock Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Block Volume Backups을 클릭합니다.\nCreate Block Volume Backup 을 클릭합니다. 백업 생성화면에서 이름을 입력하고 하단의 Create Block Volume Backup을 클릭합니다.\nBackup Type으로 풀 백업과 증분 백업을 설정할 수 있습니다.\nFull Backup: 볼륨 생성이후 모든 변경사항을 백업합니다. Incremental Backup: 마지막 Backup 이후의 모든 변경사항을 백업합니다. 볼륨 생성이후 첫 백업이 Incremental Backup인 경우 Full Backup이 발생됩니다. 백업이 완료되면 AVAILBLE 상태로 표시됩니다.\n매뉴얼 증분 백업하기 Block Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Block Volume Backups을 클릭합니다.\nCreate Block Volume Backup 을 클릭합니다.\n백업 생성화면에서 이름을 입력하고 BACKUP TYPE으로 증분 백업을 선택합니다. 그리고 하단의 Create Block Volume Backup을 클릭합니다.\n백업이 완료되면 AVAILBLE 상태로 표시됩니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/1/","tags":["block volume","backup"],"title":"3.7.1 Block Volume 백업하기"},{"categories":null,"contents":"3.8.1 Boot Volume 백업하기 Boot Volume 백업 기능은 Block Volume 백업 기능과 거의 같습니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Boot Volume 상세 페이지로 이동합니다.\n방법 #1 Compute \u0026gt; Instances 항목으로 이동합니다. 대상 인스턴스 VM을 클릭합니다. Resources \u0026gt; Boot volume으로 이동하여 Attached 된 Boot Volume을 클릭하여 상세 페이지로 이동합니다. 방법 #2 Storage \u0026gt; Block Volume 항목으로 이동합니다. 왼쪽 메뉴에서 Boot Volumes로 이동하여 원하는 Boot Volume을 클릭하여 상세 페이지로 이동합니다. Boot Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Boot Volume Backups를 클릭합니다.\nCreate Boot Volume Backup 을 클릭합니다.\n백업 생성화면에서 이름을 입력하고 하단의 Create Boot Volume Backup을 클릭합니다.\nBackup Type으로 풀 백업과 증분 백업을 설정할 수 있습니다. Block Volume과 동일합니다.\nFull Backup: 볼륨 생성이후 모든 변경사항을 백업합니다.\nIncremental Backup: 마지막 Backup 이후의 모든 변경사항을 백업합니다. 볼륨 생성이후 첫 백업이 Incremental Backup인 경우 Full Backup이 발생됩니다.\n백업이 완료되면 Available 상태로 표시됩니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/8/1/","tags":["boot volume","backup"],"title":"3.8.1 Boot Volume 백업하기"},{"categories":null,"contents":"1.1 Regions, Availability Domains OCI(Oracle Cloud Infrastructure)는 Region과 Availability Domain 안에서 호스팅 됩니다. Region은 지리적인 영역이며 Availability Domain은 한 Region 내에 위치한 데이터 센터입니다. 단일 Region은 하나 이상의 Availability Domain으로 구성됩니다.\nAvailability Domain들은 동시에 결함이 발생하거나, 다른 Availability Domain의 결함에 영향을 받지 않도록 서로 분리되어 있습니다. Availability Domain은 파워, 쿨링 및 내부 네트워크 같은 자원들을 공유하지 않아 다른 Availability Domain에 영향이 없도록 설계되어 있습니다.\nRegion 내의 모든 Availability Domain은 상호 간 높은 가용성을 제공하기 위해 서로 간에 low latency, high bandwidth network로 연결되어 있습니다.\nRegion은 다른 Region과 완전히 독립되어 있으며, Region에서 발생할 수 있는 장애를 대비해 각 Region은 다른 나라 또는 다른 대륙의 멀리 떨어진 위치에 구성되어 있으며, 다양한 요구에 따라 다른 Regions의 서비스를 사용할 수도 있습니다.\nRegion 단위로 발생할 수 있는 위험, 즉 기상 이변, 지진 등 장애 대비 법적인 문제나 다른 다양한 비즈니스적으로 충족하기 위해 Public Cloud Region 현재 41개의 Region이 있으며 10개 Region을 추가하는 계획을 가지고 있습니다. 대한민국에서는 서울, 춘천 두 개의 Region이 있습니다. Region 전체 리스트 및 제공 서비스 등 자세한 사항은 아래 링크를 참고하세요.\nhttps://www.oracle.com/cloud/public-cloud-regions/ https://www.oracle.com/cloud/data-regions/#apac Oracle is adding multiple cloud regions around the world to provide local access to cloud resources for our customers. To accomplish this quickly, we’ve chosen to launch regions in new geographies with one availability domain.\n=\u0026gt; 최근에는 고객 요구에 맞춘 빠른 확장을 위해 단일 가용 도메인을 가진 Region을 만들고 있으며, 단일 국가에 Region 2개 또는 바로 지리적으로 인접한 국가에 Region을 통해 가용성을 제공하고 있습니다. 물론 단일 가용 도메인 내에서는 Fault Domain을 통해 물리적인 가용성을 지원하고 있습니다. 이러한 정책으로 초기 설립된 Region 4개외에는 모두 가동 도메인 하나이며 서울, 춘천 Region 또한 가용 도메인 하나입니다.\nPublic Cloud Regions 목록 https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm Region Name Region Identifier Region Location Region Key Realm Key Availability Domains Australia East (Sydney) ap-sydney-1 Sydney, Australia SYD OC1 1 Australia Southeast (Melbourne) ap-melbourne-1 Melbourne, Australia MEL OC1 1 Brazil East (Sao Paulo) sa-saopaulo-1 Sao Paulo, Brazil GRU OC1 1 Brazil Southeast (Vinhedo) sa-vinhedo-1 Vinhedo, Brazil VCP OC1 1 Canada Southeast (Montreal) ca-montreal-1 Montreal, Canada YUL OC1 1 Canada Southeast (Toronto) ca-toronto-1 Toronto, Canada YYZ OC1 1 Chile (Santiago) sa-santiago-1 Santiago, Chile SCL OC1 1 France Central (Paris) eu-paris-1 Paris, France CDG OC1 1 France South (Marseille) eu-marseille-1 Marseille, France MRS OC1 1 Germany Central (Frankfurt) eu-frankfurt-1 Frankfurt, Germany FRA OC1 3 India South (Hyderabad) ap-hyderabad-1 Hyderabad, India HYD OC1 1 India West (Mumbai) ap-mumbai-1 Mumbai, India BOM OC1 1 Israel Central (Jerusalem) il-jerusalem-1 Jerusalem, Israel MTZ OC1 1 Italy Northwest (Milan) eu-milan-1 Milan, Italy LIN OC1 1 Japan Central (Osaka) ap-osaka-1 Osaka, Japan KIX OC1 1 Japan East (Tokyo) ap-tokyo-1 Tokyo, Japan NRT OC1 1 Mexico Central (Queretaro) mx-queretaro-1 Queretaro, Mexico QRO OC1 1 Netherlands Northwest (Amsterdam) eu-amsterdam-1 Amsterdam, Netherlands AMS OC1 1 Saudi Arabia West (Jeddah) me-jeddah-1 Jeddah, Saudi Arabia JED OC1 1 Singapore (Singapore) ap-singapore-1 Singapore,Singapore SIN OC1 1 South Africa Central (Johannesburg) af-johannesburg-1 Johannesburg, South Africa JNB OC1 1 South Korea Central (Seoul) ap-seoul-1 Seoul, South Korea ICN OC1 1 South Korea North (Chuncheon) ap-chuncheon-1 Chuncheon, South Korea YNY OC1 1 Spain Central (Madrid) eu-madrid-1 Madrid, Spain MAD OC1 1 Sweden Central (Stockholm) eu-stockholm-1 Stockholm, Sweden ARN OC1 1 Switzerland North (Zurich) eu-zurich-1 Zurich, Switzerland ZRH OC1 1 UAE Central (Abu Dhabi) me-abudhabi-1 Abu Dhabi, UAE AUH OC1 1 UAE East (Dubai) me-dubai-1 Dubai, UAE DXB OC1 1 UK South (London) uk-london-1 London, United Kingdom LHR OC1 3 UK West (Newport) uk-cardiff-1 Newport, United Kingdom CWL OC1 1 US East (Ashburn) us-ashburn-1 Ashburn, VA IAD OC1 3 US Midwest (Chicago) us-chicago-1 Chicago, IL ORD OC1 3 US West (Phoenix) us-phoenix-1 Phoenix, AZ PHX OC1 3 US West (San Jose) us-sanjose-1 San Jose, CA SJC OC1 1 ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/1/","tags":["region","availability domain","AD"],"title":"1.1 Regions, Availability Domains"},{"categories":null,"contents":"2.1 Oracle Cloud Free Tier 계정 생성 Oracle Cloud Infrastructure을 무료로 사용해 보기 위한 30일 동안 US 300$ 한도 내에서 사용할 수 있는 Trial을 제공하고 있습니다. 또한 Always Free 항목으로 제공하는 자원에 대해서 30일 이후도 항시 무료로 사용할 수 있습니다.\nhttps://www.oracle.com/cloud/free/\n30-day free trial에서 이용 가능한 서비스 목록\nhttps://www.oracle.com/cloud/free/#free-cloud-trial Oracle Cloud Free Tier 계정 생성 절차 Free Tier 신청\nhttps://www.oracle.com/kr/cloud/free/ 에 접속하여 무료로 시작하기를 클릭하여 신청합니다.\n계정 정보 입력\n기본 정보를 입력하고 내 전자메일 확인을 클릭하면 입력한 이메일로 링크가 전달됩니다.\n메일 수신함을 확인합니다. Click here를 클릭하면 Free Tier 등록 링크로 이동하게 됩니다.\n등록 링크로 이동하여 추가 계정 정보를 입력합니다.\n비밀번호를 입력\n클라우드 계정 이름(Cloud Account Name)\n홈 영역\nHome Region은 생성후 변경이 불가하니 주의하여 선택합니다. Free Tier에서는 Service Limit으로 인해 Region 하나만 사용할 수 있습니다. 주의하여 선택합니다. 이용 약관 동의\n주소 정보\n휴대폰 전화번호\n지급 검증\n무료 체험 시작\nFree Tier 생성중\nFree Tier Account 생성중\nFree Tier Account 준비 완료 메일 수신\n","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/1/","tags":["trial","cloud account"],"title":"2.1 Oracle Cloud Free Tier 계정 생성"},{"categories":null,"contents":"1.8.1.1 Service Mesh 없는 마이크로서비스 앱 배포 본 내용은 아래 Istio 문서 상에 있는 내용을 재 확인하는 내용으로 마이크로 서비스 앱을 사용하는 데 있어서 어떤 문제가 발생할 수 있는지, 왜 Istio 같은 Service Mesh 필요한지에 대해 알아보는 내용입니다.\nhttps://istio.io/latest/docs/examples/microservices-istio/bookinfo-kubernetes/ https://istio.io/latest/docs/examples/microservices-istio/add-new-microservice-version/ 테스트 마이크로 서비스 앱(Bookinfo) 배포 테스트 앱\nProduct, Review, Details, Ratings의 4개 마이크로 서비스 앱으로 구성되어 있습니다.\n먼저 여기서는 Reviews 서비스는 v1만 배포합니다.\n아직 istio 설정이 안된 상태에서 배포 및 결과를 확인합니다.\n앱 배포\nkubectl apply -l version!=v2,version!=v3 -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml 스케일\nkubectl scale deployments --all --replicas 3 외부 접근을 위한 서비스 오픈\n테스트 편의를 위해 Load Balancer 타입으로 오픈합니다.\nkubectl patch svc productpage -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; 배포 상태 확인\n각 앱이 3개의 Pod이며 각 Pod는 Container 1개로 구성된 것을 확인할 수 있습니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-6997d94bb9-8hc7b 1/1 Running 0 30s details-v1-6997d94bb9-cjsn7 1/1 Running 0 31s details-v1-6997d94bb9-sv2p5 1/1 Running 0 30s productpage-v1-58b4c9bff8-9x5r4 1/1 Running 0 29s productpage-v1-58b4c9bff8-b87xs 1/1 Running 0 28s productpage-v1-58b4c9bff8-sjj5h 1/1 Running 0 28s ratings-v1-b8f8fcf49-2njx4 1/1 Running 0 28s ratings-v1-b8f8fcf49-4x5fj 1/1 Running 0 28s ratings-v1-b8f8fcf49-xmfwq 1/1 Running 0 27s reviews-v1-5896f547f5-47fmg 1/1 Running 0 27s reviews-v1-5896f547f5-8f5dk 1/1 Running 0 27s reviews-v1-5896f547f5-hkhtq 1/1 Running 0 27s 서비스 확인\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE details ClusterIP 10.96.74.5 \u0026lt;none\u0026gt; 9080/TCP 17m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 4d13h productpage LoadBalancer 10.96.122.193 129.154.xx.xxx 9080:30930/TCP 17m ratings ClusterIP 10.96.88.44 \u0026lt;none\u0026gt; 9080/TCP 17m reviews ClusterIP 10.96.29.37 \u0026lt;none\u0026gt; 9080/TCP 17m 메인 페이지 접속\nhttp://{productpage external ip}:9080/productpage 각 서비스가 호출되어 정상적인 페이지가 보이는 것을 알 수 있습니다. Review v1에서는 아직 Rating 서비스와 연동되지 않아 Rating 정보는 보이지 않습니다. Product page 호출 코드 확인\n소스 파일: https://github.com/istio/istio/blob/release-1.18/samples/bookinfo/src/productpage/productpage.py\nreviews 호출 주소를 보면 http://{reviewsHostname}.{servicesDomain}:{reviewsPort}/reviews 인걸 알 수 있습니다.\n실제 호출 로그\n실제 로그를 보면 http://reviews:9080/reviews 로 정상 호출 되었으며, 배포시의 ClusterIP 타입의 reviews Service를 통해 같은 네임스페이스 상에서는 reviews를 주소로하여 정상 호출된 것을 알 수 있습니다.\n$ kubectl logs -l app=productpage -f ... DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): reviews:9080 send: b\u0026#39;GET /reviews/0 HTTP/1.1\\r\\nHost: reviews:9080\\r\\nuser-agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\\r\\nAccept-Encoding: gzip, deflate\\r\\nAccept: */*\\r\\nConnection: keep-alive\\r\\nX-B3-TraceId: b4eb54b1b2a1e29d\\r\\nX-B3-SpanId: b4eb54b1b2a1e29d\\r\\nX-B3-Sampled: 1\\r\\n\\r\\n\u0026#39; reply: \u0026#39;HTTP/1.1 200 OK\\r\\n\u0026#39; header: X-Powered-By: Servlet/3.1 header: Content-Type: application/json header: Date: Wed, 05 Jul 2023 04:32:21 GMT header: Content-Language: en-US header: Content-Length: 358 DEBUG:urllib3.connectionpool:http://reviews:9080 \u0026#34;GET /reviews/0 HTTP/1.1\u0026#34; 200 358 INFO:werkzeug:::ffff:10.244.0.128 - - [05/Jul/2023 04:32:21] \u0026#34;GET /productpage HTTP/1.1\u0026#34; 200 - Reviews 새 버전 배포 Reviews Service 확인\nSelector에서 보이는 것처럼 app=reviews 레이블이 달린 Pod로 분배하고 있습니다. 현재 배포되어있는 Reviews v1의 세 개의 Pod의 Endpoints로 분배되고 있습니다.\n$ kubectl describe svc reviews Name: reviews Namespace: default Labels: app=reviews service=reviews Annotations: \u0026lt;none\u0026gt; Selector: app=reviews Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.96.29.37 IPs: 10.96.29.37 Port: http 9080/TCP TargetPort: 9080/TCP Endpoints: 10.244.0.151:9080,10.244.0.27:9080,10.244.1.22:9080 Session Affinity: None Events: \u0026lt;none\u0026gt; 이 상태에서 Reviews 앱의 새로운 버전인 v2를 개발하여 배포하게 된다면, 아마 배포해서 정상동작 확인후 서비스 라우팅이 되도록 app=review를 추가하게 될 것입니다. 이후 일부 요청을 서비스 하도록 실제 운영하다 문제 발생시 원복시키거나, 문제가 없는 경우, v1로 가는 요청을 줄이고, v2로 가는 요청을 점진적으로 늘려가면서 실제 v2로 이관하게 될 것입니다.\nReviews v2 배포 Reviews v2 버전을 배포합니다.\n라우팅에서 일단 제외하기 위해 app: reviews_test로 레이블을 변경하여 배포합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml | sed \u0026#39;s/app: reviews/app: reviews_test/\u0026#39; | kubectl apply -l app=reviews_test,version=v2 -f - 배포 결과 확인\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ... reviews-v1-5896f547f5-47fmg 1/1 Running 0 6m51s 10.244.1.22 10.0.10.220 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-5896f547f5-8f5dk 1/1 Running 0 6m51s 10.244.0.151 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v1-5896f547f5-hkhtq 1/1 Running 0 6m51s 10.244.0.27 10.0.10.41 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; reviews-v2-869797b54b-2sxjw 1/1 Running 0 22s 10.244.0.152 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Reviews v2 Pod 테스트\nproductpage Pod에서 Reviews v2 Pod로 테스트하면 좋겠지만, productpage Pod에 테스트용 curl 없는 관계로 별도 클라이언트용 Pod를 배포하여 테스트합니다.\n테스트 클라이언트 Pod 배포\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/sleep/sleep.yaml 테스트\nREVIEWS_V2_POD_IP=$(kubectl get pod -l app=reviews_test,version=v2 -o jsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;) echo $REVIEWS_V2_POD_IP kubectl exec $(kubectl get pod -l app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -- curl -sS \u0026#34;$REVIEWS_V2_POD_IP:9080/reviews/7\u0026#34; 테스트 결과\n아래와 같이 v2 버전이 잘 동작함을 확인합니다.\n$ REVIEWS_V2_POD_IP=$(kubectl get pod -l app=reviews_test,version=v2 -o jsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;) $ echo $REVIEWS_V2_POD_IP 10.244.0.152 $ kubectl exec $(kubectl get pod -l app=sleep -o jsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -- curl -sS \u0026#34;$REVIEWS_V2_POD_IP:9080/reviews/7\u0026#34; {\u0026#34;id\u0026#34;: \u0026#34;7\u0026#34;,\u0026#34;podname\u0026#34;: \u0026#34;reviews-v2-869797b54b-2sxjw\u0026#34;,\u0026#34;clustername\u0026#34;: \u0026#34;null\u0026#34;,\u0026#34;reviews\u0026#34;: [{ \u0026#34;reviewer\u0026#34;: \u0026#34;Reviewer1\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;An extremely entertaining play by Shakespeare. The slapstick humour is refreshing!\u0026#34;, \u0026#34;rating\u0026#34;: {\u0026#34;stars\u0026#34;: 5, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}},{ \u0026#34;reviewer\u0026#34;: \u0026#34;Reviewer2\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;Absolutely fun and entertaining. The play lacks thematic depth when compared to other plays by Shakespeare.\u0026#34;, \u0026#34;rating\u0026#34;: {\u0026#34;stars\u0026#34;: 4, \u0026#34;color\u0026#34;: \u0026#34;black\u0026#34;}}]} Reviews v2의 서비스 라우팅에 추가 version=v2 레이블이 달린 Review v2 Pod에 app=reviews 레이블을 추가합니다.\nkubectl label pod -l version=v2 app=reviews --overwrite Reviews Service 재확인\n아래와 같이 v2가 추가가 되어 Endpoints에 이전 3개에서 4개로 변경된 것을 알 수 있습니다.\n$ kubectl describe svc reviews Name: reviews ... Selector: app=reviews ... Endpoints: 10.244.0.151:9080,10.244.0.152:9080,10.244.0.27:9080 + 1 more... ... 메인 페이지 접속\nhttp://{productpage external ip}:9080/productpage\n페이지를 새로고침하면 4번 중 1번은 Review v2 Pod로 라우팅되어 아래 그림과 같이 별점 표시가 되는 것을 볼 수 있습니다.\nReviews v2 문제 확인후 재배포 서비스 중에 문제가 발생한 경우 다음과 같이 삭제합니다.\n위에서 서비스 라우팅을 위해 기존 Pod의 label을 변경했기 때문에, 변경된 Pod(app=reviews)외에 app=reviews_test의 Deployment도 아직 남아 있습니다. 둘다 삭제합니다. (실제로는 이렇게 하면 안되겠네요. 근데, Deployment 내에 정의된 Pod Template의 label은 변경이 안되네요\u0026hellip;) kubectl delete deployment reviews-v2 kubectl delete pod -l app=reviews,version=v2 문제 해결후 재배포\n최종 배포시에는 아래와 같이 배포후 v2의 갯수를 늘리고 v1은 삭제하게 될 것입니다. 실제 상황에서는 점진적으로 v2을 늘리고 v1을 줄이는 요구사항도 있을 것입니다.\nkubectl apply -l app=reviews,version=v2 -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml kubectl scale deployment reviews-v2 --replicas=3 kubectl delete deployment reviews-v1 실제 배포시 대응 방법 실제 환경에서 마이크로 서비스 업데이트시의 안정적인 배포를 위해 새 버전 배포후 테스트, 점진적인 새 버전으로의 이관을 위한 여러가지 방안들이 나왔습니다. 그 대표적인 방법으로 가장 많이 사용하고 있는 것은 크게 Service Mesh와 Netflix\u0026rsquo;s Hystrix를 기반한 방법입니다.\nService Mesh: Istio로 대표되는 서비스 메쉬를 사용하는 방법은 sidecar로 모듈을 Service Mesh 단에서 추가하는 방법입니다. Pod에 Application Container외에 Sidecar Container가 추가되고, Sidecar를 이용해 서비스 라우팅 및 추가 기능을 제공하는 방식입니다. 애플리케이션 코드에 별도 작업이 필요하지 않는 장점이 있습니다. 애플리케이션 코드 구현: Netflix\u0026rsquo;s Hystrix 예와 같이 서비스 라우팅, 보안, 모니터링 등에 필요한 기능을 라이브러리 모듈에서 제공하는 기능을 사용하는 방식입니다. 라이브러리를 사용하기 때문 코딩 자체량은 많지 않고, Annotation 등을 사용하게 됩니다. Spring 프레임워크에서는 Spring Cloud로 발전하여 기능을 제공하고 있습니다. ","lastmod":"2021-12-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/service-mesh/istio/1.sampleapp-without-istio/","tags":["oss","service mesh","istio"],"title":"1.8.1.1 Service Mesh 없는 마이크로서비스 앱 배포"},{"categories":null,"contents":"1.5.2.4.1 EFK(Elasticsearch, Fluentd, Kibana) 오픈 소스로 쿠버네티스 로그 모니터링을 하는 데 가장 많이 사용되는 구성으로 다음 세가지 구성을 확인해 봅니다.\nElasticsearch: 수집저장된 로그를 검색 및 분석하는 역할 수행 Fluentd 또는 Fluent Bit: 쿠버네티스 클러스터에서 로그를 수집하는 역할을 수행 Kibana: 로그를 쿼리하고, 시각화 해주는 역할 수행 여기서는 Fluent Bit를 사용하는 예시입니다.\nElastic Search + Kibana 설치 설치용 namespace를 만듭니다.\nkubectl create ns logging Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami 설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\nhttps://github.com/bitnami/charts/tree/master/bitnami/elasticsearch/#parameters\n예시\nelasticsearch 내장 kibana를 함께 설치 kibana 접근 URL을 nginx ingress controller, letsencrypt를 통한 인증서 사용하는 예시 이전 실습 1.2.2.2.2.2 NGINX Ingress Controller에서 TLS termination(feats. Let’s Encrypt) 설치 기준에서 설치하는 예시임 global: kibanaEnabled: true kibana: ingress: enabled: true hostname: kibana.ingress.thekoguryo.xyz annotations: cert-manager.io/cluster-issuer: letsencrypt-staging tls: true ingressClassName: nginx elasticsearch helm chart 설치\nhelm install elasticsearch -f values.yaml bitnami/elasticsearch --version 19.9.5 -n logging 설치\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\n$ helm install elasticsearch -f values.yaml bitnami/elasticsearch --version 19.9.5 -n logging NAME: elasticsearch LAST DEPLOYED: Wed Jul 5 01:59:09 2023 NAMESPACE: logging STATUS: deployed ... Elasticsearch can be accessed within the cluster on port 9200 at elasticsearch.logging.svc.cluster.local To access from outside the cluster execute the following commands: kubectl port-forward --namespace logging svc/elasticsearch 9200:9200 \u0026amp; curl http://127.0.0.1:9200/ 설치된 elastic search 내부 주소와 포트를 확인합니다. 이후 Fluentd에서 로그 전송을 위해 사용할 주소입니다.\n주소: elasticsearch.logging.svc.cluster.local 포트: 9200 Pod가 모두 기동할때 까지 기다립니다.\nkubectl get pod -n logging --watch Fluent Bit 구성 FluentBit Helm Chart 저장소를 추가합니다.\nhelm repo add fluent https://fluent.github.io/helm-charts ElasticSearch로 로그를 포워딩하기 위한 설정값을 작성합니다.\nReplace_Dots On: 다음과 같이 labels의 key에 app.kubernetes.io와 같이 *.*이 포함된 경우 ElasticSearch 전송시 오류가 발생합니다. 이를 방지하기 위해 추가합니다.\napiVersion: v1 kind: Pod metadata: ... labels: ... app.kubernetes.io/name: storefront ... Suppress_Type_Name On: ElasticSearch 8에서 _type 관련 오류가 발생하는 것을 방지하기 위해 추가합니다.\nRemoval of mapping types cat \u0026lt;\u0026lt;EOF \u0026gt; myvalues.yaml config: outputs: | [OUTPUT] Name es Match * Host elasticsearch.logging.svc.cluster.local Port 9200 tls Off tls.verify Off Retry_Limit False Logstash_Format On Logstash_Prefix logstash Trace_Error On Replace_Dots On Suppress_Type_Name On EOF OKE 클러스터에 FluentBit을 설치합니다.\nhelm upgrade --install fluent-bit fluent/fluent-bit -f myvalues.yaml -n logging 참고 문서 https://docs.fluentbit.io/manual/installation/kubernetes#installation Kibana 설정 설치한 kibana을 웹 브라우저로 접속합니다. ingress로 지정한 주소로 접속합니다.\n예) https://kibana.ingress.thekoguryo.xyz Welcome to Elastic 화면이 나오면 Explore on my own을 클릭하여 홈으로 이동합니다.\n왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n인덱스 패턴을 만들기 위해 Create data view를 클릭합니다.\n인덱스 패턴을 생성합니다.\n오른쪽에서 보듯이 Fluent Bit에서 전송된 로그는 logstash-로 시작합니다.\nName: logstash-*\nIndex pattern: logstash-*\nTimestamp field: @timestamp\n데이타 뷰가 추가된 결과를 볼 수 있습니다.\n왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n생성한 인덱스 패턴을 통해 수집된 로그를 확인할 수 있습니다.\n테스트 앱의 로그를 확인하기 위해 Add filter를 클릭하여 namespace_name=default 로 지정합니다. 테스트 앱을 접속합니다.\n예) https://blue.ingress.thekoguryo.xyz/efk-test 로그 확인\n아래와 같이 kibana에서 테스트 앱의 로그를 확인할 수 있습니다.\nEFK를 통해 OKE 상의 로그를 수집하는 예시였습니다. EFK에 대한 상세 내용은 제품 관련 홈페이지와 커뮤니티 사이트를 참고하기 바랍니다.\n","lastmod":"2023-07-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/open-source/elasticsearch/1.install-efk/","tags":["oss","logging","efk","fluentbit"],"title":"1.5.2.4.1 EFK(Elasticsearch, Fluentd, kibana)"},{"categories":null,"contents":"1.5.1.1 OCI Logging을 사용한 Control Plane 로그 모니터링 OKE 클러스터를 생성후, 모니터링 및 문제해결을 위해 쿠버네티스 Control Plane 상의 프로세스에 대한 로그 모니터링이 필요할 수 있습니다. 컨트롤 플레인상의 프로세스에 대한 로그는 OKE 서비스 로그로 간주되어 OCI 서비스 로그를 다루는 방법과 동일하게 설정하여 사용할 수 있습니다.\nOKE 서비스 로그로 제공하는 Control Plane 상의 프로세스에 대한 제공 로그는 다음과 같습니다.\nkube-scheduler 로그: 스케줄러 결정 같은 kube-scheduler에서 발생하는 로그, 에러, 이벤트 kube-controller-manager 로그: replica 수에 맞게 배포에 대한 재조정 같은 kube-controller-manager에서 발생하는 로그, 에러, 이벤트 cloud-controller-manager 로그: 로드 밸런서 프로비저닝 같은 cloud-controller-manager에서 발생하는 로그, 에러, 이벤트 kube-apiserver 로그: Kubernetes API 서버로 전송된 모든 요청같은 kube-apiserver 로그, 에러, 이벤트 Log Group 만들기 Log Group은 로그들을 관리하는 말 그대로 로그의 묶음 단위 입니다. 로그들을 담는 그릇으로 그룹이 없는 경우에 먼저 만듭니다.\n좌측 상단 햄버거 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging \u0026gt; Log Groups로 이동합니다.\nCreate Log Group을 클릭하여 로그 그룹을 만듭니다.\nName: 예, oke-log-group OKE 서비스로 활성화하기 좌측 상단 햄버거 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging \u0026gt; Logs로 이동합니다.\n위치할 Compartment를 선택합니다.\nEnable service log를 클릭합니다.\nSelect Resource\nContainer Engine for Kubernetes 서비스를 선택하고, 대상 OKE 클러스터를 선택합니다.\nConfigure Log\n대상 로그를 지정합니다. 특정 프로세스 또는 전체 로그를 선택합니다.\n로그 이름을 입력합니다.\nName: 예, oke-control-plane-log-for-oke-cluster-1 Show Advanced Options를 클릭하여 필요하면 추가 설정을 합니다.\nLog Location: 속할 로그 그룹을 앞서 만든 Log Group으로 선택합니다. Log Retention: 보관 기간을 지정합니다. 최대 6개월(180일) 까지 OCI Logging에서 설정 완료후 Enable Log를 클릭합니다.\nOCI 서비스 콘솔에서 Log 화면으로 다시 돌아갑니다.\n화면에서 Resources \u0026gt; Explore Log에서 로그를 조회 할 수 있습니다. 검색을 위해서는 로그목록 오른쪽 위에 있는 Explore with Log Search를 클릭합니다.\nCustom filters 항목에서 source='cloud-controller-manager' 같이 검색값으로 조회하면 됩니다. Custom filters에 값을 입력하고 엔터키를 꼭 칩니다.\n검색된 로그 데이터를 확인할 수 있습니다.\nOCI Logging 에 수집된 로그는 필요시, OCI Service Connector Hub를 통해 타시스템으로 전달할 수 있습니다.\n","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/control-plane-logs/1.oci-logging/","tags":["oke","oci logging","logging","control-plane"],"title":"1.5.1.1 OCI Logging을 사용한 Control Plane 로그 모니터링"},{"categories":null,"contents":"1.5.2.1 OCI Logging을 사용한 애플리케이션 로그 모니터링 OKE 상에 배포되어 있는 애플리케이션 로그 모니터링을 OCI Logging 서비스를 통해 모니터링할 수 있습니다.\nOCI Logging 서비스 사용 권한 설정 Worker Node에 대한 Dynamic Group 만들기\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\nOKE 클러스터가 있는 Compartment의 OCID를 확인합니다.\n좌측 Dynamic Group 메뉴로 이동하여 아래 규칙을 가진 Dynamic Group을 만듭니다.\ninstance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39; Name: 예, oke-nodes-dynamic-group Dynamic Group에 대한 OCI Logging 서비스 권한 부여하기\n좌측 Policy 메뉴로 이동하여 아래 규칙을 가진 Policy을 만듭니다. 방금 생성한 Dynamic Group에 대한 Policy를 만듭니다.\nallow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use log-content in compartment \u0026lt;compartment-name\u0026gt; Name: 예, oke-logging-policy 컨테이너를 위한 Custom Log 설정하기 Log Group 만들기\nLog Group은 로그들을 관리하는 말 그대로 로그의 묶음 단위 입니다. 로그들을 담는 그릇으로 그룹이 없는 경우에 먼저 만듭니다.\n좌측 상단 햄버거 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging \u0026gt; Log Groups로 이동합니다.\nCreate Log Group을 클릭하여 로그 그룹을 만듭니다.\nName: 예, oke-log-group Custom Log 만들기\nCustom Log는 OCI 서비스 로그이외 애플리케이션에서 수집하는 로그를 말합니다. Custom Log를 정의하고, 이에 대한 로그 수집기를 정의합니다.\nResources \u0026gt; Logs 메뉴로 이동하여 Create custom log를 클릭합니다.\n로그 이름과 보관 주기 등을 설정하여 custom log를 만듭니다.\nName: 예, oke-custom-log-for-containers 이해를 돕고자 Agent는 별도로 설정합니다. 여기서는 일단 Add configuration later 선택\nAgent Configuration 설정\nAgent Configuration는 로그를 수집하는 agent를 설정하는 부분입니다.\nLogging \u0026gt; Agent Configurations 메뉴로 이동하여 Create agent log를 클릭합니다.\nAgent 설정 정보를 입력합니다.\nName: 예, oke-agent-conf-for-containers Host Group: Agent가 설치될 대상을 지정합니다. 여기서는 앞서 만든 Dynamic Group으로 지정합니다. Worker Node가 동적으로 생성,삭제가 될 텐데, 그 Worker Nodes의 그룹이 앞서 만든 Dynamic Group에 속한 컴퓨트 인스턴스입니다. Agent 설정 부분에서 로그가 위치한 경로 및 수집된 로그의 전달 위치를 지정합니다.\nlog input:\nWorker Node 컴퓨트 인스턴스 내에서 수집할 로그가 위치한 경로를 지정합니다. 사전에 약속된 다음 위치를 지정합니다.입력하고 엔터키를 꼭 칩니다.\n/var/log/containers/*.log\nlog destination: 수집한 로그를 전달한 OCI Log 이름을 입력합니다. 앞서 생성한 custom log 이름을 지정합니다.\n참고: Worker Node VM상에 컨테이너 로그 위치\nWorker Node VM에 SSH로 접속이 가능한 환경, 예) bastion host에서 Worker Node에 접속해 보면 컨테이너 로그 위치는 다음과 같습니다.\n[opc@bastion-host ~]$ ssh opc@10.0.10.193 Activate the web console with: systemctl enable --now cockpit.socket Last login: Wed Jul 5 08:48:08 2023 from 10.0.20.229 [opc@oke-cjakv5mqtna-nlhwbs5hpiq-s4dpxie44pa-2 ~]$ sudo su [root@oke-cjakv5mqtna-nlhwbs5hpiq-s4dpxie44pa-2 opc]# cd /var/log/containers/ [root@oke-cjakv5mqtna-nlhwbs5hpiq-s4dpxie44pa-2 containers]# ls -la total 20 drwxr-xr-x. 2 root root 12288 Jul 5 08:46 . drwxr-xr-x. 15 root root 4096 Jul 2 03:43 .. ... lrwxrwxrwx. 1 root root 101 Jun 30 15:22 coredns-5885f64884-nfmdn_kube-system_coredns-058942ad4a45004af37030ae13cfafbb0c5d7299ec061d392ed6f512bd1ca5ce.log -\u0026gt; /var/log/pods/kube-system_coredns-5885f64884-nfmdn_2bcc9282-d6b8-4427-9763-89530a48500a/coredns/0.log ... lrwxrwxrwx. 1 root root 104 Jul 5 08:46 nginx-docker-hub-67c59cc7d5-p9g9t_default_nginx-3744e08196e1f115d5d77bc95975ffb40c1bcf1669a6386feb83674834505e29.log -\u0026gt; /var/log/pods/default_nginx-docker-hub-67c59cc7d5-p9g9t_d4c014bf-b395-4250-add2-c3efd94ac4ec/nginx/0.log ... /var/log/containers/*.log는 위에서 보는 것처럼 링크라서 kubernetes namespace 기준으로 하고 싶다면, 로그 경로를 default namespace인 경우/var/log/pods/default_*/*/*.log 이렇게 해도 되겠습니다.\n로깅 테스트 애플리케이션 로그 확인을 위해 이전 가이드에 샘플로 배포된 nginx 앱을 접속해 봅니다.\n발생한 POD 로그는 다음과 같습니다.\n$ kubectl logs -f nginx-docker-hub-67c59cc7d5-p9g9t ... 127.0.0.6 - - [05/Jul/2023:08:50:42 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#34; \u0026#34;10.179.86.108\u0026#34; 동일한 로그가 Worker Node VM 상에서도 로그를 조회해 보면 /var/log/containers/*.log 위치에 발생하는 것을 확인 할 수 있습니다.\n[root@oke-cjakv5mqtna-nlhwbs5hpiq-s4dpxie44pa-2 containers]# pwd /var/log/containers [root@oke-cjakv5mqtna-nlhwbs5hpiq-s4dpxie44pa-2 containers]# tail -f nginx-docker-hub-67c59cc7d5-p9g9t_default_nginx-3744e08196e1f115d5d77bc95975ffb40c1bcf1669a6386feb83674834505e29.log ... 2023-07-05T08:50:42.746094280+00:00 stdout F 127.0.0.6 - - [05/Jul/2023:08:50:42 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 200 615 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#34; \u0026#34;10.179.86.108\u0026#34; OCI 서비스 콘솔에서 Log 화면으로 다시 돌아갑니다.\n화면에서 Resources \u0026gt; Explore Log에서 로그를 조회 할 수 있습니다. 검색을 위해서는 로그목록 오른쪽 위에 있는 Explore with Log Search를 클릭합니다.\nCustom filters 항목에서 data.message='\u0026quot;*customlogtest*\u0026quot; 같이 검색값으로 조회하면 됩니다. Custom filters에 값을 입력하고 엔터키를 꼭 칩니다.\nLog Agent를 통해 수집되는 주기가 있어 5분 내외가 걸릴 수 있습니다. Log Flush가 안되어 계속 기다려도 로그 조회가 안될 수 있으니, 테스트 URL 여러번 반복 접속합니다. 검색된 로그 데이터를 확인할 수 있습니다.\n","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/1.oci-logging/","tags":["oke","oci logging","logging"],"title":"1.5.2.1 OCI Logging을 사용한 애플리케이션 로그 모니터링"},{"categories":null,"contents":"1.2.1.1 OCI Load Balancer로 서비스하기 앱 배포해보기 가장 흔한 형태인 Public Container Registry인 Docker Hub에서 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest 배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\n$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-5bfd857f89-9rh8q 1/1 Running 0 31s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 80m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 31s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-5bfd857f89 1 1 1 31s Load Balancer 타입으로 서비스 만들기 클라이언트 서비스를 위해 LoadBalancer Type으로 서비스를 생성합니다.\n서비스 생성 결과를 확인하면 아래와 같이 LoadBalancer 타입으로 생성되어 Public IP가 할당 된 것을 볼 수 있습니다.\n$ kubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc service/nginx-docker-hub-svc exposed $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 82m nginx-docker-hub-svc LoadBalancer 10.96.209.198 150.230.xxx.xx 80:32298/TCP 62s 서비스 주소인 Public IP로 접속하면, 연결되는 것을 볼 수 있습니다.\n$ curl http://150.230.xxx.xx \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; $ OCI 콘솔에서 Load Balancer 확인하기 콘솔에서 Networking \u0026gt; Load Balancers 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n아래 그림과 같이 kubernetes 상에 생성된 서비스와 동일한 Public IP로 Load Balancer가 생성된 것을 알 수 있습니다.\nLoad Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n상세화면에서 좌측 하단 Resources \u0026gt; Listeners로 이동합니다.\nkubernetes에서 Load Balancer 생성시 지정한 (kubectl expose ~~~~ --port 80) 명령에 따라 80 포트로 Listen 하고 있는 걸 알 수 있습니다.\nResources \u0026gt; Backend Set으로 이동합니다. 생성된 Backend Set을 클릭하여 상세화면으로 이동합니다.\n좌측 하단 Resources \u0026gt; Backends로 이동합니다.\n세 개의 백엔드 노드의 NodePort 32298 포트로 부하 분산하는 것을 볼 수 있습니다.\n다시 터미널로 이동하여 서비스와 노드 정보를 조회합니다.\nkubectl get svc kubectl get nodes -o wide 조회결과\n조회 해보면 OCI Load Balancer 가 Worker Nodes 3개로, 각 노드의 Node Port인 32298으로 부하 분산 되는 것을 알 수 있습니다. 이처럼 kubernetes에서 Load Balancer Type 서비스를 생성하면, OCI Load Balancer와 연동되어 자동으로 자원이 생성됩니다.\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 91m nginx-docker-hub-svc LoadBalancer 10.96.209.198 150.230.xxx.xx 80:32298/TCP 9m51s $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.107 Ready node 87m v1.25.4 10.0.10.107 \u0026lt;none\u0026gt; Oracle Linux Server 8.7 5.15.0-6.80.3.1.el8uek.x86_64 cri-o://1.25.1-111.el8 10.0.10.143 Ready node 87m v1.25.4 10.0.10.143 \u0026lt;none\u0026gt; Oracle Linux Server 8.7 5.15.0-6.80.3.1.el8uek.x86_64 cri-o://1.25.1-111.el8 10.0.10.39 Ready node 87m v1.25.4 10.0.10.39 \u0026lt;none\u0026gt; Oracle Linux Server 8.7 5.15.0-6.80.3.1.el8uek.x86_64 cri-o://1.25.1-111.el8 OCI 콘솔에서 Security List 확인하기 콘솔에서 Networking \u0026gt; Virtual Cloud Networks 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\nOKE 클러스터가 사용하는 VCN으로 이동합니다.\nSubnet을 보면 그림과 같이, 3개의 서브넷이 있습니다.\noke-k8sApiEndpoint-subnet~~: Kubernetes API Endpoint를 위한 서브넷 oke-svclbsubnet-~~: Load Balancer가 사용하는 서브넷 oke-nodesubnet-~~: Worker Node 들이 사용하는 서브넷 Resources \u0026gt; Security Lists로 이동하면, 위 3개의 서브넷이 사용하는 3개의 Security List가 있습니다.\nLoad Balancer용 서브넷 확인\n먼저 oke-svclbsubnet-~~ 의 상세화면으로 이동합니다. Ingress/Egress Rule을 확인해 보면, 외부에서 80 포트로 수신하고, Worker Node들의 Node Port로 송신할 수 있도록 자동으로 추가된 것을 볼수 있습니다.\n다시 VCN 상세 화면으로 이동하여, Worker Nodes용 Security List를 확인합니다.\nOCI Load Balancer에서 Node Port로 요청을 수신할 수 있도록 자동으로 규칙이 추가된 것을 볼수 있습니다.\n위와 같이 OKE 클러스터에 kubernetes 명령으로 Load Balancer 서비스 타입을 생성하면, 그에 따라 OCI Load Balancer가 생성되고, 관련 Security List에도 등록되는 것을 알 수 있습니다.\n","lastmod":"2024-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/load-balancer/1.default-load-balancer/","tags":["oke"],"title":"1.2.1.1 OCI Load Balancer로 서비스하기"},{"categories":null,"contents":"1.4.1.1 OCIR 이미지 사용하기 Oracle Cloud Infrastructure Registry(OCIR)는 오라클이 제공하는 관리형 컨테이너 레지스트리로 Docker V2 API를 지원하며, Open Container Initiate 호환 컨테이너 레지스트리입니다. docker cli를 통해 이미지를 Push, Pull 해서 사용할 수 있으며, Kubernetes 클러스터에서도 사용할 수 있습니다.\nOCIR에 이미지를 사용하기 위해서는 먼저 등록 작업이 필요하며, 앞서 예제에서 사용한 nginx 이미지를 아래 절차에 따라 등록해 봅니다.\nOCIR Repository 만들기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Registry로 이동합니다.\nList Scope에서 대상 Compartment를 선택합니다.\n이미지를 Push하기 전에 먼저 OCIR에 repository를 생성이 필요합니다.\nCreate repository를 클릭하여 아래와 같이 nginx repository를 생성합니다. Push, Pull 모두 인증 테스트를 위해 Access 모드를 Private으로 선택합니다.\n생성 완료\nRepository 화면에서 Namespace를 복사해 둡니다.\nOCI Auth Token 만들기 docker cli로 docker hub에 이미지를 등록하거나, 가져올때 username/password로 docker login을 통해 로그인을 합니다. OCIR에도 마찬가지로 로그인이 필요하며, password 대신 보안을 위해 Auth Token을 사용합니다.\n우측 상단 사용자의 Profile 아이콘을 클릭하여 My profile로 이동합니다.\n아래 그림상의 유저의 username은 Identity Domain 이름인 Default를 제외한 kildong@example.com입니다. 왼쪽 아래 Resources \u0026gt; Auth Token으로 이동합니다.\nAuth Token 생성을 위해 Generate Token을 클릭합니다.\n설명을 입력하고 생성합니다. Auth Token은 생성시에만 볼수 있으므로 복사해 둡니다.\nOCIR 로그인 및 이미지 Push 앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 docker cli로 로그인 합니다.\nOCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io region-key: 예, yny region-identifier: 예, ap-chuncheon-1 전체 Region별 OCIR 주소: Availability by Region Username: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다. \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능 Password: 앞서 생성한 로그인할 유저의 Auth Token kildong@cloudshell:~ (ap-chuncheon-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnxxxxxxxxgq\u0026#34; } kildong@cloudshell:~ (ap-chuncheon-1)$ docker login yny.ocir.io Username: cnxxxxxxxxgq/kildong@example.com Password: WARNING! Your password will be stored unencrypted in /home/kildong/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 이미지 Push\nOCIR에 생성한 Repository로 Push 하기 위해 아래 형식으로 태그를 한 후 Push 하면 됩니다. \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt;:\u0026lt;tag\u0026gt; nginx:latest 예시 docker pull nginx:latest docker tag nginx:latest yny.ocir.io/cnxxxxxxxxgq/nginx:latest docker push yny.ocir.io/cnxxxxxxxxgq/nginx:latest OCIR 확인\nOCI 서비스 콘솔로 다시 돌아가서 대상 Compartment 기준으로 Push한 이미지가 정상적으로 등록된 것을 알 수 있습니다.\n실수를 막기 위한 참고 사항 다음과 같은 상황에서는 docker push하면 어떻게 될까요?\nPush 전에 OCIR Repository를 사전에 만들지 않은 경우\n=\u0026gt; 사전에 OCIR Repository를 만들지 않으면, 기본 설정에 의해 root compartment 쪽에 push 됩니다.\ndev/nginx:latest(또는 bitnami/nginx:latest)와 같이 이미지 이름 앞에 추가 적인 경로가 있는 경우에 OCIR Repository를 dev 로만 만든 경우\n=\u0026gt; dev/nginx 까지를 Repository 이름으로 생각합니다. 그래서 OCIR에서 dev/nginx를 Repository Name으로 입력하여 생성합니다. 그렇게 만들지 않는 경우 동일하게 root compartment 쪽에 push 됩니다.\nContainer Registry 우측 상단에 Settings를 클릭하여 설정정보를 보면 아래와 같이 대상 repository가 없는 경우 root compartment에 private repository를 자동으로 새로 만들고 push 하는 것이 기본 값으로 체크되어 있습니다.\nOCIR 이미지로 OKE 클러스터에 배포 OCIR 이미지 배포 테스트\n가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-ocir --image=yny.ocir.io/cnxxxxxxxxgq/nginx:latest 배포 결과 아래와 같이 private repository로 인증문제로 이미지를 가져오는 오류가 발생한 것을 알수 있습니다.\nkildong@cloudshell:~ (ap-chuncheon-1)$ kubectl create deployment nginx-ocir --image=yny.ocir.io/cnxxxxxxxxgq/nginx:latest deployment.apps/nginx-ocir created kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ocir-56d7b8d55c-t667j 0/1 ImagePullBackOff 0 15s kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl describe pod nginx-ocir-56d7b8d55c-t667j Name: nginx-ocir-56d7b8d55c-t667j ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning Failed 15s (x2 over 27s) kubelet Failed to pull image \u0026#34;yny.ocir.io/cnxxxxxxxxgq/nginx:latest\u0026#34;: rpc error: code = Unknown desc = reading manifest latest in yny.ocir.io/cnxxxxxxxxgq/nginx: denied: Anonymous users are only allowed read access on public repos Warning Failed 15s (x2 over 27s) kubelet Error: ErrImagePull Normal BackOff 1s (x2 over 26s) kubelet Back-off pulling image \u0026#34;yny.ocir.io/cnxxxxxxxxgq/nginx:latest\u0026#34; Warning Failed 1s (x2 over 26s) kubelet Error: ImagePullBackOff OCIR Private Repository 이미지 배포 테스트 - imagepullsecret\nPrivate Repository에서 이미지를 가져와서 사용하려면 인증을 위한 secret을 등록해서 사용해야 합니다. 아래 절차에 따라 secret을 만들어 사용합니다.\n앞서 Auth Token을 사용하여 docker login을 하였습니다. 사용자 홈 밑에 .docker/config.json에 인증정보가 저장됩니다.\nkildong@cloudshell:~ (ap-chuncheon-1)$ docker login yny.ocir.io Username: cnxxxxxxxxgq/kildong@example.com Password: WARNING! Your password will be stored unencrypted in /home/kildong/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded 위 인증 정보를 통해 secret을 만듭니다.\nkubectl create secret generic ocir-secret \\ --from-file=.dockerconfigjson=$HOME/.docker/config.json \\ --type=kubernetes.io/dockerconfigjson 또는 docker login 정보 없이 직접 secret을 만들 수도 있습니다.\nkubectl create secret docker-registry \u0026lt;secret-name\u0026gt; --docker-server=\u0026lt;region-key\u0026gt;.ocir.io --docker-username=\u0026#39;\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;oci-username\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;oci-auth-token\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email-address\u0026gt;\u0026#39; 아래와 같이 imagepullsecret을 사용하여 다시 배포합니다.\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-ocir name: nginx-ocir spec: replicas: 1 selector: matchLabels: app: nginx-ocir template: metadata: labels: app: nginx-ocir spec: containers: - name: nginx image: yny.ocir.io/cnxxxxxxxxgq/nginx:latest imagePullSecrets: - name: ocir-secret 아래와 같이 Running 상태로 정상 배포되는 것을 볼 수 있습니다.\nkildong@cloudshell:~ (ap-chuncheon-1)$ kubectl create secret generic ocir-secret \\ \u0026gt; --from-file=.dockerconfigjson=$HOME/.docker/config.json \\ \u0026gt; --type=kubernetes.io/dockerconfigjson secret/ocir-secret created kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl get secret NAME TYPE DATA AGE ocir-secret kubernetes.io/dockerconfigjson 1 46s kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f nginx-ocir-deployment.yaml deployment.apps/nginx-ocir configured kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-86bcf7867c-tk4m5 1/1 Running 0 86s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 152m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 7m21s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-86bcf7867c 1 1 1 87s OCIR Private Repository 이미지 배포 테스트 - default imagepullsecret\n매번 imagepullsecret을 지정하는 것이 불편한 경우 기본으로 사용할 Container Repository에 대한 인증을 default로 저장하여 사용할 수도 있습니다.\nnamespace에 default serviceaccount가 있는데, 여기에 아래와 같이 imagepullsecret을 추가합니다.\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39; 그 결과 아래와 같이 default serviceaccount에 기본적으로 사용할 imagesecret이 추가되었습니다\nkildong@cloudshell:~ (ap-chuncheon-1)$ kubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39; serviceaccount/default patched kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl get sa default -o yaml apiVersion: v1 imagePullSecrets: - name: ocir-secret kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2023-05-16T07:52:12Z\u0026#34; name: default namespace: default resourceVersion: \u0026#34;37675\u0026#34; uid: 477e180e-82af-4ff4-a1cd-b9bd12763078 앞서 배포한 yaml을 삭제하고 인증정보가 없어 처음 실패한 명령으로 다시 배포합니다.\nkubectl delete deployment nginx-ocir kubectl create deployment nginx-ocir --image=yny.ocir.io/cnxxxxxxxxgq/nginx:latest 결과확인하면 default imagepullsecret을 사용하여 정상 배포됨을 알 수 있습니다.\nkildong@cloudshell:~ (ap-chuncheon-1)$ kubectl delete deployment nginx-ocir deployment.apps \u0026#34;nginx-ocir\u0026#34; deleted kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl create deployment nginx-ocir --image=yny.ocir.io/cnxxxxxxxxgq/nginx:latest deployment.apps/nginx-ocir created kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-56d7b8d55c-65d22 1/1 Running 0 12s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 156m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 12s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-56d7b8d55c 1 1 1 13s ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/container-registry/ocir/1.deploy-ocir-image/","tags":["oke"],"title":"1.4.1.1 OCIR 이미지 사용하기"},{"categories":null,"contents":"1.1.3.1 Cloud Shell로 OKE 클러스터 연결하기 Cloud Shell Cloud Shell은 공식 문서에서 설명처럼 Oracle Cloud 콘솔에서 제공하는 웹 브라우저 기반 리눅스 터미널입니다. 작은 가상머신으로 구동된다고 이해하시면 되며, Oracle Cloud 콘솔에 접속한 유저에 대해 사전 인증된 OCI CLI를 제공하며, 추가적인 cli 및 설정들을 기본 구성하여 제공합니다.\nOKE 클러스터 접속을 위한 Kubernetes CLI인 kubectl로 기본 설치되어 제공합니다.\n무료로 사용할 수 있고, 인증 및 기본 툴 들이 구성되어 있기 때문 바로 사용할 수 있는 이점이 있습니다.\nCloud Shell로 OKE 클러스터 연결하기 생성한 OKE 클러스터 상세 페이지로 이동합니다.\nAccess Cluster를 클릭합니다.\n두 가지 접근 방법 중에 Cloud Shell Access을 선택합니다.\nCloud Shell Access: OCI에서 제공하는 Cloud Shell을 통해 접근합니다. 현재 접속한 사용자가 작업하기 위한 기본 설정 및 관련 cli들이 구성되어 있습니다. Local Access: 로컬 PC 환경에서 처음 접속하기 위해 필요한 작업부터 시작하는 방법입니다. Step #1. Launch Cloud Shell\nLaunch Cloud Shell를 클릭하거나, 우측 상단에 있는 링크를 클릭하여 Cloud Shell에 접속합니다.\n접속한 환경에서 다음 명령을 실행해 보면 oci cli가 설치되어 있으며, 접속이 가능한 상태임을 알 수 있습니다.\noci -v oci os ns get Step #2. kubeconfig 파일 생성하기\n생성된 OKE 클러스터 접속을 위한 kubeconfig을 생성하기 위해 Access Your Cluster의 두 번째 단계 내용을 Cloud Shell에서 실행합니다.\nCloud Shell는 사용자 테넌시 외부에 있는 환경으로 Kubernetes API를 Public Endpoint을 제공하는 경우에만 접근할 수 있습니다. OKE 클러스터 연결 확인\nkubectl cluster-info 를 실행하면 생성된 kubeconfig를 통해 클러스터에 접속됨을 확인할 수 있습니다.\nkubectl로 접속이 되는 것을 확인했습니다. 이제 kubectl로 일반적인 쿠버네티스를 명령을 수행하여 사용하면 됩니다.\n앱 배포해보기 가장 흔한 형태인 Public Container Registry인 Docker Hub에서 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest 배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\n$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-5bfd857f89-9rh8q 1/1 Running 0 31s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 80m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 31s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-5bfd857f89 1 1 1 31s ","lastmod":"2024-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/basics/3.access-cluster/1.cloudshell-access/","tags":["oke"],"title":"1.1.3.1 Cloud Shell로 클러스터 연결하기"},{"categories":null,"contents":"1.1.3.2 로컬 환경에서 클러스터 연결하기 OKE 클러스터 접속을 위해서는 #1. OCI CLI 설치 및 인증 설정, #2. kubectl 설치 및 kubeconfig 파일 생성의 작업이 필요합니다.\nOCI CLI 설치 및 환경 구성 OCI CLI 설치\n공식 문서 또는 아래 문서를 참고하여 OCI OCI를 설치합니다.\nhttps://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cliinstall.htm 14.1.1 OCI CLI 설치하기 Oracle Linux 기준 예시\n설치\nbash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34; 설치 확인\noci --version OCI CLI Config File설정\n아래 문서를 참고하여 Config File을 설정합니다.\n14.1.1.3 토큰 기반 인증 Config File 설정 14.1.1.4 API Key 기반 인증 Config File 설정 연결 확인\nOCI CLI를 설정한 로컬환경에서 간단한 명령인 oci os ns get을 실행하여 연결 확인\n$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnxxxxxxxxgq\u0026#34; } kubectl CLI 설치 및 환경 구성 kubectl CLI 설치\n공식 문서를 참고하여 kubectl OCI를 설치합니다.\nhttps://kubernetes.io/docs/tasks/tools/install-kubectl-linux/ Linux 기준 예시\n설치\ncurl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl kubeconfig 파일 생성하기\n생성한 OKE 클러스터 상세 페이지에서 Access Cluster를 클릭합니다.\nLocal Access 를 클릭합니다.\nPUBLIC_ENDPOINT\nKubernetes API 접근을 Public IP로 접근할 때 사용합니다. OKE 클러스터 생성시 Kubernetes API에 Public IP를 부여한 경우에 사용 가능한 방법입니다.\nCloud Shell에서 사용한 것과 동일합니다.\nPRIVATE_ENDPOINT\nKubernetes API 접근을 Private IP로 접근할 때 사용합니다. PRIVATE_ENDPOINT, 즉 Private IP로 접근을 해야 하므로, 내부 IP로 접근이 가능한 위치에서 수행할 때 사용합니다.\nbastion host는 외부에서 SSH로 접근 가능하게 22 포트 오픈이 필요하며, 내부적으로는 OKE 클러스터의 Kubernetes API 및 Worker Nodes 들에 접근이 가능해야 합니다. kubeconfig 파일 생성 및 클러스터에 접속 확인 [opc@bastion-host ~]$ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.ap-chuncheon-1.aaaaaaaaq4ltd357todnpr2xqi5lu2orcp26jrdt4lz573ytdc4vz7x3dqla --file $HOME/.kube/config --region ap-chuncheon-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT New config written to the Kubeconfig file /home/opc/.kube/config [opc@bastion-host ~]$ kubectl cluster-info Kubernetes control plane is running at https://10.0.0.4:6443 CoreDNS is running at https://10.0.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. [opc@bastion-host ~]$ kubectl로 접속이 되는 것을 확인했습니다. 이제 kubectl로 일반적인 쿠버네티스를 명령을 수행하여 사용하면 됩니다.\n앱 배포해보기 가장 흔한 형태인 Public Container Registry인 Docker Hub에서 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest 배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\n$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created $ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-5bfd857f89-9rh8q 1/1 Running 0 31s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 80m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 31s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-5bfd857f89 1 1 1 31s ","lastmod":"2024-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/basics/3.access-cluster/2.local-access/","tags":["oke"],"title":"1.1.3.2 로컬 환경에서 클러스터 연결하기"},{"categories":null,"contents":"8.1 File Storage 구성 예시 서버간 공유 파일 시스템으로 File Storage를 통해 여러 개의 클라이언트 서버에서 파일을 공유할 수 있습니다. 필요로 하는 보안규칙을 이해하기 위해 아래 그림과 같이 기본 구성된 VCN의 Subnet과 별도의 Subnet 상에 File Storage를 구성하도록 해보겠습니다.\nSecurity List\n아래 그림에서는 VCN 상의 특정 Subnet에서 접근이 가능하도록 Source IP를 VCN의 CIDR로 지정하였습니다. 특정 Subnet에서만 접근하려고 하면 Security List의 Source IP의 CIDR로 제어하면 됩니다. 같은 Subnet상의 Compute Instance에서 접근하는 경우에도 반드시 Security List에 등록되어 있어야 합니다. Export Option\nSecurity List외에 Export Option을 통해 Client 별로 접근 제어를 할 수 있습니다. ","lastmod":"2020-01-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/1/","tags":["file storage"],"title":"8.1 File Storage 구성 예시"},{"categories":null,"contents":"14.2.1 OCI Request Signature Version 1 앞서 설명한 것처럼 OCI REST API를 호출하기 위해서는 요청 서명을 추가해야 합니다. draft-cavage-http-signatures-08 스펙에 기초하며 개략적인 절차는 다음과 같습니다.\nHTTPS (SSL protocol TLS 1.2) 요청 메시지를 만듭니다. 요청 메시지에 따른 서명대상 문자열을 만듭니다. 개인키와 RSA-SHA256을 사용해 서명대상 문자열을 서명(암호화)합니다. 요청 메시지 Authorization 헤더에 서명된 문자열 및 필요한 추가 정보를 추가합니다. OCI REST API 호출 자세한 절차는 공식 문서와 다음 블로그에서 잘 설명하고 있습니다.\nOracle Cloud Infrastructure Documentation - Request Signatures Oracle Cloud Infrastructure (OCI) REST call walkthrough with curl 서명 샘플 또한 공식 문서에서는 여러가지 언어 및 명령행에서 실행할 수 있는 샘플을 제공하고 있습니다.\nSample Code ","lastmod":"2019-05-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/1/","tags":["rest api","request signature"],"title":"14.2.1 OCI Request Signature Version 1"},{"categories":null,"contents":"10.1 Load Balancer 구성 예시 인스턴스의 가용성을 보장하기 위해 OCI Load Balancer를 통해 여러 개의 대상 서버로 부하를 분산할 수 있습니다. 대상 서버도 가용성 보장을 위해 서로 다른 AD 또는 동일 AD 상의 서로 다른 Fault Domain에 일반적으로 구성해야 합니다. 그리고 OCI Load Balancer는 Fail Over를 위해 자체적으로 이중화되어 구성됩니다.\n","lastmod":"2019-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/1/","tags":["load balancer"],"title":"10.1 Load Balancer 구성 예시"},{"categories":null,"contents":"6.1 Compartment Compartment는 자원들을 쉽게 관리할 수 있도록 하는 논리적인 개념으로 폴더 구조라고 생각하면 됩니다. Tenancy가 생성되면 최초로 Root Compartment 하나가 만들어져 있으며, 관리자가 Root Compartment 하위로 새로운 Compartment를 추가할 수 있습니다. 모든 OCI 자원들은 특정 Compartment에 속하게 되며 Compartment 단위로 사용자들의 접근 정책을 관리할 수 있습니다.\n최초 Tenancy가 만들어지면, Root Compartment가 만들어져 있으며, 모든 Cloud 자원들을 담을 수 있습니다. Root 폴더라고 생각하면 됩니다.\nTenancy 내에 Compartment를 구성하기 전에 고려할 사항이 많겠지만, 그중 아래 사항을 고려하여 구성합니다.\n자원(예시, 인스턴스, Block Storage Volume, VCN, Subnet)을 생성할 시점에, 자원을 담을 Compartment를 지정해야 함 Compartment를 삭제하려면 Compartment에 속한 모든 자원은 삭제 또는 Terminate 시켜야 함 사용자 그룹에 대해 전체 Tenancy에 대한 권한을 부여할 수 있으며, Compartment 단위로도 부여할 수 있음. Compartment 구성 예시 OCI 문서 가이드에서는 크게 아래 두 가지 방법을 예로 들고 있습니다. Compartment 구성시 이를 참조하여 구성합니다.\n예시 #1, 그냥 하나만 사용하기 Tenancy를 소규모 조직이 사용하는 경우 최초 생성된 Root Compartment 하나만 사용합니다. 다만, 기능 테스트 등을 고려하여 오라클 문서에서는 Sandbox Compartment 하나는 적어도 생성한 다음에 일반 사용자그룹에 Sandbox에 대해서는 많은 권한을 부여하되, Root Compartment에 대해서는 엄격하게, 세부적인 권한을 부여하도록 하는 방법을 예로 들고 있습니다.\n예시 #2, 부서, 프로젝트 등을 고려한 Compartment 구성 Root Compartment, Sandbox Compartment 이외에 부서별, 프로젝트별 등을 고려해 Compartment를 구성하여 해당 Compartment 별로 세부적인 권한을 부여하도록 하는 방법을 예로 들고 있습니다.\nSandbox Compartment 만들기 다음 과정을 통해 Compartment를 만들 수 있습니다. 이전 장에서 이미 만든 경우는 생략합니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments 항목으로 이동합니다. Create Compartment 클릭 생성 정보 입력 Name: \u0026ldquo;Sandbox\u0026rdquo; 입력 Description: 설명 입력, 예) 이름과 동일하게 \u0026ldquo;Sandbox\u0026rdquo; Parent Compartment: 상위 Compartment 지정, Root Compartment 선택 Create Compartment 클릭 Production Compartment 만들기 테스트를 위해 여분으로 이름을 Production으로 하는 Compartment를 하나 더 만듭니다. 다음 정보로 Compartment 생성 Name: \u0026ldquo;Production\u0026rdquo; 입력 Description: 설명 입력, 예) 이름과 동일하게 \u0026ldquo;Production\u0026rdquo; Parent Compartment: 상위 Compartment 지정, Root Compartment 선택 ","lastmod":"2019-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/1/","tags":["compartment"],"title":"6.1 Compartment"},{"categories":null,"contents":"5.1 Linux 인스턴스에 Apache HTTP Server 설치 생성한 Instance에 SSH 명령을 통해 접속합니다. 아래 명령은 Oracle Enterprise Linux 기준입니다.\nssh –i \u0026lt;private_key\u0026gt; \u0026lt;username\u0026gt;@\u0026lt;public-ip-address\u0026gt; Apache HTTP Server 설치\nsudo yum -y install httpd OS 방화벽에서 Apache HTTP용 포트, 80 포트 개방\nsudo firewall-cmd --permanent --add-port=80/tcp 팁 방화벽 개방 포트\nApache 서버 디폴트 포트이외 포트, 다른 응용프로그램 사용시 실제 개방이 필요한 포트에 맞게 위 명령을 수정하여 해당 포트를 방화벽에서 개방합니다. 방화벽 변경정보 다시 반영\nsudo firewall-cmd --reload Apache 시작\nsudo systemctl start httpd sudo systemctl enable httpd 테스트를 위해 서버의 Root Index Document 생성\nsudo su echo \u0026#39;Hello Apache\u0026#39; \u0026gt;/var/www/html/index.html 포트가 OS에서 개방되었는지 확인합니다.\nsudo firewall-cmd --zone=public --list-ports 설치 예시 $ ssh -i privateKey opc@129.154.xx.xxx FIPS mode initialized The authenticity of host \u0026#39;129.154.54.163 (129.154.xx.xxx)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:svS10DcrVps4neg/t9lYoutfow+V5a6UHU2VrC3UJ2Y. ECDSA key fingerprint is SHA1:2rtJUm8jt0QiYR7b+I3Ra6j4Yac. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;129.154.54.163\u0026#39; (ECDSA) to the list of known hosts. Activate the web console with: systemctl enable --now cockpit.socket Last login: Thu May 4 07:03:42 2023 from 132.145.xx.xx [opc@examplelinuxinstance ~]$ sudo yum -y install httpd Last metadata expiration check: 2:30:13 ago on Thu 04 May 2023 06:45:55 AM GMT. Dependencies resolved. ========================================================================================================================================= Package Architecture Version Repository Size ========================================================================================================================================= Installing: httpd x86_64 2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5 ol8_appstream 1.4 M Installing dependencies: apr x86_64 1.6.3-12.el8 ol8_appstream 129 k apr-util x86_64 1.6.1-6.el8 ol8_appstream 105 k httpd-filesystem noarch 2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5 ol8_appstream 43 k httpd-tools x86_64 2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5 ol8_appstream 110 k mod_http2 x86_64 1.15.7-5.module+el8.7.0+21029+de29ba63.4 ol8_appstream 155 k oracle-logos-httpd noarch 84.5-1.0.1.el8 ol8_baseos_latest 29 k Installing weak dependencies: apr-util-bdb x86_64 1.6.1-6.el8 ol8_appstream 24 k apr-util-openssl x86_64 1.6.1-6.el8 ol8_appstream 27 k Enabling module streams: httpd 2.4 Transaction Summary ========================================================================================================================================= Install 9 Packages Total download size: 2.0 M Installed size: 5.4 M Downloading Packages: (1/9): oracle-logos-httpd-84.5-1.0.1.el8.noarch.rpm 1.0 MB/s | 29 kB 00:00 (2/9): apr-util-1.6.1-6.el8.x86_64.rpm 3.2 MB/s | 105 kB 00:00 (3/9): apr-util-bdb-1.6.1-6.el8.x86_64.rpm 742 kB/s | 24 kB 00:00 (4/9): apr-util-openssl-1.6.1-6.el8.x86_64.rpm 774 kB/s | 27 kB 00:00 (5/9): apr-1.6.3-12.el8.x86_64.rpm 1.4 MB/s | 129 kB 00:00 (6/9): httpd-tools-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64.rpm 3.5 MB/s | 110 kB 00:00 (7/9): httpd-filesystem-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.noarch.rpm 766 kB/s | 43 kB 00:00 (8/9): mod_http2-1.15.7-5.module+el8.7.0+21029+de29ba63.4.x86_64.rpm 2.3 MB/s | 155 kB 00:00 (9/9): httpd-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64.rpm 9.2 MB/s | 1.4 MB 00:00 ----------------------------------------------------------------------------------------------------------------------------------------- Total 9.1 MB/s | 2.0 MB 00:00 Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Preparing : 1/1 Installing : apr-1.6.3-12.el8.x86_64 1/9 Running scriptlet: apr-1.6.3-12.el8.x86_64 1/9 Installing : apr-util-bdb-1.6.1-6.el8.x86_64 2/9 Installing : apr-util-openssl-1.6.1-6.el8.x86_64 3/9 Installing : apr-util-1.6.1-6.el8.x86_64 4/9 Running scriptlet: apr-util-1.6.1-6.el8.x86_64 4/9 Installing : httpd-tools-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 5/9 Running scriptlet: httpd-filesystem-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.noarch 6/9 Installing : httpd-filesystem-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.noarch 6/9 Installing : oracle-logos-httpd-84.5-1.0.1.el8.noarch 7/9 Installing : mod_http2-1.15.7-5.module+el8.7.0+21029+de29ba63.4.x86_64 8/9 Installing : httpd-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 9/9 Running scriptlet: httpd-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 9/9 Verifying : oracle-logos-httpd-84.5-1.0.1.el8.noarch 1/9 Verifying : apr-1.6.3-12.el8.x86_64 2/9 Verifying : apr-util-1.6.1-6.el8.x86_64 3/9 Verifying : apr-util-bdb-1.6.1-6.el8.x86_64 4/9 Verifying : apr-util-openssl-1.6.1-6.el8.x86_64 5/9 Verifying : httpd-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 6/9 Verifying : httpd-filesystem-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.noarch 7/9 Verifying : httpd-tools-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 8/9 Verifying : mod_http2-1.15.7-5.module+el8.7.0+21029+de29ba63.4.x86_64 9/9 Installed: apr-1.6.3-12.el8.x86_64 apr-util-1.6.1-6.el8.x86_64 apr-util-bdb-1.6.1-6.el8.x86_64 apr-util-openssl-1.6.1-6.el8.x86_64 httpd-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 httpd-filesystem-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.noarch httpd-tools-2.4.37-51.0.1.module+el8.7.0+21029+de29ba63.5.x86_64 mod_http2-1.15.7-5.module+el8.7.0+21029+de29ba63.4.x86_64 oracle-logos-httpd-84.5-1.0.1.el8.noarch Complete! [opc@examplelinuxinstance ~]$ sudo firewall-cmd --permanent --add-port=80/tcp success [opc@examplelinuxinstance ~]$ sudo firewall-cmd --reload success [opc@examplelinuxinstance ~]$ sudo systemctl start httpd [opc@examplelinuxinstance ~]$ sudo systemctl enable httpd Created symlink /etc/systemd/system/multi-user.target.wants/httpd.service → /usr/lib/systemd/system/httpd.service. [opc@examplelinuxinstance ~]$ sudo su [root@examplelinuxinstance opc]# echo \u0026#39;Hello Apache\u0026#39; \u0026gt;/var/www/html/index.html [root@examplelinuxinstance opc]# [root@examplelinuxinstance opc]# [root@examplelinuxinstance opc]# exit exit [opc@examplelinuxinstance ~]$ curl http://127.0.0.1:80/index.html Hello Apache [opc@examplelinuxinstance ~]$ sudo firewall-cmd --zone=public --list-ports 80/tcp ","lastmod":"2019-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/1/","tags":["linux","apache"],"title":"5.1 Linux 인스턴스에 Apache HTTP Server 설치"},{"categories":null,"contents":"14.1.1.1 Windows에서 OCI CLI 설치하기 관리자 모드로 PowerShell 실행\nPowerShell의 remote execution policy 구성을 위해 다음 명령 실행\nSet-ExecutionPolicy RemoteSigned 정책 변경을 위해 Y 응답 설치 스크립트 실행을 위해 다음 실행\npowershell -NoProfile -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.ps1\u0026#39;))\u0026#34; Python 설치여부: Y 응답 설치 경로에 대한 확인 후 설정 CLI의 새 버전 업데이트 확인시 Y 응답 PATH에 CLI 추가 요청시 Y 응답 설치 후 현재 창에는 신규 등록한 PATH가 적용되지 않을 수 있으므로, 새로운 PowerShell을 열어 OCI CLI를 실행합니다.\n설치 예시\nPS C:\\WINDOWS\\system32\u0026gt; Set-ExecutionPolicy RemoteSigned 실행 규칙 변경 실행 정책은 신뢰하지 않는 스크립트로부터 사용자를 보호합니다. 실행 정책을 변경하면 about_Execution_Policies 도움말 항목(https://go.microsoft.com/fwlink/?LinkID=135170)에 설명된 보안 위험에 노출될 수 있습니다. 실행 정책을 변경하시겠습니까? [Y] 예(Y) [A] 모두 예(A) [N] 아니요(N) [L] 모두 아니요(L) [S] 일시 중단(S) [?] 도움말 (기본값은 \u0026#34;N\u0026#34;): Y PS C:\\WINDOWS\\system32\u0026gt; powershell -NoProfile -ExecutionPolicy Bypass -Command \u0026#34;iex ((New-Object System.Net.WebClient).DownloadString(\u0026#39;https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.ps1\u0026#39;))\u0026#34; ****************************************************************************** You have started the OCI CLI Installer in interactive mode. If you do not wish to run this in interactive mode, please include the -AcceptAllDefaults option. If you have the script locally and would like to know more about input options for this script, then you can run: help .\\install.ps1 If you would like to know more about input options for this script, refer to: https://github.com/oracle/oci-cli/blob/master/scripts/install/README.rst ****************************************************************************** 자세한 정보 표시: No valid Python installation found. Python is required to run the CLI. Install Python now? (Entering \u0026#34;n\u0026#34; will exit the installation script) [Y] Yes [N] No [?] 도움말 (기본값은 \u0026#34;Y\u0026#34;): Y 자세한 정보 표시: Downloading Python... 자세한 정보 표시: Download Complete! Installer executable written to: C:\\Users\\thekoguryo\\AppData\\Local\\Temp\\tmp18AF.tmp.exe 자세한 정보 표시: Installing Python to C:\\Users\\thekoguryo\\Python... 자세한 정보 표시: Successfully installed Python! 자세한 정보 표시: Downloading install script to C:\\Users\\thekoguryo\\AppData\\Local\\Temp\\tmpC089.tmp 자세한 정보 표시: C:\\Users\\thekoguryo\\Python False False 자세한 정보 표시: Using Python executable: C:\\Users\\thekoguryo\\Python\\python.exe to run install script... 자세한 정보 표시: Arguments to python script: \u0026#34;C:\\Users\\thekoguryo\\AppData\\Local\\Temp\\tmpC089.tmp\u0026#34; -- Verifying Python version. -- Python version 3.8.5 okay. ===\u0026gt; In what directory would you like to place the install? (leave blank to use \u0026#39;C:\\Users\\thekoguryo\\lib\\oracle-cli\u0026#39;): -- Creating directory \u0026#39;C:\\Users\\thekoguryo\\lib\\oracle-cli\u0026#39;. -- We will install at \u0026#39;C:\\Users\\thekoguryo\\lib\\oracle-cli\u0026#39;. ===\u0026gt; In what directory would you like to place the \u0026#39;oci.exe\u0026#39; executable? (leave blank to use \u0026#39;C:\\Users\\thekoguryo\\bin\u0026#39;): -- Creating directory \u0026#39;C:\\Users\\thekoguryo\\bin\u0026#39;. -- The executable will be in \u0026#39;C:\\Users\\thekoguryo\\bin\u0026#39;. ===\u0026gt; In what directory would you like to place the OCI scripts? (leave blank to use \u0026#39;C:\\Users\\thekoguryo\\bin\\oci-cli-scripts\u0026#39;): -- Creating directory \u0026#39;C:\\Users\\thekoguryo\\bin\\oci-cli-scripts\u0026#39;. -- The scripts will be in \u0026#39;C:\\Users\\thekoguryo\\bin\\oci-cli-scripts\u0026#39;. ===\u0026gt; Currently supported optional packages are: [\u0026#39;db (will install cx_Oracle)\u0026#39;] What optional CLI packages would you like to be installed (comma separated names; press enter if you don\u0026#39;t need any optional packages)?: -- The optional packages installed will be \u0026#39;\u0026#39;. -- Trying to use python3 venv. -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\Python\\\\python.exe\u0026#39;, \u0026#39;-m\u0026#39;, \u0026#39;venv\u0026#39;, \u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\u0026#39;] -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\\\\Scripts\\\\python.exe\u0026#39;, \u0026#39;-m\u0026#39;, \u0026#39;pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--upgrade\u0026#39;, \u0026#39;pip\u0026#39;] Collecting pip Downloading pip-21.3.1-py3-none-any.whl (1.7 MB) |████████████████████████████████| 1.7 MB 6.8 MB/s Installing collected packages: pip Attempting uninstall: pip Found existing installation: pip 20.1.1 Uninstalling pip-20.1.1: Successfully uninstalled pip-20.1.1 Successfully installed pip-21.3.1 -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\\\\Scripts\\\\pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;C:\\\\Users\\\\thekoguryo\\\\AppData\\\\Local\\\\Temp\\\\tmp7z8s7qbi\u0026#39;, \u0026#39;wheel\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting wheel Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB) Installing collected packages: wheel Successfully installed wheel-0.37.1 -- Executing: [\u0026#39;C:\\\\Users\\\\thekoguryo\\\\lib\\\\oracle-cli\\\\Scripts\\\\pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;C:\\\\Users\\\\thekoguryo\\\\AppData\\\\Local\\\\Temp\\\\tmp7z8s7qbi\u0026#39;, \u0026#39;oci_cli\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting oci_cli Downloading oci_cli-3.4.2-py3-none-any.whl (23.8 MB) |████████████████████████████████| 23.8 MB 6.8 MB/s Collecting cryptography\u0026lt;=3.4.7,\u0026gt;=3.2.1 Downloading cryptography-3.4.7-cp36-abi3-win_amd64.whl (1.6 MB) |████████████████████████████████| 1.6 MB ... Collecting pyOpenSSL==19.1.0 Downloading pyOpenSSL-19.1.0-py2.py3-none-any.whl (53 kB) |████████████████████████████████| 53 kB 955 kB/s Collecting click==7.1.2 Downloading click-7.1.2-py2.py3-none-any.whl (82 kB) |████████████████████████████████| 82 kB 5.8 MB/s Collecting six\u0026gt;=1.15.0 Downloading six-1.16.0-py2.py3-none-any.whl (11 kB) Collecting oci==2.54.0 Downloading oci-2.54.0-py2.py3-none-any.whl (12.2 MB) |████████████████████████████████| 12.2 MB 6.4 MB/s Collecting arrow\u0026gt;=1.0.0 Downloading arrow-1.2.1-py3-none-any.whl (63 kB) |████████████████████████████████| 63 kB ... Collecting jmespath==0.10.0 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting pytz\u0026gt;=2016.10 Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB) |████████████████████████████████| 503 kB 6.8 MB/s Collecting certifi Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB) |████████████████████████████████| 149 kB ... Collecting PyYAML\u0026lt;6,\u0026gt;=5.4 Downloading PyYAML-5.4.1-cp38-cp38-win_amd64.whl (213 kB) |████████████████████████████████| 213 kB ... Collecting terminaltables==3.1.0 Downloading terminaltables-3.1.0.tar.gz (12 kB) Preparing metadata (setup.py) ... done Collecting python-dateutil\u0026lt;3.0.0,\u0026gt;=2.5.3 Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB) |████████████████████████████████| 247 kB ... Collecting circuitbreaker\u0026lt;2.0.0,\u0026gt;=1.3.1 Downloading circuitbreaker-1.3.2.tar.gz (7.9 kB) Preparing metadata (setup.py) ... done Collecting cffi\u0026gt;=1.12 Downloading cffi-1.15.0-cp38-cp38-win_amd64.whl (179 kB) |████████████████████████████████| 179 kB 6.4 MB/s Collecting pycparser Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB) |████████████████████████████████| 118 kB 6.8 MB/s Building wheels for collected packages: terminaltables, circuitbreaker Building wheel for terminaltables (setup.py) ... done Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15355 sha256=a28562e8abfb78420764123ed4224e44167c5bc969996d1745bc5c1116b2d8b2 Stored in directory: c:\\users\\thekoguryo\\appdata\\local\\temp\\tmp7z8s7qbi\\wheels\\08\\8f\\5f\\253d0105a55bd84ee61ef0d37dbf70421e61e0cd70cef7c5e1 Building wheel for circuitbreaker (setup.py) ... done Created wheel for circuitbreaker: filename=circuitbreaker-1.3.2-py3-none-any.whl size=6017 sha256=ac910cffc04f0ed6e90f2b65da6241bdaf9848fcf160dc03b1ec821db75600b8 Stored in directory: c:\\users\\thekoguryo\\appdata\\local\\temp\\tmp7z8s7qbi\\wheels\\97\\d2\\3d\\8ad7bff00d846a770cdc0ed208f0fae657c983e675d658c1d5 Successfully built terminaltables circuitbreaker Installing collected packages: pycparser, cffi, six, cryptography, pytz, python-dateutil, pyOpenSSL, circuitbreaker, certifi, terminaltables, PyYAML, oci, jmespath, click, arrow, oci-cli Successfully installed PyYAML-5.4.1 arrow-1.2.1 certifi-2021.10.8 cffi-1.15.0 circuitbreaker-1.3.2 click-7.1.2 cryptography-3.4.7 jmespath-0.10.0 oci-2.54.0 oci-cli-3.4.2 pyOpenSSL-19.1.0 pycparser-2.21 python-dateutil-2.8.2 pytz-2021.3 six-1.16.0 terminaltables-3.1.0 ===\u0026gt; Modify PATH to include the CLI and enable tab completion in PowerShell now? (Y/n): Y -- -- ** Close and re-open PowerShell to reload changes to your PATH ** -- In order to run the autocomplete script, you may also need to set your PowerShell execution policy to allow for running local scripts (as an Administrator run Set-ExecutionPolicy RemoteSigned in a PowerShell prompt) -- -- Installation successful. -- Run the CLI with C:\\Users\\thekoguryo\\bin\\oci.exe --help 자세한 정보 표시: Successfully installed OCI CLI! PS C:\\WINDOWS\\system32\u0026gt; ","lastmod":"2018-12-24T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/1/","tags":["windows","CLI"],"title":"14.1.1.1 Windows에서 OCI CLI 설치하기"},{"categories":null,"contents":"16.1 Load Balancer의 인증서를 OCI Certificate로 관리하기 OCI Certificate는 Load Balancer, API Gateway와의 연동을 지원합니다. Compute 인스턴스 앞에 그림과 같이 OCI Load Balancer를 두고, OCI Certificate 상의 인증서를 통해 관리할 수 있습니다.\nVCN 구성 및 Backend 웹서버 만들기 10.1 Load Balancer 구성 예시 ~ 10.4 Load Balancer를 위한 Subnet 만들기 을 참고하여 Load Balancer를 구성하기 위한 준비를 합니다.\nOCI Certificate 서비스에서 Certificate Authority 만들기 OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security \u0026gt; Dynamic Group을 클릭합니다.\nCA를 위한 Dynamic Group을 만듭니다.\nName: 예시, ca-dynamic-group resource.type=\u0026#39;certificateauthority\u0026#39; Policy를 만듭니다.\nName: 예시, ca-policy Allow dynamic-group \u0026lt;YourDynamicGroupName\u0026gt; to use keys in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group \u0026lt;YourDynamicGroupName\u0026gt; to manage objects in compartment \u0026lt;YourCompartmentName\u0026gt; OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security \u0026gt; Vault를 클릭합니다.\n새 Vault를 만듭니다.\nName: 예시, my-vault 만들어진 Vault로 이동합니다.\n새 Master Encryption Key를 만듭니다.\nOCI Certificate에서 사용하기 위해서는 HSM 모드로, 비대칭 키(여기서는 RSA 기반 키)를 만듭니다.\nOnly HSM asymmetric key is allowed\nOCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security \u0026gt; Certificate \u0026gt; Certificate Authorities를 클릭합니다.\nCreate Certificate Authority를 클릭합니다.\nCertificate Authority 생성 정보를 입력합니다.\n기본 정보\nCA 타입: Root Certificate Authority Name: 예시, my-oci-root-ca Subject Information\nCommon Name: 예시, My OCI Root CA Authority Configuration\n앞서 만든 Vault의 Master Encryption Key를 선택합니다.\nExpire Rule\nMaximum Validity Duration for Certificates (Days): 하위 인증서의 최대 유효 기간으로 원하는 값으로 변경합니다. 예, 365 Revocation Configuration\nSkip Revocation을 체크합니다. Summary\n설정 내용 확인후 CA를 만듭니다.\nOCI Certificate 만들기 앞서 만든 CA 화면으로 이동합니다.\nResources \u0026gt; Certificates에서 Issue Certificate을 클릭합니다.\nCertificate 생성 정보를 입력합니다.\n기본 정보\nCA 타입: Issued by internal CA\nName: 예시, web.example.com\nSubject Information\nCommon Name:\n예시, web.example.com Subject Alternative Names\nDNS Name: HTTPS로 접속시 사용할 주소명을 입력합니다. 예시, web.example.com 필요하면, 접속시 사용할 DNS Name 또는 LB의 고정 IP를 IP Address 더 추가합니다. Certificate Configuration\n나머지는 기본설정 그대로 사용 Rules\n그대로 사용 Summary\n설정 내용 확인후 Certificate를 만듭니다. \u0026ldquo;The validity period 7838778727 exceeds the maximum validity period allowed 7776000000.\u0026ldquo;과 같은 오류가 발생시 만료 날짜(Not Valid After)를 앞당겨 설정합니다.\nLoad Balancer 만들기 10.5 Load Balancer 만들기 을 참고하여 Load Balancer를 구성하기 위한 준비를 합니다.\nConfigure Listener\nListener Name:\n이름 입력, 예) ExampleLB-Listener Specify the type of traffic your listener handles: HTTPS 선택\nOCI Certificate의 SSL 인증서 연동을 테스트하기 위해 HTTPS를 선택합니다. SSL certificate\nOCI Certificate 서비스 선택 앞서 만든 인증서 선택, 예, web.example.com Manage Logging\n에러 로그와 액세스 로그를 OCI Log 서비스를 사용하도록 설정할 수 있습니다. 일단 여기서는 사용하지 않습니다. Submit을 클릭하여 Load Balancer를 생성합니다.\nLoad Balancer 연결확인 생성한 Load Balancer의 Public IP(예, 144.22.xx.xx)를 확인합니다.\n여기서는 테스트 편의상 클라이언트 호스트 파일에 등록합니다. (구매한 도메인을 사용하는 경우, DNS 서버에 등록하면 됩니다.)\n# Mac / Linux # /etc/hosts 144.22.xx.xx web.example.com 웹브라우저에서 https로 접속합니다. https://web.example.com\n아래와 같은 결과가 나오면 정상적으로 HTTPS로 접속한 것입니다. 다만, 사설 CA을 사용하여 발급된 인증서를 Load Balancer의 인증서로 사용했기 때문에, 아래와 같이 보안 경고가 뜹니다. 고급 항목을 클릭하고, 경고를 무시하고 해당 페이지로 이동합니다.\n아래와 같이 페이지에 접속할 수 있습니다.\n인증서를 확인합니다.\n이름을 포함한 인증서 정보를 볼수 있습니다. OCI Certificate에서 생성한 CA와 Certificate 정보를 확인할 수 있습니다. 이후 인증서 갱신 테스트를 위해 현재 유효성 기간을 확인합니다.\n클라이언트 브라우저에서 개인 정보 보호 오류 해결하기 OCI Certificate에서 만든 Root CA는 사설 CA이기 때문에 위와 같이 브라우저에서 해당 CA로 발급한 인증서를 사용하는 웹사이트에 대해서 개인 정보 보안 오류가 발생합니다. 클라이언트 OS에서 사전에 설치된, 알려진 기관의 인증서가 아니기 때문에 발생하는 오류입니다. 해당 오류가 뜨지 않게 하기 위해서는 클라이언트에 사설 CA를 등록하는 과정이 필요합니다.\nOCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security \u0026gt; Certificate \u0026gt; Certificate Authorities를 클릭합니다.\n앞서 만든 Certificate Authoritiy인 my-oci-root-ca을 클릭합니다.\nResources \u0026gt; Versions을 클릭하고, 현재 버전의 우측 액션 메뉴에서 View Content를 클릭합니다.\nCertificate PEM을 다운로드 받습니다.\nMac 기준: 루트 인증서 추가\n클라이언트 브라우저를 실행하는 환경에 다운받은 CA를 설치합니다.\n맥 키체인 접근을 실행합니다.\n시스템 키체인 잠금 해제합니다\n새로운 키체인 항목 생성 아이콘을 클릭합니다.\n다운받은 (my-oci-root-ca) 인증서 파일 선택\n추가된 인증서를 더블 클릭합니다.\nSSL을 항상 신뢰로 변경합니다. 창을 닫으면, 관리자 암호 입력후 저장할 수 있습니다.\n인증서 설정이 완료되었습니다.\n인증서가 등록후 다시 웹페이지를 접속하면 인증오류가 발생하지 합니다. 오류가 발생하는 경우 브라우저 캐시를 클리어하고 다시 시도합니다.\nOCI Certificate 서비스에서 인증서 갱신하기 OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security » Certificate » Certificate을 클릭합니다.\nLoad Balancer에 등록해 사용하고 있는 인증서를 클릭합니다.\n만료를 가정하여 Renew Certificate을 클릭하여 인증서를 갱신합니다.\n기존 인증서와 차이를 위해 만료기간을 다른 날짜로 변경합니다.\n새 버전으로 갱신되었습니다.\nLoad Balancer 재접속하여 인증서 갱신 확인 앞서 Load Balancer에 https로 접속한 웹브라우저로 돌아갑니다.\n현재 화면을 재접속합니다.\n만료날짜를 통해 갱신된 인증서가 적용된 것을 알 수 있습니다.\n인증서 자동 갱신 인증서를 갱신합니다. 기본값을 사용하면, 90일간 유효한 인증서가 발급됩니다.\nResources \u0026gt; Rules 로 이동합니다.\n기본값은 유효기간 90일로, 90일간 인증서는 유효하며, 만료 30일전에 자동 갱신되는 규칙입니다. 갱신된 인증서 또한 90일간 유효합니다.\n테스트상 자동 갱신을 빠르게 확인하기 위해, 사전 갱신 기간을 만료 89일 전으로 변경합니다. 즉 내일 다시 갱신되도록 설정합니다.\nRenewal Interval (Days): 인증서가 자동으로 갱신되는 빈도, 주기를 입력합니다. 현재 버전의 인증서가 만료되기 이전일로 설정합니다. 인증서가 갱신되기 전에, 이전 인증서로 만료되도록은 설정할 수 없습니다. Advance Renewal Period (Days): 만료일 기준으로 며칠 전에 미리 갱신할 지 사전 갱신 일자를 설정합니다. 다음 날인(3월 31일)에 자동으로 갱신되어 버전 4가 만들어진 것을 확인할 수 있습니다.\n","lastmod":null,"permalink":"https://thekoguryo.github.io/oci/chapter16/1/","tags":["certificate","load balancer"],"title":"16.1 Load Balancer의 인증서를 OCI Certificate로 관리하기"},{"categories":null,"contents":"17.2 Kafka Connect, Debezium로 MySQL CDC 구성하기 Kafka에서 Debezium Connector를 통해 데이터 변경분 캡쳐(CDC)를 수행하고, JDBC Connector를 통해 대상 시스템에 동기화는 것을 구현하는 경우가 있습니다. 여기서는 Kafka를 대신하여 Kafka 호환 서비스인 OCI Streaming을 사용할 수 있는 지, 사용시 유의사항이 있는 지를 확인해 보고자 합니다. 그래서 여기서는 Kafka에 대한 자세한 설명보다는 OCI Streaming로 가능 여부 확인에 우선합니다.\nSource MySQL -\u0026gt; Debezium Connector -\u0026gt; OCI Streaming -\u0026gt; JDBC Connector -\u0026gt; Target MySQL 구성으로 CDC를 구성하도록 하겠습니다.\n먼저 볼 것\nOCI Streaming - Kafka Connect, Debezium로 PostgreSQL CDC 구성하기을 먼저 본 것을 전제로 합니다.\nSource, Target MySQL 데이터베이스 구성 먼저, Source, Target으로 사용할 MySQL 데이터베이스 인스턴스를 만듭니다. 설치 편의상 debezium에서 제공하는 컨테이너 이미지를 사용하여, OCI Container Instance 서비스로 사용할 환경을 만듭니다.\nSource MySQL 생성 정보\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Instances 로 이동합니다.\n아래 정보로 Container Instance를 생성합니다.\nName: mysql-source Networking: 편의상 Public Subnet 사용 컨테이너 이미지 Registry hostname: docker.io Repository: debezium/example-mysql Tag: 2.7 환경 변수 MYSQL_ROOT_PASSWORD=password123 MYSQL_USER=mysqluser MYSQL_PASSWORD=password123 MYSQL_DATABASE=sourcedb 고급 옵션 Startup options \u0026gt; Command arguments에 다음 추가 --default-authentication-plugin=mysql_native_password Security List Ingress 규칙에 3306 포트를 개방합니다.\nTarget MySQL 생성 정보\n아래 항목만 달리하여 Container Instance를 하나 더 만듭니다. Name: mysql-target 환경 변수 MYSQL_DATABASE=targetdb mysql client 설치\nMySQL에 접속한 툴로 mysql client를 설치합니다.\nMac 기준\nbrew install mysql-client echo \u0026#39;export PATH=\u0026#34;/opt/homebrew/opt/mysql-client/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc # 확인 mysql --version Oracle Linux 8\nsudo dnf install -y mysql # 확인 mysql --version Source DB에 CDC 관련 권한 설정\nroot로 MySQL에 접속\nmysql --host \u0026lt;Source-MySQL-Public-IP\u0026gt; -u root --password=password123 복제를 위한 권한 mysqluser 유저에 설정\nGRANT SELECT, RELOAD, SHOW DATABASES, LOCK TABLES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39;; Source DB에 CDC 대상 테이블 생성\nmysqluser 로 접속\nmysql --host \u0026lt;Source-MySQL-Public-IP\u0026gt; -u mysqluser --password=password123 샘플 테이블 생성\nuse sourcedb; create table employees ( emp_no int not null, birth_date date not null, first_name varchar(14) not null, last_name varchar(16) not null, gender enum (\u0026#39;M\u0026#39;,\u0026#39;F\u0026#39;) not null, hire_date date not null, primary key (emp_no) ); Target DB 접속 확인\nmysqluser 로 접속\nmysql --host \u0026lt;Target-MySQL-Public-IP\u0026gt; -u mysqluser --password=password123 샘플 테이블 없음을 확인합니다.\nuse targetdb; select * from employees; OCI Streaming 서비스 구성 및 연결 준비 OCI Streaming - Kafka Connect, Debezium로 PostgreSQL CDC 구성하기과 동일합니다.\nKafka Connect 설치 및 구성 Kafka 및 Connector 설치\nOCI Streaming - Kafka Connect, Debezium로 PostgreSQL CDC 구성하기과 동일합니다.\nMySQL 관련 Connector 설치\nSource DB에 CDC를 위한 Debezium MySQL Connector 2.x 버전을 설치합니다.\nwget https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/2.6.1.Final/debezium-connector-mysql-2.6.1.Final-plugin.tar.gz tar zxvf debezium-connector-mysql-2.6.1.Final-plugin.tar.gz -C ./kafka/plugins/ JDBC Connector에서 사용할 MySQL JDBC Driver를 설치합니다.\nwget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-j-8.4.0.tar.gz tar xvzfp mysql-connector-j-8.4.0.tar.gz */*.jar cp mysql-connector-j-8.4.0/mysql-connector-j-8.4.0.jar ./kafka/plugins/confluentinc-kafka-connect-jdbc-10.7.6/lib/ 참조 - https://docs.confluent.io/kafka-connectors/jdbc/current/jdbc-drivers.html#mysql-server Connector 설치를 확인합니다.\n$ ls ./kafka/plugins/ confluentinc-kafka-connect-jdbc-10.7.6 debezium-connector-mysql Kafka Connect Properties 설정\nOCI Streaming - Kafka Connect, Debezium로 PostgreSQL CDC 구성하기과 동일합니다.\nKafka Connect 실행\n설정 파일을 사용하여 실행합니다.\n./kafka/bin/connect-distributed.sh connect-distributed.properties Source DB 연결을 위한 Debezium Connector 설정 Source DB 연결을 위한 설정 파일(connector-source-mysql.json)을 만듭니다.\n{ \u0026#34;name\u0026#34;: \u0026#34;source-mysql\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.debezium.connector.mysql.MySqlConnector\u0026#34;, \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;database.hostname\u0026#34;: \u0026#34;xxx.xx.xx.xxx\u0026#34;, \u0026#34;database.port\u0026#34;: \u0026#34;3306\u0026#34;, \u0026#34;database.user\u0026#34;: \u0026#34;mysqluser\u0026#34;, \u0026#34;database.password\u0026#34;: \u0026#34;password123\u0026#34;, \u0026#34;database.server.id\u0026#34;:\u0026#34;223344\u0026#34;, \u0026#34;database.server.name\u0026#34;: \u0026#34;source-mysql\u0026#34;, \u0026#34;database.ssl.mode\u0026#34;: \u0026#34;preferred\u0026#34;, \u0026#34;snapshot.mode\u0026#34;: \u0026#34;never\u0026#34;, \u0026#34;tombstones.on.delete\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;include.query\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;topic.prefix\u0026#34;: \u0026#34;source-mysql\u0026#34;, \u0026#34;database.include.list\u0026#34;: \u0026#34;sourcedb\u0026#34;, \u0026#34;time.precision.mode\u0026#34;: \u0026#34;connect\u0026#34;, \u0026#34;schema.history.internal.kafka.bootstrap.servers\u0026#34;: \u0026#34;cell-1.streaming.${REGION}.oci.oraclecloud.com:9092\u0026#34;, \u0026#34;schema.history.internal.kafka.topic\u0026#34;: \u0026#34;source-mysql.schema.history\u0026#34;, \u0026#34;include.schema.changes\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;schema.history.internal.skip.unparseable.ddl\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;schema.history.internal.producer.security.protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, \u0026#34;schema.history.internal.producer.sasl.mechanism\u0026#34;: \u0026#34;PLAIN\u0026#34;, \u0026#34;schema.history.internal.producer.sasl.jaas.config\u0026#34;: \u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;${TENANCY_NAME}/${USER_NAME}/${STREAM_POOL_OCID}\\\u0026#34; password=\\\u0026#34;${AUTH_TOKEN}\\\u0026#34;;\u0026#34;, \u0026#34;schema.history.internal.producer.ssl.endpoint.identification.algorithm\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;schema.history.internal.consumer.security.protocol\u0026#34;: \u0026#34;SASL_SSL\u0026#34;, \u0026#34;schema.history.internal.consumer.sasl.mechanism\u0026#34;: \u0026#34;PLAIN\u0026#34;, \u0026#34;schema.history.internal.consumer.sasl.jaas.config\u0026#34;: \u0026#34;org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\u0026#34;${TENANCY_NAME}/${USER_NAME}/${STREAM_POOL_OCID}\\\u0026#34; password=\\\u0026#34;${AUTH_TOKEN}\\\u0026#34;;\u0026#34;, \u0026#34;schema.history.internal.consumer.ssl.endpoint.identification.algorithm\u0026#34;: \u0026#34;https\u0026#34; } } database.hostname: Source DB의 IP\ndatabase.user: DB 접속 유저명\ndatabase.password: 유저 패스워드\ndatabase.dbname : 앞서 CREATE DATABASE 명령으로 만든 Source DB상의 DATABASE 이름\ntopic.prefix: OCI Streaming에 만들어질 Topic들의 Prefix\n예, source-mysql employees 테이블의 변경분은 ${topic.prefix}.${DATABASE_NAME}.${TABLE_NAME} 형식으로 source-mysql.sourcedb.employees 이름으로 Topic이 만들어지게 됨 database.include.list: Source MySQL에서 바라볼 데이터베이스 지정\ntime.precision.mode: connect: date/time/datetime 포맷 변환을 위해 필요, 하지 않을 경우, birth_date: \u0026lsquo;1953-09-02\u0026rsquo;가 Target에서는 -5965처럼될 수 있습니다.\nschema.history.internal\nkafka.bootstrap.servers: Kafka Connection Settings의 Bootstrap Servers 값 사용 *.sasl.jaas.config: Kafka Connection Settings의 SASL Connection Strings 값에서 AUTH_TOKEN만 변경하여 사용, 이중 따옴표라 \\\u0026quot;을 주의합니다. 설정 파일을 사용해 Source DB를 위한 Connector를 배포합니다.\ncurl --location --request POST \u0026#39;http://localhost:8083/connectors\u0026#39; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;@connector-source-mysql.json\u0026#39; 현재 배포된 Connector를 조회합니다.\ncurl localhost:8083/connectors | jq 필요시 삭제후 설정 파일 변경후 다시 배포합니다.\ncurl --location --request DELETE \u0026#39;http://localhost:8083/connectors/source-mysql\u0026#39; Connector를 배포하면 \u0026ldquo;schema.history.internal.kafka.topic\u0026quot;으로 지정한 이름으로 자동생성이 시도되는데, Kafka Connect 로그에서 다음과 같은 오류가 발생합니다.\nCaused by: java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.UnsupportedVersionException: Creating topics with default partitions/replication factor are only supported in CreateTopicRequest version 4+. The following topics need values for partitions and replicas: [source-mysql.schema.history] 해당 Topic 생성시 partitions/replication factor 설정 항목을 알수 없어, 일단 OCI 콘솔에서 직접 생성해 줍니다.\nStream Name: source-mysql.schema.history\nStream Pool: my-stream-pool\nNumber of Partitions: 1\nTarget DB 연결을 위한 JDBC Sink Connector 설정 Target DB 연결을 위한 설정 파일(connector-target-mysql.json)을 만듭니다.\n{ \u0026#34;name\u0026#34;: \u0026#34;target-mysql\u0026#34;, \u0026#34;config\u0026#34;: { \u0026#34;connector.class\u0026#34;: \u0026#34;io.confluent.connect.jdbc.JdbcSinkConnector\u0026#34;, \u0026#34;tasks.max\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;connection.url\u0026#34;: \u0026#34;jdbc:mysql://xxx.xx.xxx.xx:3306/targetdb\u0026#34;, \u0026#34;connection.user\u0026#34;: \u0026#34;mysqluser\u0026#34;, \u0026#34;connection.password\u0026#34;: \u0026#34;password123\u0026#34;, \u0026#34;topics.regex\u0026#34;: \u0026#34;source-mysql.sourcedb.(.*)\u0026#34;, \u0026#34;table.name.format\u0026#34;:\u0026#34;${topic}\u0026#34;, \u0026#34;auto.create\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;auto.evolve\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;delete.enabled\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;insert.mode\u0026#34;: \u0026#34;upsert\u0026#34;, \u0026#34;pk.mode\u0026#34;: \u0026#34;record_key\u0026#34;, \u0026#34;key.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, \u0026#34;key.converter.schemas.enable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;value.converter\u0026#34;: \u0026#34;org.apache.kafka.connect.json.JsonConverter\u0026#34;, \u0026#34;value.converter.schemas.enable\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;transforms\u0026#34;: \u0026#34;unwrap, route\u0026#34;, \u0026#34;transforms.unwrap.type\u0026#34;: \u0026#34;io.debezium.transforms.ExtractNewRecordState\u0026#34;, \u0026#34;transforms.unwrap.drop.tombstones\u0026#34;: \u0026#34;false\u0026#34;, \u0026#34;transforms.unwrap.delete.handling.mode\u0026#34;: \u0026#34;none\u0026#34;, \u0026#34;transforms.route.type\u0026#34;: \u0026#34;org.apache.kafka.connect.transforms.RegexRouter\u0026#34;, \u0026#34;transforms.route.regex\u0026#34;: \u0026#34;([^.]+)\\\\.([^.]+)\\\\.([^.]+)\u0026#34;, \u0026#34;transforms.route.replacement\u0026#34;: \u0026#34;$3\u0026#34; } } connection.url: Target DB 접속을 위한 JDBC URL 입력 connection.user: DB 접속 유저명 connection.password: 유저 패스워드 topics.regex: Source DB에서 여러 테이블의 변경분을 가져오는 경우를 고려해, 정규식으로 입력 예시, ${topic.prefix}.${DATABASE_NAME}.${TABLE_NAME} 형식으로 source-mysql.sourcedb.employees table.name.format: Target DB에 만들 테이블 이름 형식, ${topic}으로 하면 실제 앞 부분이 제외되고 테이블 이름을 가져옮, 예, employees auto.create: Target DB에 해당 테이블이 없을 경우, 자동으로 만들지 여부 지정 auto.evolve: Target DB에 해당 테이블과 스키마가 다를 경우, 자동으로 반영할 지 여부 지넝 delete.enabled: Target DB에 해당 테이블 삭제 가능 여부 지정 transform.route: topic 이름이 source-mysql.sourcedb.employees인 경우, table.name을 고정값으로 하지 않고, \u0026ldquo;table.name.format\u0026rdquo;:\u0026quot;${topic}\u0026ldquo;게 하면, 버그 인지 .employees 처럼 .이 붙어, Target DB에 SQL를 날려, 쿼리 오류가 발생합니다. transform.route을 통해서 앞 부분을 자르고 세번째 테이블이름만으로 라우팅되게 합니다. 설정 파일을 사용해 Target DB를 위한 Connector를 배포합니다.\ncurl --location --request POST \u0026#39;http://localhost:8083/connectors\u0026#39; --header \u0026#39;Content-Type: application/json\u0026#39; --data \u0026#39;@connector-target-mysql.json\u0026#39; 현재 배포된 Connector를 조회합니다.\ncurl localhost:8083/connectors | jq 필요시 삭제후 설정 파일 변경후 다시 배포합니다.\ncurl --location --request DELETE \u0026#39;http://localhost:8083/connectors/target-mysql\u0026#39; OCI 콘솔에서 Streams 목록을 확인해 보면, 새로 Topic 생성된 것을 확인할 수 있습니다.\nCDC 테스트 Source DB에 데이터 변경분 발생\n데이터베이스 접속\nmysql --host \u0026lt;Source-MySQL-Public-IP\u0026gt; -u mysqluser --password=password123 DATABASE 선택\nuse sourcedb; 새 데이터 삽입\nINSERT INTO employees VALUES (10001,\u0026#39;1953-09-02\u0026#39;,\u0026#39;Georgi\u0026#39;,\u0026#39;Facello\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;1986-06-26\u0026#39;); 데이터를 확인합니다.\nmysql\u0026gt; select * from employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | +--------+------------+------------+-----------+--------+------------+ 1 row in set (0.00 sec) Kafka Connect 로그에 다음과 같이 로그가 발생합니다.\n그리고 OCI 콘솔에서 확인해 보면, 테이블에 해당하는 Topic이 추가로 생성됩니다.\nTarget DB에 데이터 확인\n데이터베이스 접속\nmysql --host \u0026lt;Target-MySQL-Public-IP\u0026gt; -u mysqluser --password=password123 DATABASE 접속\nuse targetdb; 현재 데이터 확인합니다. 테이블이 생성되고 데이터가 들어간 것을 확인할 수 있습니다.\nmysql\u0026gt; select * from employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | +--------+------------+------------+-----------+--------+------------+ 1 row in set (0.00 sec) mysql\u0026gt; Target DB에 동일 테이블이 생성되고 데이터도 동기화되었습니다.\nKafka Connect 로그 확인\nKafka Connect의 실행로그를 확인하면, 아래와 같이 변경분이 확인하고, Target DB에 테이블이 없음을 확인하고 생성했다는 로그를 확인할 수 있습니다. 또한 변경분을 전달했다는 메시지도 확인할 수 있습니다.\nconnect-distributed.properties에서 OCI Streaming에서 사용하는 연결방식만 사용하면, 기존 Kafka들 대체하여, Kafka Connect, Connector 플러그인들을 활용하여 CDC를 수행할 수 있는 것을 확인하였습니다.\n추가 참고사항 PacketTooBigException\nCaused by: java.sql.SQLException: Exception chain: com.mysql.cj.jdbc.exceptions.PacketTooBigException: Packet for query is too large (5,673 \u0026gt; 2,048). You can change this value on the server by setting the \u0026#39;max_allowed_packet\u0026#39; variable. at io.confluent.connect.jdbc.sink.JdbcSinkTask.getAllMessagesException(JdbcSinkTask.java:165) at io.confluent.connect.jdbc.sink.JdbcSinkTask.put(JdbcSinkTask.java:111) ... 12 more 방안\nmysql\u0026gt; show variables where Variable_name = \u0026#39;max_allowed_packet\u0026#39;; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | max_allowed_packet | 2048 | +--------------------+-------+ 1 row in set (0.00 sec) mysql\u0026gt; SET GLOBAL max_allowed_packet = 8192; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; exit Bye 재접속 mysql\u0026gt; show variables where Variable_name = \u0026#39;max_allowed_packet\u0026#39;; +--------------------+-------+ | Variable_name | Value | +--------------------+-------+ | max_allowed_packet | 8192 | +--------------------+-------+ 1 row in set (0.00 sec) mysql\u0026gt; ","lastmod":"2024-05-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter17/oci-oss-cdc-mysql-debezium/","tags":["oci streaming","kafka connect","cdc","debezium"],"title":"17.2 Kafka Connect, Debezium로 MySQL CDC 구성하기"},{"categories":null,"contents":"1.2.5.2 Preemptible Capacity Node Pool 추가하기 Preemptible Node Pool 만들기 Node Pool 만들기\nOCI 콘솔에 로그인합니다.\n대상 OKE 클러스터로 이동합니다.\n클러스터 상세정보에서 Resources \u0026gt; Node Pools을 클릭합니다.\nAdd node pool을 클릭합니다.\n생성할 Node Pool의 기본 정보를 입력합니다.\nName: pool-preemptible\nNode type: Managed\nVersion: 예, v1.26.7\nNode Placement Configuration:\nWorker Node가 위치할 서브넷 지정\nShow advanced options \u0026gt; Capacity type에서 Preemptible capacity를 선택합니다.\nShape and image: 기본 선택된 VM.Standard.E3.Flex를 선택합니다.\nNode count: 1\nPod communication: Worker Node가 위치할 서브넷 지정\n나머지 항목은 요건에 맞게 설정합니다.\nAdd를 클릭하여 Node Pool을 추가합니다.\n생성된 Node Pool 확인하기\n생성된 Node를 조회해 봅니다.\n$ kubectl get nodes -L kubernetes.io/arch,name --sort-by=\u0026#39;{.metadata.labels.name}\u0026#39; NAME STATUS ROLES AGE VERSION ARCH NAME 10.0.10.158 Ready node 18d v1.26.7 amd64 oke-cluster-1 10.0.10.42 Ready node 10d v1.26.7 amd64 oke-cluster-1 10.0.10.43 Ready node 10d v1.26.7 amd64 oke-cluster-1 10.0.10.248 Ready node 8h v1.26.7 arm64 pool-arm 10.0.10.8 Ready node 78m v1.26.7 amd64 pool-preemptible OKE 클러스터 상의 Node Pool 구성\nPreemptible 노드에는 생성시 기본적으로 taint가 걸려있습니다.\n$ kubectl describe node 10.0.10.8 Name: 10.0.10.8 Roles: node Labels: beta.kubernetes.io/arch=amd64 ... oci.oraclecloud.com/oke-is-preemptible=true ... ... Taints: oci.oraclecloud.com/oke-is-preemptible:NoSchedule Preemptible Node Pool에 우선하여 애플리케이션 배포하기\nPreemptible 노드는 비용이 저렴한 대신, OCI가 필요한 경우, 임의로 종료하여, 자원을 회수해 갈 수 있습니다. 그래서 여기서는, Preemptible 노드에만 배포하게 설정하기보다는, Preemptible 노드에 우선하되, 다른 노드에도 배포될 수 있는 방법으로 설정하겠습니다.\n앞선 배포 파일에 추가 설정을 합니다.\nPreemptible 노드에도 배포될 수 있도록 taint에 대응하는 toleration을 설정합니다.\nnodeAffinity중에서 perferered 옵션을 사용하여, preemptible 노드가 있는 경우 해당 노드에 우선 배포되고, preemptible 노드가 없는 경우 다른 노드에도 배포될 수 있도록 설정합니다.\n우선 배포외 다른 노드에도 배포되는 것을 확인하기 위해 Preemptible 노드의 CPU 자원을 초과하도록 resources.requests.cpu: 250m 추가합니다.\n# nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 10 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 resources: requests: cpu: 250m tolerations: - key: \u0026#34;oci.oraclecloud.com/oke-is-preemptible\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; affinity: nodeAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: \u0026#34;oci.oraclecloud.com/oke-is-preemptible\u0026#34; operator: In values: - \u0026#34;true\u0026#34; 다시 배포하고 배포된 Node를 확인합니다.\n대부분의 Pod가 우선 조건에 의해 Preemptible 노드인, 10.0.10.8 노드에서 실행중인 것을 볼 수 있습니다. $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-deployment-7bc559645f-2pzst 1/1 Running 0 15s 10.0.10.93 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-59zlg 1/1 Running 0 15s 10.0.10.174 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-7lpwb 1/1 Running 0 15s 10.0.10.224 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-8hd6n 1/1 Running 0 15s 10.0.10.231 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-bn2r4 1/1 Running 0 15s 10.0.10.14 10.0.10.43 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-czdg2 1/1 Running 0 15s 10.0.10.236 10.0.10.158 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-g792z 1/1 Running 0 15s 10.0.10.253 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-kh697 1/1 Running 0 15s 10.0.10.204 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-qrlv9 1/1 Running 0 15s 10.0.10.122 10.0.10.8 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-deployment-7bc559645f-rbttb 1/1 Running 0 15s 10.0.10.29 10.0.10.42 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ","lastmod":"2024-03-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/nodepool/2.preemptible-capacity/","tags":["oke","preemptible","nodepool"],"title":"1.2.5.2 Preemptible Capacity Node Pool 추가하기"},{"categories":null,"contents":"1.3.2 Horizontal Pod Autoscaler CPU 또는 메모리 기반의 Horizontal Pod Autoscaler를 사용하기 위해서는 먼저 Metrics Server의 설치가 필요합니다. Metrics Server가 설치가 되어 있는지 확인합니다.\nkubectl -n kube-system get deployment/metrics-server 샘플 애플리케이션을 배포합니다.\nkubectl apply -f https://k8s.io/examples/application/php-apache.yaml 배포 파일\n500m cpu limit: 최대 500 밀리코어까지 사용할 수 있게 지정. 0.5 코어 200m cpu request: 기본 200 밀리코어까지 사용할 수 있게 지정, 0.2 코어로 해당 자원은 보장받습니다. ... spec: containers: - name: php-apache image: registry.k8s.io/hpa-example ports: - containerPort: 80 resources: limits: cpu: 500m requests: cpu: 200m ... HPA 설정\nPod의 CPU 상태에 따라 최소 1개에서 최대 10개로 replica를 만들도록 HPA를 설정합니다.\nkubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10 HPA 설정을 조회합니다.\n현재는 CPU 부하가 없는 상태로 Pod 1입니다.\n$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 66s 부하를 발생합니다.\n부하 발생용 Pod를 만듭니다.\nkubectl run -it --rm load-generator --image=busybox /bin/sh Pod 안에서 다음명령을 실행하여 부하를 발생시킵니다.\nwhile true; do wget -q -O- http://php-apache; done 실행결과\n$ kubectl run -it --rm load-generator --image=busybox /bin/sh If you don\u0026#39;t see a command prompt, try pressing enter. / # / # while true; do wget -q -O- http://php-apache; done OK!OK!OK!OK!OK!OK!OK!OK!... 새 터미널을 열어서 다시 HPA 설정을 다시 조회합니다. 부하로 인해 Pod가 7개까지 늘어 난 것을 볼 수 있습니다.\n$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 50%/50% 1 10 7 8m25s $ kubectl top pod NAME CPU(cores) MEMORY(bytes) load-generator 17m 0Mi php-apache-7495ff8f5b-cngzz 147m 11Mi php-apache-7495ff8f5b-g6hsr 105m 11Mi php-apache-7495ff8f5b-h4wn6 72m 11Mi php-apache-7495ff8f5b-jw9mq 76m 11Mi php-apache-7495ff8f5b-kcbgw 147m 11Mi php-apache-7495ff8f5b-vpcrj 150m 11Mi php-apache-7495ff8f5b-wsf8c 73m 11Mi 부하를 발생한 터미널로 돌아가 부하를 중지합니다. Ctrl+C 눌러 중지하고, exit 명령으로 컨테이너 밖으로 나옵니다.\n현재 HPA 상태를 조회해 봅니다. 부하발생기의 종료로 현재 평균 CPU는 0이지만, 여전히 Pod는 7개 그대로인 상태입니다.\n$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 7 34m 기본 설정이라면, 5분이 지난후 다시 조회해 보면, Pod가 1개로 줄어든 것을 볼 수 있습니다.\n$ kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 0%/50% 1 10 1 40m HPA 관련 추가적인 설정과 기본 다운스케일 안정화 윈도우 설정값 등은 공식 문서를 참고합니다.\nHorizontal Pod Autoscaling \u0026gt; Example: change downscale stabilization window ","lastmod":"2024-01-26T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/autoscaling/2.hpa/","tags":["oke","autoscaling","hpa"],"title":"1.3.2 Horizontal Pod Autoscaler"},{"categories":null,"contents":"1.2.1.2 OCI Load Balancer 상세 설정하기 OCI Load Balancer는 OCI에서 제공하고 있는 L7 로드 밸런서입니다. HTTP/HTTPS, HTTP/2, TCP의 프로토콜을 지원합니다. HTTP 기반 서비스 사용시 OCI Certificate을 통해 인증서 서비스를 연동하거나, OCI WAF, WAA 등과 연동을 할 수 있습니다.\n기본 생성 Load Balancer 추가 설정없이 Service Type만 Load Balancer로 지정하는 경우, 이전에 주로 사용하던 Fixed Shape 형태의 로드 밸런서로 100Mbps 사이즈로 생성되며, 기본적으로 Public Subnet을 전제로 만들어 지게 됩니다.\n기본 Load Balancer 생성 명령\nkubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc 기본 Load Balancer 생성 YAML\n# nginx-docker-hub-svc.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-docker-hub-svc spec: ports: - port: 80 targetPort: 80 selector: app: nginx-docker-hub type: LoadBalancer 생성된 OCI Load Balancer\nOCI Load Balancer 상세 설정하기 OCI Load Balancer의 설정가능한 annotations 전체 항목 에서 보듯이 annotation을 통해 추가적인 설정 기능을 제공합니다.\nOCI Load Balancer를 사용하는 경우, 전체 annotations 중에서 아래 조합으로 기본적인 설정을 할 수 있겠습니다.\n# nginx-docker-hub-svc-v2.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-docker-hub-svc annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape: \u0026#34;flexible\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-backend-protocol: \u0026#34;HTTP\u0026#34; spec: ports: - port: 80 targetPort: 80 selector: app: nginx-docker-hub type: LoadBalancer loadBalancerIP: 158.179.xxx.xxx annotation 설정값 설명\n항목 값 설명 oci.oraclecloud.com/load-balancer-type \u0026quot;lb\u0026quot; Default: \u0026quot;lb\u0026quot;\n- OCI Load Balancer 사용,\n* L7지원\n\u0026quot;nlb\u0026quot; - OCI Network Load Balancer 사용시\n* L4 지원 service.beta.kubernetes.io/oci-load-balancer-shape \u0026quot;flexible\u0026quot; Default: \u0026quot;100Mbps\u0026quot;\nflexible shape 선택 service.beta.kubernetes.io/oci-load-balancer-shape-flex-min \u0026quot;10\u0026quot; flexible 유형인 경우 최소 용량 지정 service.beta.kubernetes.io/oci-load-balancer-shape-flex-max \u0026quot;10\u0026quot; flexible 유형인 경우 최대 용량 지정 service.beta.kubernetes.io/oci-load-balancer-backend-protocol \u0026quot;HTTP\u0026quot; Default: \u0026quot;TCP\u0026quot;Load Balancer의 Listener의 프로토콜을 지정WAF, WAA 지원을 위해 HTTP으로 지정 spec.loadBalancerIP: Reserved Public IP를 사전에 만들어 가지고 있는 경우, 해당 IP를 직접 입력하여, 생성되는 Load Balancer에 원하는 IP를 부여합니다.\n생성결과\nFlexible Shape, Reserved Public IP\nBandwidth\nProtocol\n추가사항: Internal Load Balancer\n생성되는 위치가 Private Subnet인 경우, 아래 옵션을 추가로 지정합니다.\nmetadata: annotations: service.beta.kubernetes.io/oci-load-balancer-internal: \u0026#34;true\u0026#34; ... 지정하지 않는 경우 Kubernetes상의 오류와 함께 OCI Load Balancer 자체가 생성되지 않습니다. 위 항목 지정시, spec.loadBalancerIP는 동작하지 않습니다. ","lastmod":"2024-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/load-balancer/2.load-balancer-annotation/","tags":["oke"],"title":"1.2.1.2 OCI Load Balancer 상세 설정하기"},{"categories":null,"contents":"1.2.1.3 OCI Network Load Balancer 상세 설정하기 OCI Network Load Balancer는 OCI에서 제공하고 있는 L4 로드 밸런서입니다. TCP, UDP의 프로토콜을 지원합니다. 무료 서비스입니다.\n기본 생성 Load Balancer 추가 설정없이 Service Type만 Load Balancer로 지정하는 경우, 이전에 주로 사용하던 Fixed Shape 형태의 로드 밸런서로 100Mbps 사이즈로 생성되며, 기본적으로 Public Subnet을 전제로 만들어 지게 됩니다.\n기본 Load Balancer 생성 명령\nkubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc 기본 Load Balancer 생성 YAML\n# nginx-docker-hub-svc.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-docker-hub-svc spec: ports: - port: 80 targetPort: 80 selector: app: nginx-docker-hub type: LoadBalancer 생성된 OCI Load Balancer\nOCI Network Load Balancer 상세 설정하기 OCI Network Load Balancer의 설정가능한 annotations 전체 항목 에서 보듯이 annotation을 통해 추가적인 설정 기능을 제공합니다.\nOCI Network Load Balancer를 사용하는 경우, 전체 annotations 중에서 아래 조합으로 기본적인 설정을 할 수 있겠습니다.\n# nginx-docker-hub-svc-nlb.yaml --- apiVersion: v1 kind: Service metadata: name: nginx-docker-hub-svc annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;nlb\u0026#34; spec: ports: - port: 80 targetPort: 80 selector: app: nginx-docker-hub type: LoadBalancer loadBalancerIP: 158.179.xxx.xxx annotation 설정값 설명\n항목 값 설명 oci.oraclecloud.com/load-balancer-type \u0026quot;nlb\u0026quot; Default: \u0026quot;lb\u0026quot; - OCI Load Balancer 사용\n\u0026quot;nlb\u0026quot; - L4을 지원하는 OCI Network Load Balancer, 무료 서비스 spec.loadBalancerIP: Reserved Public IP를 사전에 만들어 가지고 있는 경우, 해당 IP를 직접 입력하여, 생성되는 Load Balancer에 원하는 IP를 부여합니다. 생성 결과\nNetwork Load Balancer로 생성, Reserved Public IP 추가사항: Internal Load Balancer\n생성되는 위치가 Private Subnet인 경우, 아래 옵션을 추가로 지정합니다.\nmetadata: annotations: service.beta.kubernetes.io/oci-load-balancer-internal: \u0026#34;true\u0026#34; ... 지정하지 않는 경우 Kubernetes상의 오류와 함께 OCI Load Balancer 자체가 생성되지 않습니다. 위 항목 지정시, spec.loadBalancerIP는 동작하지 않습니다. ","lastmod":"2024-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/load-balancer/3.network-load-balancer-annotation/","tags":["oke"],"title":"1.2.1.3 OCI Network Load Balancer 상세 설정하기"},{"categories":null,"contents":"4.1.1.2 Spring Boot에서 OCI Cache with Redis 사용하기 Spring Boot에서는 캐쉬를 설정할 수 있고, 캐쉬 서버로 Redis를 많이 사용합니다. OCI Cache with Redis로 만든 Redis 클러스터도 Spring Boot, Redis 코드에서 잘 연결되는 지 확인해 봅니다.\n예제 Caching Data with Spring으로 기본 캐쉬 따라하기 Caching Data with Spring을 따라 순서대로 진행합니다. \u0026lsquo;OCI Cache with Redis를 캐쉬 서버로 연동\u0026rsquo; 전까지는 해당 링크 내용과 동일합니다.\n소소를 Download 받아, 압축해제하거나, Git 명령(git clone https://github.com/spring-guides/gs-caching.git)을 사용하여 복제합니다. cd gs-caching/initial 캐쉬설정하기 전 소스를 빌드하고 실행합니다.\n./mvnw clean package java -jar target/caching-initial-0.0.1-SNAPSHOT.jar 애플리케이션을 실행합니다. 각 조회가 매번 3초 가량 걸리는 것을 볼 수 있습니다.\n2023-11-16T11:34:38.459+09:00 INFO 4132 --- [ main] com.example.caching.AppRunner : .... Fetching books 2023-11-16T11:34:41.463+09:00 INFO 4132 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T11:34:44.465+09:00 INFO 4132 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T11:34:47.468+09:00 INFO 4132 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} Enable caching src/main/java/com/example/caching/SimpleBookRepository.java 에 @Cacheable(\u0026quot;books\u0026quot;)를 추가하여, getByIsbn() 메서드의 결과를 books 캐쉬에 캐쉬하도록 설정합니다.\npackage com.example.caching; import org.springframework.cache.annotation.Cacheable; import org.springframework.stereotype.Component; @Component public class SimpleBookRepository implements BookRepository { @Override @Cacheable(\u0026#34;books\u0026#34;) public Book getByIsbn(String isbn) { simulateSlowService(); return new Book(isbn, \u0026#34;Some book\u0026#34;); } // Don\u0026#39;t do this at home private void simulateSlowService() { try { long time = 3000L; Thread.sleep(time); } catch (InterruptedException e) { throw new IllegalStateException(e); } } } src/main/java/com/example/caching/CachingApplication.java 에 @EnableCaching annotation을 추가합니다.\n캐쉬 라이브러리를 별도로 지정하지 않으면, 디폴트로 ConcurrentHashMap을 사용합니다. package com.example.caching; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cache.annotation.EnableCaching; @SpringBootApplication @EnableCaching public class CachingApplication { public static void main(String[] args) { SpringApplication.run(CachingApplication.class, args); } } 소스를 빌드하고 실행합니다.\n./mvnw clean package java -jar target/caching-initial-0.0.1-SNAPSHOT.jar 처음 조회가 3초 가량 걸리고, 이후 2번은 캐쉬 데이터를 사용하여, 바로 응답한 것을 볼 수 있습니다.\n2023-11-16T11:53:55.346+09:00 INFO 5782 --- [ main] com.example.caching.AppRunner : .... Fetching books 2023-11-16T11:53:58.351+09:00 INFO 5782 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T11:53:58.355+09:00 INFO 5782 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T11:53:58.356+09:00 INFO 5782 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} OCI Cache with Redis를 캐쉬 서버로 연동하기 dependency에 redis를 추가합니다.\nMaven 기준 pom.xml에 다음 추가 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;\tsrc/main/application.properties 파일을 생성하고 다음 항목을 추가합니다.\n아래 host, port는 개발환경을 기준으로 앞선 실습에서의 Bastion 서비스를 통해 포트포워딩으로 연동하는 예시입니다. spring.redis.host=localhost spring.redis.port=6379 spring.redis.ssl=true Redis 설정을 위한 Config 클래스 파일( src/main/java/com/example/caching/RedisConfig.java)을 만듭니다.\nlettuce 라이브러리를 사용하는 예시입니다. redis-cli에서 --tls 옵션을 사용한 것 처럼 lettuceClientConfigurationBuilder.useSsl().disablePeerVerification()을 꼭 추가합니다. application.properties에 값을 사용하여 host, port 값과 ssl 설정여부 설정하는 예시입니다. package com.example.caching; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Bean; import org.springframework.beans.factory.annotation.Value; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.connection.RedisStandaloneConfiguration; import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory; import org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration; import org.springframework.data.redis.connection.RedisNode; @Configuration public class RedisConfig { @Value(\u0026#34;${spring.redis.host}\u0026#34;) private String host; @Value(\u0026#34;${spring.redis.port}\u0026#34;) private int port; @Value(\u0026#34;${spring.redis.ssl}\u0026#34;) private boolean ssl; @Bean public RedisConnectionFactory redisConnectionFactory() { final RedisNode redisNode = RedisNode.newRedisNode() .listeningAt(host, port) .build(); // Connecting as a Standalone Redis server final RedisStandaloneConfiguration redisStandaloneConfiguration = new RedisStandaloneConfiguration(); redisStandaloneConfiguration.setHostName(host); redisStandaloneConfiguration.setPort(port); final LettuceClientConfiguration.LettuceClientConfigurationBuilder lettuceClientConfigurationBuilder = LettuceClientConfiguration.builder(); if (ssl) { lettuceClientConfigurationBuilder.useSsl().disablePeerVerification(); } final LettuceClientConfiguration lettuceClientConfiguration = lettuceClientConfigurationBuilder.build(); return new LettuceConnectionFactory(redisStandaloneConfiguration, lettuceClientConfiguration); } } 캐쉬되는 데이터 모델( src/main/java/com/example/caching/Book.java)을 Serializable 하도록 설정합니다.\npackage com.example.caching; import java.io.Serializable; public class Book implements Serializable { ... } 소스를 빌드하고 실행합니다.\n./mvnw clean package java -jar target/caching-initial-0.0.1-SNAPSHOT.jar 처음 조회가 3초 가량 걸리고, 이후 2번은 빠르게 실행되는 것을 볼 수 있습니다. 결과는 앞선 기본 ConcurrentHashMap를 저장소로 사용할 때랑 동일합니다.\n2023-11-16T12:53:16.367+09:00 INFO 12783 --- [ main] com.example.caching.AppRunner : .... Fetching books 2023-11-16T12:53:20.008+09:00 INFO 12783 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T12:53:20.058+09:00 INFO 12783 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T12:53:20.100+09:00 INFO 12783 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} redis-cli로 Redis 클러스터에서 결과확인 redis-cli로 포트 포워딩된 localhost, 6379 포트로 접속합니다.\nkeys, get 명령으로 데이터가 들어간 것을 확인 할 수 있습니다.\n$ redis-cli --tls -h localhost localhost:6379\u0026gt; keys * 1) \u0026#34;books::isbn-1234\u0026#34; localhost:6379\u0026gt; get books::isbn-1234 \u0026#34;\\xac\\xed\\x00\\x05sr\\x00\\x18com.example.caching.Book\\x81\\xa5yv\\x90|\\xa3\\xe8\\x02\\x00\\x02L\\x00\\x04isbnt\\x00\\x12Ljava/lang/String;L\\x00\\x05titleq\\x00~\\x00\\x01xpt\\x00\\tisbn-1234t\\x00\\tSome book\u0026#34; localhost:6379\u0026gt; 추가 테스트 추가 테스트를 위해 src/main/java/com/example/caching/AppRunner.java 파일을 수정합니다.\npackage com.example.caching; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.stereotype.Component; @Component public class AppRunner implements CommandLineRunner { private static final Logger logger = LoggerFactory.getLogger(AppRunner.class); private final BookRepository bookRepository; public AppRunner(BookRepository bookRepository) { this.bookRepository = bookRepository; } @Override public void run(String... args) throws Exception { logger.info(\u0026#34;.... Fetching books\u0026#34;); logger.info(\u0026#34;isbn-1234 --\u0026gt;\u0026#34; + bookRepository.getByIsbn(\u0026#34;isbn-1234\u0026#34;)); logger.info(\u0026#34;isbn-4567 --\u0026gt;\u0026#34; + bookRepository.getByIsbn(\u0026#34;isbn-4567\u0026#34;)); logger.info(\u0026#34;isbn-1234 --\u0026gt;\u0026#34; + bookRepository.getByIsbn(\u0026#34;isbn-1234\u0026#34;)); logger.info(\u0026#34;isbn-4567 --\u0026gt;\u0026#34; + bookRepository.getByIsbn(\u0026#34;isbn-4567\u0026#34;)); } } src/main/application.properties 파일에 다음 항목을 추가합니다.\nlogging.level.org.springframework.cache=TRACE 다시 빌드하고 실행합니다.\n./mvnw clean package java -jar target/caching-initial-0.0.1-SNAPSHOT.jar 실행 로그 확인\nisbn-1234 조회: 두 번 모두 Cache entry for key 'isbn-1234' found in cache 'books' 와 같이 Cache Hit 되었습니다. isbn-4567 조회: No cache entry for key 'isbn-4567' in cache(s) [books] - 첫번째는 Cache Miss Cache entry for key 'isbn-4567' found in cache 'books' - 두번째는 Cache Hit 2023-11-16T13:06:42.373+09:00 INFO 34105 --- [ main] com.example.caching.AppRunner : .... Fetching books 2023-11-16T13:06:42.375+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-1234\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.SimpleBookRepository.getByIsbn(java.lang.String)] caches=[books] | key=\u0026#39;\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-16T13:06:42.890+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Cache entry for key \u0026#39;isbn-1234\u0026#39; found in cache \u0026#39;books\u0026#39; 2023-11-16T13:06:42.890+09:00 INFO 34105 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T13:06:42.890+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-4567\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.SimpleBookRepository.getByIsbn(java.lang.String)] caches=[books] | key=\u0026#39;\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-16T13:06:42.932+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : No cache entry for key \u0026#39;isbn-4567\u0026#39; in cache(s) [books] 2023-11-16T13:06:42.932+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-4567\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.SimpleBookRepository.getByIsbn(java.lang.String)] caches=[books] | key=\u0026#39;\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-16T13:06:45.996+09:00 INFO 34105 --- [ main] com.example.caching.AppRunner : isbn-4567 --\u0026gt;Book{isbn=\u0026#39;isbn-4567\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T13:06:45.997+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-1234\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.SimpleBookRepository.getByIsbn(java.lang.String)] caches=[books] | key=\u0026#39;\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-16T13:06:46.042+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Cache entry for key \u0026#39;isbn-1234\u0026#39; found in cache \u0026#39;books\u0026#39; 2023-11-16T13:06:46.043+09:00 INFO 34105 --- [ main] com.example.caching.AppRunner : isbn-1234 --\u0026gt;Book{isbn=\u0026#39;isbn-1234\u0026#39;, title=\u0026#39;Some book\u0026#39;} 2023-11-16T13:06:46.043+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-4567\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.SimpleBookRepository.getByIsbn(java.lang.String)] caches=[books] | key=\u0026#39;\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-16T13:06:46.088+09:00 TRACE 34105 --- [ main] o.s.cache.interceptor.CacheInterceptor : Cache entry for key \u0026#39;isbn-4567\u0026#39; found in cache \u0026#39;books\u0026#39; 2023-11-16T13:06:46.088+09:00 INFO 34105 --- [ main] com.example.caching.AppRunner : isbn-4567 --\u0026gt;Book{isbn=\u0026#39;isbn-4567\u0026#39;, title=\u0026#39;Some book\u0026#39;} redis-cli로 다시 조회해 보면 새로 추가한 isbn-4567도 들어가 있는 것을 볼 수 있습니다.\n$ redis-cli --tls -h localhost localhost:6379\u0026gt; keys * 1) \u0026#34;books::isbn-4567\u0026#34; 2) \u0026#34;books::isbn-1234\u0026#34; localhost:6379\u0026gt; 참고 https://spring.io/guides/gs/caching/ https://www.bezkoder.com/spring-boot-redis-cache-example/ ","lastmod":"2023-11-16T00:00:01Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/databases/redis/cache-with-redis/2.cache-with-redis-in-springboot/","tags":["cache","redis","spring-boot"],"title":"4.1.1.2 Spring Boot에서 OCI Cache with Redis 사용하기"},{"categories":null,"contents":"4.2.2 Oracle Database Operator for Kubernetes (OraOperator) 지난 2023년 7월, Kubernetes Operator 방식을 지원하는 Oracle Database Operator for Kubernete가 Production 릴리즈로 v1.0.0이 출시되었습니다. 줄여서 OraOperator라 부릅니다.\nOraOperator goes Production GitHub - Oracle Database Operator for Kubernetes Oracle Database Operator for Kubernetes를 사용하여, 쿠버네티스 클러스터내에 데이터베이스를 생성할 수 있습니다. 또한 Operator를 통해 쿠버네티스 YAML 설정으로, 쿠버네티스 외부에 있는 OCI 상의 또는 On-Premise 상의 오라클 데이터베이스에 대한 라이프사이클 관리를 지원합니다. 데이터베이스 별 가능한 오퍼레이션 내역은 Features Summary에서 확인합니다.\nOracle Database Operator for Kubernetes\n지원하는 데이터베이스\nContainerized DB on Kubernetes Cluster\nContainerized Single Instance databases (SIDB) Containerized Sharded databases (SHARDED) OCI-Only DB\nOracle Base Database Cloud Service (BDBCS) Oracle Autonomous Database: Oracle Autonomous Database shared Oracle Cloud Infrastructure (OCI) (ADB-S) Oracle Autonomous Database on dedicated Cloud infrastructure (ADB-D) Oracle Autonomous Container Database (ACD) (infrastructure) the infrastructure for provisionning Autonomous Databases. On-Premise DB\nOracle Multitenant Databases (CDB/PDBs) 설치 및 테스트된 쿠버네티스 플랫폼 Oracle Container Engine for Kubernetes (OKE)는 물론 그외 쿠버네티스에서도 사용할 수 있습니다. 공식 GitHub Repository 문서상으로 설치 테스트된 쿠버네티스 환경은 다음과 같습니다.\nOracle Container Engine for Kubernetes (OKE) with Kubernetes 1.24 Oracle Linux Cloud Native Environment(OLCNE) 1.6 Minikube with version v1.29.0 Azure Kubernetes Service Amazon Elastic Kubernetes Service Red Hat OKD Red Hat OpenShift Oracle Database Operator for Kubernetes 설치 cert-manager 설치 이미 설치되어 있지 않다면, 아래와 같이 설치합니다.\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.1/cert-manager.yaml OraOperator 설치 OraOperator를 설치합니다.\nkubectl apply -f https://raw.githubusercontent.com/oracle/oracle-database-operator/main/oracle-database-operator.yaml OraOperator의 기동을 확인합니다. 기본 3중화되어 설치됩니다.\n$ kubectl get pods -n oracle-database-operator-system NAME READY STATUS RESTARTS AGE oracle-database-operator-controller-manager-758497dd49-42snr 1/1 Running 0 60s oracle-database-operator-controller-manager-758497dd49-8x2rq 1/1 Running 0 60s oracle-database-operator-controller-manager-758497dd49-mmxw6 1/1 Running 0 60s 쿠버네티스내 오라클 데이터베이스 쿠버네티스내에서 컨테이너 형식으로 구동되는 데이터베이스는 Single Instance와 Sharded 데이터베이스를 현재 지원합니다. RAC는 아직 쿠버네티스내 컨테이너 형식으로는 지원되지 않습니다. Data Guard는 Preview 상태로 지원중입니다.\nContainerized Single Instance databases (SIDB) Containerized Sharded databases (SHARDED) 여기에서는 단일 인스턴스 데이터베이스 SIDB를 Free 데이터베이스, Exress Edition, Enterprise Edition 이 세 가지의 각 기본 생성하는 과정을 확인해 봅니다.\n#1. Oracle Database 23c Free – Developer Release 설치하기 2023년 4월 Oracle Database 23c Free – Developer Release이 Oracle Database의 새로운 무료 오퍼링으로 출시되었습니다. 데이터베이스 컨테이너 이미지는 Oracle Container Registry를 통해 제공합니다. 하지만, Oracle Free Use Terms and Conditions에 따라 계정 인증이나 라이선스 사용 문서 동의없이 바로 사용할 수 있습니다.\n참고사항\nProvisioning Oracle Database Free is supported for release 23c (23.2.0) and later releases.\nFor Free database, only single replica mode (i.e. replicas: 1) is supported.\nFor Free database, you cannot change the init parameters i.e. cpuCount, processes, sgaTarget or pgaAggregateTarget.\nOracle Enterprise Manager is not supported from release 23c and later release.\n데이터베이스 생성용 YAML 파일을 작성합니다. 예시 설치 구성 YAML 파일(config/samples/sidb/singleinstancedatabase_free.yaml)을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/oracle/oracle-database-operator/main/config/samples/sidb/singleinstancedatabase_free.yaml 필요하면 예시 파일을 수정합니다.\n# # Copyright (c) 2023, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # apiVersion: database.oracle.com/v1alpha1 kind: SingleInstanceDatabase metadata: name: freedb-sample namespace: default spec: ## Use only alphanumeric characters for sid sid: FREE ## DB edition edition: free ## Secret containing SIDB password mapped to secretKey adminPassword: secretName: freedb-admin-secret ## Database image details image: ## Oracle Database Free is only supported from DB version 23.2 onwards pullFrom: container-registry.oracle.com/database/free:latest prebuiltDB: true ## size is the required minimum size of the persistent volume ## storageClass is specified for automatic volume provisioning ## accessMode can only accept one of ReadWriteOnce, ReadWriteMany persistence: size: 50Gi ## oci-bv applies to OCI block volumes. Use \u0026#34;standard\u0026#34; storageClass for dynamic provisioning in Minikube. Update as appropriate for other cloud service providers storageClass: \u0026#34;oci-bv\u0026#34; accessMode: \u0026#34;ReadWriteOnce\u0026#34; ## Count of Database Pods. Should be 1 for express edition. replicas: 1 persistence: OKE 기준 예시로 oci-bv StorageClass를 사용합니다. 필요시 각자환경에 맞게 수정합니다.\n$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci oracle.com/oci Delete Immediate false 57d oci-bv (default) blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 57d 서비스 타입: 기본은 NodePort 타입입니다. Load Balancer를 사용하고자 하거나, 그와 더불어 Internal용으로 Private Load Balancer를 쓸 경우 아래 내용을 참고하여 추가로 설정합니다.\n## Type of service . Applicable on cloud enviroments only ## if loadBalService : false, service type = \u0026#34;NodePort\u0026#34; else \u0026#34;LoadBalancer\u0026#34; loadBalancer: false ## \u0026#39;listenerPort\u0026#39; and \u0026#39;tcpsListenerPort\u0026#39; fields customizes port cofigurations for normal and tcps database listeners ## \u0026#39;tcpsListenerPort\u0026#39; will come in effect only when \u0026#39;enableTCPS\u0026#39; field is set ## If loadBalancer is enabled, the listenerPort, tcpsListenerPort will be the load balancer ports ## If loadBalancer is disabled, the listenerPort, tcpsListenerPort will be the node ports(should be in range 30000-32767) ## If enableTCPS is set, and listenerPort is commented/not mentioned in the YAML file, only TCPS endpoint will be exposed #listenerPort: 30001 #tcpsListenerPort: 30002 ## Service Annotations (Cloud provider specific), for configuring the service (e.g. private LoadBalancer service) #serviceAnnotations: # service.beta.kubernetes.io/oci-load-balancer-internal: \u0026#34;true\u0026#34; 관리자 암호를 위한 Secret을 데이터베이스 생성용 YAML에서 지정한 이름으로 만듭니다.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - ## Oracle Database Free Admin password secret apiVersion: v1 kind: Secret metadata: name: freedb-admin-secret namespace: default type: Opaque stringData: ## Specify your DB password here oracle_pwd: Welcome_SIDB_1234 EOF 데이터베이스 생성용 YAML 파일을 통해 데이터베이스를 생성합니다.\nkubectl apply -f singleinstancedatabase_free.yaml 처음 실행하는 경우를 포함하여 Worker Node에 이미지가 없으면, 이미지를 가져오는데 시간이 걸립니다. 컨테이너 이미지 파일 용량이 커서 오래 걸릴 수 있습니다. (2023년 10월 기준, 9.16GB)\n$ kubectl get events --sort-by=\u0026#39;.metadata.creationTimestamp\u0026#39; -A default 4m53s Normal Pulling pod/freedb-sample-awfu6 Pulling image \u0026#34;container-registry.oracle.com/database/free:latest\u0026#34; default 3s Normal Pulled pod/freedb-sample-awfu6 Successfully pulled image \u0026#34;container-registry.oracle.com/database/free:latest\u0026#34; in 4m49.676290519s (4m49.676389866s including waiting) default 3s Normal Created pod/freedb-sample-awfu6 Created container init-permissions default 2s Normal Started pod/freedb-sample-awfu6 Started container init-permissions ... 데이터베이스 상태 status가 Healthy인지 확인합니다.\n$ kubectl get SingleInstanceDatabase NAME EDITION STATUS ROLE VERSION CONNECT STR TCPS CONNECT STR OEM EXPRESS URL freedb-sample Free Healthy PRIMARY 23.3.0.23.09 10.0.10.225:32750/FREE Unavailable Unavailable CONNECT STR 칼럼에 있는 Connection String도 확인합니다.\nNodePort로 오픈된 것을 볼 수 있습니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES freedb-sample-awfu6 1/1 Running 0 14m 10.244.0.13 10.0.10.225 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE freedb-sample ClusterIP 10.96.210.28 \u0026lt;none\u0026gt; 1521/TCP 12m freedb-sample-ext NodePort 10.96.133.142 \u0026lt;none\u0026gt; 5500:31389/TCP,1521:32750/TCP 12m kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 57d Connection String 상의 주소로 접근 가능한 곳에서 연결할 수 있습니다. 여기서는 테스트로 해당 Pod에 들어가서 데이터베이스 연결을 확인해봅니다.\n$ kubectl exec -it freedb-sample-awfu6 -- /bin/bash Defaulted container \u0026#34;freedb-sample\u0026#34; out of: freedb-sample, init-permissions (init), init-prebuiltdb (init) bash-4.4$ sqlplus sys/Welcome_SIDB_1234@10.0.10.225:32750/FREE as sysdba SQL*Plus: Release 23.0.0.0.0 - Production on Thu Oct 26 15:34:09 2023 Version 23.3.0.23.09 Copyright (c) 1982, 2023, Oracle. All rights reserved. Connected to: Oracle Database 23c Free Release 23.0.0.0.0 - Develop, Learn, and Run for Free Version 23.3.0.23.09 SQL\u0026gt; #2. Oracle Database Express Edition (XE) 설치하기 무료 오라클 데이터베이스인 Express Edition을 컨테이너 이미지로 설치합니다. 데이터베이스 컨테이너 이미지는 Oracle Container Registry를 통해 제공합니다. 하지만, Oracle Free Use Terms and Conditions에 따라 계정 인증이나 라이선스 사용 문서 동의없이 바로 사용할 수 있습니다.\n참고사항\nProvisioning Oracle Database express edition is supported for release 21c (21.3.0) only.\nFor XE database, only single replica mode (i.e. replicas: 1) is supported.\nFor XE database, you cannot change the init parameters i.e. cpuCount, processes, sgaTarget or pgaAggregateTarget.\n데이터베이스 생성용 YAML 파일을 작성합니다. 예시 설치 구성 YAML 파일(config/samples/sidb/singleinstancedatabase_express.yaml)을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/oracle/oracle-database-operator/main/config/samples/sidb/singleinstancedatabase_express.yaml 필요하면 예시 파일을 수정합니다.\n서비스 타입: 기본은 NodePort 타입입니다. 여기서는 기능 확인차 Load Balancer를 사용하도록 설정을 추가합니다. # # Copyright (c) 2023, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # apiVersion: database.oracle.com/v1alpha1 kind: SingleInstanceDatabase metadata: name: xedb-sample namespace: default spec: ## Use only alphanumeric characters for sid sid: XE ## DB edition edition: express ## Secret containing SIDB password mapped to secretKey adminPassword: secretName: xedb-admin-secret ## Database image details image: pullFrom: container-registry.oracle.com/database/express:latest prebuiltDB: true ## size is the required minimum size of the persistent volume ## storageClass is specified for automatic volume provisioning ## accessMode can only accept one of ReadWriteOnce, ReadWriteMany persistence: size: 50Gi ## oci-bv applies to OCI block volumes. Use \u0026#34;standard\u0026#34; storageClass for dynamic provisioning in Minikube. Update as appropriate for other cloud service providers storageClass: \u0026#34;oci-bv\u0026#34; accessMode: \u0026#34;ReadWriteOnce\u0026#34; ## Count of Database Pods. Should be 1 for express edition. replicas: 1 ## Type of service . Applicable on cloud enviroments only ## if loadBalService : false, service type = \u0026#34;NodePort\u0026#34; else \u0026#34;LoadBalancer\u0026#34; loadBalancer: true 관리자 암호를 위한 Secret을 데이터베이스 생성용 YAML에서 지정한 이름으로 만듭니다.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - ## Oracle Database Free Admin password secret apiVersion: v1 kind: Secret metadata: name: xedb-admin-secret namespace: default type: Opaque stringData: ## Specify your DB password here oracle_pwd: Welcome_SIDB_1234 EOF 데이터베이스 생성용 YAML 파일을 통해 데이터베이스를 생성합니다.\nkubectl apply -f singleinstancedatabase_express.yaml 처음 실행하는 경우를 포함하여 Worker Node에 이미지가 없으면, 이미지를 가져오는데 시간이 걸립니다. 컨테이너 이미지 파일 용량이 커서 오래 걸릴 수 있습니다. (2023년 10월 기준, 11.4GB)\n데이터베이스 상태 status가 Healthy인지 확인합니다.\n$ kubectl get SingleInstanceDatabase NAME EDITION STATUS ROLE VERSION CONNECT STR TCPS CONNECT STR OEM EXPRESS URL xedb-sample Express Healthy PRIMARY 21.3.0.0.0 150.230.xxx.xx:1521/XE Unavailable https://150.230.xxx.xx:5500/em CONNECT STR 칼럼, OEM EXPRESS URL 칼럼값을 확인합니다.\nLoadBalancer로 오픈된 것을 볼 수 있습니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES xedb-sample-o83o6 1/1 Running 0 16m 10.244.0.12 10.0.10.97 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 57d xedb-sample ClusterIP 10.96.111.26 \u0026lt;none\u0026gt; 1521/TCP 7m18s xedb-sample-ext LoadBalancer 10.96.142.44 150.230.xxx.xx 5500:32274/TCP,1521:32566/TCP 7m18s OEM EXPRESS URL 주소로 접속합니다. Self-Signed 인증서 문제 확인 후 그대로 접속합니다.\n접속 주소: 예, https://150.230.xxx.xx:5500/em Username: SYS Password: 관리자 Secret 생성시 사용한 패스워드 로그인한 Enterprise Manager(EM) 화면 예시입니다.\n호스트에서 SQL Developer로 접속해 보기 개발자 데스크탑/랩탑에 사용할 클라이언트를 가정하여, SQL Developer로 아래와 같이 접속할 수 있습니다.\n새 데이터베이스 접속을 만듭니다.\n접속 세부 정보: Connection String에 있던 정보 사용 사용자 이름/비밀번호: SYS / 관리자 패스워드 롤: SYS 유저로 접속시 SYSDBA 선택 접속 테스트를 하면 성공하는 것을 볼 수 있습니다.\n#3. Oracle Database Enterprise Edition 설치하기 Oracle Database Operator for Kubernetes (OraOperator)은 무료이지만, Oracle Database Enterprise Edition을 사용할 경우에는 관련 라이센스가 필요합니다. 그래서 Oracle Container Registry에서 관련 라이센스 약관 동의 과정과 관련 작업이 필요합니다.\nOCR(Oracle Container Registry)에서 이미지 가져오기 앞서 무료버전과는 달리, 여기서 사용할 컨테이너 이미지는 Oracle Container Registry에 로그인과, 라이센스 동의하는 과정이 필요합니다. 또한 이미지를 가져올때 사용할 Auth Token도 생성하여 사용해야 합니다.\nOracle Container Registry에 접속하여, Database 저장소로 이동합니다.\n저장소 중에서 enterprise를 선택합니다.\n라이센스 동의를 위해 로그인합니다.\nOracle Account로 로그인합니다. 계정이 없는 경우, 계정을 만듭니다.\n라이센스 동의화면에서, 한국어로 표준 약관을 확인하고, 동의를 클릭합니다.\ndocker login을 위한 Auth Token이 필요합니다. 로그인 유저 정보에서 Auth Token을 클릭합니다.\nAuth Token을 생성합니다.\n생성된 키를 복사해 둡니다. 복사후에는 다시 확인할 수 없으니, 잘 보관해 둡니다.\nOracle Container Registry에서 Oracle Database Enterprise Edition에서 아래 이미지 태그에서 보면, Default 서버 이외에 미러 사이트를 제공하고 있습니다. 서울, 춘천 미러사이트도 제공하고 있습니다.\ndocker login 명령으로 Oracle Container Registry로 로그인합니다. (필요하면, 미러 사이트 주소를 사용합니다) 앞서 사용한 Oracle Account와 Auth Token으로 로그인합니다. 로그인 후에는 Oracle Database Enterprise Edition Docker Image를 잘 가져 오는 것을 볼 수 있습니다.\n당장 여기서 사용하지 않으면, 용량이 크므로 가져오는 것만 확인하고 취소합니다.\n$ docker pull container-registry.oracle.com/database/enterprise:latest Error response from daemon: Head \u0026#34;https://container-registry.oracle.com/v2/database/enterprise/manifests/latest\u0026#34;: unauthorized: authentication required $ docker login container-registry.oracle.com Username: \u0026lt;oracle sso username\u0026gt; Password: \u0026lt;Auth Token - Generated Secret Key\u0026gt; Login Succeeded $ docker pull container-registry.oracle.com/database/enterprise:latest latest: Pulling from database/enterprise 7c8051acdded: Pulling fs layer e8e925221939: Download complete 588ca0fc7bfb: Downloading [=======\u0026gt; ] 1.957kB/12.98kB ... ^C 로그인한 정보를 기준으로 Image Pull Secret을 생성합니다.\nkubectl create secret generic oracle-container-registry-secret --from-file=.dockerconfigjson=.docker/config.json --type=kubernetes.io/dockerconfigjson Oracle Database Enterprise Edition 설치하기 참고사항\nFor ease of use, the storage class oci-bv is specified in the singleinstancedatabase_create.yaml. This storage class facilitates dynamic provisioning of the OCI block volumes on the Oracle OKE for persistent storage of the database. The supported access mode for this class is ReadWriteOnce. For other cloud providers, you can similarly use their dynamic provisioning storage classes. It is beneficial to have the database replica pods more than or equal to the number of available nodes if ReadWriteMany access mode is used with the OCI NFS volume. By doing so, the pods get distributed on different nodes and the database image is downloaded on all those nodes. This helps in reducing time for the database fail-over if the active database pod dies. Supports Oracle Database Enterprise Edition (19.3.0), and later releases. To pull the database image faster from the container registry, so that you can bring up the SIDB instance quickly, you can use the container-registry mirror of the corresponding cluster\u0026rsquo;s region. For example, if the cluster exists in Mumbai region, then you can use the container-registry-bom.oracle.com mirror. For more information on container-registry mirrors, follow the link https://blogs.oracle.com/wim/post/oracle-container-registry-mirrors-in-oracle-cloud-infrastructure. To update the init parameters like sgaTarget and pgaAggregateTarget, refer the initParams section of the singleinstancedatabase.yaml file. 데이터베이스 생성용 YAML 파일을 작성합니다. 예시 설치 구성 YAML 파일(config/samples/sidb/singleinstancedatabase_create.yaml)을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/oracle/oracle-database-operator/main/config/samples/sidb/singleinstancedatabase_create.yaml 필요하면 예시 파일을 수정합니다.\n서비스 타입: 기본은 NodePort 타입입니다. 여기서는 기능 확인차 Load Balancer를 사용하도록 설정을 추가합니다. image.pullFrom: 필요하면, 미러 사이트로 변경합니다. # # Copyright (c) 2023, Oracle and/or its affiliates. # Licensed under the Universal Permissive License v 1.0 as shown at http://oss.oracle.com/licenses/upl. # apiVersion: database.oracle.com/v1alpha1 kind: SingleInstanceDatabase metadata: # Creates base sidb-sample. Use singleinstancedatabase_clone.yaml for cloning # and singleinstancedatabase_patch.yaml for patching name: sidb-sample namespace: default spec: ## Use only alphanumeric characters for sid sid: ORCL1 ## DB edition. edition: enterprise ## Secret containing SIDB password mapped to secretKey adminPassword: secretName: db-admin-secret ## DB character set charset: AL32UTF8 ## PDB name pdbName: orclpdb1 ## Enable/Disable ArchiveLog. Should be true to allow DB cloning archiveLog: true ## Database image details image: pullFrom: container-registry.oracle.com/database/enterprise:latest pullSecrets: oracle-container-registry-secret ## size is the required minimum size of the persistent volume ## storageClass is specified for automatic volume provisioning ## accessMode can only accept one of ReadWriteOnce, ReadWriteMany persistence: size: 100Gi ## oci-bv applies to OCI block volumes. Use \u0026#34;standard\u0026#34; storageClass for dynamic provisioning in Minikube. Update as appropriate for other cloud service providers storageClass: \u0026#34;oci-bv\u0026#34; accessMode: \u0026#34;ReadWriteOnce\u0026#34; ## Count of Database Pods. replicas: 1 ## Type of service . Applicable on cloud enviroments only ## if loadBalService : false, service type = \u0026#34;NodePort\u0026#34; else \u0026#34;LoadBalancer\u0026#34; loadBalancer: true 관리자 암호를 위한 Secret을 데이터베이스 생성용 YAML에서 지정한 이름으로 만듭니다.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - ## Oracle Database Free Admin password secret apiVersion: v1 kind: Secret metadata: name: db-admin-secret namespace: default type: Opaque stringData: ## Specify your DB password here oracle_pwd: Welcome_SIDB_1234 EOF 데이터베이스 생성용 YAML 파일을 통해 데이터베이스를 생성합니다.\nkubectl apply -f singleinstancedatabase_create.yaml 처음 실행하는 경우를 포함하여 Worker Node에 이미지가 없으면, 이미지를 가져오는데 시간이 걸립니다. 컨테이너 이미지 파일 용량이 커서 오래 걸릴 수 있습니다. (2023년 10월 기준, 8.58GB)\n데이터베이스 상태 status가 Healthy인지 확인합니다.\n$ kubectl get SingleInstanceDatabase NAME EDITION STATUS ROLE VERSION CONNECT STR TCPS CONNECT STR OEM EXPRESS URL sidb-sample Enterprise Healthy PRIMARY 21.3.0.0.0 158.180.xx.x:1521/ORCL1 Unavailable https://158.180.xx.x:5500/em CONNECT STR 칼럼, OEM EXPRESS URL 칼럼값을 확인합니다.\nOEM EXPRESS URL 주소로 접속합니다. Self-Signed 인증서 문제 확인 후 그대로 접속합니다.\n접속 주소: 예, https://158.180.xx.x:5500/em Username: SYS Password: 관리자 Secret 생성시 사용한 패스워드 Container Name: 입력안함 로그인한 Enterprise Manager(EM) 화면 예시입니다.\n호스트에서 SQL Developer로 접속해 보기 개발자 데스크탑/랩탑에 사용할 클라이언트를 가정하여, SQL Developer로 아래와 같이 접속할 수 있습니다.\n새 데이터베이스 접속을 만듭니다.\n접속 세부 정보: Connection String에 있던 정보 사용 사용자 이름/비밀번호: SYS / 관리자 패스워드 롤: SYS 유저로 접속시 SYSDBA 선택 접속 테스트를 하면 성공하는 것을 볼 수 있습니다.\n멀티 Replica로 실행하기 스케일합니다.\nkubectl scale --replicas=2 SingleInstanceDatabase/sidb-sample 스케일 후 Pod를 조회합니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE sidb-sample-8xe7h 1/1 Running 0 4m43s sidb-sample-a28ph 0/1 Running 0 2m47s 스케일 후 나중에 기동한 두 번째 Pod의 로그를 확인해 봅니다.\n$ kubectl logs -f sidb-sample-a28ph ... Connected to an idle instance. SQL\u0026gt; ORACLE instance shut down. SQL\u0026gt; ORACLE instance started. Total System Global Area 1610612080 bytes Fixed Size 9686384 bytes Variable Size 503316480 bytes Database Buffers 1090519040 bytes Redo Buffers 7090176 bytes SQL\u0026gt; Disconnected from Oracle Database 21c Enterprise Edition Release 21.0.0.0.0 - Production Version 21.3.0.0.0 [2023:10:30 08:28:06]: Releasing lock .ORCL1.create_lck [2023:10:30 08:28:06]: Lock released .ORCL1.create_lck [2023:10:30 08:28:06]: Acquiring lock .ORCL1.exist_lck with heartbeat 30 secs 현재 RAC를 지원하지 않습니다. 그래서 현재 버전에서 Replica만 2개 이상으로 늘리면, 실제 실행중인 Active Pod 이외의 Pod는 컨테이너는 실행되었지만, 아직 READY 상태가 아닙니다. Active Pod 장애시 Fail Over를 준비하고 있는 상태이며, 로그 상에서 lock을 통해 Active Pod로 전환할 지를 체크하고 있는 것을 볼 수 있습니다.\nPV의 AccessMode에 따라 ReadWriteOnce인 경우 같은 Worker Node에 Replica Pod가 추가 생성됩니다. File Storage 서비스를 사용하여 ReadWriteMany인 경우에는 다른 Worker Node에 Replica Pod들이 추가 생성됩니다. 두 번째이후의 Pod들은 빠른 Fail Over를 위한 준비 상태로 대기한다고 이해하면 됩니다. 미리 만들어 놓지 않으면, 실행될 Worker Node에 이미지 파일이 없는 경우 이미지 Pull 시간, 그리고 DB 생성 시간 등의 시간 까지 Fail Over를 위해 신규 Pod가 Ready가 될때 까지 총 시간이 길어지기 때문에 이 시간을 줄이는 용도로 사용할 수 있습니다.\n","lastmod":"2023-10-30T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/databases/oracle-database/2.oracle-database-operator/","tags":["kubernetes","free","database","developer"],"title":"4.2.2 Oracle Database Operator for Kubernetes"},{"categories":null,"contents":"1.10.2 cloud-init으로 kubelet 옵션 변경하기 Worker Node 생성시 커스텀 cloud-init 스크립트를 실행할 수 있습니다. OKE에서 수행해야 하는 디폴트 스크립트에 추가하여, Worker Node 생성시 수행하고 싶은 스크립트을 입력하면 됩니다.\n참조문서: Using Custom Cloud-init Initialization Scripts to Set Up Managed Nodes Worker Node에서 실행중인 kubelet 상의 옵션 값을 변경하고자 하는 경우, 이 cloud-init을 사용하여 변경할 수 있습니다.\n참조문서: Example 3: Using a Custom Cloud-init Script to Set kubelet-extra-args on Managed Nodes kubelet-extra-args으로 kubelet 옵션 변경하기 먼저, kubelet에서 변경가능한 옵션을 확인합니다.\n이중 예시로 다음 옵션을 변경해 보겠습니다. -v, \u0026ndash;v Level Number for the log level verbosity Example 3에서 설명하고 있는 것처럼 OKE 디폴트 스크립트(/var/run/oke-init.sh) 실행시 추가 파라미터로 kubelet-extra-args을 통해 추가해 주면 됩니다.\nverbosity 로그 레벨을 디버그 레벨로 하기 위해 --kubelet-extra-args \u0026quot;--v=4\u0026quot; 추가한 예시입니다. #!/bin/bash curl --fail -H \u0026#34;Authorization: Bearer Oracle\u0026#34; -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode \u0026gt;/var/run/oke-init.sh bash /var/run/oke-init.sh --kubelet-extra-args \u0026#34;--v=4\u0026#34; kubelet-config-file로 kubelet 옵션 변경하기 단일 항목(예시, serialize-image-pulls) 변경하기\nkubelet 옵션중에 일부는 kubelet-config-file을 사용하도록 변경되었습니다.\n예시, \u0026ndash;serialize-image-pulls Default: true Pull images one at a time. We recommend not changing the default value on nodes that run docker daemon with version \u0026lt; 1.9 or an aufs storage backend. Issue #10959 has more details. (DEPRECATED: This parameter should be set via the config file specified by the Kubelet\u0026rsquo;s --config flag. See kubelet-config-file for more information.) 실제 Worker Node 접속하여 kubelet을 확인해 보면 사용중인 config 파일(/etc/kubernetes/kubelet-config.json)을 확인할 수 있습니다.\n[opc@oke-c7fk4xxodaa-nk4a2kfionq-sfwfrelxbnq-1 ~]$ systemctl status kubelet -l ● kubelet.service - Kubernetes Kubelet Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─kubelet-default-args.conf, kubelet-extra-args.conf Active: active (running) since Tue 2023-07-18 07:41:37 UTC; 25min ago Main PID: 13482 (kubelet) Tasks: 11 (limit: 99872) Memory: 50.5M CGroup: /system.slice/kubelet.service └─13482 /usr/bin/kubelet --config /etc/kubernetes/kubelet-config.json --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --cloud-provider=external ... config 파일을 확인해 보면, 이미 serializeImagePulls을 디폴트인 true가 아닌, false로 변경하여 사용하고 있는 것을 볼 수 있습니다.\n{ \u0026#34;apiVersion\u0026#34;: \u0026#34;kubelet.config.k8s.io/v1beta1\u0026#34;, ... \u0026#34;serializeImagePulls\u0026#34;: false, ... } kubelet config 파일의 내용을 변경하고, kubelet을 재시작하는 스크립트를 cloud-init에 아래와 같이 추가할 수 있습니다.\n#!/bin/bash curl --fail -H \u0026#34;Authorization: Bearer Oracle\u0026#34; -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode \u0026gt;/var/run/oke-init.sh bash /var/run/oke-init.sh --kubelet-extra-args \u0026#34;--v=4\u0026#34; sed -i \u0026#39;s/^ \u0026#34;serializeImagePulls\u0026#34;: .*/ \u0026#34;serializeImagePulls\u0026#34;: true,/\u0026#39; /etc/kubernetes/kubelet-config.json systemctl daemon-reload systemctl restart kubelet 여러 항목(예시, cpu-manager-policy) 변경하기\nCPU Manager를 사용하기 위해 cpuManagerPolicy를 static으로 변경하는 경우 kubeReserved 또는 systemReserved도 반드시 설정해야 합니다.\n예시, Field Description cpuManagerPolicy string cpuManagerPolicy is the name of the policy to use. Requires the CPUManager feature gate to be enabled. Default: \u0026ldquo;none\u0026rdquo; kubeReserved map[string]string kubeReserved is a set of ResourceName=ResourceQuantity (e.g. cpu=200m,memory=150G) pairs that describe resources reserved for kubernetes system components. Currently cpu, memory and local storage for root file system are supported. See https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ for more details. Default: nil systemReserved map[string]string systemReserved is a set of ResourceName=ResourceQuantity (e.g. cpu=200m,memory=150G) pairs that describe resources reserved for non-kubernetes components. Currently only cpu and memory are supported. See http://kubernetes.io/docs/user-guide/compute-resources for more detail. Default: nil config 파일을 확인해 보면, 이미 cpuManagerPolicy은 따로 정의되지 않았습니다.\nkubelet config 파일에 여러 항목을 변경 및 추가하기 위해 다음과 같이 jq -s add를 사용하는 방법이 있습니다. kubelet을 재시작하는 스크립트를 cloud-init에 아래와 같이 추가할 수 있습니다.\n#!/bin/bash curl --fail -H \u0026#34;Authorization: Bearer Oracle\u0026#34; -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode \u0026gt;/var/run/oke-init.sh bash /var/run/oke-init.sh --kubelet-extra-args \u0026#34;--v=4\u0026#34; cat \u0026lt;\u0026lt;EOF \u0026gt; updated-option.json { \u0026#34;serializeImagePulls\u0026#34;: false, \u0026#34;cpuManagerPolicy\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;kubeReserved\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;500m\u0026#34; } } EOF cp /etc/kubernetes/kubelet-config.json kubelet-config.json jq -s add kubelet-config.json updated-option.json \u0026gt; /etc/kubernetes/kubelet-config.json rm -rf /var/lib/kubelet/cpu_manager_state systemctl daemon-reload systemctl restart kubelet 현재 kubelet 설정값 확인하기 kubectl로 현재 Worker Nodes를 조회합니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.232 Ready node 22h v1.26.2 대상 Node의 Name으로 다음 명령을 실행해 해당 Nodes의 kubelet 현재 옵션값들을 확인할 수 있습니다.\nkubectl get --raw \u0026#34;/api/v1/nodes/\u0026lt;nodename\u0026gt;/proxy/configz\u0026#34; | jq 실행예시 cloud-init으로 변경 전 값들을 확인할 수 있습니다. $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.232 Ready node 22h v1.26.2 $ kubectl get --raw \u0026#34;/api/v1/nodes/10.0.10.232/proxy/configz\u0026#34; | jq { \u0026#34;kubeletconfig\u0026#34;: { ... \u0026#34;cpuManagerPolicy\u0026#34;: \u0026#34;none\u0026#34;, ... \u0026#34;serializeImagePulls\u0026#34;: false, ... \u0026#34;logging\u0026#34;: { ... \u0026#34;verbosity\u0026#34;: 2, ... }, ... } } cloud-init으로 kubelet 옵션 변경하기 클러스터 생성시 advance options 에서 cloud-init을 입력하거나, 이미 생성된 Node Pool의 Edit 화면에서 Initialization Script에 앞서 작성한 스크립트를 복사합니다.\n추가된 내용으로 클러스터를 생성하거나, 기존 Node Pool인 경우 Worker Node를 재생성합니다.\n새로 생성된 Worker Node에 대해서 cloud-init이 잘 적용되었는 지, kubelet 현재 옵션값들을 확인해봅니다.\nkubectl get --raw \u0026#34;/api/v1/nodes/\u0026lt;nodename\u0026gt;/proxy/configz\u0026#34; | jq 실행예시 cpuManagerPolicy: static**, , kubeReserved, serializeImagePulls: false, verbosity: 4 인 것을 확인할 수 있습니다. $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.232 Ready node 22h v1.26.2 10.0.10.48 Ready node 1m25s v1.26.2 $ kubectl get --raw \u0026#34;/api/v1/nodes/10.0.10.48/proxy/configz\u0026#34; | jq { \u0026#34;kubeletconfig\u0026#34;: { ... \u0026#34;cpuManagerPolicy\u0026#34;: \u0026#34;static\u0026#34;, ... \u0026#34;serializeImagePulls\u0026#34;: false, ... \u0026#34;kubeReserved\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;500m\u0026#34; }, ... \u0026#34;logging\u0026#34;: { ... \u0026#34;verbosity\u0026#34;: 4, ... }, ... } } 이렇게 cloud-init을 통해 생성되는 Worker Node의 kubelet 옵션값을 변경할 수 있습니다.\n","lastmod":"2023-07-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/customize/2.kubelet-options/","tags":["oke","cloud-init","kubelet"],"title":"1.10.2 cloud-init으로 kubelet 옵션 변경하기"},{"categories":null,"contents":"1.2.2.1.2 OCI Native Ingress Controller에서 PATH 기반 라우팅 PATH 기반 기본 라우팅 테스트 가장 기본적인 라우팅으로 URL PATH에 따라 라우팅 서비스를 달리하는 경우입니다.\n테스트를 위한 샘플 앱을 배포합니다.\n배경 색깔이 다른 두 개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80 ingress 설정 YAML(native-ic-ingress-path-basic.yaml)을 작성합니다.\n/blue 요청은 nginx-blue-svc 로 라우팅 /green 요청은 nginx-green-svc로 라우팅 spec.ingressClassName에 앞서 생성한 IngressClass의 이름을 입력하였습니다. 나머지는 Nginx Ingress Controller 때와 큰 차이가 없습니다. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: native-ic-ingress-path-basic spec: ingressClassName: native-ic-ingress-class rules: - http: paths: - path: /blue pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 - http: paths: - path: /green pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 작성한 native-ic-ingress-path-basic.yaml을 배포합니다.\n여기서 OCI Load Balancer의 IP를 확인할 수 있습니다. $ kubectl apply -f native-ic-ingress-path-basic.yaml ingress.networking.k8s.io/native-ic-ingress-path-basic created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE native-ic-ingress-path-basic native-ic-ingress-class * 158.180.xx.xxx 80 5s 지금 상태에서 Load Balancer에서 Pod로 접속이 불가합니다. 그림과 같이 OCI Load Balancer에서 Backend Set에서 오류가 나는 것을 볼 수 있습니다.\nSecurity List에 관련 보안 정책을 추가해 줍니다. OCI VCN-Native Pod Networking CNI를 사용하기 때문 Pod Subnet으로 접속가능하도록 규칙 추가가 필요합니다.\nLoad Balancer Subnet -\u0026gt; Pod Subnet(10.0.40.0/24)의 대상 포트(80): oke-svclbseclist-~~ Security List 업데이트\nEgress 규칙: Stateless Destination IP Protocol Source Port Range Destination Port Range No 10.0.40.0/24 TCP All 80 Load Balancer Subnet -\u0026gt; Pod Subnet(10.0.40.0/24)의 대상 포트(80): oke-pods-seclist-~~ Security List 업데이트\nIngress Rules: Stateless Source IP Protocol Source Port Range Destination Port Range No 10.0.20.0/24 TCP All 80 앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n/blue 요청\n/green 요청\nPOD 정보 확인\n정의한 PATH에 따라 각각 blue, green 앱이 배포된 POD로 라우팅 된 것을 웹페이지 배경색 및 POD IP로 알 수 있습니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-blue-565656fd64-rm49s 1/1 Running 0 95m 10.0.40.52 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-green-8c6dc77b6-hk7h7 1/1 Running 0 95m 10.0.40.16 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; OCI Load Balancer 설정 내용 확인 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Networking \u0026gt; Load Balancers \u0026gt; Load Balancer로 이동합니다.\n생성된 OCI Native Ingress Controller의 Load Balancer를 클릭합니다.\nResources \u0026gt; Listener에서 기본값으로 80 포트로 등록된 것을 볼 수 있습니다.\nResources \u0026gt; Backend sets에서 default외에 두개의 Backend Set이 등록된 것을 볼 수 있습니다. 각각 native-ic-ingress-path-basic에서 두 Path에서 라우팅되는 서비스에 현재 있는 Pod가 Backend로 등록되어 있습니다.\n그 중 /blue의 Backend 서비스인 nginx-blue-svc로 등록된 화면입니다. 다시 Load Balancer에서 Resources \u0026gt; Routing policies을 선택하면, 2개의 라우팅 규칙이 등록되어 있습니다. 클릭하여 등록된 규칙을 보면, native-ic-ingress-path-basic에서 요청한 그대로 각각 /blue, /green Path에 따라 대상 Backend Set로 라우팅하는 규칙이 등록된 것을 볼 수 있습니다.\n참고 https://kubernetes.io/docs/concepts/services-networking/ingress/#deprecated-annotation\nDeprecated annotation Before the IngressClass resource and ingressClassName field were added in Kubernetes 1.18, Ingress classes were specified with a kubernetes.io/ingress.class annotation on the Ingress. This annotation was never formally defined, but was widely supported by Ingress controllers.\nThe newer ingressClassName field on Ingresses is a replacement for that annotation, but is not a direct equivalent. While the annotation was generally used to reference the name of the Ingress controller that should implement the Ingress, the field is a reference to an IngressClass resource that contains additional Ingress configuration, including the name of the Ingress controller.\nNginx Ingress Controller에서 kubernetes.io/ingress.class annotation을 사용하였고, 인터넷 상의 많은 예시가 그렇게 되어 있지만, 공식적으로는 ingressClassName 사용을 권고하고 있으니, OCI Native Ingress Controller의 예시에 있는 ingressClassName 을 여기에서는 사용합니다.\n","lastmod":"2023-06-12T00:00:01Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/oci-native-ingress/2.ingress-path/","tags":["ingress-controller","oci-ingress-controller"],"title":"1.2.2.1.2 PATH 기반 라우팅"},{"categories":null,"contents":"2.2 OKE 클러스터 만들기 - Virtual Nodes Virtual Nodes를 위한 OCI IAM Policy 만들기 OKE Serverless 서비스가 Virtual Nodes 생성할 있도록 권한을 줍니다. 사용중인 테넌시내에서 Virtual Nodes을 사용하겠다고 알려주는 것이기 때문에, 아래 내용을 변경없이 그대로 복사해서 Policy를 만들면 됩니다. 각자 테넌시 환경에 맞게 변경하는 내용이 아닙니다.\nOCI Documentation \u0026gt; Container Engine \u0026gt; Required IAM Policies for Using Virtual Nodes Name: 예, oke-virtual-nodes-policy define tenancy ske as ocid1.tenancy.oc1..aaaaaaaacrvwsphodcje6wfbc3xsixhzcan5zihki6bvc7xkwqds4tqhzbaq define compartment ske_compartment as ocid1.compartment.oc1..aaaaaaaa2bou6r766wmrh5zt3vhu2rwdya7ahn4dfdtwzowb662cmtdc5fea endorse any-user to associate compute-container-instances in compartment ske_compartment of tenancy ske with subnets in tenancy where ALL {request.principal.type=\u0026#39;virtualnode\u0026#39;,request.operation=\u0026#39;CreateContainerInstance\u0026#39;,request.principal.subnet=2.subnet.id} endorse any-user to associate compute-container-instances in compartment ske_compartment of tenancy ske with vnics in tenancy where ALL {request.principal.type=\u0026#39;virtualnode\u0026#39;,request.operation=\u0026#39;CreateContainerInstance\u0026#39;,request.principal.subnet=2.subnet.id} endorse any-user to associate compute-container-instances in compartment ske_compartment of tenancy ske with network-security-group in tenancy where ALL {request.principal.type=\u0026#39;virtualnode\u0026#39;,request.operation=\u0026#39;CreateContainerInstance\u0026#39;} OKE Serverless 클러스터 만들기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여 생성합니다.\nName\n예) oke-serverless-cluster-1 Kubernetes API Endpoint\nPublic API로 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kuberentes version\nVirtual Nodes는 v1.25.4 이상에서 지원됩니다.\nVirtual Nodes는 아직 버전 업그레이드를 지원하지 않으므로, 사용을 원하는 버전을 선택합니다.\nNode type: Virtual을 선택합니다.\nNode count: 기본값인 3을 그대로 사용합니다.\nPod shape: Pod개 배포될 Shape을 선택합니다. E3.Flex, E4.Flex Shape 중에 선택합니다.\nManaged Nodes 처럼 Worker Node의 Compute 인스턴스의 Shape을 고정하는 것이 아닙니다. Pod 자원 요청시 지정하는 requests, limits에 따라 자원이 할당되며, Serverless로 OKE가 관리합니다. 입력한 정보를 리뷰하고 클러스터를 생성합니다.\nQuick Create로 클러스터를 생성시 기본 네트워크 자원이 함께 생성됩니다.\n클러스터 생성 확인\n생성된 클러스터 상세정보를 그림과 같이 Enhanced Cluster이며, Network 타입은 VCN-Native Pod Networking을 사용합니다. Flannel CNI는 지원하지 않습니다.\nNode Pool 상세 정보에서 Resources \u0026gt; Nodes 정보를 보면 생성된 Worker Nodes를 확인할 수 있습니다. Virtual Nodes로 Pod가 위치할 가상 노드로, Compute 인스턴스가 따라 생성되지 않습니다.\nOKE Serverless 클러스터 연결하기\n연결하는 방법은 기존 OKE 클러스터를 연결하는 방법과 동일합니다.\n생성한 OKE 클러스터 상세 페이지로 이동하여, Access Cluster를 클릭합니다. 가이드에 따라 사용환경에서 클러스터에 연결합니다.\n노드 정보를 조회해 봅니다.\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.12 Ready \u0026lt;none\u0026gt; 3m52s v1.26.2-0.2.169-230516185737 10.0.10.12 \u0026lt;none\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 10.0.10.78 Ready \u0026lt;none\u0026gt; 3m47s v1.26.2-0.2.169-230516185737 10.0.10.78 \u0026lt;none\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; 10.0.10.85 Ready \u0026lt;none\u0026gt; 3m53s v1.26.2-0.2.169-230516185737 10.0.10.85 \u0026lt;none\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; \u0026lt;unknown\u0026gt; ","lastmod":"2023-05-22T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-virtual-nodes/2.create-oke-cluster-virtual-nodes/","tags":["oke","enhanced cluster","virtual nodes"],"title":"2.2 OKE 클러스터 만들기 - Virtual Nodes"},{"categories":null,"contents":"3.2 OKE 클러스터 만들기 - Self-Managed Nodes 사전 요구 사항 확인 Cluster 요구사항\nControl Plane Nodes 버전: 1.25 이상 Enhanced Cluster Flannel CNI 사용 할 것 (VCN-Native Pod Networking CNI는 지원하지 않음) Image 요구사항\n2023년 3월 28일 이후 릴리즈된 아래 이미지 중 사용 OKE Worker Node Oracle Linux 7.x Images OKE Worker Node Oracle Linux 8.x Images. OKE 클러스터 준비하기 위 요구사항에 맞게 OKE 클러스터를 준비합니다. 아래 내용은 편의상 Quick Create 모드로 클러스터를 생성시의 과정을 설명하는 내용입니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여 생성합니다.\nName\n예) oke-cluster-self-managed Kubernetes API Endpoint\nPublic API로 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kuberentes version\n1.25 이상 요구됩니다. 여기서는 v1.26.7을 선택합니다. Node type: Managed를 선택합니다.\nNode count: 기본값인 3을 그대로 사용합니다.\nNext를 클릭합니다.\n기본 Network type: OCI_VCN_IP_NATIVE입니다. Flannel CNI로 변경할 필요가 있습니다. 변경을 위해 아래에 Save as stack을 클릭합니다.\n저장하면, Resource Manager의 Stack으로 저장됩니다.\n저장후 클러스터 생성은 Cancel 합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Resource Manager \u0026gt; Stacks로 이동합니다.\n저장한 Stack을 클릭합니다.\nEdit 아래 Edit Terrafrom configuration in code editor를 클릭합니다.\nStack 아래에 main.tf 파일을 더블클릭합니다.\nCNI 설정을 OCI_VCN_IP_NATIVE에서 FLANNEL_OVERLAY로 변경합니다.\n수정전\n...\tcluster_pod_network_options { cni_type = \u0026#34;OCI_VCN_IP_NATIVE\u0026#34; } ...\tnode_pool_pod_network_option_details { cni_type = \u0026#34;OCI_VCN_IP_NATIVE\u0026#34; } ...\t수정후\n...\tcluster_pod_network_options { cni_type = \u0026#34;FLANNEL_OVERLAY\u0026#34; } ...\tnode_pool_pod_network_option_details { cni_type = \u0026#34;FLANNEL_OVERLAY\u0026#34; } ...\tStack을 우클릭한 변경사항을 저장하고 Stack Plan을 실행합니다.\n클라우드 콘솔에서 Stacks로 다시 이동합니다.\nPlan이 잘 실행된 것을 확인하고, Apply를 클릭하여 변경된 정보로 OKE 클러스터를 생성합니다.\nApply가 완료된 것을 확인합니다.\n요구사항에 맞는 클러스터가 준비되었습니다.\nSelf-Managed 노드를 위한 OCI IAM Dynamic Group 및 Policy 만들기 생성한 노드가 OKE 클러스터에 조인할 수 있도록 권한을 추가하는 과정입니다.\nOCI Documentation \u0026gt; Container Engine \u0026gt; Creating a Dynamic Group and a Policy for Self-Managed Nodes Self-Managed 노드로 추가할 Compute 인스턴스를 포함하는 Dynamic Group을 만듭니다.\nName: 예, oke-self-managed-node-dyn-grp ALL {instance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39;} 생성한 Dynamic Group을 위한 Policy를 만듭니다.\nName: 예, oke-self-managed-node-policy Allow dynamic-group oke-self-managed-node-dyn-grp to {CLUSTER_JOIN} in compartment \u0026lt;compartment-name\u0026gt; 더 조건을 제한하기 위해서는 where 조건에 조인하려는 클러스터를 명시합니다. Allow dynamic-group oke-self-managed-node-dyn-grp to {CLUSTER_JOIN} in compartment \u0026lt;compartment-name\u0026gt; where target.cluster.id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39; Self-Managed 노드를 위한 cloud-init 스크립트 준비하기 Kubernetes API private endpoint 준비\n클러스터 상세 정보에서 해당정보를 확인합니다. 단 이후 사용할 시에는 포트넘버 없이 사용합니다. 예, 10.0.0.13\nbase64-encoded CA certificate 얻기\n아래 명령으로 확인합니다.\noci ce cluster create-kubeconfig --cluster-id \u0026lt;cluster-ocid\u0026gt; --region \u0026lt;region-identifier\u0026gt; --file - | grep -oE \u0026#34;LS0t.*\u0026#34; 예시\n$ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.ap-chuncheon-1.aaaaaaaa67cdjzuxf5j2s6yhdvtft6vrrfievmdx65xyqx3hncut2lfsgbja --region ap-chuncheon-1 --file - | grep -oE \u0026#34;LS0t.*\u0026#34; LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURpRENDQW5DZ0F3SUJBZ0lRSi9VMGN0RHpkVmN2M2hjSms......0tLS0tCg== 다음과 같이 cloud-init 스크립트를 준비합니다.\n#!/usr/bin/env bash bash /etc/oke/oke-install.sh \\ --apiserver-endpoint \u0026#34;\u0026lt;cluster-endpoint\u0026gt;\u0026#34; \\ --kubelet-ca-cert \u0026#34;\u0026lt;base64-encoded-certificate\u0026gt;\u0026#34; 예시\n#!/usr/bin/env bash bash /etc/oke/oke-install.sh \\ --apiserver-endpoint \u0026#34;10.0.0.13\u0026#34; \\ --kubelet-ca-cert \u0026#34;LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURpRENDQW5DZ0F3SUJBZ0lRSi9VMGN0RHpkVmN2M2hjSms......0tLS0tCg==\u0026#34; Self-Managed 노드 만들기 아래 링크를 클릭하여, 사용할 Image의 OCID를 확인합니다.\n2023년 3월 28일 이후 릴리즈된 아래 이미지 중 사용\nOKE Worker Node Oracle Linux 7.x Images OKE Worker Node Oracle Linux 8.x Images 예시\n쿠버네티스 버전, CPU 유형에 맞게 Image를 선택하고, Read more를 클릭합니다.\n사용할 리전의 Image OCID를 복사해 둡니다.\n좌측 상단 햄버거 메뉴에서 Compute \u0026gt; Compute \u0026gt; Instances로 이동합니다.\n컴퓨트 인스턴스를 생성합니다.\nName: 예, instance-self-managed\nImage:\nChange image를 클릭합니다. My images \u0026gt; Image OCID를 선택후 복사해둔 Image OCID를 입력합니다. Primary VNIC information\nOKE Worker Nodes가 위치한 VCN과 Subnet을 선택합니다. Add SSH keys\n컴퓨트 인스턴스 접속시 사용할 SSH Key의 Public Key를 입력합니다. 제일 아래 Show advanced options를 클릭\nManagement \u0026gt; Paste cloud-init script에 앞서 준비한 cloud-init 스크립트를 입력합니다.\n필요한 추가 설정을 하고 컴퓨트 인스턴스를 생성합니다.\nOKE Serverless 클러스터 연결하기 연결하는 방법은 기존 OKE 클러스터를 연결하는 방법과 동일합니다.\n생성한 OKE 클러스터 상세 페이지로 이동하여, Access Cluster를 클릭합니다. 가이드에 따라 사용환경에서 클러스터에 연결합니다.\n노드 정보를 조회해 봅니다.\n인스턴스 생성후 몇 분후에 클러스터에 조인된 것을 확인할 수 있습니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.106 Ready node 100m v1.26.7 10.0.10.211 Ready node 100m v1.26.7 10.0.10.229 NotReady \u0026lt;none\u0026gt; 82s v1.26.7 10.0.10.242 Ready node 100m v1.26.7 $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.106 Ready node 100m v1.26.7 10.0.10.211 Ready node 100m v1.26.7 10.0.10.229 Ready \u0026lt;none\u0026gt; 114s v1.26.7 10.0.10.242 Ready node 100m v1.26.7 해당 노드를 조회해 봅니다.\n$ kubectl describe node 10.0.10.229 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 7m52s kube-proxy Normal Starting 9m41s kubelet Starting kubelet. Normal NodeHasSufficientMemory 9m41s kubelet Node 10.0.10.229 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 9m41s kubelet Node 10.0.10.229 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 9m41s kubelet Node 10.0.10.229 status is now: NodeHasSufficientPID Normal Synced 9m40s cloud-node-controller Node synced successfully Normal RegisteredNode 9m38s node-controller Node 10.0.10.229 event: Registered Node 10.0.10.229 in Controller Normal NodeAllocatableEnforced 9m38s kubelet Updated Node Allocatable limit across pods Normal NodeReady 7m55s kubelet Node 10.0.10.229 status is now: NodeReady 노드의 레이블을 조회해 봅니다.\n$ kubectl get node 10.0.10.229 -o json | jq \u0026#39;.metadata.labels\u0026#39; { ... \u0026#34;displayName\u0026#34;: \u0026#34;instance-self-managed\u0026#34;, ... \u0026#34;oci.oraclecloud.com/node.info.byon\u0026#34;: \u0026#34;true\u0026#34;, ... } BYON(Bring Your Own Nodes) 레이블을 조회해 봅니다.\n$ kubectl get nodes -L oci.oraclecloud.com/node.info.byon NAME STATUS ROLES AGE VERSION NODE.INFO.BYON 10.0.10.106 Ready node 154m v1.26.7 10.0.10.211 Ready node 154m v1.26.7 10.0.10.229 Ready \u0026lt;none\u0026gt; 55m v1.26.7 true 10.0.10.242 Ready node 154m v1.26.7 Self-Managed 노드 클러스터에 조인되었습니다. 쿠버네티스 배포 자원에 대한 제약은 따로 없으니, Managed Nodes와 동일하게, 배포하여 사용하면 됩니다.\n참고 How to Deploy Self Managed Nodes on OKE ","lastmod":"2023-05-22T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-self-managed-nodes/2.create-oke-cluster-self-managed-nodes/","tags":["oke","enhanced cluster","self-managed nodes"],"title":"3.2 OKE 클러스터 만들기 - Self-Managed Nodes"},{"categories":null,"contents":"1.9.2 Workload Identity 사용하기 Workload Identity: 애플리케이션 컨테이너 Pod에서 OCI 자원에 대해 API Call을 하는 경우, OCI Policy로 권한 설정이 필요합니다. 기본적으로는 애플리케이션내에서 User 인증을 사용하는 방법 또는 Worker Nodes 그룹에 대해 권한 설정하는 방법이 있습니다. 여기에서 추가해 Workload Identity는 Pod 단위로 권한 설정을 지원합니다.\n쿠버네티스 클러스터에서 구동하는 애플리케이션에서 OCI 자원을 접근할 필요가 있는 경우, 이 때 클러스터 전체가 아닌, 해당 애플리케이션 workload에 한정하여, 권한을 부여하는 것이 보안상으로 안전합니다. OKE 클러스터, namespace, service account의 조합으로 workload 자원을 특정하여 workload identity로 지정하고, 이 workload identity에 대해서 OCI IAM Policy를 통해 다른 OCI 자원에 대한 권한을 부여할 수 있습니다.\n2023년 5월 현재 기준, Workload Identity는 OCI SDK와 함께 사용할 수 있습니다. 지원하는 OCI SDK는 다음과 같습니다.\nGo SDK v65.32.0 (and later) Java SDK v2.54.0 (and later) Workload Identity에 대한 Policy 설정하기 애플리케이션 컨테이너가 위치할 namespace이 없는 경우 먼저 생성합니다.\nkubectl create namespace \u0026lt;namespace-name\u0026gt; 쿠버네티스 Service Account를 생성합니다.\nkubectl create serviceaccount \u0026lt;service-account-name\u0026gt; --namespace \u0026lt;namespace-name\u0026gt; 실행 예시\n$ kubectl create namespace workload-identity-ns namespace/workload-identity-ns created $ kubectl create serviceaccount workload-identity-sa --namespace workload-identity-ns serviceaccount/workload-identity-sa created OCI 콘솔 - OKE 클러스터 상세정보에서 클러스터 OCID를 얻습니다.\n대상 Workload Identity에 대한 Policy를 설정합니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Policies 항목으로 이동합니다.\nCreate Policy 클릭하여 Policy를 생성합니다.\nWorkload Identity 정보를 이용해 다음과 같이 Policy를 지정합니다.\nallow any-user to \u0026lt;verb\u0026gt; \u0026lt;resource\u0026gt; in \u0026lt;location\u0026gt; where all { request.principal.type = \u0026#39;workload\u0026#39;, request.principal.namespace = \u0026#39;\u0026lt;namespace-name\u0026gt;\u0026#39;, request.principal.service_account = \u0026#39;\u0026lt;service-account-name\u0026gt;\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} 실행 예시 - 애플리케이션 컨테이너에서 OCI Object Storage에 있는 오브젝트 조회권한 예시\n# oke-workload-identity-policy allow any-user to inspect objects in compartment oci-hol-xx where all { request.principal.type = \u0026#39;workload\u0026#39;, request.principal.namespace = \u0026#39;workload-identity-ns\u0026#39;, request.principal.service_account = \u0026#39;workload-identity-sa\u0026#39;, request.principal.cluster_id = \u0026#39;ocid1.cluster.oc1.ap-chuncheon-1.aaaaaaaa_____yg3a\u0026#39;} 애플리케이션 컨테이너에서 OCI Java SDK를 사용하는 예시 Java 애플리케이션을 작성합니다. 예시는 Spring Boot를 사용하여, 컨테이너 이미지를 작성후 OKE에 배포하였습니다.\npom.xml 에 필요한 라이브러리를 Dependency를 추가합니다.\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.oci.sdk\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;oci-java-sdk-addons-oke-workload-identity\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.54.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.oci.sdk\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;oci-java-sdk-common\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.54.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.oracle.oci.sdk\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;oci-java-sdk-objectstorage\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.54.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Java 코드에서 OCI 인증 부분을 변경 OkeWorkloadIdentityAuthenticationDetailsProvider를 사용합니다.\n/* Config the Container Engine for Kubernetes workload identity provider */ OkeWorkloadIdentityAuthenticationDetailsProvider provider = new OkeWorkloadIdentityAuthenticationDetailsProvider .OkeWorkloadIdentityAuthenticationDetailsProviderBuilder() .build(); 가져온 provider를 통해 OCI 자원에 대한 필요한 작업을 수행합니다.\n/* Configure the client to use workload identity provider*/ ObjectStorage client = ObjectStorageClient.builder().region(Region.AP_CHUNCHEON_1).build(provider); GetNamespaceResponse namespaceResponse = client.getNamespace(GetNamespaceRequest.builder().build()); String namespaceName = namespaceResponse.getValue(); Dockerfile을 작성하여 컨테이너 이미지를 작성하여 Registry에 등록합니다.\nOKE 클러스터에 배포하기 위한 manifest yaml 파일을 다음과 같이 작성합니다.\npod 정의 spec에 serviceAccountName에 앞서 만든 service account 입력 pod 정의 spec에 automountServiceAccountToken: true 입력 --- apiVersion: apps/v1 kind: Deployment metadata: name: workload-identity-deployment namespace: workload-identity-ns spec: replicas: 1 selector: matchLabels: app: workload-identity template: metadata: labels: app: workload-identity spec: serviceAccountName: workload-identity-sa automountServiceAccountToken: true containers: - name: workload-identity image: thekoguryo/oke-workload-identity:0.0.1 imagePullPolicy: Always ports: - containerPort: 8080 env: - name: OCI_AUTH value: \u0026#34;OkeWorkloadIdentity\u0026#34; - name: OCI_REGION value: \u0026#34;ap-chuncheon-1\u0026#34; 작성한 manifest yaml 파일을 OKE 클러스터에 배포합니다.\nkubectl apply -f oke-deployment.yaml 배포된 자원내역을 확인합니다.\n$ kubectl get sa,deploy,pod,svc -n workload-identity-ns NAME SECRETS AGE serviceaccount/default 0 22h serviceaccount/workload-identity-sa 0 22h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/workload-identity-deployment 1/1 1 1 22h NAME READY STATUS RESTARTS AGE pod/workload-identity-deployment-bf798d654-gsp77 1/1 Running 0 80m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/workload-identity-service LoadBalancer 10.96.74.56 146.56.xxx.xx 80:31742/TCP 22h 배포된 애플리케이션을 테스트합니다.\nVisibility: Public은 Workload Identity와 상관없이 조회되므로, Visibility: Private인 Bucket을 대상으로 조회하여 Workload Identity가 잘 동작하여, 결과가 조회되는지 확인합니다. $ curl http://146.56.xxx.xx/objects?bucket_name=ExampleBucket { \u0026#34;data\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;images/icons8-anonymous-mask-100.png\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;images/icons8-mando-100.png\u0026#34; } ] } 예제 애플리케이션 코드 소스 파일 https://github.com/TheKoguryo/oke/tree/main/oke-workload-identity Spring Boot 2.7.11 Maven Build 사용 JDK 11 사용 OKE 클러스터 배포용 manifest yaml https://github.com/TheKoguryo/oke/blob/main/oke-workload-identity/oke-deployment.yaml env.name=OCI_REGION 대상 OCI Bucktet이 있는 OCI Region으로 각자 환경에 맞게 변경 필요 참고 문서 OCI Documentation \u0026gt; Container Engine \u0026gt; Granting Workloads Access to OCI Resources OCI Blog \u0026gt; OKE Workload Identity: Greater control of access ","lastmod":"2023-05-19T00:00:01Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/enhanced-cluster/2.workload-identity/","tags":["oke","enhanced cluster","workload identity"],"title":"1.9.2 Workload Identity 사용하기"},{"categories":null,"contents":"6.2 CLI로 Container Instance 만들기 OCI CLI, SDK, REST API 등을 통해 OCI 자원을 관리할 수 있습니다. OCI CLI로 Container Instance를 생성하는 방법을 확인해 봅니다. 편의상 CLI 서브 파라미터 보다는 JSON 메시지 하나를 만들어 사용하였습니다. 해당 JSON 메시지를 REST API를 통해 요청시도 사용 가능합니다.\nOCI CLI로 Container Instance 만들기 필요하면 사용할 요청 JSON 샘플 메시지를 생성하여 참조합니다.\noci container-instances container-instance create --generate-full-command-json-input \u0026gt; sample.json 생성에 필요한 값을 미리 확인합니다.\ncompartmentId: Container Instance를 생성할 대상 Compartment의 OCID\navailabilityDomain: 아래 명령 또는 OCI 콘솔에서 Availability Domain의 name 확인\n$ oci iam availability-domain list { \u0026#34;data\u0026#34;: [ { \u0026#34;compartment-id\u0026#34;: \u0026#34;ocid1.tenancy.oc1..aaaaaaaa______\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;ocid1.availabilitydomain.oc1..aaaaaaaa______\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;hoLX:AP-CHUNCHEON-1-AD-1\u0026#34; } ] } vnics.subnetId: Container Instance가 사용할 Subnet의 OCID\n생성을 위한 JSON 요청 파일을 만듭니다.\n이전 장에서 OCI 콘솔에서 생성한 내용을 동일한 설정으로 이번에는 CLI를 통해서 생성합니다.\n이전 인스턴스가 있는 경우, 호스트명(vnics.hostnameLabel)과 Private IP(vnics.privateIp)만 겹치지 않게 변경합니다.\n2번에서 확인한 각 항목을 본인 환경값으로 변경\n{ \u0026#34;displayName\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;compartmentId\u0026#34;: \u0026#34;ocid1.compartment.oc1..aaaaaaaa_____\u0026#34;, \u0026#34;availabilityDomain\u0026#34;: \u0026#34;hoLX:AP-CHUNCHEON-1-AD-1\u0026#34;, \u0026#34;shape\u0026#34;: \u0026#34;CI.Standard.E4.Flex\u0026#34;, \u0026#34;shapeConfig\u0026#34;: { \u0026#34;ocpus\u0026#34;: 2, \u0026#34;memoryInGBs\u0026#34;: 8 }, \u0026#34;vnics\u0026#34;: [ { \u0026#34;subnetId\u0026#34;: \u0026#34;ocid1.subnet.oc1.ap-chuncheon-1.aaaaaaaa_____\u0026#34;, \u0026#34;isPublicIpAssigned\u0026#34;: true, \u0026#34;privateIp\u0026#34;: \u0026#34;10.0.0.102\u0026#34;, \u0026#34;hostnameLabel\u0026#34;: \u0026#34;wordpress-2\u0026#34; } ], \u0026#34;containers\u0026#34;: [ { \u0026#34;displayName\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;mysql:8.0.31\u0026#34;, \u0026#34;environmentVariables\u0026#34;: { \u0026#34;MYSQL_DATABASE\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;MYSQL_ROOT_PASSWORD\u0026#34;: \u0026#34;wordpressonmysql\u0026#34;, \u0026#34;MYSQL_PASSWORD\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;MYSQL_USER\u0026#34;: \u0026#34;wordpress\u0026#34; }, \u0026#34;resourceConfig\u0026#34;: { \u0026#34;vcpusLimit\u0026#34;: 1, \u0026#34;memoryLimitInGBs\u0026#34;: 4 }, \u0026#34;arguments\u0026#34;: [ \u0026#34;--default-authentication-plugin=mysql_native_password\u0026#34; ] }, { \u0026#34;displayName\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;wordpress:latest\u0026#34;, \u0026#34;environmentVariables\u0026#34;: { \u0026#34;WORDPRESS_DB_HOST\u0026#34;: \u0026#34;127.0.0.1\u0026#34;, \u0026#34;WORDPRESS_DB_NAME\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;WORDPRESS_DB_USER\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;WORDPRESS_DB_PASSWORD\u0026#34;: \u0026#34;wordpress\u0026#34; }, \u0026#34;resourceConfig\u0026#34;: { \u0026#34;vcpusLimit\u0026#34;: 1, \u0026#34;memoryLimitInGBs\u0026#34;: 4 } } ] } CLI로 Container Instance를 생성합니다.\noci container-instances container-instance create --from-json=file://wordpress.json 생성요청 결과\n$ oci container-instances container-instance create --from-json=file://wordpress.json { \u0026#34;data\u0026#34;: { ... }, \u0026#34;etag\u0026#34;: \u0026#34;78a83e80b952c63142ce72ebccdeccbf6706f298bff711ac164530d97a1f8ea4\u0026#34;, \u0026#34;opc-work-request-id\u0026#34;: \u0026#34;ocid1.computecontainerworkrequest.oc1.ap-chuncheon-1.ab4w4ljrnukka4ce4kirehpiwgz7m6e3446exb3nbg6vty2qvgdh6suivpia\u0026#34; } 콘솔에서 생성여부를 확인합니다.\n","lastmod":"2023-05-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/container-instances/2.create-container-instances-by-cli/","tags":["container-instances","cli"],"title":"6.2 CLI로 Container Instance 만들기"},{"categories":null,"contents":"11.2 GoDaddy의 서브 도메인 관리를 OCI DNS 서비스에 delegation 하기 이미 사용하는 Domain Name과 DNS 서버가 있는 경우에, 기존 관리하는 방식을 사용하면서, OCI를 통해서 서비스하는 자원들에 대해 별도로 관리하고자 하는 경우가 있습니다. 서브 도메인을 분리하여, 분리된 서브 도메인에 대한 DNS 관리는 OCI DNS 서비스에 위임하는 경우에 대해서 알아보고자 합니다.\n테스트를 위해 GoDaddy 사이트에서 구매한 도메인 네임(thekoguryo.xyz)을 사용하였습니다. 이전에 사용한 freenom은 2023년 4월 기준 정상 서비스가 되지 않아, GoDaddy를 이용해 테스트 하였습니다.\n기본 DNS 관리 환경에서 사용중인 도메인이 있습니다. 예, thekoguryo.xyz 기본 DNS 관리 환경에서 sub1.thekoguryo.xyz 서브 도메인을 관리하고 있습니다. 새로 사용하는 OCI 상의 자원들을 위해 sub2 서브 도메인을 생성하고, sub2.thekoguryo.xyz는 OCI DNS 서비스에 관리를 위임하고자 합니다. 즉, web2.sub2.thekoguryo.xyz 등 새로운 호스트의 DNS는 OCI DNS 서비스에만 등록하면 되도록 하고자 합니다. 기존 DNS 관리 환경 - GoDaddy 예시 GoDaddy에 접속합니다.\n사용하는 도메인에 대해서 Manage DNS 화면으로 이동합니다.\n아래 예시와 같이 서브 도메인 sub1에 대해서 레코드를 등록해 관리하고 있는 상태입니다.\n등록된 주소를 통해 접속해 봅니다. 등록된 DNS 정보가 잘 동작하고 있습니다.\nOCI DNS 서비스 설정 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; DNS Management \u0026gt; Zones 항목으로 이동합니다.\nPublic zones에서 Create Zone 클릭\n생성정보 입력\nMethod: Manual\nZone type: Primary\nZone name: 관리할 서브 도메인을 입력합니다. 예, sub2.thekoguryo.xyz\nCreate 클릭\n생성된 zone을 클릭합니다.\nZone 정보에서 네임 서버 주소들을 확인할 수 있습니다.\n기존 DNS 관리 환경에서 서브도메인 관리 위임하기 - GoDaddy 예시 GoDaddy 화면으로 돌아갑니다.\n사용하는 도메인에 대해서 Manage DNS 화면으로 이동합니다.\nNS 타입으로 Name에 서브 도메인 명을 입력합니다. Value에는 앞서 OCI DNS의 Zone 정보에 있는 네임서버 주소를 입력합니다. Add More Records를 클릭하여, 네임서버 주소 총 4개를 추가하고 저장합니다.\n이제 서브도메인 sub2.thekoguryo.xyz에 대해서는 OCI DNS 서버로 위임이 되었습니다.\nOCI DNS 서비스에서 DNS 레코드 추가하기 OCI 콘솔에서 생성한 DNS Zone 화면으로 이동합니다.\n왼쪽 아래 Resources \u0026gt; Records로 이동하면 생성된 항목을 볼 수 있습니다. Zone 내부에 NS 유형과 SOA 유형의 레코드가 생성되어 있습니다. NS는 네임 서버 레코드, SOA는 권한 시작 레코드입니다.\n새 레코드를 추가하기 위해 Add Record를 클릭합니다.\nDNS Records로 컴퓨트 인스턴스에 있는 Public IP를 레코드를 추가해 봅니다.\nRecord Type: A - IPv4 Address\nName: web\nTTL: 30, 우측 자물쇠는 클릭하여 잠금 해제 후 TTL 값 입력\nRdata mode: Basic\nAddress: 매핑할 인스턴스의 Public IP\nSubmit 클릭\n변경분을 반영하기 위해 Publish change 클릭\n확인 창이 뜨면 한번 더 Publish change 클릭\n레코드 추가 및 반영 완료\n등록된 주소를 통해 접속해 봅니다. 등록된 DNS 정보가 잘 동작하고 있습니다.\nDNS 질의 테스트 GoDaddy에서 직접 관리하는 주소(web1.sub1.thekoguryo.xyz)를 dig +trace 명령으로 DNS 질의과정을 확인해 봅니다. GoDaddy에 Name Server로 등록되어 있던, ns52.domaincontrol.com에서 최종 IP를 가져오는 것을 볼 수 있습니다.\n$ dig +trace web1.sub1.thekoguryo.xyz ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; +trace web1.sub1.thekoguryo.xyz ;; global options: +cmd .\t492520\tIN\tNS\tl.root-servers.net. .\t492520\tIN\tNS\tk.root-servers.net. .\t492520\tIN\tNS\tj.root-servers.net. .\t492520\tIN\tNS\tg.root-servers.net. .\t492520\tIN\tNS\te.root-servers.net. .\t492520\tIN\tNS\td.root-servers.net. .\t492520\tIN\tNS\th.root-servers.net. .\t492520\tIN\tNS\tb.root-servers.net. .\t492520\tIN\tNS\ta.root-servers.net. .\t492520\tIN\tNS\tf.root-servers.net. .\t492520\tIN\tNS\tc.root-servers.net. .\t492520\tIN\tNS\tm.root-servers.net. .\t492520\tIN\tNS\ti.root-servers.net. .\t492520\tIN\tRRSIG\tNS 8 0 518400 20230506170000 20230423160000 60955 . ...... ;; Received 1109 bytes from 168.126.63.1#53(168.126.63.1) in 8 ms xyz.\t172800\tIN\tNS\tz.nic.xyz. xyz.\t172800\tIN\tNS\tx.nic.xyz. xyz.\t172800\tIN\tNS\ty.nic.xyz. xyz.\t172800\tIN\tNS\tgenerationxyz.nic.xyz. xyz.\t86400\tIN\tDS\t3599 8 2 B9733869BC84C86BB59D102BA5DA6B27B2088552332A39DCD54BC4E8 D66B0499 xyz.\t86400\tIN\tDS\t3599 8 1 3FA3B264F45DB5F38BEDEAF1A88B76AA318C2C7F xyz.\t86400\tIN\tRRSIG\tDS 8 1 86400 20230507050000 20230424040000 60955 . ...... ;; Received 686 bytes from 202.12.27.33#53(m.root-servers.net) in 7 ms thekoguryo.xyz.\t3600\tIN\tNS\tns52.domaincontrol.com. thekoguryo.xyz.\t3600\tIN\tNS\tns51.domaincontrol.com. 2tjms8vm0h8n7j9e748d19gpnujm0emj.xyz. 3600 IN NSEC3 1 1 0 - 2TJVVSOC6CRR8B060M9FH0MPV66FBD17 NS SOA RRSIG DNSKEY NSEC3PARAM 2tjms8vm0h8n7j9e748d19gpnujm0emj.xyz. 3600 IN RRSIG NSEC3 8 2 3600 20230511041210 20230411070205 53358 xyz. ...... 2372ggrgccgilepboq3aol06ev9fc9e0.xyz. 3600 IN NSEC3 1 1 0 - 23A0806OHH7S36DBS305MO6OUMIVAI41 NS DS RRSIG 2372ggrgccgilepboq3aol06ev9fc9e0.xyz. 3600 IN RRSIG NSEC3 8 2 3600 20230429224834 20230331074050 53358 xyz. ...... ;; Received 593 bytes from 212.18.249.42#53(generationxyz.nic.xyz) in 44 ms web1.sub1.thekoguryo.xyz. 600\tIN\tA\t152.69.xxx.xxx thekoguryo.xyz.\t3600\tIN\tNS\tns52.domaincontrol.com. thekoguryo.xyz.\t3600\tIN\tNS\tns51.domaincontrol.com. ;; Received 124 bytes from 173.201.73.26#53(ns52.domaincontrol.com) in 143 ms GoDaddy에서 OCI DNS 서비스로 위임한 주소(web2.sub2.thekoguryo.xyz)를 dig +trace 명령으로 DNS 질의과정을 확인해 봅니다. GoDaddy에 Name Server로 등록되어 있던, ns51.domaincontrol.com을 거쳐 OCI Name Server인 ns1.p68.dns.oraclecloud.net에서 최종 IP를 가져오는 것을 볼 수 있습니다.\n$ dig +trace web2.sub2.thekoguryo.xyz ; \u0026lt;\u0026lt;\u0026gt;\u0026gt; DiG 9.10.6 \u0026lt;\u0026lt;\u0026gt;\u0026gt; +trace web2.sub2.thekoguryo.xyz ;; global options: +cmd .\t488765\tIN\tNS\tm.root-servers.net. .\t488765\tIN\tNS\tk.root-servers.net. .\t488765\tIN\tNS\te.root-servers.net. .\t488765\tIN\tNS\tc.root-servers.net. .\t488765\tIN\tNS\th.root-servers.net. .\t488765\tIN\tNS\tb.root-servers.net. .\t488765\tIN\tNS\tf.root-servers.net. .\t488765\tIN\tNS\tg.root-servers.net. .\t488765\tIN\tNS\tj.root-servers.net. .\t488765\tIN\tNS\td.root-servers.net. .\t488765\tIN\tNS\ti.root-servers.net. .\t488765\tIN\tNS\ta.root-servers.net. .\t488765\tIN\tNS\tl.root-servers.net. .\t488765\tIN\tRRSIG\tNS 8 0 518400 20230506170000 20230423160000 60955 . ...... ;; Received 1109 bytes from 168.126.63.1#53(168.126.63.1) in 11 ms xyz.\t172800\tIN\tNS\tgenerationxyz.nic.xyz. xyz.\t172800\tIN\tNS\tz.nic.xyz. xyz.\t172800\tIN\tNS\tx.nic.xyz. xyz.\t172800\tIN\tNS\ty.nic.xyz. xyz.\t86400\tIN\tDS\t3599 8 1 3FA3B264F45DB5F38BEDEAF1A88B76AA318C2C7F xyz.\t86400\tIN\tDS\t3599 8 2 B9733869BC84C86BB59D102BA5DA6B27B2088552332A39DCD54BC4E8 D66B0499 xyz.\t86400\tIN\tRRSIG\tDS 8 1 86400 20230507050000 20230424040000 60955 . ...... ;; Received 686 bytes from 192.36.148.17#53(i.root-servers.net) in 52 ms thekoguryo.xyz.\t3600\tIN\tNS\tns52.domaincontrol.com. thekoguryo.xyz.\t3600\tIN\tNS\tns51.domaincontrol.com. 2tjms8vm0h8n7j9e748d19gpnujm0emj.xyz. 3600 IN NSEC3 1 1 0 - 2TJVVSOC6CRR8B060M9FH0MPV66FBD17 NS SOA RRSIG DNSKEY NSEC3PARAM 2tjms8vm0h8n7j9e748d19gpnujm0emj.xyz. 3600 IN RRSIG NSEC3 8 2 3600 20230511041210 20230411070205 53358 xyz. ...... 2372ggrgccgilepboq3aol06ev9fc9e0.xyz. 3600 IN NSEC3 1 1 0 - 23A0806OHH7S36DBS305MO6OUMIVAI41 NS DS RRSIG 2372ggrgccgilepboq3aol06ev9fc9e0.xyz. 3600 IN RRSIG NSEC3 8 2 3600 20230429224834 20230331074050 53358 xyz. ...... ;; Received 593 bytes from 185.24.64.42#53(y.nic.xyz) in 44 ms sub2.thekoguryo.xyz.\t3600\tIN\tNS\tns4.p68.dns.oraclecloud.net. sub2.thekoguryo.xyz.\t3600\tIN\tNS\tns3.p68.dns.oraclecloud.net. sub2.thekoguryo.xyz.\t3600\tIN\tNS\tns2.p68.dns.oraclecloud.net. sub2.thekoguryo.xyz.\t3600\tIN\tNS\tns1.p68.dns.oraclecloud.net. ;; Received 148 bytes from 97.74.105.26#53(ns51.domaincontrol.com) in 138 ms web2.sub2.thekoguryo.xyz. 30\tIN\tA\t146.56.xxx.xxx sub2.thekoguryo.xyz.\t86400\tIN\tNS\tns1.p68.dns.oraclecloud.net. sub2.thekoguryo.xyz.\t86400\tIN\tNS\tns3.p68.dns.oraclecloud.net. sub2.thekoguryo.xyz.\t86400\tIN\tNS\tns2.p68.dns.oraclecloud.net. sub2.thekoguryo.xyz.\t86400\tIN\tNS\tns4.p68.dns.oraclecloud.net. ;; Received 164 bytes from 108.59.161.68#53(ns1.p68.dns.oraclecloud.net) in 9 ms 서브도메인에 대한 DNS 관리를 OCI DNS 서비스에 위임하는 것이 잘 동작하는 것을 확인하였습니다.\n","lastmod":"2023-04-24T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter11/2/","tags":["DNS","subdomain","GoDaddy"],"title":"11.2 GoDadday의 서브 도메인을 OCI DNS 서비스에 delegation 하기"},{"categories":null,"contents":"16.2 OCI Certificate에 외부 인증서 임포트하기 외부 발급업체에서 발급한 인증서를 임포트해서 사용하는 경우에 대한 테스트입니다.\n외부 SSL 인증서 발급하기 SSL 인증서를 발급받습니다. 여기서는 Self-Signed 인증서가 아닌 무료 인증서를 사용하겠습니다.\n무료 SSL 인증서 발급받기 압축파일을 풀면 다음 세가지 파일이 포함되어 있습니다.\nOCI Certificate 만들기 OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security \u0026gt; Certificates \u0026gt; Certificates을 클릭합니다.\nCreate Certificate를 클릭합니다\nResources \u0026gt; Certificates에서 Issue Certificate을 클릭합니다.\nCertificate 생성 정보를 입력합니다.\n기본 정보\nCA 타입: Imported\nName:\nSubject Information\n해당 사항 없음 Certificate Configuration\n순서대로 해당 되는 내용을 복사해서 입력합니다. 예, Certificate: certificate.crt, Certificate Chain: ca_bundle.crt, Private Key: private.crt Rules\n해당 사항 없음 Summary\n설정 내용 확인후 Certificate를 만듭니다. 생성 완료\n생성 오류 발생시 생성 요청 직후, 아래와 같이 The certificate chain in the configuration details of the certificate is invalid 이런 오류가 발생하는 경우가 있습니다. 트러블 슈팅에 나온 것처럼 몇 가지로 인해 발생할 수 있지만, 정상적인 인증서에서도 Certificate Chain에 상위 인증서들이 없는 경우 흔히 발생하는 오류입니다.\n​ 참고 OCI Certificates \u0026gt; Troublieshooting\u0026gt; Validation of Certificate Chain Fails Certificate Chain을 생성하는 방법 중 하나로 다음 사이트에 접속합니다.\nhttps://whatsmychaincert.com/\nCertificate(예, certificate.crt) 파일 내용을 복사해 붙입니다. Include Root Certificate을 체크하고, 생성 버튼을 클릭합니다.\n생성된 파일이 자동으로 다운로드 됩니다.\nCertificate을 다시 생성합니다. 임포트될 인증서 설정에서 Certificate Chain을 이전 사이트에서 생성한 Certificate Chain 값을 붙입니다.\n인증서를 생성합니다. Certificate Chain 문제였다면, 이제는 정상 생성 될 것입니다.\nLoad Balancer 만들기 10.5 Load Balancer 만들기 을 참고하여 Load Balancer를 구성하기 위한 준비를 합니다.\nConfigure Listener\nListener Name:\n이름 입력, 예) ExampleLB-Listener Specify the type of traffic your listener handles: HTTPS 선택\nOCI Certificate의 SSL 인증서 연동을 테스트하기 위해 HTTPS를 선택합니다. SSL certificate\nOCI Certificate 서비스 선택 앞서 임포트하여 만든 인증서 선택 Manage Logging\n에러 로그와 액세스 로그를 OCI Log 서비스를 사용하도록 설정할 수 있습니다. 일단 여기서는 사용하지 않습니다. Submit을 클릭하여 Load Balancer를 생성합니다.\nLoad Balancer의 Public IP를 구입한 DNS 관리화면을 통해 도메인에 맞게 설정합니다.\nLoad Balancer 연결확인 생성한 Load Balancer의 Public IP를 확인합니다.\n웹브라우저에서 https로 접속합니다.\n인증서를 확인합니다.\n이름을 포함한 인증서 정보를 볼수 있습니다.\nOCI Certificate 서비스에서 인증서 갱신하기 임포트한 인증서에서 Renew Certificate을 클릭합니다.\n외부 인증서를 임포트 한 것이므로 실제 원 인증서 갱신은 해당 사이트에서 됩니다.\n임포트 유형의 OCI Certificate은 원 사이트에서 갱신된 인증서로 업데이트하는 방식입니다.\n임포트를 통해 새 버전으로 갱신되었습니다.\nLoad Balancer 재접속하여 인증서 갱신 확인 앞서 Load Balancer에 https로 접속한 웹브라우저로 돌아갑니다.\n현재 화면을 재접속합니다.\n만료날짜를 통해 갱신된 인증서가 적용된 것을 알 수 있습니다.\n","lastmod":"2023-03-31T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter16/2/","tags":["certificate","load balancer"],"title":"16.2 OCI Certificate에 외부 인증서 임포트하기"},{"categories":null,"contents":"1.4.2 Image Registry로 Nexus Repository 사용하기 컴퓨트 인스턴스에 Nexus 설치 Step 1 - 컴퓨트 인스턴스 생성\n시스템 요구사항에 맞춰 컴퓨트 인스턴스를 생성합니다. 여기서는 최소사항에 맞춰 설치하겠습니다.\n요구사항: https://help.sonatype.com/repomanager3/product-information/system-requirements#SystemRequirements-CPU OS: Oracle Linux 8 CPU: 4 OCPU Memory: 8 GB Step 2 – JDK 설치\n참조 문서 - https://blogs.oracle.com/developers/post/how-to-install-oracle-java-in-oracle-cloud-infrastructure\nNexus Repository Manager의 설치 요구사항에 맞춰 Java 1.8 버전을 설치합니다.\n요구사항 - https://help.sonatype.com/repomanager3/product-information/system-requirements#SystemRequirements-Java Oracle Linux 8 인스턴스상의 Repository에서 설치 가능한 JDK 버전을 확인합니다.\n$ yum list jdk* Available Packages jdk-11.x86_64 2000:11.0.17-ga ol8_oci_included ... jdk-17.x86_64 2000:17.0.5-ga ol8_oci_included ... jdk1.8.x86_64 2000:1.8.0_361-fcs ol8_oci_included JDK 1.8을 설치합니다.\nsudo yum install -y jdk1.8 설치 버전 확인\n$ java -version java version \u0026#34;1.8.0_361\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_361-b09) Java HotSpot(TM) 64-Bit Server VM (build 25.361-b09, mixed mode) Step 3 – Oracle Linux 8에 Sonatype Nexus Repository 설치하기\nhttps://www.atlantic.net/dedicated-server-hosting/how-to-install-sonatype-nexus-repository-manager-on-oracle-linux-8/\nNexus를 실행할 유저를 생성합니다.\nsudo adduser nexus 최신 버전을 다운로드 받습니다. SELinux로 인한 실행시 오류를 막기 위해 /opt 위치에 설치합니다.\ncd /opt sudo wget -O nexus.tar.gz https://download.sonatype.com/nexus/3/latest-unix.tar.gz 다운로드 받은 파일을 압축해제합니다.\nsudo tar -xvf nexus.tar.gz 폴더명을 변경합니다.\nsudo mv nexus-3* nexus3 압축 해제된 Nexus 폴더의 소유자와 권한을 변경합니다.\nsudo chown -R nexus:nexus /opt/nexus3 sudo chown -R nexus:nexus /opt/sonatype-work 설정파일에서 실행 유저를 변경합니다.\nsudo vi /opt/nexus3/bin/nexus.rc 다음 내용 변경\nrun_as_user=\u0026#34;nexus\u0026#34; Step 4 – Systemd Service File 생성\n서비스로써 인스턴스 재시작시 실행될 수 있도록 관련 파일을 설정합니다.\n다음 파일을 생성합니다.\nsudo vi /etc/systemd/system/nexus.service 다음 내용 추가\n[Unit] Description=nexus service After=network.target [Service] Type=forking LimitNOFILE=65536 ExecStart=/opt/nexus3/bin/nexus start ExecStop=/opt/nexus3/bin/nexus stop User=nexus Restart=on-abort TimeoutSec=600 [Install] WantedBy=multi-user.target 서비스 활성화\nsudo systemctl daemon-reload sudo systemctl enable nexus.service sudo systemctl start nexus.service 서비스 상태 확인\nsystemctl status nexus 정상 시작시 상태 - active (running) 상태\n● nexus.service - nexus service Loaded: loaded (/etc/systemd/system/nexus.service; enabled; vendor preset: disabled) Active: active (running) since Mon 2023-03-13 09:30:22 GMT; 5s ago Process: 12799 ExecStart=/opt/nexus3/bin/nexus start (code=exited, status=0/SUCCESS) Main PID: 13014 (java) Tasks: 55 (limit: 48440) Memory: 903.3M CGroup: /system.slice/nexus.service └─13014 /usr/java/jdk1.8.0_361-amd64/bin/java -server -Dinstall4j.jvmDir=/usr/java/jdk1.8.0_361-amd64 -Dexe4j.moduleName=/opt/nexus3/bin/nexus -XX:+Unl\u0026gt; Mar 13 09:30:22 nexus systemd[1]: Starting nexus service... Mar 13 09:30:22 nexus systemd[1]: Started nexus service. 로그 확인\ntail -f /opt/sonatype-work/nexus3/log/nexus.log 정상 기동시 - 8081 포트로 서비스 됩니다.\ntail -f /opt/sonatype-work/nexus3/log/nexus.log 2023-03-13 09:30:53,941+0000 INFO [jetty-main-1] *SYSTEM org.eclipse.jetty.server.handler.ContextHandler - Started o.e.j.w.WebAppContext@3e0f2b26{Sonatype Nexus,/,file:///opt/nexus3/public/,AVAILABLE} 2023-03-13 09:30:53,975+0000 INFO [jetty-main-1] *SYSTEM org.eclipse.jetty.server.AbstractConnector - Started ServerConnector@3d2ab5e1{HTTP/1.1, (http/1.1)}{0.0.0.0:8081} 2023-03-13 09:30:53,975+0000 INFO [jetty-main-1] *SYSTEM org.eclipse.jetty.server.Server - Started @31643ms 2023-03-13 09:30:53,975+0000 INFO [jetty-main-1] *SYSTEM org.sonatype.nexus.bootstrap.jetty.JettyServer - ------------------------------------------------- Started Sonatype Nexus OSS 3.49.0-02 ------------------------------------------------- 2023-03-13 09:31:52,105+0000 INFO [periodic-4-thread-1] *SYSTEM org.sonatype.nexus.repository.httpclient.internal.HttpClientFacetImpl - Repository status for nuget.org-proxy changed from READY to AVAILABLE - reason n/a for n/a Step 5 - 방화벽에서 서비스 포트 열기\n컴퓨트 인스턴스에서 OS 상의 방화벽에서 대상 포트를 엽니다.\nNexus Repository Manager UI: 8081\nImage Registry 호스트용: 5000\nDocker Hub 프락시용: 5001\n# OS 방화벽에서 Nexus Repository Manager UI, 8081 포트 개방 sudo firewall-cmd --permanent --add-port=8081/tcp # OS 방화벽에서 호스트용, 프락시용 포트, 5000,5001 포트 개방 sudo firewall-cmd --permanent --add-port=5000/tcp sudo firewall-cmd --permanent --add-port=5001/tcp # 방화벽 변경정보 다시 반영 sudo firewall-cmd --reload 컴퓨트 인스턴스가 속한 Security List에 관련 포트를 Ingress Rule에 추가합니다.\n포트는 동일: 8081, 5000, 5001 Step 6 – Nexus Repository Manager 접속 및 Repository 설정\nNexus Repository Manager 접속합니다.\n주소: http://\u0026lt;인스턴스 IP\u0026gt;:8081 컴퓨트 인스턴스에 관리자 패스워드를 확인하여 Sign in 합니다.\nusername: admin\n패스워드: 최초 로그인 후 아래 파일은 자동으로 삭제됩니다.\ncat /opt/sonatype-work/nexus3/admin.password 패스워드를 변경합니다.\n익명으로 접근에 대한 권한을 설정합니다.\n설정을 완료합니다.\n이미지가 저장될 Blob Store를 만듭니다.\ndocker-hosted: File 타입으로 만듭니다.\ndocker-hub도 같은 방법으로 만듭니다.\n필요에 따라 컴퓨트 인스턴스에 Block Volume을 마운트해서 해당 경로로 지정하는 방법도 있습니다.\ndocker-hosted Repository를 만듭니다.\nRepositories \u0026gt; Create Repository 클릭합니다.\nRecipe에서 docker (hosted)를 선택\n아래 정보를 입력하여 생성합니다.\nName: docker-hosted HTTP: 포트 5000 번 Allow anonymous docker pull: 체크 - 로그인 없이 docker pull 할 수 있는지, 즉 Public Repository 인지 설정하는 부분 Blob store: 앞서 만든 docker-hosted 선택 docker-hub Repository를 만듭니다.\nRepositories \u0026gt; Create Repository 클릭합니다.\nRecipe에서 docker (proxy)를 선택\n다른 Registry(예, Docker Hub)의 이미지를 캐쉬로 저장하는 유형입니다. 아래 정보를 입력하여 생성합니다.\nName: docker-hub HTTP: 포트 5001 번 Allow anonymous docker pull: 체크 - 로그인 없이 docker pull 할 수 있는지, 즉 Public Repository 인지 설정하는 부분 Remote storage: Docker Hub의 주소인 https://registry-1.docker.io 입력 Docker Index: Use Docker Hub 선택 Blob store: 앞서 만든 docker-hosted 선택 나머지는 기본값 사용 Allow anonymous docker pull을 사용하기 위해 Security \u0026gt; Realms에서 Docker Bearer Token Realm을 추가합니다.\nDocker Bearer Token Realm을 오른쪽 Active 화면으로 이동시킵니다.\n적용을 위해 Save합니다.\nAllow anonymous docker pull을 사용하기 위해 Acnonymous Access를 활성화합니다.\n설치구성한 Nexus Repository을 Image Registry로 사용하기 테스트 Docker 클라이언트로 테스트하기\n생성한 Repository를 HTTP로만 서비스를 하도록 설정하였기 때문에, Docker 클라이언트에서 insecure-registries 설정을 해야 연결이 가능합니다. 내부 네트워크에서 사용하는 경우, HTTP로만 하는 경우가 있어, 해당 내용을 기준으로 테스트합니다.\nHTTPS로 서비스시는 SSL Certificate 발급 및 등록 작업이 필요합니다. 여기서는 편의상 HTTP로만 사용합니다.\nDocker 클라이언트를 설치합니다.\n/etc/docker/daemon.json 파일이 없는 경우 생성하여 아래 내용을 추가합니다.\n{ \u0026#34;insecure-registries\u0026#34; : [\u0026#34;10.0.30.250:5000\u0026#34;, \u0026#34;10.0.30.250:5001\u0026#34;] } 서비스 재시작\nsudo service docker restart docker-hub Repository에서 docker pull 합니다. 현재 docker-hub는 없어도, Docker Hub를 통해 가져오는 것을 볼 수 있습니다.\n$ docker pull 10.0.30.250:5001/busybox:latest latest: Pulling from busybox 1487bff95222: Pull complete Digest: sha256:c118f538365369207c12e5794c3cbfb7b042d950af590ae6c287ede74f29b7d4 Status: Downloaded newer image for 10.0.30.250:5001/busybox:latest 10.0.30.250:5001/busybox:latest Nexus Repository Manager 접속하여 Browse에서 docker-hub를 클릭합니다. Docker Hub에서 가져와서 캐쉬된 것을 볼 수 있습니다.\ndocker-hosted Repository에 로그인합니다.\n$ docker login 10.0.30.250:5000 Username: admin Password: WARNING! Your password will be stored unencrypted in /home/opc/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded docker pull 해서 docker-hosted Repository에 push 하기\ndocker pull\n$ docker pull nginx:latest latest: Pulling from library/nginx Digest: sha256:aa0afebbb3cfa473099a62c4b32e9b3fb73ed23f2a75a65ce1d4b4f55a5c2ef2 Status: Downloaded newer image for nginx:latest docker.io/library/nginx:latest tag 하기\n$ docker tag nginx:latest 10.0.30.250:5000/nginx:latest docker-hosted Repository에 push\n$ docker push 10.0.30.250:5000/nginx:latest The push refers to repository [10.0.30.250:5000/nginx] 101af4ba983b: Pushed d8466e142d87: Pushed 83ba6d8ffb8c: Pushed e161c82b34d2: Pushed 4dc5cd799a08: Pushed 650abce4b096: Pushed latest: digest: sha256:942ae2dfd73088b54d7151a3c3fd5af038a51c50029bfcfd21f1e650d9579967 size: 1570 Nexus Repository Manager 접속하여 Browse에서 docker-hosted를 클릭합니다. Docker 클라이언트로 push한 이미지가 등록된 것을 확인할 수 있습니다.\nOKE 클러스터에서 Nexus에서 이미지 가져오기\nOKE는 쿠버네티스 버전 1.20.8 부터 Docker 대신 CRI-O를 컨테이너 런타임으로 사용하고 있습니다. HTTP로 컨테이너 이미지를 가져오기 위해서는 Docker Client에서 설정한 insecure-registries에 대응되는 추가 설정이 필요합니다.\nWorker Node에 SSH로 접속하여 Nexus에서 이미지를 잘 가져오는 지 확인해 봅니다.\n[opc@oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-0 ~]$ sudo crictl pull 10.0.30.250:5000/nginx:latest E0314 02:07:35.127303 35031 remote_image.go:238] \u0026#34;PullImage from image service failed\u0026#34; err=\u0026#34;rpc error: code = Unknown desc = pinging container registry 10.0.30.250:5000: Get \\\u0026#34;https://10.0.30.250:5000/v2/\\\u0026#34;: http: server gave HTTP response to HTTPS client\u0026#34; image=\u0026#34;10.0.30.250:5000/nginx:latest\u0026#34; FATA[0000] pulling image: rpc error: code = Unknown desc = pinging container registry 10.0.30.250:5000: Get \u0026#34;https://10.0.30.250:5000/v2/\u0026#34;: http: server gave HTTP response to HTTPS client HTTP를 사용하는 경우 오류가 발생합니다. /etc/containers/registries.conf.d 폴더 하위에 레지스트리관련 추가 파일을 생성합니다.\n예, /etc/containers/registries.conf.d/my-private-registry.conf Nexus 컴퓨트 인스턴스의 Private IP을 사용한 예시입니다. insecure = true로 설정합니다. Nexus 컴퓨트 인스턴스가 OKE 클러스터와 같은 VCN인 경우 인스턴스의 Internal FQDN(예, nexus.oketoolssubnet.okecluster1.oraclevcn.com)를 Private IP 대신 사용할 수 있습니다. [[registry]] location = \u0026#34;10.0.30.250:5000\u0026#34; insecure = true [[registry]] location = \u0026#34;nexus.oketoolssubnet.okecluster1.oraclevcn.com:5000\u0026#34; insecure = true [[registry]] prefix = \u0026#34;docker.io\u0026#34; insecure = false blocked = false location = \u0026#34;docker.io\u0026#34; [[registry.mirror]] location = \u0026#34;nexus.oketoolssubnet.okecluster1.oraclevcn.com:5001\u0026#34; insecure = true 설정을 반영합니다.\nsudo systemctl daemon-reload sudo systemctl restart crio Nexus에서 이미지를 잘 가져오는 지 다시 확인해 봅니다.\n[opc@oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-0 ~]$ sudo crictl pull 10.0.30.250:5000/nginx:latest Image is up to date for 10.0.30.250:5000/nginx@sha256:942ae2dfd73088b54d7151a3c3fd5af038a51c50029bfcfd21f1e650d9579967 [opc@oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-0 ~]$ sudo crictl images IMAGE TAG IMAGE ID SIZE 10.0.30.250:5000/nginx latest 904b8cb13b932 146MB ... OKE 클러스터에서 실행하면, 위에서 설정한 Worker Node에 Pod가 배포되는 경우 이미지를 가져와 Running 상태가 됩니다.\n작업한 oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-0(10.0.10.38) Worker Node에서 Running 상태이며 나머지 노드에 배포된 Pod는 ErrImagePull 에러가 발생하였습니다. $ kubectl get nodes -o wide -L hostname NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME HOSTNAME 10.0.10.16 Ready node 5m5s v1.24.1 10.0.10.16 \u0026lt;none\u0026gt; Oracle Linux Server 8.6 5.4.17-2136.314.6.2.el8uek.x86_64 cri-o://1.24.1-76.el8 oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-2 10.0.10.38 Ready node 60m v1.24.1 10.0.10.38 \u0026lt;none\u0026gt; Oracle Linux Server 8.6 5.4.17-2136.314.6.2.el8uek.x86_64 cri-o://1.24.1-76.el8 oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-0 10.0.10.93 Ready node 5m33s v1.24.1 10.0.10.93 \u0026lt;none\u0026gt; Oracle Linux Server 8.6 5.4.17-2136.314.6.2.el8uek.x86_64 cri-o://1.24.1-76.el8 oke-cdajpvfkfoq-nkm2nn3kbfa-sb34tztom5a-1 $ kubectl create deploy nginx --image=10.0.30.250:5000/nginx:latest --replicas=6 deployment.apps/nginx created $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-67c575b4fb-8kc6v 1/1 Running 0 10s 10.244.4.151 10.0.10.38 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-67c575b4fb-8rjxx 0/1 ErrImagePull 0 10s 10.244.5.5 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-67c575b4fb-g82k4 0/1 ErrImagePull 0 10s 10.244.5.133 10.0.10.16 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-67c575b4fb-q2nr9 0/1 ErrImagePull 0 10s 10.244.5.134 10.0.10.16 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-67c575b4fb-qt5h5 0/1 ErrImagePull 0 10s 10.244.5.4 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-67c575b4fb-zm5n6 0/1 ErrImagePull 0 10s 10.244.5.135 10.0.10.16 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 에러 없이 실행하기 위해서는 모든 Worker Node에 동일한 작업이 필요합니다. 생성되는 Worker Node에 자동으로 적용하기 위해서는 Custom Cloud-init Initialization Script을 활용하여, Worker Node 인스턴스 생성시 관련 스크립트가 자동으로 실행되게 설정하면 됩니다.\nNode Pool 생성 또는 수정시 고급 옵션에 Initialization script 항목이 있습니다. Custom Cloud Init Script Template을 다운로드 받아 OKE가 실행하는 oke-init.sh 다음에 실행할 커스텀 스크립트를 그림과 같이 추가합니다. 적용후 신규 생성되는 Worker Node에 적용이 되므로 기존 노드가 있는 경우 삭제후 재생성합니다. ","lastmod":"2023-03-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/container-registry/nexus/1.nexus-repository/","tags":["oss","nexus","image registry"],"title":"1.4.2 Image Registry로 Nexus Repository 사용하기"},{"categories":null,"contents":"1.5.2.2.2 Management Agent를 사용한 모니터링 애플리케이션에서 내보내는 메트릭 정보를 수집할 수 있는 OCI Management Agent를 GitHub 저장소를 통해 제공합니다. 쿠버네티스를 위해서 Helm Chart 형식으로 제공하며, OKE에 설치하여 OKE 클러스터에서 수집되는 메트릭 정보를 OCI Logging Analytics를 통해 모니터링할 수 있습니다.\n설치를 위해 필요한 도구 아래 도구가 필요하고 대상 OKE 클러스터에 접근이 가능해야 합니다. 아래 모든 툴이 설치되어 있는 cloud shell을 사용하면 편리합니다.\nOCI CLI kubectl helm OKE Cluster 관련 Policy 설정필요 방법 #1. Dynamic Group 방식 사용하기\nManagement Agent에 대한 Dynamic Group을 만듭니다. OKE 클러스터에 설치될 것이므로 OKE 클러스터의 Worker Node가 위치한 Compartment를 기준으로 합니다.\n# mgmtagent-dynamic-group ALL {resource.type=\u0026#39;managementagent\u0026#39;, resource.compartment.id=\u0026#39;\u0026lt;management_agent_compartment_OCID\u0026gt;\u0026#39;} Policy 설정\n# mgmtagent-kubernetes-monitoring-policy ALLOW DYNAMICGROUP \u0026lt;DG-Group-Name\u0026gt; TO USE METRICS IN COMPARTMENT \u0026lt;metrics-compartment-name\u0026gt; WHERE target.metrics.namespace = \u0026#39;mgmtagent_kubernetes_metrics\u0026#39; 방법 #2. Policy 조건 사용하기\nPolicy 설정\n# mgmtagent-kubernetes-monitoring-policy ALLOW ANY-USER TO USE METRICS IN COMPARTMENT \u0026lt;metrics-compartment-name\u0026gt; WHERE ALL {request.principal.type=\u0026#39;managementagent\u0026#39;, request.principal.compartment.id=\u0026#39;\u0026lt;management_agent_compartment_OCID\u0026gt;\u0026#39;, target.metrics.namespace = \u0026#39;mgmtagent_kubernetes_metrics\u0026#39;} OKE에 Management Agent 설치하기 Management Agent 컨테이너 이미지 만들기\nOCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Management Agents \u0026gt; Downloads and Keys로 이동합니다\nAgent for LINUX (X86_64)를 ZIP 파일 형식의 파일을 다운로드 받습니다.\nCloud Shell을 시작합니다.\nManagement Agent 컨테이너 이미지 생성용 Dockerfile을 다운로드 받습니다.\ngit clone https://github.com/oracle/docker-images.git 다운로드 받은 Agent를 Cloud Shell에 업로드합니다.\n다운로드 받은 Agent 파일을 Dockerfile 있는 위치로 복사합니다.\ncp oracle.mgmt_agent.zip docker-images/OracleManagementAgent/dockerfiles/latest/ Dockerfile 있는 폴더로 이동합니다.\ncd docker-images/OracleManagementAgent/dockerfiles/latest/ Docker CLI로 컨테이너 이미지를 빌드합니다. 아래는 Agent 버전으로 태그를 단 예시입니다.\ndocker build -t oracle/mgmtagent-container:230207.1529 . 배포한 OCIR에 Docker CLI로 로그인합니다.\n빌드한 이미지를 사용할 OCIR에 등록합니다.\n춘천 리전 OCIR에 등록하는 예시 docker tag oracle/mgmtagent-container:230207.1529 yny.ocir.io/\u0026lt;tenant-namespace\u0026gt;/oracle/mgmtagent-container:230207.1529 docker push yny.ocir.io/\u0026lt;tenant-namespace\u0026gt;/oracle/mgmtagent-container:230207.1529 Management Agent 설치 키 다운로드\nOCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Management Agents \u0026gt; Downloads and Keys로 이동합니다\n화면 아래에 Create Key 클릭\n이름과 Compartment를 지정하고 키를 생성합니다.\n다운로드 받은 키 파일은 Helm 차트 배포시 사용할 것입니다.\nManagement Agent Helm 배포\nCloud Shell로 돌아갑니다.\nManagement Agent Helm 차트를 GitHub에서 최신 버전을 다운로드하고 압축을 해제합니다.\nwget https://github.com/oracle-quickstart/oci-management-agent/releases/download/v1.0.9/mgmtagent-helm.zip unzip mgmtagent-helm.zip -d mgmtagent-helm mgmtagent-helm/values.yaml에서 배포를 위한 설정값을 확인합니다.\nmgmtagent: # Copy the downloaded Management Agent Install Key file under root helm directory as resources/input.rsp installKey: resources/input.rsp # Follow steps documented at https://github.com/oracle/docker-images/tree/main/OracleManagementAgent to build docker image. image: # Replace this value with actual docker image URL for Management Agent url: # Image secrets to use for pulling container image (base64 encoded content of ~/.docker/config.json file) secret: # By default, this app will create namespace in the following property and deploy all new resources in that namespace. You can set this to false if you want to use an already existing namespace # Update the namespace name below if required createNamespace: true # Kubernetes namespace to create and install this helm chart in namespace: mgmtagent # By default, metric server will be deployed and used by Management Agent to collect metrics. You can set this to false if you already have metric server installed on your cluster deployMetricServer: true # Kubernetes Cluster details to monitor kubernetesCluster: # OCI Compartment Id to push Kubernetes Monitoring metrics. If not specified default is same as Agent compartment compartmentId: # Kubernetes cluster name name: # Kubernetes cluster Namespace to monitor namespace: kube-system mgmtagent.installKey 설정\n앞서 다운로드 받은 Agent 키 파일 이름을 업로드 하고 이름을 input.rsp로 변경합니다.\n키 파일을 다음 위치로 복사합니다\ncp input.rsp mgmtagent-helm/resources mgmtagent.image 설정\nurl - 앞서 OCIR에 등록한 Management Agent 컨테이너 이미지의 주소를 입력합니다.\nsecret - Docker CLI로 대상 OCIR에 로그인한 경우 아래 명령으로 base64로 인코딩한 값을 입력합니다.\nbase64 ~/.docker/config.json | tr -d \u0026#39;\\n\\r\u0026#39; 결과 예시\nimage: # Replace this value with actual docker image URL for Management Agent url: yny.ocir.io/\u0026lt;tenant-namespace\u0026gt;/oracle/mgmtagent-container:230207.1529 # Image secrets to use for pulling container image (base64 encoded content of ~/.docker/config.json file) secret: ewoJImF1dGhzIjogewo......ChsaW51eCkiCgl9Cn0= kubernetesCluster 설정\ncompartmentId - OKE Worker Node와 Agent가 같은 Compartment 인 경우 그대로 두고 다를 때 Worker Node가 있는 Compartment의 OCID를 입력합니다.\nname - OKE Cluster의 이름을 입력합니다.\n결과 예시\n# Kubernetes Cluster details to monitor kubernetesCluster: # OCI Compartment Id to push Kubernetes Monitoring metrics. If not specified default is same as Agent compartment compartmentId: # Kubernetes cluster name name: oke-cluster-1 # Kubernetes cluster Namespace to monitor namespace: kube-system deployMetricServer 설정\nmetric-server도 함께 설치할 수 있지만, 여기서는 별도로 최신 버전을 설치하겠습니다.\ndeployMetricServer: false 현재 Kubernetes v1.25를 사용하는 경우 현재 최신버전인 metric-server를 0.6.2 설치가 필요합니다. 아래와 같이 별도로 설치합니다.\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml 다른 값들은 기본값을 사용합니다.\n실제 배포될 Kubernetes yaml 파일을 통해 설정값을 검증합니다.\nhelm template --values values.yaml . 배포\nhelm install mgmtagent . 배포 결과를 확인합니다.\npod/mgmtagent-0 - 실행중인 Management Agent pod\nconfigmap/mgmtagent-monitoring-config - 모니터링 대상 OKE 클러스터 설정 정보\nsecret/mgmtagent-container-registry-key - OCIR 인증 정보\nsecret/mgmtagent-input-rsp: Management Agent 설치 키 정보\n$ kubectl get pod,cm,secret -n mgmtagent NAME READY STATUS RESTARTS AGE pod/mgmtagent-0 1/1 Running 0 2m41s NAME DATA AGE configmap/kube-root-ca.crt 1 2m43s configmap/mgmtagent-monitoring-config 1 2m43s NAME TYPE DATA AGE secret/mgmtagent-container-registry-key kubernetes.io/dockerconfigjson 1 2m44s secret/mgmtagent-input-rsp Opaque 1 2m44s Agent가 메트릭을 잘 전달하고 있는지를 확인합니다.\nkubectl exec -n=mgmtagent --stdin --tty mgmtagent-0 -- tail -100 /opt/oracle/mgmt_agent/agent_inst/log/mgmt_agent_client.log | grep MetricUploadInvocation | grep rsp 아래와 같이 200 응답이 보이는 지 확인합니다. 404가 보이는 경우 Policy 설정을 확인하세요.\n2023-02-24 06:07:50,563 [SendQueue.1 (SenderManager_sender)-47] INFO - MetricUploadInvocation \u0026lt;--rsp[9ARPMGGMIQCAVWWKKDCB6L102QTY91RW/760D6060C29077717C3E951C8E88F63C/4FB4B617C87A5FF13C37FB4C93DD5241]\u0026lt;-- POST https://telemetry-ingestion.ap-chuncheon-1.oraclecloud.com/20180401/metrics: [200] Management Agent 및 메트릭 확인\nOCI 콘솔에 로그인합니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Management Agents \u0026gt; Agent로 이동합니다.\nAgent가 등록된 것을 확인합니다.\n등록된 Agent를 클릭하면, 기본 Agent에 대한 메트릭을 볼 수 있습니다. 왼쪽 Time range에서 조회 시간을 1시간으로 변경합니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Monitoring \u0026gt; Metrics Explorer로 이동합니다.\n화면 아래 Query 부분에서 mgmtagent_kubernetes_metrics가 추가된 것을 볼 수 있고, 선택하면 제공하는 메트릭들을 볼 수 있습니다.\n메트릭 이름까지 나열되면 OKE에 설치된 Agent Pod에서 OCI Metrics까지 연동을 성공한 것입니다.\nOCI Logging Analytics 대쉬보드 사용하기 샘플 대쉬보드 등록하기\nAgent Helm 차트의 resources 폴더에 Logging Analytics용 샘플 대쉬보드를 제공합니다. 또는 직접 링크에서 mgmtagent_kubernetes_dashboard.json 파일을 다운로드 받습니다.\n왼쪽 위 내비게이션 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging Analytics \u0026gt; Dashboards로 이동합니다.\nImport Dashboards를 클릭하고 대쉬보드 파일을 임포트합니다.\nManagement Agent와 동일한 Compartment에 임포트합니다.\n임포트된 Kubernetes Monitoring Sample Dashboard를 클릭합니다.\n그림과 같이 대상 OKE 클러스터의 메트릭을 통한 대쉬보드를 볼 수 있습니다.\n대쉬보드 수정하기\n대쉬보드 오른쪽 위 Actions에서 Edit를 선택합니다.\n현재 대쉬보드상의 위젯을 편집하거나, 추가 할 수 있습니다.\n추가를 위해 Create Metric Widget를 클릭합니다.\nCompartment input은 기본값을 사용합니다.\nSource에서 추가한 mgmtagent_kubernetes_metrics을 선택합니다.\n추가된 메트릭을 기준으로 차트를 만들 수 있습니다.\n참고 OCI Blog \u0026gt; Monitoring Kubernetes Cluster using Management Agent StatefulSet\nOCI Management Agent GitHub\n","lastmod":"2023-02-23T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/logging-analytics/2.management-agent-manual/","tags":["oke","oci logging analytics","management agent"],"title":"1.5.2.2.2 Management Agent를 사용한 모니터링"},{"categories":null,"contents":"Load Balancer 문제해결 Load Balancer 생성 오류 - The following tag namespaces / keys are not authorized or not found 현상: OKE에서 kubectl로 Load Balancer 타입을 생성했을때 다음과 같은 오류 발생\n$ kubectl describe svc nginx-deployment ... Normal EnsuringLoadBalancer 12s (x3 over 34s) service-controller Ensuring load balancer Warning SyncLoadBalancerFailed 8s service-controller Error syncing load balancer: failed to ensure load balancer: creating load balancer: Error returned by LoadBalancer Service. Http Status Code: 400. Error Code: RelatedResourceNotAuthorizedOrNotFound. Opc request id: 783ddd1d645bf1efda4918a6ba52ca51/DCC20EA18C4559FD348A8597553FF298/980E1AFAF615507D4AF6F8CE665EF2FD. Message: The following tag namespaces / keys are not authorized or not found: \u0026#39;cn\u0026#39;. Operation Name: CreateLoadBalancer 원인:\nterraform-oci-oke로 OKE 클러스터를 만들었을때, 콘솔에서 OKE 클러스터 정보를 보면, Initial load balancer tags에 Defined tags가 설정되어 있음. 그래서 OKE에서 생성된는 Load Balancer는 해당 tag를 기본 설정하려고 한다. 근데, 해당 tag namespace가 cluster가 위치한 compartment와 다를 경우 권한에 없어 위와 같은 에러가 발생함. 아래 그림에서는 상위 compartment에 tagnamespace인 cn을 참조함.\n해결책\n아래 문서를 보고 추가적인 권한 설정이 필요함. 해당 OKE Cluster가 다른 compartment에 위치한 tag-namespace에 대한 권한을 부여함\nhttps://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengtaggingclusterresources.htm#contengtaggingclusterresources_iam-tag-namespace-policy\nAllow any-user to use tag-namespace in compartment \u0026lt;compartment-ocid\u0026gt; where all { request.principal.id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39; } 또는\nAllow any-user to use tag-namespace in tenancy where all {request.principal.type = \u0026#39;cluster\u0026#39;} ","lastmod":"2023-02-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/troubleshooting/load-balancer/","tags":["oke","load balancer"],"title":"Load Balancer 문제해결"},{"categories":null,"contents":"5.2.2 OCI Search 서비스를 사용한 로그 모니터링 OpenSearch와 OpenDashboards는 2021년에 ElasticSearch와 Cabana에서 포크되어 별도 오픈소스 프로젝트로 운영되고 있습니다. OCI OpenSearch는 OCI에서 제공하는 관리형 서비스입니다.\nOCI Search 서비스 사용을 위한 Policy 설정하기 OCI Search 서비스 권한 부여하기 Allow service opensearch to manage vnics in compartment \u0026lt;NETWORK_RESOURCES_COMPARTMENT\u0026gt; Allow service opensearch to manage vcns in compartment \u0026lt;NETWORK_RESOURCES_COMPARTMENT\u0026gt; Allow service opensearch to use subnets in compartment \u0026lt;NETWORK_RESOURCES_COMPARTMENT\u0026gt; Allow service opensearch to use network-security-groups in compartment \u0026lt;NETWORK_RESOURCES_COMPARTMENT\u0026gt; 오라클 클라우드 콘솔에서 OCI Search 서비스 관리 권한 부여하기 사용자가 속한 그룹이 SearchOpenSearchAdmins인 경우 예시 Allow group SearchOpenSearchAdmins to manage opensearch-family in compartment \u0026lt;CLUSTER_RESOURCES_COMPARTMENT\u0026gt; OCI OpenSearch 클러스터 만들기 오라클 클라우드 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Databases \u0026gt; OpenSearch \u0026gt; Clusters 로 이동합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n생성할 compartment 위치 및 이름, 버전을 선택합니다.\n생성할 노드 구성을 선택합니다. 세 가지 구성이 가능하며, 여기서는 Development 구성을 선택합니다.\nDevelopment: 1 마스터 노드, 1 데이타 노드, 1 OpenSearch 대쉬보드 노드 Application Search, Log Analytics: 3 마스터 노드, 3 데이타 노드, 3 OpenSearch 대쉬보드 노드 동일하며 데이타 노드의 CPU 크기가 다릅니다. 구성을 선택후 노드 크기는 모두 수정할 수 있습니다. 또한 클러스터 생성이후에 크기를 변경할 수 있습니다. 클러스터가 사용할 서브넷을 설정합니다. 여기서는 OKE 클러스터 생성시 이미 만들어진 VCN과 서브넷을 편의상 선택하였습니다.\n선택할 구성 정보를 확인하고 클러스터를 생성합니다.\nDevelopment 구성기준으로 테스트 날짜기준, 생성시간은 20분 가량이 소요되었습니다.\n생성된 클러스터 정보\n클러스터 정보에서 보듯에 OpenSearch와 OpenSearch Dashboard 모두 Private IP와 Private FQDN만 있습니다. 2022년 5월말 기준으로 아직 Public IP는 지원하지 않습니다. 클러스터의 OpenSearch API Endpoint, Private IP와 OpenSearch Dashboard의 API Endpoint 및 private IP를 확인합니다.\n클러스터 연결하기 위한 설정하기 JumpBox VM 만들기 OpenSearch Dashboard는 현재 Public IP를 지원하지 않습니다. OpenSearch Dashboard를 접근을 위해 Public IP를 보유한 JumpBox VM를 추가로 생성합니다.\n오라클 클라우드 콘솔, 좌측 상단 햄버거 메뉴에서 Compute \u0026gt; Instances로 이동합니다.\n새로 Compute VM를 만듭니다.\nName: 예) jumpbox\nNetworking: 생성한 OpenSearch 클러스터에 접근이 가능한 VCN, Public Subnet을 선택하고 Public IP를 할당합니다.\n예) 여기서는 생성한 OpenSearch 클러스터와 동일한 네트워크를 사용하였습니다.\n생성한 jumpbox VM에 접근할 수 있도록, jumpbox VM에서 OpenSearch에 접근 할 수 있도록 보안규칙을 추가합니다.\nIngress Rules 아래 예시는 jumpbox가 위치가 서브넷인 10.0.20.0/24인 경우 Stateless Source IP Protocol Source Port Range Destination Port Range Description No 0.0.0.0/0 TCP All 22 SSH No 10.0.20.0/24 TCP All 5601 OpenSearch Dashboard No 10.0.20.0/24 TCP All 9200 OpenSearch API OpenSearch Dashboard 접속확인 SSH 터널링을 통해 jumpbox VM을 통해 OpenSearch Dashboard를 접속합니다.\nOpenSearch Cluster정보 기준, OpenSearch Dashboard private ip가 10.0.20.197인 경우 ssh -L 5601:10.0.20.197:5601 opc@jumpbox jumpbox VM에 접속한후 Dashboard API Endpoint에 접속이 되는지 테스트합니다.\n아래와 같이 접속이 되는 것을 알 수 있으면, 현재 Self Signed 된 인증서를 사용하는 것을 알 수 있습니다. $ ssh -L 5601:10.0.20.197:5601 opc@jumpbox Activate the web console with: systemctl enable --now cockpit.socket Last login: Fri Jun 3 04:51:54 2022 from 192.188.170.86 [opc@jumpbox ~]$ curl -v --insecure https://amaaaaaauzduwfyafs5lx2a77vnnbvz5wt6gbez6r3apldbqzn725kszn2xq.opendashboard.ap-chuncheon-1.oci.oracleiaas.com:5601 ... * SSL connection using TLSv1.2 / ECDHE-RSA-AES128-GCM-SHA256 * ALPN, server accepted to use http/1.1 * Server certificate: * subject: CN=*.opendashboard.ap-chuncheon-1.oci.oracleiaas.com * start date: Apr 5 02:07:20 2022 GMT * expire date: Oct 2 02:08:20 2022 GMT * issuer: OU=opc-device:71:a6:a1:f5:40:ae:13:ba:7b:10:71:5e:7e:84:e3:a6:6f:c1:f4:21:61:3a:e3:da:8e:30:2a:56:82:30:cc:9d; CN=PKISVC CrossRegion Intermediate r2 * SSL certificate verify result: self signed certificate in certificate chain (19), continuing anyway. \u0026gt; GET / HTTP/1.1 \u0026gt; Host: amaaaaaauzduwfyafs5lx2a77vnnbvz5wt6gbez6r3apldbqzn725kszn2xq.opendashboard.ap-chuncheon-1.oci.oracleiaas.com:5601 \u0026gt; User-Agent: curl/7.61.1 \u0026gt; Accept: */* \u0026gt; \u0026lt; HTTP/1.1 302 Found \u0026lt; location: /app/home \u0026lt; osd-name: kibana-54c5f88dc9-gt57f \u0026lt; cache-control: private, no-cache, no-store, must-revalidate \u0026lt; content-length: 0 \u0026lt; Date: Fri, 03 Jun 2022 04:58:56 GMT \u0026lt; Connection: keep-alive \u0026lt; * Connection #0 to host amaaaaaauzduwfyafs5lx2a77vnnbvz5wt6gbez6r3apldbqzn725kszn2xq.opendashboard.ap-chuncheon-1.oci.oracleiaas.com left intact SSH 터널링이 되어 있으므로, 로컬에서 브라우저로 OpenSearch Dashboard에 접속을 확인합니다.\n접속주소: https://localhost:5601 OKE에서 OpenSearch로 로그 보내기 EFK 구성에서 ElasticSearch와 Kibana 대신 OpenSearch와 OpenSearch Dashboard를 사용합니다.\nFluentd 구성 Fluentd 설치를 위한 Service Account를 생성하고 관련 권한을 정의합니다.\nconfigmap 추가 설정정의\nFluentd 관련 설정은 컨테이너 내에 /fluentd/etc/ 하위에 .conf 파일로 모두 정의 되어 있습니다. 이 파일들을 재정의 할 수 있습니다. OCI OpenSearch 클러스터가 현재 기본적으로 Self Signed 인증서를 사용합니다. 연결을 위해 꼭 필요한 기본 항목만 남겨둡니다. 주석 처리된 부분을 그냥 두고 값을 설정하지 않으면 실행시 오류가 발생합니다. fluentd damonset 정의\n설정한 configmap 사용을 위해 Fluentd 문서상의 YAML을 일부 변경하셨습니다. configmap으로 재정의한 fluent.conf을 사용하도록 변경하고 있습니다. OpenSearch Dashboard가 사용하는 Self Signed CA를 설정하고 있습니다. OpenSearch Dashboard가 사용하는 Self Signed CA 등록 SSH로 접속한 jumpbox VM 다시 돌아갑니다.\n다음 명령어로 OpenSearch Endpoint의 Self Signed CA를 다운로드 합니다.\n클러스터의 OpenSearch API Endpoint openssl s_client -showcerts -connect ${OPENSEARCH_API_ENDPOINT}:9200 \u0026lt;/dev/null | sed -n -e \u0026#39;/-.BEGIN/,/-.END/ p\u0026#39; \u0026gt; opensearch-ca.crt 실행 예 opc@jumpbox ~]$ openssl s_client -showcerts -connect amaaaaaauzduwfyafs5lx2a77vnnbvz5wt6gbez6r3apldbqzn725kszn2xq.opensearch.ap-chuncheon-1.oci.oracleiaas.com:9200 \u0026lt;/dev/null | sed -n -e \u0026#39;/-.BEGIN/,/-.END/ p\u0026#39; \u0026gt; opensearch-ca.crt depth=2 CN = Mission Control Root CA (transitional) verify error:num=19:self signed certificate in certificate chain verify return:1 depth=2 CN = Mission Control Root CA (transitional) verify return:1 depth=1 OU = opc-device:f7:4a:0c:95:4a:8d:a6:06:a2:1c:d2:12:40:6b:61:ae:c3:a0:e2:4c:ca:7b:3e:df:94:85:cc:39:a4:92:df:93, CN = PKISVC CrossRegion Intermediate r2 verify return:1 depth=0 CN = *.opensearch.ap-chuncheon-1.oci.oracleiaas.com verify return:1 DONE [opc@jumpbox ~]$ cat opensearch-ca.crt -----BEGIN CERTIFICATE----- MIIE6TCCAtGgAwIBAgIRAOstTmVRjci44g910KG8+YMwDQYJKoZIhvcNAQELBQAw ... 2cTH4OwgiUBHOzI8tQ== -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIFxTCCA62gAwIBAgIRAM/DvpKazV/tYwL/TDrInwIwDQYJKoZIhvcNAQELBQAw ... 37v4yd3CKzhwvmSed7aUsCofDN2WtcVgAp7GA/l12kb+EEafun7QaQI= -----END CERTIFICATE----- -----BEGIN CERTIFICATE----- MIIFQzCCAyugAwIBAgIBAjANBgkqhkiG9w0BAQsFADAxMS8wLQYDVQQDEyZNaXNz ... j3tjZCAkyg== -----END CERTIFICATE----- kubectl로 대상 OKE 클러스터에 접속 가능한 곳으로 다운로드한 opensearch-ca.crt 파일을 복사합니다.\nOKE 클러스터에 다음 명령으로 CA를 configmap으로 추가합니다.\nkubectl create configmap opensearch-ca.crt --from-file=opensearch-ca.crt -n kube-system FluentD 설치 fluentd-daemonset-opensearch-self-signed.yaml에서 FLUENT_OPENSEARCH_HOST를 대상 OpenSearch 호스트로 변경합니다.\n... env: - name: FLUENT_OPENSEARCH_HOST value: \u0026#34;amaaaaaauzduwfyafs5lx2a77vnnbvz5wt6gbez6r3apldbqzn725kszn2xq.opensearch.ap-chuncheon-1.oci.oracleiaas.com\u0026#34; - name: FLUENT_OPENSEARCH_PORT value: \u0026#34;9200\u0026#34; ... FluentD 설치\nkubectl apply -f fluentd-rbac.yaml kubectl apply -f fluentd-configmap-opensearch-self-signed.yaml kubectl apply -f fluentd-daemonset-opensearch-self-signed.yaml OKE 클러스터 Worker Node에서 OpenSearch로 로그를 보낼수 있도록 보안규칙을 추가합니다.\nWorker Node의 보안규칙\n대상: 예) oke-nodeseclist-quick-oke-cluster-1-04cdcc334\nEgress Rules:\n아래 예시는 OpenSearch 위치가 서브넷인 10.0.20.0/24인 경우\nStateless Destination IP Protocol Source Port Range Destination Port Range Description No 10.0.20.0/24 TCP All 9200 OpenSearch API OpenSearch의 보안규칙\n대상: 예) oke-svclbseclist-quick-oke-cluster-1-04cdcc334\nIngress Rules:\n아래 예시는 Worker Node가 위치가 서브넷인 10.0.10.0/24인 경우\nStateless Source IP Protocol Source Port Range Destination Port Range Description No 10.0.10.0/24 TCP All 9200 OpenSearch API 로그 확인\nfluentd Pod가 기동하면, 로그를 통해 OpenSearch와 연결 오류 없이 정상 동작하는 지 확인합니다.\n$ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE ... fluentd-bz62z 1/1 Running 0 99s fluentd-c54qn 1/1 Running 0 99s fluentd-x5l2h 1/1 Running 0 99s ... $ kubectl logs -f fluentd-t7p87 -n kube-system -f 2022-06-03 07:28:46 +0000 [info]: parsing config file is succeeded path=\u0026#34;/fluentd/etc/fluent.conf\u0026#34; ... 2022-06-03 07:28:48 +0000 [info]: adding match pattern=\u0026#34;**\u0026#34; type=\u0026#34;opensearch\u0026#34; ... 2022-06-03 07:28:48 +0000 [info]: #0 starting fluentd worker pid=15 ppid=7 worker=0 2022-06-03 07:28:48 +0000 [info]: #0 [in_tail_container_logs] following tail of /var/log/containers/kube-flannel-ds-stmfq_kube-system_install-cni-f69fde150aaaf779132c4fc2f2315fc30e703e9e6c39127be21fbec137d7b735.log 2022-06-03 07:28:48 +0000 [info]: #0 [in_tail_container_logs] following tail of /var/log/containers/proxymux-client-p7nzp_kube-system_proxymux-client-fa4352b8f7ba57325ea3ef2dc6af8016a585df67a0f870058f76761b75c074e7.log OpenSearch Dashboard 설정 SSH 터널링을 통해 jumpbox VM으로 접속하지 않은 경우 다시 접속합니다.\nSSH 터널링이 되어 있으므로, 로컬에서 브라우저로 OpenSearch Dashboard에 접속을 확인합니다.\n접속주소: https://localhost:5601 왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Discover 를 클릭합니다.\nCreate index pattern을 클릭합니다.\n인덱스 패턴을 생성합니다.\n아래쪽에 보이는 3개의 소스 중에서 fluentd 소스를 사용합니다.\nIndex pattern name: fluentd Time field: time 인덱스 패턴이 추가된 결과를 볼 수 있습니다.\n왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Discover 를 클릭합니다.\n생성한 인덱스 패턴을 통해 수집된 로그를 확인할 수 있습니다.\n테스트 앱의 로그를 확인하기 위해 Add filter를 클릭하여 namespace_name=default 로 지정합니다. 테스트 앱을 접속합니다.\n예) http://129.154.xxx.xxx/?opensearch-test 로그 확인\n아래와 같이 OpenSearch Dashboard에서 테스트 앱의 로그를 확인할 수 있습니다.\n","lastmod":"2022-05-31T00:00:00Z","permalink":"https://thekoguryo.github.io/archives/oracle-cloudnative/oci-services/logging/2.oci-opensearch-self-signed/","tags":["oke","oci opensearch","opensearch"],"title":"5.2.2 OpenSearch 기반 OCI Search 서비스를 사용한 로그 모니터링 - Self-Signed"},{"categories":null,"contents":"1.2.3.2.1 File Storage 사용하기(CSI Driver 기반) - 있는 File System 쓰기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)를 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 노드상의 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 방법, RWX 접근 모드로 사용하는 방법을 확인해 보겠습니다.\n업데이트 Jan. 13, 2022일자 기준 업데이트 Support for provisioning Kubernetes Persistent Volume Claims (PVCs) on File Storage service로 OKE에서 File Storage Service에 대한 CSI Driver 공식적으로 출시되었으며, OKE 공식 문서에도 사용가이드가 포함되었습니다. 이전에 OCI 블로그를 통해 설명된 Flex Volume Driver가 아닌 공식지원하는 CSI Driver 기준으로 사용할 것을 권장합니다. File System 만들기 관련 문서를 참고하여 File System을 만듭니다.\n8. 네트워크 파일 시스템(NFS)인 File Storage 사용하기 OCI Documentation \u0026gt; File Storage OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; File Storage \u0026gt; File Systems 항목으로 이동합니다.\nCreate File System 클릭\n생성정보 입력\n기본값을 그대로 사용해도 되지만 편의상 이름을 변경합니다.\nFile System Information\nName: 우측 Edit Details를 클릭하여 원하는 이름으로 변경, oke-fss-system 입력 Export Information : 기본값 사용\nMount Target Information\nCreate New Mount Target 선택 New Mount Target Name: 우측 Edit Details를 클릭하여 원하는 이름으로 변경, oke-fss-mount-target 입력 Virtual Cloud Network: OKE 클러스터가 사용하는 VCN 선택 Subnet: Subnet 선택, 예, oke-svclbsubnet-~~~ 생성 후 File System 화면에서 Mount Target을 클릭하여 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\nMount Target OCID: \u0026hellip;wtcaaa\nIP Address: 예) 10.0.20.12**\nExport Path: 예) /oke-fss-system\nSecurity List 설정\nMount Target의 속한 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\nOKE Worker Nodes Subnet -\u0026gt; 생성한 Mount Target으로 접근 허용 File Storage 서비스를 이용하여 Persistent Volume 사용하기 Persistent Volume (PV) 만들기\nspec.csi.driver: fss.csi.oraclecloud.com를 사용 spec.csi.volumeHandle: \u0026lt;FileSystemOCID\u0026gt;:\u0026lt;MountTargetIP\u0026gt;:\u0026lt;path\u0026gt; 형식으로 아래와 같이 설정 spec.accessModes: FFS CSI Driver는 RWX - ReadWriteMany를 지원하므로 테스트를 위해 ReadWriteMany 접근 모드로 지정 # oke-fss-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: oke-fss-pv spec: capacity: storage: 50Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain csi: driver: fss.csi.oraclecloud.com volumeHandle: ocid1.filesystem.oc1.ap_chuncheon_1.aaaaa_____wtcaaa:10.0.20.12:/oke-fss-system Persistent Volume Claim(PVC) 만들기\nstorageClassName: \u0026ldquo;\u0026ldquo;로 설정 volumeName: 앞서 만든 PV의 이름 지정 # oke-fss-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: oke-fss-pvc spec: accessModes: - ReadWriteMany storageClassName: \u0026#34;\u0026#34; resources: requests: storage: 50Gi volumeName: oke-fss-pv PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\nReadWriteMany 접근모드를 사용하므로 앞선 Block Volume을 PV 사용하는 예제와 달리 replica를 복수개로 지정할 수 있습니다.\n# nginx-deployment-fss-pvc.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-fss-pvc name: nginx-fss-pvc spec: replicas: 3 selector: matchLabels: app: nginx-fss-pvc template: metadata: labels: app: nginx-fss-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: oke-fss-pvc 실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\n$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created $ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created $ kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/oke-fss-pv 50Gi RWX Retain Bound default/oke-fss-pvc 24s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 50Gi RWX 18s $ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-cc4d6b798-s9jrl 1/1 Running 0 27s 10.244.1.4 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-cc4d6b798-snr2t 1/1 Running 0 27s 10.244.0.134 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-cc4d6b798-wcvpl 1/1 Running 0 27s 10.244.0.6 10.0.10.107 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 파일 쓰기 테스트\n아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\n$ kubectl exec -it nginx-fss-pvc-cc4d6b798-s9jrl -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.39\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; $ kubectl exec -it nginx-fss-pvc-cc4d6b798-s9jrl -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.39 $ kubectl exec -it nginx-fss-pvc-cc4d6b798-snr2t -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.39 $ kubectl exec -it nginx-fss-pvc-cc4d6b798-wcvpl -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.39 참고 문서 OCI Documentation \u0026gt; Container Engine \u0026gt; Setting Up Storage for Kubernetes Clusters ","lastmod":"2024-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/storage/file-storage-service/1.file-storage/","tags":["oke"],"title":"1.2.3.2.1 File Storage 사용하기"},{"categories":null,"contents":"14.2.2 Bash 샘플 클라이언트 (예전 스타일) 2022년 기준으로 oci cli(즉, oci raw-request)를 사용하는 것으로 변경되었습니다. 최신 내용은 아래 링크를 참조하세요.\nhttps://docs.cloud.oracle.com/iaas/Content/API/Concepts/signingrequests.htm#Bash 아래는 이전에 OCI 문서에서 제공하던 oci-curl bash 함수를 이용하는 방법을 사용합니다. 백업을 위해 남겨 놓습니다.\nStep 1. oci-curl 함수 준비 # Version: 1.0.2 # Usage: # oci-curl \u0026lt;host\u0026gt; \u0026lt;method\u0026gt; [file-to-send-as-body] \u0026lt;request-target\u0026gt; [extra-curl-args] # ex: # oci-curl iaas.us-ashburn-1.oraclecloud.com get \u0026#34;/20160918/instances?compartmentId=some-compartment-ocid\u0026#34; # oci-curl iaas.us-ashburn-1.oraclecloud.com post ./request.json \u0026#34;/20160918/vcns\u0026#34; function oci-curl { # TODO: update these values to your own local tenancyId=\u0026#34;ocid1.tenancy.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;; local authUserId=\u0026#34;ocid1.user.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;; local keyFingerprint=\u0026#34;20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34\u0026#34;; local privateKeyPath=\u0026#34;/Users/someuser/.oci/oci_api_key.pem\u0026#34;; local alg=rsa-sha256 local sigVersion=\u0026#34;1\u0026#34; local now=\u0026#34;$(LC_ALL=C \\date -u \u0026#34;+%a, %d %h %Y %H:%M:%S GMT\u0026#34;)\u0026#34; local host=$1 local method=$2 local extra_args local keyId=\u0026#34;$tenancyId/$authUserId/$keyFingerprint\u0026#34; case $method in \u0026#34;get\u0026#34; | \u0026#34;GET\u0026#34;) local target=$3 extra_args=(\u0026#34;${@: 4}\u0026#34;) local curl_method=\u0026#34;GET\u0026#34;; local request_method=\u0026#34;get\u0026#34;; ;;\t\u0026#34;delete\u0026#34; | \u0026#34;DELETE\u0026#34;) local target=$3 extra_args=(\u0026#34;${@: 4}\u0026#34;) local curl_method=\u0026#34;DELETE\u0026#34;; local request_method=\u0026#34;delete\u0026#34;; ;;\t\u0026#34;head\u0026#34; | \u0026#34;HEAD\u0026#34;) local target=$3 extra_args=(\u0026#34;--head\u0026#34; \u0026#34;${@: 4}\u0026#34;) local curl_method=\u0026#34;HEAD\u0026#34;; local request_method=\u0026#34;head\u0026#34;; ;; \u0026#34;post\u0026#34; | \u0026#34;POST\u0026#34;) local body=$3 local target=$4 extra_args=(\u0026#34;${@: 5}\u0026#34;) local curl_method=\u0026#34;POST\u0026#34;; local request_method=\u0026#34;post\u0026#34;; local content_sha256=\u0026#34;$(openssl dgst -binary -sha256 \u0026lt; $body | openssl enc -e -base64)\u0026#34;; local content_type=\u0026#34;application/json\u0026#34;; local content_length=\u0026#34;$(wc -c \u0026lt; $body | xargs)\u0026#34;; ;;\t\u0026#34;put\u0026#34; | \u0026#34;PUT\u0026#34;) local body=$3 local target=$4 extra_args=(\u0026#34;${@: 5}\u0026#34;) local curl_method=\u0026#34;PUT\u0026#34; local request_method=\u0026#34;put\u0026#34; local content_sha256=\u0026#34;$(openssl dgst -binary -sha256 \u0026lt; $body | openssl enc -e -base64)\u0026#34;; local content_type=\u0026#34;application/json\u0026#34;; local content_length=\u0026#34;$(wc -c \u0026lt; $body | xargs)\u0026#34;; ;;\t*) echo \u0026#34;invalid method\u0026#34;; return;; esac # This line will url encode all special characters in the request target except \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;=\u0026#34;, and \u0026#34;\u0026amp;\u0026#34;, since those characters are used # in the request target to indicate path and query string structure. If you need to encode any of \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;=\u0026#34;, or \u0026#34;\u0026amp;\u0026#34;, such as when # used as part of a path value or query string key or value, you will need to do that yourself in the request target you pass in. local escaped_target=\u0026#34;$(echo $( rawurlencode \u0026#34;$target\u0026#34; ))\u0026#34;\tlocal request_target=\u0026#34;(request-target): $request_method $escaped_target\u0026#34; local date_header=\u0026#34;date: $now\u0026#34; local host_header=\u0026#34;host: $host\u0026#34; local content_sha256_header=\u0026#34;x-content-sha256: $content_sha256\u0026#34; local content_type_header=\u0026#34;content-type: $content_type\u0026#34; local content_length_header=\u0026#34;content-length: $content_length\u0026#34; local signing_string=\u0026#34;$request_target\\n$date_header\\n$host_header\u0026#34; local headers=\u0026#34;(request-target) date host\u0026#34; local curl_header_args curl_header_args=(-H \u0026#34;$date_header\u0026#34;) local body_arg body_arg=() if [ \u0026#34;$curl_method\u0026#34; = \u0026#34;PUT\u0026#34; -o \u0026#34;$curl_method\u0026#34; = \u0026#34;POST\u0026#34; ]; then signing_string=\u0026#34;$signing_string\\n$content_sha256_header\\n$content_type_header\\n$content_length_header\u0026#34; headers=$headers\u0026#34; x-content-sha256 content-type content-length\u0026#34; curl_header_args=(\u0026#34;${curl_header_args[@]}\u0026#34; -H \u0026#34;$content_sha256_header\u0026#34; -H \u0026#34;$content_type_header\u0026#34; -H \u0026#34;$content_length_header\u0026#34;) body_arg=(--data-binary @${body}) fi local sig=$(printf \u0026#39;%b\u0026#39; \u0026#34;$signing_string\u0026#34; | \\ openssl dgst -sha256 -sign $privateKeyPath | \\ openssl enc -e -base64 | tr -d \u0026#39;\\n\u0026#39;) curl \u0026#34;${extra_args[@]}\u0026#34; \u0026#34;${body_arg[@]}\u0026#34; -X $curl_method -sS https://${host}${escaped_target} \u0026#34;${curl_header_args[@]}\u0026#34; \\ -H \u0026#34;Authorization: Signature version=\\\u0026#34;$sigVersion\\\u0026#34;,keyId=\\\u0026#34;$keyId\\\u0026#34;,algorithm=\\\u0026#34;$alg\\\u0026#34;,headers=\\\u0026#34;${headers}\\\u0026#34;,signature=\\\u0026#34;$sig\\\u0026#34;\u0026#34; }\t# url encode all special characters except \u0026#34;/\u0026#34;, \u0026#34;?\u0026#34;, \u0026#34;=\u0026#34;, and \u0026#34;\u0026amp;\u0026#34; function rawurlencode { local string=\u0026#34;${1}\u0026#34; local strlen=${#string} local encoded=\u0026#34;\u0026#34; local pos c o\tfor (( pos=0 ; pos\u0026lt;strlen ; pos++ )); do c=${string:$pos:1} case \u0026#34;$c\u0026#34; in [-_.~a-zA-Z0-9] | \u0026#34;/\u0026#34; | \u0026#34;?\u0026#34; | \u0026#34;=\u0026#34; | \u0026#34;\u0026amp;\u0026#34; ) o=\u0026#34;${c}\u0026#34; ;; * ) printf -v o \u0026#39;%%%02x\u0026#39; \u0026#34;\u0026#39;$c\u0026#34; esac encoded+=\u0026#34;${o}\u0026#34; done echo \u0026#34;${encoded}\u0026#34; } Step 2. 연결정보 설정 위 샘플 코드를 복사하여 oci-curl.sh 이름으로 저장합니다.\noci-curl.sh 내용중에 연결정보를 사용자에 맞게 업데이트합니다.\n연결정보 예시\n아래 정보를 찾는 방법은 14.1.1.4 API Key 기반 인증 Config File 설정를 참고합니다. local tenancyId=\u0026#34;ocid1.tenancy.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;; local authUserId=\u0026#34;ocid1.user.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u0026#34;; local keyFingerprint=\u0026#34;20:3b:97:13:55:1c:5b:0d:d3:37:d8:50:4e:c5:3a:34\u0026#34;; local privateKeyPath=\u0026#34;/Users/someuser/.oci/oci_api_key.pem\u0026#34;; 아래 명령을 수행하면 oci-curl 함수를 현재 세션에서 실행할 수 있게 됩니다.\noracle@ubuntu:~/oci-curl$ . ./oci-curl.sh oracle@ubuntu:~/oci-curl$ oci-curl invalid method oracle@ubuntu:~/oci-curl$ Step 3. 사용자 조회 REST API 실행 ListUsers 설명\n문서 링크: https://docs.cloud.oracle.com/iaas/api/#/en/identity/20160918/User/ListUsers Endpoint: identity.us-ashburn-1.oraclecloud.com GET /20160918/users/ Parameters compartmentId: tenancy의 OCID 사용방법\noci-curl \u0026lt;host\u0026gt; \u0026lt;method\u0026gt; [file-to-send-as-body] \u0026lt;request-target\u0026gt; [extra-curl-args] 사용예시 - tenancy내 모든 사용자 조회 아래와 같이 사용자가 조회되는 것을 확인할 수 있습니다.\nextra-curl-args로 -i를 입력하면 응답메시지 헤더를 확인할 수 있습니다. oracle@ubuntu:~/oci-curl$ oci-curl identity.us-ashburn-1.oraclecloud.com GET \u0026#34;/20160918/users/?compartmentId=ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34; -i HTTP/1.1 200 OK Date: Sun, 19 May 2019 08:15:10 GMT Content-Type: application/json Content-Length: 3152 Connection: keep-alive opc-request-id: /2BF47D321833EFD084DBAAC718095658/E21191A3BE4584980FA4A533A6E50927 opc-next-page: AAAAAAAAAAJleUpyYVdRaU9pSTBPRGNpTENKbGJtTWlPaUpCTWpVMlIwTk5JaXdpWVd4bklqb2laR2x5SW4wLi53d0wzM0RiMnVMNGJld0FjLmVyQjNfU3RfbVpmNTRkQS1GRVlLaFk4N196S3dzakVRUVhicnZZeUowSVJxYWdpQnZSVWtEQW9ZWlRNX0hKQ1RzcEVFYTU1Qkt0cEFBdlJXdlRrdWlvNlBRRWpoUG5FNDhCSXpjZUd5UTlOOFBOdkVNRzFoLTEzRUJHbzJzSFJ0Q0hpTU8weFRMRHI4UjlhVHI4SnFjZVJyMXVkT3hlZWRVMmJ2Y1pZenlUM05aTXAzeG5HMDc4ZGpKTHBmbWViWFhtSHBLb1JrQ0JXZHl0VTlkNzRIYksyU19STlk5WWlKSGRRczhjenVzOEE2MGNJS2JDY1MyZ0FGVFhsTmI3UnNXS05ZaHlhTXZUcVZVX05JUTJETkstbVB1TUdjRFYwQU5GTkg1NzdISDRFbDY3R0w4TlAwOVBfeFprSFZrOWNFU0xfMXRHWDRUcUF5d0NoMVVrak5Nc2drb21VbkMzMWNQVThTdjBFQ1ZsS2ZjVGlmTGxCVkxqS0o1a1o1R3RoWUJVLXhJcGxHenJXNjZRUWFsbTlVM2VPVFMxbDFKZi1HS0F3ZGlDY0E1aU1ldWtxNE92d3JmT3Z6aXFGdktJUFZoeldpeHNQaF81ZVE5UzgxWlJmSURHU1dpNU85cUdXZ2I3WnA0bVhnOWtIaGFqY2YyMjJLdlNSTS0xSVFLcnlJazVDUzhranVZR2hJNE9Oa2JrUXIuWloyejcyZ05EUGVCODNZdFVJZEptZw== Cache-Control: no-cache, no-store, must-revalidate opc-limit: 25 Pragma: no-cache X-Content-Type-Options: nosniff [ { \u0026#34;capabilities\u0026#34; : { \u0026#34;canUseConsolePassword\u0026#34; : true, \u0026#34;canUseApiKeys\u0026#34; : true, \u0026#34;canUseAuthTokens\u0026#34; : true, \u0026#34;canUseSmtpCredentials\u0026#34; : true, \u0026#34;canUseCustomerSecretKeys\u0026#34; : true }, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;identityProviderId\u0026#34; : null, \u0026#34;externalIdentifier\u0026#34; : null, \u0026#34;timeModified\u0026#34; : \u0026#34;2019-05-13T04:56:33.114Z\u0026#34;, \u0026#34;isMfaActivated\u0026#34; : false, \u0026#34;id\u0026#34; : \u0026#34;ocid1.user.oc1..aaaaaaaa2um5iz27ms3cf43tp77k6tjjn4kbzjrilajem4xaiyl5vqeXXXXXX\u0026#34;, \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;oci.admin\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;OCI Admin\u0026#34;, \u0026#34;timeCreated\u0026#34; : \u0026#34;2019-05-13T04:55:06.156Z\u0026#34;, \u0026#34;freeformTags\u0026#34; : { }, \u0026#34;definedTags\u0026#34; : { }, \u0026#34;lifecycleState\u0026#34; : \u0026#34;ACTIVE\u0026#34; }, { \u0026#34;capabilities\u0026#34; : { \u0026#34;canUseConsolePassword\u0026#34; : true, \u0026#34;canUseApiKeys\u0026#34; : true, \u0026#34;canUseAuthTokens\u0026#34; : true, \u0026#34;canUseSmtpCredentials\u0026#34; : true, \u0026#34;canUseCustomerSecretKeys\u0026#34; : true }, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;identityProviderId\u0026#34; : null, \u0026#34;externalIdentifier\u0026#34; : null, \u0026#34;timeModified\u0026#34; : \u0026#34;2019-05-13T04:11:10.299Z\u0026#34;, \u0026#34;isMfaActivated\u0026#34; : false, \u0026#34;id\u0026#34; : \u0026#34;ocid1.user.oc1..aaaaaaaaeqzpkd5u7humc3xinp3ika4sjhnhqj5jbvfcvdqg4tdx4jqXXXXX\u0026#34;, \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;sandboxer\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;sandboxer\u0026#34;, \u0026#34;timeCreated\u0026#34; : \u0026#34;2019-05-13T04:09:32.205Z\u0026#34;, \u0026#34;freeformTags\u0026#34; : { }, \u0026#34;definedTags\u0026#34; : { }, \u0026#34;lifecycleState\u0026#34; : \u0026#34;ACTIVE\u0026#34; } ]oracle@ubuntu:~/oci-curl$ Step 4. 사용자 생성 REST API 실행 CreateUser 설명\n문서 링크: https://docs.cloud.oracle.com/iaas/api/#/en/identity/20160918/User/CreateUser Endpoint: identity.us-ashburn-1.oraclecloud.com POST /20160918/users/ 요청메시지 예시 { \u0026#34;compartmentId\u0026#34; : \u0026#34;tenancy OCID\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;사용자 이름\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;설명\u0026#34; } 사용방법\noci-curl \u0026lt;host\u0026gt; \u0026lt;method\u0026gt; [file-to-send-as-body] \u0026lt;request-target\u0026gt; [extra-curl-args] 사용예시 - 사용자 생성 아래와 같이 사용자가 만들어 지는 것을 확인할 수 있습니다.\noracle@ubuntu:~/oci-curl$ cat create_user_request.json { \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;KilDong OCI\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;kildong.oci@example.com\u0026#34; } oracle@ubuntu:~/oci-curl$ oci-curl identity.us-ashburn-1.oraclecloud.com POST ./create_user_request.json \u0026#34;/20160918/users/\u0026#34; -i HTTP/1.1 200 OK Date: Sun, 19 May 2019 08:33:39 GMT Content-Type: application/json Content-Length: 748 Connection: keep-alive opc-request-id: /3010DE4E4BFBF1963248FEC32FC1FFBA/FB514FBFDEAA1C6845BCAA66C2B4C31D Cache-Control: no-cache, no-store, must-revalidate ETag: 42e800af061123f725163d2b538d1f9560022422 Pragma: no-cache Location: http://identity.us-ashburn-1.oraclecloud.com/20160918/users/ocid1.user.oc1..aaaaaaaaniw34appawah7sicksca37hhzhq7pvfkmhwskf4gbkt3ctxXXXXXX X-Content-Type-Options: nosniff { \u0026#34;capabilities\u0026#34; : { \u0026#34;canUseConsolePassword\u0026#34; : true, \u0026#34;canUseApiKeys\u0026#34; : true, \u0026#34;canUseAuthTokens\u0026#34; : true, \u0026#34;canUseSmtpCredentials\u0026#34; : true, \u0026#34;canUseCustomerSecretKeys\u0026#34; : true }, \u0026#34;emailVerified\u0026#34; : false, \u0026#34;identityProviderId\u0026#34; : null, \u0026#34;externalIdentifier\u0026#34; : null, \u0026#34;timeModified\u0026#34; : \u0026#34;2019-05-19T08:33:39.788Z\u0026#34;, \u0026#34;isMfaActivated\u0026#34; : false, \u0026#34;id\u0026#34; : \u0026#34;ocid1.user.oc1..aaaaaaaaniw34appawah7sicksca37hhzhq7pvfkmhwskf4gbkt3ctxXXXXX\u0026#34;, \u0026#34;compartmentId\u0026#34; : \u0026#34;ocid1.tenancy.oc1..aaaaaaaa4xqu77ge5lsioskp53247ohk7rs3bfyodsb2bf6h6mhahlzXXXXX\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;kildong.oci@example.com\u0026#34;, \u0026#34;description\u0026#34; : \u0026#34;KilDong OCI\u0026#34;, \u0026#34;timeCreated\u0026#34; : \u0026#34;2019-05-19T08:33:39.788Z\u0026#34;, \u0026#34;freeformTags\u0026#34; : { }, \u0026#34;definedTags\u0026#34; : { }, \u0026#34;lifecycleState\u0026#34; : \u0026#34;ACTIVE\u0026#34; }oracle@ubuntu:~/oci-curl$ 생성결과 확인 ","lastmod":"2022-01-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/2/","tags":["rest api","bash"],"title":"14.2.2 Bash 샘플 클라이언트"},{"categories":null,"contents":"15.2 Resource Manager을 위한 Stack Template Stack Template OCI 콘솔에서 내비게이션 메뉴의 Developer Services \u0026gt; Resource Manager \u0026gt; Stacks을 클릭합니다.\nCreate Stack 을 클릭합니다.\n두번째 Template 옵션을 선택하고 Select Template을 클릭합니다.\n제공하고 있는 템플릿을 볼 수 있습니다.\nQuickstarts\nService\nArchitect\nPrivate\n지금인 비어 있지만, Private Template을 등록하는 작업을 통해 Tenancy 내에 공유할 템플릿을 활용할 수 있습니다.\nDeploy to Oracle Cloud 앞으로 OCI 문서 및 OCI 예제 사이트에서 아래와 같은 버튼을 보게 될 것입니다.\n그러면 OCI Console을 로그인후 자동으로 Stack 등록화면으로 이동하게 됩니다.\nReference Solution Architecture OCI 상에 배포시 참조 모델을 위해 Reference 사이트를 제공하고 있습니다. 해당 사이트에서도 동일하게 링크을 통해 Reference 모델을 Stack Template 형태로 배포할 수 있게 지원하고 있습니다.\nOracle Cloud Infrastructure Architecture Center\n","lastmod":"2022-01-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter15/2/","tags":["resource manage","terraform"],"title":"15.2 Resource Manager을 위한 Stack Template"},{"categories":null,"contents":"14.1.2.2 CLI를 통한 파일 업로드 아래 페이지의 일부를 정리한 내용입니다. 전체 내용은 다음 링크를 참고하세요. https://docs.cloud.oracle.com/iaas/Content/Object/Tasks/managingobjects.htm https://docs.oracle.com/en-us/iaas/tools/oci-cli/3.4.2/oci_cli_docs/cmdref/os/object/put.html Object Storage Namespace을 조회합니다.\noci os ns get Bucket상의 Object 목록 조회\noci os object list -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; 실행예시\nPS D:\\\u0026gt; oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnzdbcmqu9s8\u0026#34; } PS D:\\\u0026gt; oci os object list -ns cnzdbcmqu9s8 -bn ExampleBucketForCLI { \u0026#34;prefixes\u0026#34;: [] } Bucket에 Object 업로드\noci os object put -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; --file \u0026lt;file_location\u0026gt; --name \u0026lt;object_name\u0026gt; --no-multipart Windows 실행예시\nPS D:\\ForObjectStorage\u0026gt; dir 디렉터리: D:\\ForObjectStorage Mode LastWriteTime Length Name ---- ------------- ------ ---- d----- 2022-01-12 오후 7:01 images -a---- 2022-01-12 오후 5:27 152 index.html PS D:\\ForObjectStorage\u0026gt; oci os object put -bn ExampleBucketForCLI --file index.html { \u0026#34;etag\u0026#34;: \u0026#34;4235d07c-1ef4-42db-b64a-4fb045de2fcd\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 02:42:23 GMT\u0026#34;, \u0026#34;opc-content-md5\u0026#34;: \u0026#34;B8Iehaa6j58u+qMiWnNldg==\u0026#34; } PS D:\\ForObjectStorage\u0026gt; oci os object list -ns cnzdbcmqu9s8 -bn ExampleBucketForCLI { \u0026#34;data\u0026#34;: [ { \u0026#34;archival-state\u0026#34;: null, \u0026#34;etag\u0026#34;: \u0026#34;4235d07c-1ef4-42db-b64a-4fb045de2fcd\u0026#34;, \u0026#34;md5\u0026#34;: \u0026#34;B8Iehaa6j58u+qMiWnNldg==\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;index.html\u0026#34;, \u0026#34;size\u0026#34;: 152, \u0026#34;storage-tier\u0026#34;: \u0026#34;Standard\u0026#34;, \u0026#34;time-created\u0026#34;: \u0026#34;2022-01-18T02:42:23.945000+00:00\u0026#34;, \u0026#34;time-modified\u0026#34;: \u0026#34;2022-01-18T02:42:23.945000+00:00\u0026#34; } ], \u0026#34;prefixes\u0026#34;: [] } Bucket에 Object들 벌크 업로드\noci os object bulk-upload -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; --src-dir \u0026lt;source_directory_location\u0026gt; --no-multipart Windows 실행예시\nPS D:\\ForObjectStorage\u0026gt; oci os object bulk-upload -bn ExampleBucketForCLI --src-dir . Uploaded images/icons8-oracle-96.png [####################################] 100% Uploaded index.html [####################################] 100% { \u0026#34;skipped-objects\u0026#34;: [], \u0026#34;upload-failures\u0026#34;: {}, \u0026#34;uploaded-objects\u0026#34;: { \u0026#34;images/icons8-oracle-96.png\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;388a319c-b5d0-4a51-bacf-9d9fe84bc621\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:05:04 GMT\u0026#34;, \u0026#34;opc-content-md5\u0026#34;: \u0026#34;HkJF80OO/DlmKI7usBCg4Q==\u0026#34; }, \u0026#34;index.html\u0026#34;: { \u0026#34;etag\u0026#34;: \u0026#34;883369a7-4701-4bd9-8351-3fbaada61fbe\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:05:05 GMT\u0026#34;, \u0026#34;opc-content-md5\u0026#34;: \u0026#34;B8Iehaa6j58u+qMiWnNldg==\u0026#34; } } } 결과 확인\n업로드 소스 폴더\n업로드 된 Objects\n폴더안의 파일은 그림처럼 업로드 되었습니다. 다만 cli로 업로드 된 파일은 application/octet-stream 타입임을 알 수 있습니다.\nMultipart 업로드 큰 사이즈의 파일, 예를 들어 Custom Image 파일을 업로드할 때 업로드 시간이 많이 걸리게 됩니다. 이런경우 단일 파일을 여러 파일로 나누어 병렬로 업로드 하면 더 빠르게 업로드 할 수 있습니다. 또한 전송시 네트워크 에러 발생시 에러난 파트별도 재시도를 하여 보다 안정적으로 큰 파일을 업로드 할 수 있습니다.\noci os object put -ns \u0026lt;object_storage_namespace\u0026gt; -bn \u0026lt;bucket_name\u0026gt; --file \u0026lt;file_location\u0026gt; --name \u0026lt;object_name\u0026gt; --part-size \u0026lt;upload_part_size_in_MB\u0026gt; --parallel-upload-count \u0026lt;maximum_number_parallel_uploads\u0026gt; Windows 실행예시\n2.28GB 파일을 100 MB 단위로 나누어서 동시 5개로 병렬로 업로드하는 예시로 3분 57초 걸림.\nPS D:\\ForObjectStorage\u0026gt; date 2022년 1월 18일 화요일 오후 12:25:22 PS D:\\ForObjectStorage\u0026gt; oci os object put -bn ExampleBucketForCLI --file big_file.iso --part-size 100 --parallel-upload-count 5 Upload ID: 3b38b958-3010-4d6d-5e84-55d189b787cc Split file into 24 parts for upload. Uploading object [####################################] 100% { \u0026#34;etag\u0026#34;: \u0026#34;2217a0e6-2ae3-4ae8-9507-03d2f36a38ec\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:29:18 GMT\u0026#34;, \u0026#34;opc-multipart-md5\u0026#34;: \u0026#34;mnHbvK3Eed2+o54P1P7VgA==-24\u0026#34; } PS D:\\ForObjectStorage\u0026gt; date 2022년 1월 18일 화요일 오후 12:29:19 PS D:\\ForObjectStorage\u0026gt; 결과확인\n참고사항\nOCI CLI \u0026ndash;no-multipart 옵션 설명에서 처럼 128MiB이상이면, 옵션이 없어도 기본적으로 다중 분할하여 업로드 됩니다.\nBy default, files above 128 MiB will be transferred in multiple parts, then combined.\nPS D:\\ForObjectStorage\u0026gt; oci os object put -bn ExampleBucketForCLI --file big_file.iso Upload ID: 75e4106e-bf1f-ffd0-5630-c78b68b6168b Split file into 19 parts for upload. Uploading object [####################################] 100% { \u0026#34;etag\u0026#34;: \u0026#34;cf5d681b-90a3-409c-b732-1e6c6028c046\u0026#34;, \u0026#34;last-modified\u0026#34;: \u0026#34;Tue, 18 Jan 2022 03:37:46 GMT\u0026#34;, \u0026#34;opc-multipart-md5\u0026#34;: \u0026#34;eFqxsX7ROrZUQ0HrQYI/zw==-19\u0026#34; } ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/2/2/","tags":["object storage","CLI","upload"],"title":"14.1.2.2 CLI를 통한 파일 업로드"},{"categories":null,"contents":"12.2 Monitoring Alarm 만들기 Compute 인스턴스에 설정한 모니터링 중에서 CPU 사용률이 지정한 수치에 이르면 메일을 발송하도록 알람을 생성해 보겠습니다.\nOCI 콘솔에서 내비게이션 메뉴의 Observability \u0026amp; Management \u0026gt; Monitoring \u0026gt; Alarm Definition에서 정의 자원 모니터링 메트릭의 옵션 메뉴에서 정의 위 두 가지 방법이 있지만, 본 문서에서는 선택한 모니터링 메트릭으로 Alarm의 메트릭이 자동으로 설정되어 사용이 쉬운 두 번째 방법으로 진행하겠습니다.\n메트릭에서 Alarm 만들기 앞서 설정한 Compute 인스턴스의 Resources \u0026gt; Metrics 선택\nCPU Utilization의 옵션에서 Create an Alarm on the Query 클릭 Alarm 정의 복잡한 Alarm 조건은 고급모드의 Query로 가능하며 아래는 기본모드로 UI에 선택하는 방식입니다.\nAlarm 정의\nAlarm Name: 만들 Alarm 이름 입력, 예) CPU Alarm Alarm severity: 레벨을 선택, 일단은 기본값인 Critical 선택 Alarm body: Alarm 메시지로, 이후에 Email 전달할 메시지를 입력 Metric description\nMetric name: CpuUtilization을 선택합니다. Metric dimensions\n선택한 Compartment내 모든 Compute 인스턴스를 대상으로 하기 위해 추가적인 조건은 지정하지 않습니다. Trigger rule\n알람을 설정할 조건을 지정합니다. 앞서 설정한 메트릭 기준(CPU 사용룔이 1분 평균)이 50% 이상일 때 Notification 정의 Notification 대상 선택\n알람이 기동할 때 통지할 Notification 서비스의 대상 Topic을 지정합니다. 이미 Topic이 있는 경우에 선택하면 되지만, 없는 경우 만들기 위해 Create a topic 클릭\n대상 Topic 만들기\nDestination으로 만든 Topic 선택된 것을 확인합니다.\n아래쪽 Save alarm 클릭\nNotification - EMail 구독 확인 앞서 Topic의 구독 프로토콜을 이메일로 정하면 대상 이메일로 구독 확인 메일이 발송됩니다.\n수신된 메일을 확인하고 인증 링크를 클릭합니다.\n구독 확인 창\nNotification - EMail Topic 상태 확인 앞서 생성한 CPU Alarm의 Notification 대상 클릭\n이메일 구독이 Active 상태임을 확인\nAlarm 발생 테스트 모니터링과 Alarm을 설정한 Compute 인스턴스에 SSH로 접속합니다.\nstress 툴 설치\n# EPEL(Extra Packages for Enterprise Linux) 활성화 sudo yum-config-manager --enable ol8_developer_EPEL # 테스트 툴 설치 sudo yum install -y stress stress 수행\nsudo stress --cpu N --timeout N 실행 예시 사용할 Compute Instance의 CPU에 갯수에 맞춰 조정합니다.\n[opc@web-server-1 ~]$ sudo stress --cpu 2 --timeout 60 stress: info: [69871] dispatching hogs: 2 cpu, 0 io, 0 vm, 0 hdd stress: info: [69871] successful run completed in 60s Alarm 발생 확인 만든 Alarm Definition 화면 아래쪽에 발생한 내역을 확인할 수 있습니다. 대상내 인스턴스 중에 부하를 준 한 인스턴스에 에러 알람이 발생하였습니다.\nAlarm 메일 수신 확인\n조건이 만족하여 발생한 Alarm과, 조건이 해제되었을 때 2건의 메일이 수신되었습니다.\n아래와 같이 설정한 내용을 바탕으로 발생한 내용을 알려줍니다.\n","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/2/","tags":["monitoring","alarm"],"title":"12.2 Monitoring Alarm 만들기"},{"categories":null,"contents":"12.3.1 Compute VM에 Grafana 설치 Compute Instance 만들기 Compute Instance 생성 화면으로 이동합니다.\n이미지 소스를 Oracle Linux 목록에 있는 Oracle Linux Cloud Developer Image 이미지로 선택합니다.\nOracle Cloud Developer Image는 OCI CLI 등 개발툴이 이미 설치되어 있습니다.\nhttps://docs.oracle.com/en-us/iaas/oracle-linux/developer/index.htm\nCompute 인스턴스를 생성합니다.\n생성된 Compute 인스턴스의 OCID를 복사해 둡니다.\n그라파나가 설치된 Compute Instance에 권한 설정하기 Compute 인스턴스에 설치된 그라파나 OCI 플러그인 OCI API를 통해 메트릭 정보 등을 가져오기 위해서는 관련 권한이 필요합니다. Compute 인스턴스에게 주는 권한은 Instance Principal을 설정해야 합니다. 그 절차는 Compute 인스턴스를 Dynamic Group으로 지정하고, 해당 Group에 권한을 주는 방식입니다.\nDynamic Group 설정 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Domains로 이동한 후, Default Domain을 클릭합니다.\n메뉴에서 Dynamic groups로 이동합니다.\nCreate Dynamic Group을 클릭합니다.\n룰 작성 부분 오른쪽 Rule builder를 클릭하여 툴을 통해 룰을 추가합니다.\n매칭 기준: Instance OCID\nValue: 복사해둔 Compute 인스턴스의 OCID\nDynamic Group을 생성합니다.\nName: 예) grafana-instance-dynamic-group\nPolicy 설정 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Policies로 이동합니다.\nCreate Policy을 클릭합니다.\n다음 두 권한을 가지는 Policy를 생성합니다.\nallow dynamic-group grafana-instance-dynamic-group to read metrics in tenancy allow dynamic-group grafana-instance-dynamic-group to read compartments in tenancy Policy 생성화면\nOCI CLI 접속 확인 SSH로 앞서 생성한 Compute 인스턴스로 접속합니다.\n다음명령으로 Instance Principal 방식으로 접속합니다. OCI CLI config 파일을 설정하지 않았는데 접속되는 것을 확인 할 수 있습니다.\noci os ns get --auth instance_principal 접속 예시\n[opc@grafana ~]$ oci os ns get --auth instance_principal { \u0026#34;data\u0026#34;: \u0026#34;xxxxxxxxxxxx\u0026#34; } Grafana 설치 Compute 인스턴스에 접속하여 다음 명령들을 순서대로 실행하여 Grafana를 설치합니다.\n참고: https://grafana.com/grafana/download?edition=oss 2022년 1월 기준 8.3.3, 8.2.7이 설치후 접속 오류가 발생하여 8.1.8을 설치하여 진행하였습니다. wget https://dl.grafana.com/oss/release/grafana-9.5.1-1.x86_64.rpm sudo yum install -y grafana-9.5.1-1.x86_64.rpm sudo systemctl daemon-reload sudo systemctl start grafana-server sudo systemctl status grafana-server OCI Metrics Plugin for Grafana 설치\nsudo grafana-cli plugins install oci-metrics-datasource Grafana 포트 개방\nOS 레벨에서 포트를 개방합니다.\nsudo firewall-cmd --permanent --add-port=3000/tcp sudo firewall-cmd --reload sudo systemctl restart grafana-server Grafana가 설치된 Compute 인스턴스가 속한 Security List의 Ingress에 3000번 포트를 개방합니다.\nGrafana 로그인\n브라우저를 통해 Grafana가 설치된 Compute 인스턴스에 3000포트로 접속합니다. 기본 접속 계정은 admin/admin 입니다.\n참고 사이트 https://grafana.com/blog/2019/02/25/oracle-cloud-infrastructure-as-a-data-source-for-grafana/ https://blogs.oracle.com/cloudnative/data-source-grafana https://grafana.com/grafana/plugins/oci-metrics-datasource/ https://github.com/oracle/oci-grafana-metrics/blob/master/docs/linux.md ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/3/1/","tags":["monitoring","grafana"],"title":"12.3.1 Compute VM에 Grafana 설치"},{"categories":null,"contents":"12.3.2 Grafana 구성 Data Source 등록 Grafana에 로그인 합니다.\n왼쪽 메뉴에서 Administration \u0026gt; Data sources를 클릭합니다.\nAdd data source를 클릭합니다.\n데이터 소스 중에서 Oracle Cloud Infrastructure Metrics를 선택 OCI 데이터 소스 설정\nEnvironment: OCI Instance 선택 Default Region: 디폴트로 사용할 Region 선택 Save \u0026amp; Test를 클릭하여 접속이 잘 되는지 확인합니다.\n대시보드 만들기 왼쪽 메뉴에서 Dashboard를 클릭합니다.\n새 대쉬보드 생성을 위해 New \u0026gt; New Dashboard를 클릭합니다. Add visualization을 클릭합니다.\n화면 아래 Query 항목에서 Data source를 앞서 추가한 Oracle Cloud Infrastructure Metrics를 선택하고 Region부터 Metric까지 순서대로 하나씩 설정합니다. 작성한 그래프를 저장합니다.\n템플릿 기능 사용하기 오른쪽 위의 대시보드 설정 아이콘을 클릭합니다.\n변수를 추가하기 위해 왼쪽 메뉴에서 Variables를 선택하고 Add variable 클릭 새 변수를 추가합니다. Preview of values에서 보는 것 처럼 쿼리 결과를 변수로 조회해서 대시보드에서 사용할 수 있습니다.\nName: region\nQuery: regions()\n아래 2개 변수를 추가합니다.\nName Query region regions() compartment compartments() 대쉬보드로 돌아 가면 위쪽에 선택항목으로 추가된 변수가 보입니다. 실제 region과 compartment가 조회되어 나열됩니다.\n대쉬보드 Setting에서 아래 변수도 추가합니다.\nregion과 compartment를 선택하지 않으면, 아래 변수들은 연관 변수로 변수 추가시 최초 조회시 에러가 날 수 있습니다.\nName Query namespace namespaces($region,$compartment) resourcegroup resourcegroups($region, $compartment, $namespace) metric metrics($region,$compartment, $namespace, $resourcegroup) 변수 추가 완료\n대쉬보드를 저장합니다.\n처음 추가한 패널을 다시 Edit 합니다.\n아래쪽 쿼리의 파라미터로 앞서 생성한 변수로 변경합니다. 예) Region: $region. 그러면 대시보드에서 위쪽에 보이는 드랍다운 리스트에서 값을 선택하면, 변수값이 변경되어 원하는 그래프로 조회됩니다. 드랍다운 리스트에서 metric은 모니터링되는 데이터가 없을 경우 리스트가 안 보일 수 있으니, 보이지 않더라도 오류가 아니니 오해하지 않기 바랍니다. 우측 상단 Apply를 누르고 대쉬보드를 저장합니다.\n편집화면에서 패널 크기을 화면에 맞도록 크기를 조정합니다.\n작성된 대시보드에 그림과 같이 조회할 수 있습니다. 참고 사이트 https://grafana.com/blog/2019/02/25/oracle-cloud-infrastructure-as-a-data-source-for-grafana/ https://blogs.oracle.com/cloudnative/data-source-grafana oci-grafana-metrics/using.md at master · oracle/oci-grafana-metrics (github.com) ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter12/3/2/","tags":["monitoring","grafana"],"title":"12.3.2 Grafana 구성"},{"categories":null,"contents":"13.2 Autoscaling 구성 Autoscaling을 하기 위해서는 다음 사항이 필요합니다.\nInstance Pool: Autoscaling은 Instance Pool에서 현재 인스턴스 수를 설정된 메트릭에 따라 자동으로 변경하는 것으로 Instance Pool이 필요합니다. Monitoring 활성화: Compute 인스턴스는 모니터링하여 자원상태를 기반으로 하는 것으로 Monitoring이 활성화되어야 합니다. Service Limit: Autoscaling으로 새로운 인스턴스를 기동할 관련 자원이 충분해야 합니다. Step #4: Instance Pool을 이용하여 어떤 기준으로 확장/축소할지 Autoscaling Configuration 생성 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instance Pools 항목으로 이동합니다.\nAutoscaling 할 Instance Pool의 상세화면에서 More Actions \u0026gt; Create autoscaling configuration 클릭 Autoscaling Configuration\nName: 이름 입력, 예) web-server-autoscaling-config\nInstance Pool: 사용할 Instance Pool 선택, Instance Pool에서 Create Autoscaling Configuration로 이동했을 경우 자동으로 선택됨.\nAutoscaling Policy\nMetric-based autoscaling: 메트릭 값에 따라 인스턴스의 갯수를 자동으로 조절합니다. 현재는 CPU, 메모리 사용량를 기반으로 설정 가능합니다. Schedule-based autoscaling: 설정한 스케줄에 따라 정의된 인스턴스 갯수로 변경하거나, 모든 인스턴스의 상태를 변경합니다. 메트릭 기반 오토스케일 설정기반으로 생성합니다.\nAutoscaling policy name: 이름 입력, 예, web-server-autoscaling-policy\nCooldown in seconds: Cooldown 시간, 기본 300초\nAutoscaling은 Cooldown 기간동안 메트릭을 평가하여 Cooldown 시간이 지나면, 다시 Autoscaling, 즉 Instance Pool의 사이즈를 조정할 지 여부를 정합니다. Performance metric: 기준 메트릭으로 현재는 CPU Utilization, Memory Utilization을 지원\nScale-out rule: 지정한 메트릭기반의 인스턴스를 늘리는 기준 설정\nScale-in rule: 지정한 메트릭기반의 인스턴스를 줄이는 기준 설정\nScaling limits: Scale In \u0026amp; Out시의 유지할 최소 인스턴스, 최대 인스턴스를 지정\n테스트를 위해 1 인스턴스에서 CPU 부하 50% 초과 발생시 인스턴스 추가하고, CPU 부하 10% 미만시 다시 1개로 줄이는 정책으로 아래와 같이 설정합니다.\n(참고) 스케줄 기반 오토스케일 설정\nScale pool size: 설정한 스케줄에 따라 인스턴스 수를 지정할 수 있습니다. Change lifecycle state of all instances: 설정한 스케줄에 따라 인스턴스의 상태를 변경할 수 있습니다. 주말에 인스턴스 종료, 월요일 아침 시작 형태가 가능합니다. Another policy: 메트릭 기반과 달리 관리자가 지정한 스케줄에 따라 적용하는 것이기 때문에, 스케줄에 따른 여러 개의 규칙을 설정할 수 있습니다. 테스트 오토 스케일 생성완료\n화면에서 보듯이 Edit를 클릭하여 생성 이후에도 규칙을 변경할 수 있습니다. 메트릭 기반 스케줄을 스케줄 기반 스케줄로 변경하는 것은 불가합니다.\n","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter13/2/","tags":["autoscaling"],"title":"13.2 Autoscaling 구성"},{"categories":null,"contents":"8.2 FSS(File Storage Service)를 Linux VM에 마운트하여 사용하기 Subnet 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Virtual Cloud Networks 항목으로 이동합니다.\n앞서 만든 VCN인 클릭\nCreate Subnets 클릭\n생성정보 입력: 진한 글씨 항목만 입력하고 나머지는 기본값을 사용합니다.\nName: 이름 입력, fss-subnet Subnet Type: 기본값인 REGIONAL을 선택 REGIONAL: Region에 글로벌하게 있는 서브넷으로 다른 AD로 FailOver시에도 IP를 그대로 사용가능한 이점이 있음. AVAILABILITY DOMAIN-SPECIFIC: AD내에 만들어지는 서브넷 CIDR Block: 10.0.2.0/24 Route Table: Default Route Table 선택 Subnet Access: Public Subnet 선택 DHCP Options: Default DHCP Options 선택 최하단으로 스크롤하여 Create Subnet 클릭\n생성완료\nFile Storage Service용 Security List 만들기 ublic Subnet(10.0.0.0/24) 상의 VM에서 ffs-subnet(10.0.2.0/24) 상의 File Storage Service를 사용하도록 하기위해서는 관련 포트 개방이 필요합니다. 이를 위한 Security List를 만들겠습니다.\n왼쪽 Resources \u0026gt; Security Lists 클릭\nCreate Security List 클릭\n생성정보 입력\nName: 이름 입력, fss-security-list\nAllow Rules for Ingress: Public Subnet(10.0.0.0/24) 상의 VM 대상으로 열도록 아래 표 대로 입력\nStateless Source IP Protocol Source Port Range Destination Port Range No 10.0.0.0/24 TCP All 2048-2050 No 10.0.0.0/24 TCP All 111 No 10.0.0.0/24 UDP All 2048 No 10.0.0.0/24 UDP All 111 생성완료\nSubnet에 Security List 적용 앞서 만든 fss-subnet의 상세 페이지로 이동합니다.\nfss-subne의 Security List에 방금 만든 fss-security-list를 추가합니다.\nFile System 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; File Storage \u0026gt; File Systems 항목으로 이동합니다.\nCreate File System 클릭\n생성정보 입력\n기본값을 그대로 사용해도 되지만 편의상 이름을 변경합니다.\nFile System Information\nName: 우측 Edit Details를 클릭하여 원하는 이름으로 변경, fss-system 입력 Export Information : 기본값 사용\nMount Target Information\nNew Mount Target Name: 우측 Edit Details를 클릭하여 원하는 이름으로 변경, fss-mount-target 입력 Virtual Cloud Network: 사용할 VCN Subnet: 앞서 만든 File Storage를 위해 생성한 Subnet 선택, 예, fss-subnet File System 마운트 생성된 File System의 Export Path(예, /fss-system)를 클릭합니다.\n위쪽 Mount Commands를 클릭합니다.\n클라이언트에서 FSS를 Mount하기 위해 필요한 명령들을 확인합니다.\n이미지에서 OS를 선택합니다. 아래 그림은 Oracle Linux 기준입니다.\n이전 실습에서 만든 Public Subnet(10.0.0.0/24)상의 Compute Instance에 접속합니다.\n앞서 OCI 콘솔에서 확인한 Mount Commands를 순서대로 실행합니다.\n명령 예시\n# NFS 클라이언트 설치 sudo yum install nfs-utils # 마운트 폴더 생성 sudo mkdir -p /mnt/fss-system # 마운트 sudo mount 10.0.2.236:/fss-system /mnt/fss-system 결과 확인 및 테스트\n# 마운트 결과 확인 df -h # 권한 변경 sudo chmod 777 /mnt/fss-system # 파일 생성 확인 echo \u0026#39;Hello File Storage\u0026#39; \u0026gt; /mnt/fss-system/hello1.txt ls -la /mnt/fss-system/ 결과 예시\n[opc@examplelinuxinstance ~]$ # NFS 클라이언트 설치 [opc@examplelinuxinstance ~]$ sudo yum install nfs-utils Last metadata expiration check: 3:43:52 ago on Mon 08 May 2023 05:26:26 AM GMT. Package nfs-utils-1:2.3.3-57.el8_7.1.x86_64 is already installed. Dependencies resolved. Nothing to do. Complete! [opc@examplelinuxinstance ~]$ # 마운트 폴더 생성 [opc@examplelinuxinstance ~]$ sudo mkdir -p /mnt/fss-system [opc@examplelinuxinstance ~]$ # 마운트 [opc@examplelinuxinstance ~]$ sudo mount 10.0.2.236:/fss-system /mnt/fss-system [opc@examplelinuxinstance ~]$ # 마운트 결과 확인 [opc@examplelinuxinstance ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 1.8G 0 1.8G 0% /dev tmpfs 1.8G 0 1.8G 0% /dev/shm tmpfs 1.8G 81M 1.7G 5% /run tmpfs 1.8G 0 1.8G 0% /sys/fs/cgroup /dev/mapper/ocivolume-root 36G 8.7G 27G 25% / /dev/sda2 1014M 334M 681M 33% /boot /dev/sda1 100M 5.1M 95M 6% /boot/efi /dev/mapper/ocivolume-oled 10G 120M 9.9G 2% /var/oled tmpfs 363M 0 363M 0% /run/user/0 tmpfs 363M 0 363M 0% /run/user/988 tmpfs 363M 0 363M 0% /run/user/1000 10.0.2.236:/fss-system 8.0E 0 8.0E 0% /mnt/fss-system [opc@examplelinuxinstance ~]$ # 권한 변경 [opc@examplelinuxinstance ~]$ sudo chmod 777 /mnt/fss-system [opc@examplelinuxinstance ~]$ # 파일 생성 확인 [opc@examplelinuxinstance ~]$ echo \u0026#39;Hello File Storage\u0026#39; \u0026gt; /mnt/fss-system/hello1.txt [opc@examplelinuxinstance ~]$ ls -la /mnt/fss-system/ total 9 drwxrwxrwx. 2 root root 1 May 8 09:10 . drwxr-xr-x. 4 root root 36 May 8 09:10 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt drwxrwxrwx. 2 root root 0 May 8 09:10 .snapshot 마운트 정보를 확인합니다.\n$ findmnt /mnt/fss-system TARGET SOURCE FSTYPE OPTIONS /mnt/fss-system 10.0.2.236:/fss-system nfs rw,relatime,vers=3,rsize=1048576,wsize=1048576,namlen=255,hard,proto=tcp,timeo=600,retrans= VM 재기동 후에도 자동으로 마운트 하려면 /etc/fstab를 업데이트합니다.\n# # /etc/fstab # Created by anaconda on Tue Jan 17 19:39:49 2023 ... 10.0.2.236:/fss-system /mnt/fss-system nfs defaults,nofail,nosuid,resvport 0 0 ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/2/","tags":["file storage","security list","subnet"],"title":"8.2 FSS를 Linux VM에 마운트하여 사용하기"},{"categories":null,"contents":"9.2 인스턴스에서 Custom Image 만들기 중요 이미지를 만드는 동안 인스턴스가 몇 분간 오프라인 상태가 됩니다. Custom Image를 만드는 동안 인스턴스를 STOP 시키는 것을 권장하며, 실행 중인 상태에서 진행하게 되면, 강제로 중지됩니다. 중지로 인해 데이터에 문제가 발생할 수 있습니다 Custom Image를 만들 대상 Instance에 Block Volume이 장착되어 있더라도, Custom Image를 만들게 되면, Boot Volume만 포함되어 이미지가 만들어집니다.\nCustom Image 만들기 테스트 VM에 접속하여, Custom Image 만들기 전에 흔적을 만듭니다.\n[opc@examplelinuxinstance ~]$ echo \u0026#39;See you soon\u0026#39; \u0026gt; hello.txt [opc@examplelinuxinstance ~]$ cat hello.txt See you soon Custom Image를 생성하기 전에 반드시 먼저 OS 레벨에서 종료합니다.\n[opc@examplelinuxinstance ~]$ sudo su [root@examplelinuxinstance opc]# shutdown now Connection to 140.238.29.108 closed by remote host. Connection to 140.238.29.108 closed. OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nCompute Instance 목록에서 대상 인스턴스를 클릭합니다.\nOS 레벨에서 종료 여부를 다시 한번 확인한후 콘솔에서 Stop 명령으로 종료합니다.\nInstance가 중지되면 Create custom image를 클릭합니다.\nCustom Image를 저장할 Compartment를 선택하고, 이름을 입력 후 Create custom image를 클릭합니다.\nName: 예) ExampleLinuxCustomImage 아래 경고문에서 Custom Image 생성 전에 OS에서 종료 후, 인스턴스를 종료할 것을 권고하고 있습니다. 이미지 생성 중입니다.\n인스턴스 목록 화면으로 돌아가 왼쪽 Custom Images 메뉴로 가면 현재 Custom Image 목록을 확인할 수 있습니다.\nCustom Image를 새 Instance 생성하기 방법 1 Custom Images 목록에서 원하는 이미지를 선택하고, 우측 액션 메뉴에서 Create Instance를 클릭하면, 해당 이미지를 기반으로 Instance 생성화면으로 이동합니다.\n이미지가 선택된 상태로 Compute 인스턴스 생성화면이 뜹니다. 그 외 필요한 정보를 입력하고 인스턴스를 생성합니다.\n방법 2 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다. Create Instance 클릭하여 인스턴스 생성을 시작합니다. Image and Shape에서 Change Image를 통해 이미지를 변경합니다. My images \u0026gt; Custom images에서 사용가능한 커스텀 이미지를 확인할 수 있습니다. 사용할 Custom Image를 선택하고 Select Image을 클릭합니다. 그 외 필요한 정보를 입력하고 인스턴스를 생성합니다. 검증 Custom Image를 기준으로 인스턴스가 생성되었습니다.\n생성된 인스턴스에 접속하여 이전에 작성한 파일이 있는 지 확인합니다.\n$ ssh opc@152.69.xxx.xxx FIPS mode initialized The authenticity of host \u0026#39;152.69.xxx.xxx (152.69.xxx.xxx)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:d3Mv5x9ZJ9/cUqi+3n94Q9K1Acf6BE48V8N42GVyUak. ECDSA key fingerprint is SHA1:nMSNlmgEb++egZCRWp+XyY6AnCI. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;152.69.xxx.xxx\u0026#39; (ECDSA) to the list of known hosts. Activate the web console with: systemctl enable --now cockpit.socket Last login: Tue May 9 01:30:32 2023 from 132.145.85.85 [opc@examplelinuxinstance-fromcustomimage ~]$ ls hello.txt [opc@examplelinuxinstance-fromcustomimage ~]$ cat hello.txt See you soon ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter09/2/","tags":["os image","custom image","image"],"title":"9.2 인스턴스에서 Custom Image 만들기"},{"categories":null,"contents":"7.2 Bucket에 파일 올리기 Object Storage는 단일 파일 기준 최대 50 GB까지 지원합니다. OCI Console의 메시지 기준으로는 2 GiB까지만 올릴 수 있습니다. 그 이상 더 큰 파일은 SDK 또는 API를 통해 올릴 수 있습니다.\nBucket 상세 정보를 보기 위해 이름을 클릭합니다.\nUpload를 클릭합니다.\n다이얼로그가 뜨면 파일을 드래그 앤 드랍하거나 브라우저 파일 업로그 기능을 통해 올릴 수 있습니다. select files를 클릭합니다.\n원하는 파일을 선택합니다.\n다이얼로그 화면에서 Upload를 클릭합니다.\nObject Name Prefix: Prefix를 입력하면 파일명 앞에 붙으며 아래와 같이 / 가 있으면 콘솔에서 폴더로 표현됩니다. Storage Tier: Bucket: 기본에서 필요하면 다른 값으로 변경합니다. 업로드 확인후 Close를 클릭합니다.\n그림과 같이 파일이 업로드 되었고, Prefix에 /를 기준으로 폴더화 되었습니다. Object 우측 액션 메뉴를 통해 상세정보를 확인합니다.\n상세정보 화면에서 URL 및 메타정보를 확인할 수 있습니다. 또한 [Download] 받을 수도 있습니다. URL 경로 포맷은 다음과 같습니다.\n생성된 Bucket은 기본적으로 Private Bucket으로 지금 바로 URL로 파일을 내려받지는 못합니다. 추가 설정 또는 인증을 통해 내려받을 수 있습니다. https://objectstorage.\u0026lt;region_name\u0026gt;.oraclecloud.com/n/\u0026lt;object_storage_namespace\u0026gt;/b/\u0026lt;bucket\u0026gt;/o/\u0026lt;object_name\u0026gt; ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/2/","tags":["object storage","bucket"],"title":"7.2 Bucket에 파일 올리기"},{"categories":null,"contents":"4.2 Reserved Public IP 할당하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nInstance 목록 중에서 고정 IP를 부여할 Instance의 이름을 클릭하여 상세정보로 이동합니다.\n왼쪽 아래의 Resources \u0026gt; Attached VNICs를 클릭합니다.\n연결된 가상 네트워크 인터페이스 카드(VNIC) 목록이 보이며, 최초 생성시 기본 생성된 Primary VNIC이 아래와 같이 하나 보입니다.\nPrimary VNIC의 이름을 클릭합니다.\n왼쪽 아래의 Resources \u0026gt; IPv4 Addresses를 클릭합니다.\n아래 쪽에 보면 할당된 IP 주소가 보입니다. Public IP 주소를 보면 임시 주소인 Ephemeral Public IP이 부여된 걸 확인할 수 있습니다.\n수정을 위해 우측 액션 메뉴에서 Edit를 클릭합니다.\nReserved IP 밑에 적힌 \u0026ldquo;Not allowed until you unassign the existing public IP\u0026rdquo; 안내 문구처럼 예약한 Public IP를 할당하기 위해서는 현재 부여된 Ephemeral Public IP 할당 해지를 먼저 해야 합니다. No public IP로 먼저 업데이트 합니다.\nPublic IP 주소가 할당 해지되었습니다. 수정을 위해 다시 우측 액션 메뉴에서 Edit를 클릭합니다.\nReserved public ip를 선택하고 앞서 생성한 Reserved Public IP를 목록에서 할당한 예약 IP를 선택하고 업데이트합니다.\nPublic IP 주소를 보면 지정한 Reserved Public IP로 부여된 걸 확인할 수 있습니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; IP Management \u0026gt; Reserved Public IPs 항목으로 이동합니다.\n사용한 Reserved Public IP의 상태가 Assigned로 변경된 것을 확인할 수 있습니다.\n","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter04/2/","tags":["reserved public ip"],"title":"4.2 Reserved Public IP 할당하기"},{"categories":null,"contents":"6.2 사용자 만들기 팁 OCI는 접속할 수 있는 사용자는 크게 IDCS 사용자와 OCI 사용자가 있습니다. OCI는 Identity Cloud Service와 기본적으로 연결설정이 되어 있어, IDCS의 사용자가 Federated 되어 로그인할 수도 있습니다. 그리고 IDCS는 등록되어 있지 않고, OCI에만 있는 자체 사용자도 있습니다. 여기서 내용은 OCI 자체 사용자만을 대상으로 합니다. OCI 관리자 추가하기 Step 1. OCI 관리자 추가 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Users 항목으로 이동합니다.\n사용자 추가를 위해 Create User 클릭\n사용자 정보 입력\nUser Type: IAM User를 선택합니다.\nName: 사용자 이름 입력, Tenancy내에서 고유한 이름\nDescription: 설명 입력\nEmail: 이메일 입력, 암호 분실시 Forget Password 기능을 통해 암호 초기화를 위해 필요합니다.\nCreate 클릭\n사용자 상세정보에서 아래쪽 Resources \u0026gt; Groups 클릭\nAdd User to Group을 클릭하여 Administrators 그룹에 추가합니다.\n사용자 상세정보에서 위쪽에 있는 Create/Reset Password를 클릭합니다. 다이얼로그가 뜨면 다시 Create/Reset Password를 한번 더 클릭합니다.\n사용자를 위한 One Time Password가 만들어 졌습니다. IDCS 사용자는 IDCS Console에서 자동 이메일 발송기능을 제공하나 OCI 로컬 사용자에 대해서는 아직 OTP 메일 발송 기능을 제공하고 있지 않습니다. 패스워드를 사용자에게 이메일로 전달합니다.\nStep 2. OCI 사용자 최초 로그인 하기 등록한 이메일로 검증 메일이 발송됩니다. 링크를 클릭하여 로그인합니다.\nOracle Cloud Infrastructure Direct Sign-In 으로 로그인합니다.\n최초 로그인 후 패스워드를 변경합니다.\n등록된 이메일이 활성화 되었습니다. 이제 로그인 화면의 Forget password에서 이메일을 입력하면 OTP를 메일로 수신할 수 있습니다. OCI 일반 사용자 추가하기 Step 1. OCI 일반 사용자 추가 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; User 항목으로 이동합니다.\n사용자 추가를 위해 Create User 클릭\n사용자 정보 입력\nUser Type: IAM User를 선택합니다. Name: 사용자 이름 입력, Tenancy내에서 고유한 이름 Description: 설명 입력 Email: 이메일 입력, 암호 분실시 Forget Password 기능을 통해 암호 초기화를 위해 필요합니다. Create 클릭\n관리자 생성과 동일하게 사용자 상세정보에서 위쪽에 있는 Create/Reset Password를 클릭하여 사용자를 위한 One Time Password를 생성합니다. IDCS 사용자는 IDCS Console에서 자동 이메일 발송기능을 제공하나 OCI 로컬 사용자에 대해서는 아직 OTP 메일 발송 기능을 제공하고 있지 않습니다. 패스워드를 사용자에게 이메일로 전달합니다.\nStep 2. OCI 일반 사용자 최초 로그인 하기 관리자 사용자와 동일하게 이메일 검증과 패스워드는 관리자에게 전달받은 OTP를 이용해 로그인하는 과정을 수행합니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\n왼쪽 아래 대상 Compartment 선택화면에서 앞서 생성한 root Compartment만 보이고 그외 Sandbox, Production 은 권한이 없어 보이지 않는 것을 알 수 있습니다.\nroot Compartment를 선택해도 추가 권한이 없어 Compute 인스턴스 조회조차 안되는 것을 알 수 있습니다.\n","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/2/","tags":["oci user"],"title":"6.2 사용자 만들기"},{"categories":null,"contents":"3.2 가상 네트워크 환경을 위한 VCN 만들기 Virtual Cloud Network(VCN) 이란 Virtual Cloud Network(VCN)은 가상 네트워크 환경을 제공합니다. 사용자의 요구에 맞게 VCN 및 관련 자원을 구성하여 여러 가지 형태의 가상 네트워크 환경을 구성할 수 있습니다. Subnet은 VCN의 하위 요소로 단일 Availability Domain 단위로 만들어집니다. Subnet은 Route Table에 따라 트래픽은 대상 경로로 이동하며, 또한 Subnet은 Security List로 들어오고 나가는 트래픽을 제어할 수 있습니다.\nVCN 만들기 팁 VCN을 만들 때 두 가지 방법을 제공합니다. VCN만 만들기, VCN Wizard를 통해 VCN과 관련자원 함께 만들기를 제공합니다. 상세하게 구성하는 방법은 별도 문서를 참고하시고, 본 장에서는 VCN Wizard 통해 만들어진 자원을 바탕으로 빠르게 Compute Instance를 사용하고 그 개념을 이해하도록 합니다. OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Virtual Cloud Networks 항목으로 이동합니다.\n왼쪽 아래 Compartment에서 oci-hol-xx를 클릭합니다. 안 보일 경우 브라우저를 새로고침합니다.\nStart VCN Wizard을 선택합니다.\nCreate VCN with Internet Connectivity를 선택하고, Start VCN Wizard를 클릭합니다. 인터넷 연결이 되는 관련 자원들을 포함하여 VCN이 만들어지게 됩니다.\n선택한 것은 그림에서 설명하는 것 처럼, Public, Private Subnet을 포함하고, 각각 Internat Gateway, NAT Gateway를 통해 인터넷과 통신합니다. 또한 Service Gateway를 OCI 서비스를 연동하기 위해 Oracle Services Network과 통신합니다. 새 VCN의 이름(예, oci-hol-vcn)하고 사용할 Compartment를 선택합니다. 나머지는 기본값을 사용합니다.\nField Value VCN Name oci-hol-vcn Compartment 만든 Compartment 선택, 예, oci-hol-xx VCN CIDR Block 10.0.0.0/16 Use DNS Hostnames In This VCN Checked Public Subnet CIDR Block 10.0.0.0/24 Private Subnet CIDR Block 10.0.1.0/24 VCN 설정\n/16 부터 /30 까지 설정할 수 있습니다. IP Addresses Reserved for Use by Oracle을 참고하여 해당 범위를 제외하고 설정합니다. Use DNS Hostnames In This VCN 선택시 \u0026lt;hostname\u0026gt;.\u0026lt;subnet-DNS-label\u0026gt;.\u0026lt;VCN-DNS-label\u0026gt;.oraclevcn.com 형식으로 인스턴스의 FQDN이 포맷이 정해집니다. 참고 - DNS in Your Virtual Cloud Network Subnet 설정\n노트: 서브넷내의 주소 3개는 내부적으로 사용합니다\n예시 Field Value Comment Subnet CIDR 10.0.0.0/24 The first IP address 10.0.0.0 네트워크 주소 The last IP address 10.0.0.255 브로드캐스트 주소 The first host address 10.0.0.1 서브넷 디폴트 게이트웨이 주소 The remaining addresses 10.0.0.2 to 10.0.0.254 유저 사용 가능 주소 참고 문서 - Three IP Addresses in Each Subnet 설정후 Next을 클릭합니다.\n생성될 VCN과 관련 네트워크 자원들을 리뷰합니다. Gateway, Security List, Route Table이 추가적으로 만들어 지는 것을 볼 수 있습니다.\nCreate을 클릭합니다.\n인터넷 연결이 되는 관련 자원들을 포함하여 VCN이 만들어지게 됩니다.\nView Virtual Cloud Network 클릭하면, 생성된 VCN의 상세페이지로 이동하며, 만들어진 자원을 확인할 수 있습니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/2/","tags":["virtual cloud network","VCN"],"title":"3.2 가상 네트워크 환경을 위한 VCN 만들기"},{"categories":null,"contents":"3.6.2 Block Volume을 Instance에 장착하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\n앞서 생성한 대상 Instance의 이름을 클릭합니다.\nInstance 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Attached block volumes을 클릭합니다.\nAttached block volumes 버튼 클릭\n만든 Block Volume을 아래와 같이 부착합니다.\nVolume: Compartment안에 있는 리스트에서 원하는 Block Volume 선택\n앞서 만든 ExampleBlockVolume 선택 Device Path(옵션): 리스트에서 원하는 패스 선택 오라클 제공 Linux 계열 이미지를 사용하는 경우 장착될 경로를 지정할 수 있습니다. 인스턴스가 재시작하더라도 경로 변경되지 않도록 하기 위해 추가된 기능입니다. Attachment type: ISCSI, Paravirtualized 방식 중에 선택 가능하며, OCI가 자동으로 선택되는 기본 옵션 그대로 사용합니다.\niSCSI: 베어 메탈 인스턴스에 연결할 때는 iSCSI 방식이 유일한 옵션입니다. 볼륨이 부착되면, 컴퓨트 인스턴스에 로그인하여 추가적인 iscsiadm 명령들을 수행하여 iSCSI 연결을 구성해야 합니다.\nParavirtualized(반가상화): VM 인스턴스에 볼륨을 연결할 때는 추가적으로 반가상화 방식도 사용가능합니다. 반가상화 방식으로 볼륨이 부착되면, 바로 컴퓨트 인스턴스에서 사용할 수 있습니다. 추가적 명령을 실행할 필요가 없습니다. 그러나 가상화 오버헤드로 인해 더 큰 블록 볼륨에 대한 최대 IOPS 성능이 감소합니다. 자세한 내용은 VM Shapes for Paravirtualized Attached Volumes을 참조하세요.\nAccess: 단일 VM에서 사용할 것이므로 기본 Read/write 선택\nRead/write: 단일 VM에 부착하여 읽기/쓰기 가능 Read/write - shareable: 다중 VM에서 사용가능하나, 사용할 VM에서 추가적인 모듈을 설치해야 함. Read only - shareable: 다중 VM에서 사용가능하나, 읽기만 가능. 추가 모듈 설치 필요없음 선택한 설정으로 Attach 합니다.\n경고문구 확인\nBlock Volume이 VM에 부착되었지만, VM내부 OS 상의 작업이 필요합니다. 다음 장에서 이어 작업합니다.\n완료되면 Attached 상태로 되며, Instance에 장착이 완료되었습니다.\nAccess - Read/write - shareable 참고 공유 읽기/쓰기 Block Volume은 이후 다시 확인하기로 하고 콘솔에서 보이는 내용을 참고합니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/2/","tags":["block volume"],"title":"3.6.2 Block Volume을 Instance에 장착하기"},{"categories":null,"contents":"3.7.2 백업으로 새 Volume 만들기 Block Volume 백업본을 이용해 새로운 Block Volume을 생성하는 복구방식입니다. 복구된 Block Volume은 새로운 Block Volume을 장착하는 것과 같은 방법으로 Compute Instance에 장착하게 사용하면 됩니다.\n백업으로 새 Block Volume 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Block Storage \u0026gt; Block Volumes Backups 항목으로 이동합니다.\n전체 백업 목록을 확인할 수 있습니다.\n원하는 백업의 우측 액션 메뉴에서 Restore Block Volume을 클릭합니다.\n생성정보 입력화면은 처음 Block Volume 생성할 때랑 동일합니다. 생성정보를 입력합니다.\nName: 원하는 이름 입력\nCreate in Compartment: 현재 사용중인 Compartment가 기본으로 보입니다. 원하는 Compartment을 선택합니다.\nAvailability Domain: Compute Instance가 속한 Availability Domain을 선택합니다. Volume과 Instance는 반드시 같은 AD여야 합니다.\nSize: 이전 사이즈와 동일, 필요시 증설\nBackup Policies: 선택 안함.\nCross Region Replication: 여기서는 OFF\nEncryption: 지금은 OCI 제공 키 사용\nRestore Block Volume 클릭\n생성이 완료되면 Available 상태로 표시됩니다.\n복구된 Block Volume은 이제 다른 Block Volume과 동일한 Block Volume입니다. Block Volume을 장착하는 것과 같은 방법으로 장착해서 사용하면 됩니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/2/","tags":["block volume","backup"],"title":"3.7.2 백업으로 새 Volume 만들기"},{"categories":null,"contents":"3.8.2 백업으로 새 Boot Volume 만들기 Boot Volume 백업 복구 기능은 Block Volume 백업 복구 기능과 거의 같습니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Boot Volume 상세 페이지로 이동합니다.\nBoot Volume 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Boot Volume Backups를 클릭합니다.\n원하는 백업의 우측 액션 메뉴에서 Restore Boot Volume을 클릭합니다.\n생성정보 입력화면은 Block Volume 생성할 때랑 동일합니다. 생성정보를 입력합니다.\nName: 원하는 이름 입력\nCreate in Compartment: 현재 사용중인 Compartment가 기본으로 보입니다. 원하는 Compartment을 선택합니다.\nAvailability Domain: Compute Instance가 속한 Availability Domain을 선택합니다. Volume과 Instance는 반드시 같은 AD여야 합니다.\nSize: 이전 사이즈와 동일, 필요시 증설\nBackup Policies: 선택 안함.\nCross Region Replication: 여기서는 OFF\nEncryption: 지금은 OCI 제공 키 사용\nRestore Boot Volume 클릭\n생성이 완료되면 Available 상태로 표시됩니다.\n복구된 Boot Volume은 이제 다른 Boot Volume과 같은 방식으로 사용하면 됩니다. 인스턴스 생성시 사용할 수 있습니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/8/2/","tags":["boot volume","backup"],"title":"3.8.2 백업으로 새 Boot Volume 만들기"},{"categories":null,"contents":"2.2 OCI Console 접속하기 방법 #1. Welcome 이메일을 통한 접속 Free Tier Account 준비 완료 메일 수신되면 Oracle Cloud에 사인인을 클릭하여 OCI Cloud Console에 접속합니다.\n로그인 화면에서 Free Tier 신청시 입력한 관리자 이메일과 암호를 입력하여 로그인합니다.\n로그인 성공\n방법 #2. 브라우저로 직접 접속 Oracle Cloud에 로그인을 위해 브라우저를 통해 https://cloud.oracle.com으로 접속합니다.\n방법 #1: Oracle Identity Cloud Service 로그인\nOracle Identity Cloud Service로 선택합니다.\n로그인 화면에서 Free Tier 신청시 입력한 관리자 이메일과 암호를 입력하여 로그인합니다.\n로그인 완료\n로그인 성공후 프로파일을 확인하면 oracleidentitycloudservice/ 까지가 계정 이름인것을 알 수 있습니다. 방법 #2: OCI Direct Sign In\n아래쪽 OCI 직접 로그인 옵션으로 로그인합니다.\n로그인 완료\n로그인 성공후 프로파일을 확인하면 관리자 이름이 순수 계정 이름인것을 알 수 있습니다. ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/2/","tags":["oci console"],"title":"2.2 OCI Console 접속하기"},{"categories":null,"contents":"1.8.1.2 마이크로서비스 앱에 Istio 적용하기 istioctl, Helm 또는 매뉴얼로 설치가 가능합니다. 여기서는 OKE 문서에서 예시로 설명하고 있는 istioctl 기준으로 설치하고 Istio 문서에 따라 서비스에 적용하는 것을 확인해보겠습니다.\n공식 문서 https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengistio-intro-topic.htm https://istio.io/latest/docs/examples/microservices-istio/add-istio/ Istio 설치 Cloud Shell 또는 작업환경에 접속합니다.\nIstio 다운로드\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.18.0 TARGET_ARCH=x86_64 sh - Istio 경로로 이동\ncd istio-1.18.0 PATH 환경 변수에 추가\nexport PATH=$PWD/bin:$PATH 사전검증 실행\nistioctl x precheck 설치\nistioctl install --set profile=demo --set components.cni.enabled=true -y 스케일\npod\u0026rsquo;s disruption budget 기본 1로 설정되어 있는 업그레이드를 고려하여 2개 이상으로 스케일합니다.\nkubectl scale --replicas=2 deployment -n istio-system istio-egressgateway kubectl scale --replicas=2 deployment -n istio-system istio-ingressgateway kubectl scale --replicas=2 deployment -n istio-system istiod Istio Enable productpage 서비스 Istio Enable\nIstio sidecar 추가하여 재배포\nistioctl kube-inject 명령으로 productpage 앱에만 sidecar를 추가합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | sed \u0026#39;s/replicas: 1/replicas: 3/g\u0026#39; | kubectl apply -l app=productpage,version=v1 -f - 결과 확인\nproduct Pod내에 istio-proxy 컨테이너가 추가된 것을 볼 수 있습니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-6997d94bb9-8hc7b 1/1 Running 0 24m details-v1-6997d94bb9-cjsn7 1/1 Running 0 24m details-v1-6997d94bb9-sv2p5 1/1 Running 0 24m productpage-v1-5d95bbdf4-hc57h 2/2 Running 0 31s productpage-v1-5d95bbdf4-np89w 2/2 Running 0 39s productpage-v1-5d95bbdf4-r2vsm 2/2 Running 0 35s ratings-v1-b8f8fcf49-2njx4 1/1 Running 0 24m ratings-v1-b8f8fcf49-4x5fj 1/1 Running 0 24m ratings-v1-b8f8fcf49-xmfwq 1/1 Running 0 24m reviews-v1-5896f547f5-47fmg 1/1 Running 0 24m reviews-v1-5896f547f5-8f5dk 1/1 Running 0 24m reviews-v1-5896f547f5-hkhtq 1/1 Running 0 24m sleep-bc9998558-j4nfn 1/1 Running 0 16m $ kubectl describe pod productpage-v1-5d95bbdf4-hc57h Name: productpage-v1-5d95bbdf4-hc57h ... Containers: productpage: Container ID: cri-o://db9f656afa376e55c5a826a678f8aa440de22f47187f61ed70b6b34406e0df7d Image: docker.io/istio/examples-bookinfo-productpage-v1:1.17.0 ... istio-proxy: Container ID: cri-o://546cfa16ea4df577548cd6705e07a4cc757e7bcd820f505fe89fa2f9d592e5c3 Image: docker.io/istio/proxyv2:1.18.0 앞서와 동일하게 productpage 페이지를 접속합니다.\nhttp://{productpage external ip}:9080/productpage istio-proxy 로그 확인\n웹페이지가 이전과 동일하게 접속이 되고 추가적으로 istio-proxy를 통해서 거쳐가는 것을 알 수 있습니다.\n$ kubectl logs -l app=productpage -c istio-proxy -f ... [2023-07-05T04:53:41.213Z] \u0026#34;GET /details/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 178 2 1 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#34; \u0026#34;3bbac68e-a868-97f2-a325-2c9a94783a4a\u0026#34; \u0026#34;details:9080\u0026#34; \u0026#34;10.244.0.146:9080\u0026#34; outbound|9080||details.default.svc.cluster.local 10.244.0.32:37298 10.96.74.5:9080 10.244.0.32:54698 - default [2023-07-05T04:53:41.221Z] \u0026#34;GET /reviews/0 HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 358 480 480 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#34; \u0026#34;3bbac68e-a868-97f2-a325-2c9a94783a4a\u0026#34; \u0026#34;reviews:9080\u0026#34; \u0026#34;10.244.0.151:9080\u0026#34; outbound|9080||reviews.default.svc.cluster.local 10.244.0.32:35808 10.96.29.37:9080 10.244.0.32:35886 - default [2023-07-05T04:53:41.205Z] \u0026#34;GET /productpage HTTP/1.1\u0026#34; 200 - via_upstream - \u0026#34;-\u0026#34; 0 4294 520 520 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\u0026#34; \u0026#34;3bbac68e-a868-97f2-a325-2c9a94783a4a\u0026#34; \u0026#34;129.154.60.198:9080\u0026#34; \u0026#34;10.244.0.32:9080\u0026#34; inbound|9080|| 127.0.0.6:41455 10.244.0.32:9080 10.244.0.1:10500 - default 전체 서비스 Istio Enable\n다음 명령을 통해 Review v2와 나머지 서비스들에도 Istio를 활성화합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app!=reviews -f - curl -s https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v1 -f - curl -s https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v2 -f - 실행결과\nIstio-proxy가 추가 되어 Pod당 컨테이너가 2개로 보이면, bookinfo.yaml이 재적용되어 replica=1임을 참고합니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE details-v1-654f694d5-tpwnm 2/2 Running 0 37s productpage-v1-5d95bbdf4-np89w 2/2 Running 0 4m42s ratings-v1-7596d9bcfc-ktnrk 2/2 Running 0 36s reviews-v1-7d74558c8d-t9j5l 1/1 Running 0 28s reviews-v2-64f8b49667-nbws7 2/2 Running 0 15s sleep-bc9998558-j4nfn 1/1 Running 0 20m 참고\n아래와 같이 istio-injection 레이블을 달면 해당 네임스페이스에 배포되는 pod는 자동으로 istio sidecar가 추가됩니다.\nkubectl label namespace \u0026lt;NAMESPACE_NAME\u0026gt; istio-injection=enabled Istio Ingress Gateway 설정 앞서 다른 장에서 Ingress Controller로 nginx ingress controller를 사용하였습니다. 마이크로 서비스들에 대한 컨트롤을 위해 서비스들에 대해서 Istio의 Ingress Gateway를 사용하도록 설정합니다.\n참고문서\nhttps://istio.io/latest/docs/examples/microservices-istio/istio-ingress-gateway/ Gateway: 서비스 메쉬로 들어오고 나가는 트래픽에 대한 로드 밸런서 정보를 기술합니다.\nVirtualService: 트래픽에 대한 라우팅 규칙을 지정합니다.\n아래 내용으로 예제인 book-info에 대한 Istio Ingress Gateway를 생성합니다.\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: selector: istio: ingressgateway # use Istio default gateway implementation servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: bookinfo spec: hosts: - \u0026#34;*\u0026#34; gateways: - bookinfo-gateway http: - match: - uri: exact: /productpage - uri: exact: /login - uri: exact: /logout - uri: prefix: /static route: - destination: host: productpage port: number: 9080 EOF Istio Ingress Gateway를 통한 접속을 위해 IP를 확인합니다.\n$ kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-egressgateway ClusterIP 10.96.158.40 \u0026lt;none\u0026gt; 80/TCP,443/TCP 17m istio-ingressgateway LoadBalancer 10.96.152.102 158.180.xx.xxx 15021:32155/TCP,80:30404/TCP,443:30195/TCP,31400:31326/TCP,15443:31202/TCP 17m istiod ClusterIP 10.96.202.142 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/TCP 17m 접속 확인\n예시, http://158.180.xxx.xx/productpage Metric 모니터링 Prometheus \u0026amp; Grafana 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/addons/prometheus.yaml kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/addons/grafana.yaml 설치된 Grafana 대쉬보드를 외부에서 접근\n옵션 #1: istioctl dashboard grafana 명령으로 로컬 프락시를 통해 접속할 수 있습니다. Cloud Shell는 Public IP 및 외부 접속을 허용하지 않아, 외부 접속이 필요하면 관련 설정이 필요합니다. 옵션 #2: Istio Ingress Gateway를 통한 외부 접속을 설정할 수 있습니다. 설정을 통해 외부 접속 설정- https://istio.io/latest/docs/tasks/observability/gateways/ 공식 문서를 참고하여 외부 접속을 설정합니다.\nhttps://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http 외부 접속 설정 예시\n설정을 위한 도메인 설정\nIstio Ingress Gateway의 External IP를 확인하여, DNS 서버에 설정하거나, 클라이언트의 /etc/hosts 파일의 설정합니다. 사용할 도메인 주소를 입력합니다.\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.xyz\u0026#34; 외부 접속 오픈\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: grafana-gateway namespace: istio-system spec: selector: istio: ingressgateway servers: - port: number: 80 name: http-grafana protocol: HTTP hosts: - \u0026#34;grafana.${INGRESS_DOMAIN}\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: grafana-vs namespace: istio-system spec: hosts: - \u0026#34;grafana.${INGRESS_DOMAIN}\u0026#34; gateways: - grafana-gateway http: - route: - destination: host: grafana port: number: 3000 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: grafana namespace: istio-system spec: host: grafana trafficPolicy: tls: mode: DISABLE --- EOF 테스트 데이타 발생\nproductpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다. 테스트 앱 페이지 예시, http://158.180.xxx.xx/productpage Grafana 접속\n예) http://grafana.istio.thekoguryo.xyz\n설정한 주소로 접속하면 Istio에서 제공하고 있는 기본 대쉬보드를 확인할 수 있습니다.\nGrafana 대쉬보드를 보면 각 서비스의 처리 청보를 확인할 수 있습니다.\n분산 추적(Distributed Tracing) 마이크로 서비스는 클라이언트의 요청을 분산된 서비스를 통해 처리한 후 클라이언트에게 응답합니다. 예시로 사용하고 있는 Bookinfo 앱도 productpage 서비스가 뒷단 서비스에서 취합한 정보를 클라이언트에게 제공하고 있습니다. 이러한 분산환경에서 클라이언트의 요청에 대한 서비스 들간의 호출 정보를 추적하기 위한 여러가지 툴들이 있습니다. Istio에서는 Envoy의 분산 추적에 대한 기능을 활용하여, Jaeger, Zipkin, LightStep 등의 툴을 활용할 수 있습니다. 여기서는 Zipkin을 통한 방법을 확인해 보겠습니다. 그외 툴들은 관련 페이지를 참고하세요.\nhttps://istio.io/latest/docs/tasks/observability/distributed-tracing/overview/ Zipkin\nZipkin 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/addons/extras/zipkin.yaml 설치된 Zipkin 페이지를 외부에서 접근\nGrafana와 동일하게 옵션 #2: 외부 접속을 사용합니다. 공식 문서를 참고하여 외부 접속을 설정합니다. https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http 외부 접속 설정 예시\n설정을 위한 도메인 설정\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.xyz\u0026#34; 외부 접속 오픈\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: tracing-gateway namespace: istio-system spec: selector: istio: ingressgateway servers: - port: number: 80 name: http-tracing protocol: HTTP hosts: - \u0026#34;tracing.${INGRESS_DOMAIN}\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: tracing-vs namespace: istio-system spec: hosts: - \u0026#34;tracing.${INGRESS_DOMAIN}\u0026#34; gateways: - tracing-gateway http: - route: - destination: host: tracing port: number: 80 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: tracing namespace: istio-system spec: host: tracing trafficPolicy: tls: mode: DISABLE --- EOF 테스트 데이타 발생\nproductpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다. Zipkin 페이지 접속합니다.\n예) http://tracing.istio.thekoguryo.xyz 서비스 추적 테스트\nFind a trace 화면에서 빨간 플러스 기호를 클릭하여 검색할 서비스를 설정하고 쿼리를 실행합니다. 조회 결과를 확인하고 추적할 건에 대해서 오른쪽 SHOW를 클릭합니다.\n해당 요청건에 대한 서비스간 호출 관계, 시간 및 각 서비스에 대한 태그 정보를 볼 수 있습니다.\nTrace ID로 추적\n각 요청건에 대해서 Span ID(Trace ID)를 통해 추적할 수 있습니다. 로그에서 확인할 수 있습니다.\n화면 오른쪽 상단의 Trace ID 검색창을 통해서 바로 검색하면, 해당 요청건을 바로 확인할 수 있습니다.\n서비스 메쉬 시각화 마이크로 서비스는 클라이언트의 요청을 분산된 서비스를 통해 처리한 후 클라이언트에게 응답합니다. 예시로 사용하고 있는 Bookinfo 앱도 productpage 서비스가 뒷단 서비스에서 취합한 정보를 클라이언트에게 제공하고 있습니다. 이러한 분산환경에서 클라이언트의 요청에 대한 서비스 들간의 호출 정보를 시각화를 Kiali를 통해 제공하고 있습니다.\nhttps://istio.io/latest/docs/tasks/observability/kiali/ Kiali 설치\nkubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.18/samples/addons/kiali.yaml 설치된 Kiali 페이지를 외부에서 접근\nGrafana와 동일하게 옵션 #2: 외부 접속을 사용합니다. 공식 문서를 참고하여 외부 접속을 설정합니다. https://istio.io/latest/docs/tasks/observability/gateways/#option-2-insecure-access-http 외부 접속 설정 예시\n설정을 위한 도메인 설정\nexport INGRESS_DOMAIN=\u0026#34;istio.thekoguryo.xyz\u0026#34; 외부 접속 오픈\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: kiali-gateway namespace: istio-system spec: selector: istio: ingressgateway servers: - port: number: 80 name: http-kiali protocol: HTTP hosts: - \u0026#34;kiali.${INGRESS_DOMAIN}\u0026#34; --- apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kiali-vs namespace: istio-system spec: hosts: - \u0026#34;kiali.${INGRESS_DOMAIN}\u0026#34; gateways: - kiali-gateway http: - route: - destination: host: kiali port: number: 20001 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: kiali namespace: istio-system spec: host: kiali trafficPolicy: tls: mode: DISABLE --- EOF 테스트 데이타 발생\nproductpage의 Istio Ingress Gateway를 통해 접속하여 테스트 데이타를 발생시킵니다. Kiali 대쉬보드 접속합니다.\n예) http://kiali.istio.thekoguryo.xyz 내비게이션 메뉴에서 Graph를 클릭하면, 서비스 간의 호출 정보를 시각화해서 볼 수 있습니다.\nReviews 새 버전 배포 Reviews v3 배포\nReviews v3 버전을 배포합니다.\nIstio가 활성화 된 상태에서 버전 업데이트를 가정하여 v3를 배포합니다.\ncurl -s https://raw.githubusercontent.com/istio/istio/release-1.18/samples/bookinfo/platform/kube/bookinfo.yaml | istioctl kube-inject -f - | kubectl apply -l app=reviews,version=v3 -f - 배포 결과 확인\n$ kubectl get pod NAME READY STATUS RESTARTS AGE ... reviews-v2-64f8b49667-bhh6j 2/2 Running 0 14m reviews-v3-6566d5c446-frgfc 2/2 Running 0 41s ... productpage의 Istio Ingress Gateway를 통해 접속하여 테스트합니다.\nReviews v2과 Reviews v3는 각각 Pod 하나로 분배규칙을 따라 설정하지 않았기 때문에, 한번씩 라우팅됩니다. Reviews v2(검은 별점)과 Reviews v3(빨간 별점)이 한번시 표시되는 것을 볼수 있습니다. 가중치 기반 라우팅\n신규 버전의 완전한 서비스 전에 검증을 위해 가중치 기반의 일부 요청만 라우팅되도록 하는 시나리오를 확인 해 봅니다.\n배포된 Reviews Pod의 버전 레이블을 확인합니다. 각각 version=v2, version=v3이 할당되어 있습니다.\n$ kubectl get pod -L version NAME READY STATUS RESTARTS AGE VERSION details-v1-654f694d5-7xs5d 2/2 Running 0 26m v1 productpage-v1-5d95bbdf4-thlpj 2/2 Running 0 26m v1 ratings-v1-7596d9bcfc-g7srf 2/2 Running 0 26m v1 reviews-v2-64f8b49667-bhh6j 2/2 Running 0 25m v2 reviews-v3-6566d5c446-frgfc 2/2 Running 0 11m v3 sleep-bc9998558-j4nfn 1/1 Running 0 84m 레이블 기준으로 라우팅을 위해 Istio에 필요한 아래 설정을 배포합니다.\nDestinationRule: 서비스 엔드포인트에 대해서 labels로 필터링. 테스트 예제에서는 Service Type이 아닌, Pod에만 version label이 있음. Pod의 label로 하는 것 같음 VirtualService: 정의한 destination에 대해서 weight 기준으로 분배함. weight의 총합은 100이어야함. 100번 중에 90번, 10번 이렇게 정확한 분배가 아닌, 확률로 추측됨. kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: reviews spec: hosts: - reviews http: - route: - destination: host: reviews subset: v2 weight: 90 - destination: host: reviews subset: v3 weight: 10 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata: name: reviews spec: host: reviews subsets: - name: v1 labels: version: v1 - name: v2 labels: version: v2 - name: v3 labels: version: v3 EOF 아래와 같이 반복 테스트합니다.\nISTIO_INGRESS_LB_IP=`kubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;` counter=1; \\ while [ $counter -lt 20 ]; \\ do curl -s http://${ISTIO_INGRESS_LB_IP}/productpage | grep color | head -1; counter=$(( $counter + 1 )); \\ done 테스트 결과\n20중에 18번 검은색 별표, 2번 빨간색 별표임을 알수 있습니다. 확률로 정확이 딱 안 맞을 수 있습니다.\n$ ISTIO_INGRESS_LB_IP=`kubectl get svc istio-ingressgateway -n istio-system -o jsonpath=\u0026#39;{.status.loadBalancer.ingress[0].ip}\u0026#39;` )$ $ counter=1; \\ \u0026gt; while [ $counter -lt 20 ]; \\ \u0026gt; do \u0026gt; curl -s http://${ISTIO_INGRESS_LB_IP}/productpage | grep color | head -1; \u0026gt; counter=$(( $counter + 1 )); \\ \u0026gt; done \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;red\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; \u0026lt;font color=\u0026#34;black\u0026#34;\u0026gt; Kiali에 호출 흐름 확인\n아래그림에서 보듯이 Reviews v2, v3에 설정한 가중치에 맞게 분배되는 것을 볼 수 있습니다.\n","lastmod":"2021-12-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/service-mesh/istio/2.sampleapp-with-istio/","tags":["oss","service mesh","istio"],"title":"1.8.1.2 마이크로서비스 앱에 Istio 적용하기"},{"categories":null,"contents":"1.5.2.4.2 Prometheus\u0026amp;Grafana로 모니터링 하기 오픈 소스로 쿠버네티스 메트릭 모니터링을 하는 데 가장 많이 사용되는 구성인 Prometheus\u0026amp;Grafana을 확인해 봅니다.\nPrometheus: 오픈소스 시스템 모니터링 및 Alert 툴킷 https://prometheus.io/docs/introduction/overview/ Grafana: 수집된 데이터에 대한 질의 등을 통해 대쉬보드 형태로 시각화해 주는 도구 https://grafana.com/grafana/ Prometheus 설치 설치용 namespace를 만듭니다.\nkubectl create ns monitoring Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami 설정 값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\nhttps://github.com/bitnami/charts/tree/master/bitnami/kube-prometheus#parameters\n예시\nprometheus와 alertmanager 접근 URL을 nginx ingress controller, letsencrypt를 통한 인증서 사용하는 예시 이전 실습 1.2.2.2.2.2 NGINX Ingress Controller에서 TLS termination(feats. Let’s Encrypt) 설치 기준에서 설치하는 예시임 cat \u0026lt;\u0026lt;EOF \u0026gt; values.yaml prometheus: ingress: enabled: true hostname: prometheus.ingress.thekoguryo.xyz path: / annotations: cert-manager.io/cluster-issuer: letsencrypt-staging tls: true ingressClassName: nginx alertmanager: ingress: enabled: true hostname: alertmanager.ingress.thekoguryo.xyz path: / annotations: cert-manager.io/cluster-issuer: letsencrypt-staging tls: true ingressClassName: nginx EOF prometheus helm chart 설치\nhelm install prometheus -n monitoring -f values.yaml bitnami/kube-prometheus --version 8.14.2 설치결과\nPrometheus의 내부 DNS 정보는 이후 Grafana에서 연동할 때 사용됩니다.\n$ helm install prometheus -n monitoring -f values.yaml bitnami/kube-prometheus --version 8.14.2 NAME: prometheus ... Prometheus can be accessed via port \u0026#34;9090\u0026#34; on the following DNS name from within your cluster: prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local To access Prometheus from outside the cluster execute the following commands: You should be able to access your new Prometheus installation through https://prometheus.ingress.thekoguryo.xyz ... Grafana 설치 설정 값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\nhttps://github.com/bitnami/charts/tree/master/bitnami/grafana#parameters grafana 접근 URL을 이전 장(4.1.4.2 NGINX Ingress Controller에서 TLS termination(feats. Let’s Encrypt)에서 설치한 nginx ingress controller를 사용하는 예시입니다. cat \u0026lt;\u0026lt;EOF \u0026gt; values.yaml admin: password: \u0026#34;higrafana\u0026#34; ingress: enabled: true hostname: grafana.ingress.thekoguryo.xyz path: / annotations: cert-manager.io/cluster-issuer: letsencrypt-staging tls: true ingressClassName: nginx EOF grafana helm chart 설치\nhelm install grafana -n monitoring -f values.yaml bitnami/grafana 설치결과\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\n$ helm install grafana -n monitoring -f values.yaml bitnami/grafana --version 8.4.6 NAME: grafana ... 1. Get the application URL by running these commands: https://grafana.ingress.thekoguryo.xyz/ 2. Get the admin credentials: echo \u0026#34;User: admin\u0026#34; echo \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 -d)\u0026#34; Grafana 설정 Grafana 웹 UI를 접속합니다.\nURL: values.yaml에서 설정한 ingress.hostname\n관리자 계정\nUser: admin\nPassword: values.yaml에 설정한 admin.password 또는 미입력시 다음 명령으로 확인\necho \u0026#34;Password: $(kubectl get secret grafana-admin --namespace monitoring -o jsonpath=\u0026#34;{.data.GF_SECURITY_ADMIN_PASSWORD}\u0026#34; | base64 --decode)\u0026#34; Prometheus 등록을 위해 왼쪽 메뉴에서 Configuration \u0026gt; Data sources 선택\nAdd data source 클릭\nPrometheus 선택\nPrometheus 연결정보 입력\n앞서 Prometheus 설치 로그에 확인한 Prometheus 내부 연결 정보를 입력합니다.\nhttp://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090 아래 쪽 Save \u0026amp; test 클릭\nGrafana Dashboard 추가 Kubernetes 모니터링을 위해 공개된 Grafana Dashboard를 사용할 수 있습니다.\nhttps://grafana.com/grafana/dashboards/?search=kubernetes\u0026amp;orderBy=updatedAt\u0026amp;direction=desc\u0026amp;dataSource=prometheus 위 사이트에서 검색된 대쉬보드 중 원하는 것을 선택합니다.\n대쉬보드 상세 페이지에 ID를 확인합니다.\nDashboard 등록을 위해 왼쪽 메뉴에서 Dashboards를 클릭합니다.\nNew \u0026gt; Import 클릭\n임포트할 대쉬보드 ID(예시, 15757) 입력후 Load 클릭\ndata source를 앞서 등록한 Prometheus 선택후 Import 클릭\n임포트된 대쉬보드를 통해 OKE 클러스터 모니터링\n위와 같이 공개된 대쉬보드를 사용하거나, 원하는 대쉬보드를 작성하여 Prometheus에서 수집된 메트릭 정보를 이용하여, Kubernetes 클러스터를 모니터링 할 수 있습니다.\n","lastmod":"2021-12-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/open-source/prometheusgrafana/1.install-prometheus-grafana/","tags":["oss","monitoring","prometheus","grafana"],"title":"1.5.2.4.2 Prometheus\u0026Grafana로 모니터링 하기"},{"categories":null,"contents":"1.2.2.2.2 NGINX Ingress Controller에서 PATH 기반 라우팅 PATH 기반 기본 라우팅 테스트 가장 기본적인 라우팅으로 URL PATH에 따라 라우팅 서비스를 달리하는 경우입니다.\n테스트를 위한 샘플 앱을 배포합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=ghcr.io/thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=ghcr.io/thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80 ingress 설정 YAML(path-basic.yaml)을 작성합니다.\n/blue 요청은 nginx-blue-svc 로 라우팅 /green 요청은 nginx-green-svc로 라우팅 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-path-basic spec: ingressClassName: nginx rules: - http: paths: - path: /blue pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 - http: paths: - path: /green pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 참고 기존 annotations kubernetes.io/ingress.class: nginx 에서 ingressClassName: nginx 변경 Deprecating the Ingress Class Annotation 작성한 path-basic.yaml을 배포합니다.\n$ kubectl apply -f path-basic.yaml ingress.networking.k8s.io/ingress-path-basic created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-basic nginx * 152.69.xx.xx 80 39s 앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n/blue 요청\n/green 요청\nPOD 정보 확인\n정의한 PATH에 따라 각각 blue, green 앱이 배포된 POD로 라우팅 된 것을 웹페이지 배경색 및 POD IP로 알 수 있습니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-blue-565656fd64-bcc8x 1/1 Running 0 19m 10.244.1.4 10.0.10.207 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-green-8c6dc77b6-29kl9 1/1 Running 0 19m 10.244.0.5 10.0.10.239 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Rewrite Target URL PATH 라우팅 결과를 보면 /blue, /green의 Path가 최종 라우팅 되어 실행되는 앱으로 그대로 전달되는 것을 알 수 있습니다. ingress controller에서 라우팅을 위해서만 사용하고, 실제 앱의 동작을 위해서는 수정이 필요한 경우에 사용합니다.\nhttps://kubernetes.github.io/ingress-nginx/examples/rewrite/ ingress 설정 YAML(path-rewrite-target.yaml)을 작성합니다.\npath: /blue -\u0026gt; /blue(/|$)(.*) 로 변경 annotation 추가: nginx.ingress.kubernetes.io/rewrite-target: /$2 예시 ~~/blue -\u0026gt; ~~/ 로 앱으로 전달 ~~/blue/abc -\u0026gt; ~~/abc 로 앱으로 전달 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-path-rewrite-target annotations: nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: ingressClassName: nginx rules: - http: paths: - path: /blue(/|$)(.*) pathType: ImplementationSpecific backend: service: name: nginx-blue-svc port: number: 80 - http: paths: - path: /green(/|$)(.*) pathType: ImplementationSpecific backend: service: name: nginx-green-svc port: number: 80 참고 pathType: Prefix인 경우 path에 와이드 카드가 있는 경우 경고문구가 출력됨. 이를 위해 pathType: ImplementationSpecific 변경 앞선 path-basic.yaml를 삭제하고 path-rewrite-target.yaml를 배포합니다.\n$ kubectl delete -f path-basic.yaml ingress.networking.k8s.io \u0026#34;ingress-path-basic\u0026#34; deleted $ kubectl apply -f path-rewrite-target.yaml ingress.networking.k8s.io/ingress-path-rewrite-target created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-rewrite-target nginx * 152.69.xxx.xxx 80 82s 앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n~~/blue 요청\n라우팅된 앱에서는 /blue가 빠지고 /로만 수신됨 ~~/blue/abc 요청\n라우팅된 앱에서는 /blue가 빠지고 /abc로만 수신됨 ","lastmod":"2024-01-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/nginx-ingress/lb/2.nginx-ingress-path/","tags":["oss","ingress-controller","nginx"],"title":"1.2.2.2.2 PATH 기반 라우팅"},{"categories":null,"contents":"1.4.1.2 컨테이너 이미지 스캔 OCIR는 OCI Vulnerability Scanning 서비스를 사용하여, 알려진 Common Vulnerabilities and Exposures (CVE) database를 기반으로 컨테이너 이미지에 대한 취약점 분석 기능을 제공하고 있습니다. 다음은 취약점 분석을 위한 이미지 스캔 기능을 확인해 봅니다.\n공식 문서\nOCI Documentation \u0026gt; Container Registry \u0026gt; Scanning Images for Vulnerabilities 관련 Policy 설정 스캔 서비스가 OCIR repository에 대한 권한을 부여합니다. 적용해야 하는 Policy는 위 공식 문서를 참조합니다. tenancy 전체 또는 compartment 에 대해서 지정할 수 있습니다.\n전체 테넌시\n이름: 예) ocir-scanning-images-root-policy allow service vulnerability-scanning-service to read repos in tenancy allow service vulnerability-scanning-service to read compartments in tenancy 특정 compartment\nallow service vulnerability-scanning-service to read repos in compartment \u0026lt;compartment-name\u0026gt; allow service vulnerability-scanning-service to read compartments in compartment \u0026lt;compartment-name\u0026gt; #1. OCIR Repository에서 이미지 스캐너 설정 OCIR Repository에서 Repository 단위로 이미지 스캐너 추가, 삭제 할 수 있습니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Registry로 이동합니다.\nList Scope에서 대상 Compartment를 선택합니다.\n스캐너를 설정할 Repository(예시, nginx)를 선택하고 오른쪽 Actions 메뉴에서 Add scanner를 선택합니다.\n여기에서 기본 정보를 넣고 스캐너를 만들면 됩니다.\n스캐너를 추가하면, 이후 OCI Vulnerability Scanning를 통해 검사한 결과를 볼 수 있습니다. Repository 마다 설정해야 한다는 점이 불편할 수 있습니다.\n#2. OCI Vulnerability Scanning에서 OCIR Repository에 대한 이미지 스캐닝 Container Image Scan Recipe 만들기\nOracle Cloud 콘솔에 로그입니다.\nIdentity \u0026amp; Security \u0026gt; Scanning \u0026gt; Scan Recipes 메뉴로 이동합니다.\nCreate를 클릭하여 이미지 스캔 레시피를 만듭니다.\nType: Container image Name: 예) container-image-scan-recipe Container Image Target 생성\n이미지 스캔 레시피 상세화면에서 Create Target을 클릭합니다.\n컨테이너 이미지 타겟을 만듭니다.\nName: 예) container-image-target 대상 Repository: 선택한 Compartment 내의 모든 Repository가 되도록 선택합니다. Results are typically available 15 minutes after creating a new target. 생성된 Target 상단에 표시된 문구처럼 생성후 15분 이후에 결과를 확인합니다.\n스캐닝 리포트 확인\n내비게이션 메뉴에서 Identity \u0026amp; Security \u0026gt; Scanning \u0026gt; Scanning Reports 메뉴로 이동합니다.\nContainer Image 탭으로 이동합니다.\n컨테이너 이미지 스캐닝 결과를 확인할 수 있습니다.\n원하는 이미지의 이름을 클릭하면, 취약점 분석 결과를 볼 수 있습니다\nView detail을 클릭하면, 원인과 해결책이 있는 경우 관련 정보를 알려줍니다.\n리스트된 취약점 중에 하나를 클릭합니다. 해당 취약점 기준으로 해당 취약점이 있는 호스트, 컨테이너 이미지 내역을 볼 수 있습니다\nOCIR에서 스캐닝 리포트 확인\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Registry로 이동합니다.\n원하는 컨테이너 이미지를 선택한 후, Scan results 탭을 클릭합니다.\n최근 스캔 결과에 대해서 View details를 클릭합니다.\n취약점 확인\n이미지 스캐닝 결과 확인 알려진 CVE 정보를 기반한 취약점을 확인 할 수 있으며, 취약점를 클릭하면, 실제 CVE 데이터 베이스로 이동하여, 상세 정보를 확인 할 수 있습니다.\n","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/container-registry/ocir/2.scan-image/","tags":["container registry"],"title":"1.4.1.2 컨테이너 이미지 스캔"},{"categories":null,"contents":"1.6.2 File Storage 사용하기(Flex Volume Driver) 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)을 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 RWX 접근 모드로 사용하는 방법을 확인해 보겠습니다.\nFiles Storage 만들기 관련 문서를 참고하여 File Storage를 만듭니다.\nhttps://docs.oracle.com/en-us/iaas/Content/File/home.htm\nhttps://thekoguryo.github.io/oci/chapter08/\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Storage \u0026gt; File Storage로 이동합니다\n대상 Compartment를 확인합니다.\nFile Systems에서 Create File System을 클릭합니다.\n기본 설정화면에서 간단히 아래 정보를 원하는 값이 맞게 알맞게 수정하고 생성합니다.\nFile System Information: Name Mount Target Information: New Mount Target Name Virtual Cloud Network Subnet 생성결과 확인\nFile Storage \u0026gt; Mount Target 에서 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\nMount Target OCID: \u0026hellip;sc2mia IP Address: 예) 10.0.20.194 Export Path: 예) /OKE-FFS-Strorage Security List 설정\nFile System 생성시 Mount Target의 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\nFile Storage 서비스를 이용하여 Persistent Volume 사용하기 Storage Class 만들기\n앞서 확인한 Mount Target OCID로 업데이트 후 적용\napiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: oci-fss provisioner: oracle.com/oci-fss parameters: # Insert mount target from the FSS here mntTargetId: ocid1.mounttarget.oc1.ap_seoul_1.aaaaaa4np2szmmn5nfrw4llqojxwiotboawxgzlpovwc2mjnmfsc2mia Persistent Volume (PV) 만들기\nMount Targe의 IP와 Export Path로 업데이트 후 적용\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion: v1 kind: PersistentVolume metadata: name: oke-fss-pv spec: storageClassName: oci-fss capacity: storage: 100Gi accessModes: - ReadWriteMany mountOptions: - nosuid nfs: # Replace this with the IP of your FSS file system in OCI server: 10.0.20.194 # Replace this with the Path of your FSS file system in OCI path: \u0026#34;/OKE-FFS-Storage\u0026#34; readOnly: false Persistent Volume Claime(PVC) 만들기\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: oke-fss-pvc spec: storageClassName: oci-fss accessModes: - ReadWriteMany resources: requests: storage: 100Gi volumeName: oke-fss-pv PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\n앞선 예제와 달리 replica를 복수개로 지정할 수 있습니다.\napiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-fss-pvc name: nginx-fss-pvc spec: replicas: 3 selector: matchLabels: app: nginx-fss-pvc template: metadata: labels: app: nginx-fss-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: oke-fss-pvc 실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oci-fss-storageclass.yaml storageclass.storage.k8s.io/oci-fss created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get sc,pv,pvc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/oci (default) oracle.com/oci Delete Immediate false 2d19h storageclass.storage.k8s.io/oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d19h storageclass.storage.k8s.io/oci-fss oracle.com/oci-fss Delete Immediate false 34s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 3h6m persistentvolume/oke-fss-pv 100Gi RWX Retain Bound default/oke-fss-pvc oci-fss 24s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/csi-bvs-pvc Bound csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO oci-bv 3h6m persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 100Gi RWX oci-fss 17s oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-9fb98454f-bc7hp 1/1 Running 0 24s 10.244.0.5 10.0.10.40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-bxw4x 1/1 Running 0 24s 10.244.1.18 10.0.10.15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-qm9tq 1/1 Running 0 24s 10.244.0.153 10.0.10.219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 파일 쓰기 테스트\n아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.40\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bxw4x -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-qm9tq -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 참고 문서 https://blogs.oracle.com/cloud-infrastructure/post/using-file-storage-service-with-container-engine-for-kubernetes\n","lastmod":"2021-11-10T00:00:00Z","permalink":"https://thekoguryo.github.io/archives/oracle-cloudnative/oke/6.persistent-volume/2.file-storage-by-flex-volume-driver/","tags":["oke"],"title":"1.6.2 File Storage 사용하기(Flex Volume Driver)"},{"categories":null,"contents":"1.1.2 OKE 클러스터 만들기 v1.26.7, Oracle Linux 8, VCN-Native Pod Networking CNI에서 2024년 1월 7일에 테스트 되었음\nQuick Create 모드로 클러스터 만들기 처음 OKE 클러스터를 만드는 단계로 실환경에서는 별도의 OKE 사용자 및 VCN 등 커스텀한 환경을 사용하겠지만, OKE를 이해하기 위한 처음 단계로 Quick Create 모드로 설치합니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\nList Scope에서 생성할 Compartment를 선택합니다.\n클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여 생성된 OKE 클러스터를 통해 기본 구조를 이해하고자 합니다. 아래 Submit을 클릭합니다.\nQuick Create\n아래 그림에서 설명하고 있는 것 처럼 생성될 클러스터가 속할 VCN에 자동으로 만들어지고, 필요한 자원, Kubernetes API 서브넷, 로드 밸런서 서브넷, 워커노드 서브넷 간의 Security Lists에 필요한 규칙이 자동으로 설정됩니다. 대신 생성되는 자원의 이름이 클러스터 이름을 기준으로 자동으로 정해집니다. 대신 CNI는 선택 옵션없이 VCN-Native Pod Networking으로 자동 선택됩니다. Custom Create\n클러스터 생성전에 필요한 VCN을 미리 생성해야 합니다. 관련 라우팅 규칙 및 Security List 도 사전에 설정이 필요합니다. 사용자가 사전에 자원을 만들므로, 원하는 이름 규칙으로 만들 수 있습니다. CNI을 OCI VCN-Native Pod Networking(OCI_VCN_IP_NATIVE) 또는 Flannel 중에 선택 가능합니다. 클러스터 생성 옵션 생성할 클러스터 이름을 입력합니다. 예) oke-cluster-1\n설치될 Compartment를 선택합니다.\n클러스터의 쿠버네티스 버전을 선택합니다. 예) v1.26.7\nKubernetes API Endpoint\n인터넷 상에서 Kubernetes API를 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kubernetes API를 Private IP만 할당할지, Public IP도 할당할 지를 선택합니다. Quick Create로 설치하게 되면, 이 선택에 따라 Kubernetes API가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 만들어 질지가 정해집니다. Node type\nManaged: 선택하지 않아도 기본 값입니다. Worker Nodes를 SSH로 접속할 수 있고, 사용자가 관리합니다. Virtual: Serverless로 사용자가 Worker Nodes에 대한 스케일, 업그레이드 등을 하지 않습니다. Enhanced Cluster에서만 가능합니다. Kubernetes Worker Nodes\nWorker Nodes를 접속할 수 있는 방법을 선택합니다. 기본 선택된 Private Workers를 그대로 사용 생성되는 Worker Nodes를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 이 선택에 따라 Worker Nodes가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다. Shape Worker Node로 사용할 Shape을 지정합니다. 기본 선택된 Flex Shape에서 필요에 따라 OCPU, Memory를 조정합니다.\nImage\n쿠버네티스 버전 선택이 그에 해당되는 노드 버전이 기본적으로 선택됩니다. 필요한 경우에만 변경합니다. Node count\nWorker Nodes 갯수를 지정합니다. 기본 값을 3개입니다.\nControl Plane Nodes는 OCI가 관리하는 영역으로 별도 크기 등을 지정하지 못하며, 기본적으로 다중화되어 있습니다.\nShow advanced options을 클릭합니다\nBoot Volume\nboot volume 사이즈를 조정할 수 있습니다. Worker Nodes에 실행될 컨테이너 이미지가 다운로드 되기 때문에, 애플리케이션 컨테이너 수와 이미지가 큰 경우, 그에 맞게 늘릴 필요가 있습니다.\nAdd an SSH Key\n트러블 슈팅 등을 위해 Worker Node에 SSH로 접근하기 위해서는 SSH Key 등록이 필요합니다. 사용할 SSH Key의 Public Key를 아래와 같이 등록합니다.\n클러스터 생성 정보를 모두 입력하였습니다. 아래 Next를 클릭\n기본적으로 Enhanced Cluster 타입이 선택됩니다. 여기서는 Basic Cluster 사용을 위해 화면 제일 아래 Basic Cluster Confirmation에서 Create a Basic cluster를 클릭합니다.\nReview\nCluster\nBasic Cluster 선택되었는지 확인할 것. Basic Cluster에도 필수 애드온은 설치됨.\nNetwork\nNode pools\nNode pool Version과 Image 버전이 동일한 지 확인합니다. Network type은 OCI_VCN_IP_NATIVE로 설치됩니다.\n리뷰 후 클러스터를 생성합니다.\n클러스터 생성 및 관련 네트워크 자원\n아래 그림과 같이 Quick Create로 클러스터를 생성시 기본 네트워크 자원이 함께 생성되는 것을 볼수 있습니다.\n클러스터 생성 확인 생성이 요청되면, 클러스터 생성, 노드 풀 생성, Worker Node 생성 및 구성 순으로 진행됩니다.\n클러스터 상세정보에서 Resources \u0026gt; Node Pools를 보면 생성된 pool을 볼 수 있습니다.\n생성된 Node Pool인 pool1을 클릭하여 Node Pool 상세 정보로 이동합니다.\nNode Pool 상세 정보에서 Resources \u0026gt; Nodes 정보를 보면 생성된 Worker Nodes를 확인할 수 있습니다. VM 생성후 쿠버네티스 구성 시간이 있어 Ready 상태가 될 때까지 약간의 시간이 걸립니다. 테스트 환경에서는 노드가 모두 Ready 될때 까지 10분 정도 걸렸습니다.\n더 다양한 클러스터 및 네트워크 구성 예시 Example Network Resource Configurations에서 Kubernetes API Endpoint, Worker Nodes, Service Load Balancer에 대한 몇 가지 조합을 한 예시를 확인할 수 있습니다. 이 조합들 중에서 여기서 만는Quick Create 모드에서 Public Endpoint, Private Workers를 선택하였고, Service Load Balancer는 기본 생성시는 Public이며 Kubernetes에서 Load Balancer 생성시 선택할 수 있습니다.\n","lastmod":"2024-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/basics/2.create-oke-cluster-in-quick-mode/","tags":["oke"],"title":"1.1.2 OKE 클러스터 만들기 - Quick Create 모드"},{"categories":null,"contents":"14.3.2 Terraform Configurations 작성하기 Terraform을 쓰려면 Terraform 설정 파일(.tf)에 HashiCorp Configuration Language(HCL) 형식으로 OCI 인프라 자원을 표현해야 합니다. JSON 형식도 지원하지만, 여기서는 HCL 포맷을 사용하겠습니다.\n아래 내용은 Terraform에서 가장 기초적인 일부 내용으로 전체 정보는 Terraform 및 Terraform OCI Provider 문서를 참조하기 바랍니다.\nTerraform OCI Provider 문서 OCI 콘솔에서 처음 VCN을 만든 것처럼 Terraform을 통해 VCN을 만드는 것으로 테스트 해 봅니다. 물론 콘솔에서 Wizard로 만드는 것과는 달리 정말 VCN만 만드는 예제입니다.\nOCI Provider 정의 Terraform 설정 파일에서 표현하는 자원들은 제공업체(Provider)에 따라 그 종류와 특정이 달라집니다. 다양한 업체들이 Terraform Provider로써 자신들의 자원을 정의하여 제공하고 있습니다. OCI로 Provider는 다음과 같이 설정하여 사용할 수 있습니다.\nprovider.tf\nvariable \u0026#34;tenancy_ocid\u0026#34; {} variable \u0026#34;user_ocid\u0026#34; {} variable \u0026#34;fingerprint\u0026#34; {} variable \u0026#34;private_key_path\u0026#34; {} variable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { tenancy_ocid = \u0026#34;${var.tenancy_ocid}\u0026#34; user_ocid = \u0026#34;${var.user_ocid}\u0026#34; fingerprint = \u0026#34;${var.fingerprint}\u0026#34; private_key_path = \u0026#34;${var.private_key_path}\u0026#34; region = \u0026#34;${var.region}\u0026#34; } Variable 정의 Provider 정의에서 보듯이 Variable은 Terraform 모듈을 위해 필요한 파라미터 정의를 위해 사용합니다. variable \u0026quot;tenancy_ocid\u0026quot; {}와 같이 정의하고 ${var.tenancy_ocid} 처럼 필요한 곳에서 사용할 수 있습니다. 선언은 variable \u0026quot;변수명\u0026quot; {} 형식으로 선언하여 아래 예시와 같이 추가 설정을 할 수 있습니다.\ntype (옵션): 변수 타있을 정의합니다\nstring, list, map 중 하나 표시하지 않으면 default에 사용된 값 형식에 기초로 함 표시하지 않고 default도 없는 경우는 string으로 가정 default (옵션): 디폴트 값 정의, 디폴트 값이 없으면, 호출할 때 환경변수 설정 등을 통해서 전달해야 합니다.\ndescription (옵션): 변수 설명을 입력합니다.\n예시\nvariable \u0026#34;tenancy_ocid\u0026#34; {} variable \u0026#34;user_ocid\u0026#34; {} variable \u0026#34;fingerprint\u0026#34; {} variable \u0026#34;private_key_path\u0026#34; {} variable \u0026#34;region\u0026#34; {} variable \u0026#34;AD\u0026#34; { default = \u0026#34;1\u0026#34; description = \u0026#34;Availability Domain\u0026#34; } variable \u0026#34;CPUCoreCount\u0026#34; { type = \u0026#34;string\u0026#34; default = \u0026#34;2\u0026#34; } Variable 값 설정 Terraform 실행하기 전에 아래 처럼 TF_VAR_변수명 형식으로 환경변수에 지정하여 변수 값을 지정할 수 있습니다.\nexport TF_VAR_tenancy_ocid=\u0026#34;\u0026lt;tenancy OCID\u0026gt;\u0026#34; terraform CLI 실행시 -var 옵션으로 할 수도 있습니다.\n별도 변수 파일(terraform.tfvars) 생성해도 됩니다.\n환경에 따라 변수를 달리하기 위해 파일명이 다른 경우는 terraform 실행시 -var-file 옵션으로 파일명 지정 가능합니다.\n예시\n## terraform.tfvars file tenancy_ocid=\u0026#34;\u0026lt;tenancy OCID\u0026gt;\u0026#34; Resource 설정 Resource는 생성할 OCI 자원을 나타냅니다. 아래와 같이 resource를 설정하고 terraform을 실행하면, OCI에 해당 자원이 실제로 생성됩니다. Terraform을 지원하는 OCI 자원 목록은 Terraform OCI Provider 문서에서 확인할 수 있습니다.\n예시\nvariable \u0026#34;compartment_ocid\u0026#34; {} resource \u0026#34;oci_core_vcn\u0026#34; \u0026#34;example_vcn1\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; dns_label = \u0026#34;vcn1\u0026#34; compartment_id = \u0026#34;${var.compartment_ocid}\u0026#34; display_name = \u0026#34;vcn1\u0026#34; } Data Source 설정 Data 소스는 조회할 OCI 자원을 나타냅니다. Resource와는 달리 조회 용도로만 사용됩니다. Terraform을 지원하는 OCI Data 소스는 Terraform OCI Provider 문서에서 확인할 수 있습니다.\n예시, oci_identity_availability_domains\nData Source: oci_identity_availability_domains 필수 파라미터를 이용해서 조회하는 예시 # Gets a list of Availability Domains data \u0026#34;oci_identity_availability_domains\u0026#34; \u0026#34;ADs\u0026#34; { #Required compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; } Output 설정 Output 변수는 많은 데이터 중에서 쿼리를 통해 특정 값을 조회하는 용도로 terraform 실행시 로그로 출력되며, terraform output 변수명 명령으로 조회도 됩니다.\n생성된 example_vcn1의 id 예시\noutput \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_vcn.example_vcn1.id}\u0026#34;] } Terraform Configurations 예시 provider.tf\nvariable \u0026#34;tenancy_ocid\u0026#34; {} variable \u0026#34;user_ocid\u0026#34; {} variable \u0026#34;fingerprint\u0026#34; {} variable \u0026#34;private_key_path\u0026#34; {} variable \u0026#34;region\u0026#34; {} provider \u0026#34;oci\u0026#34; { tenancy_ocid = \u0026#34;${var.tenancy_ocid}\u0026#34; user_ocid = \u0026#34;${var.user_ocid}\u0026#34; fingerprint = \u0026#34;${var.fingerprint}\u0026#34; private_key_path = \u0026#34;${var.private_key_path}\u0026#34; region = \u0026#34;${var.region}\u0026#34; } vcn.tf\nvariable \u0026#34;compartment_ocid\u0026#34; {} resource \u0026#34;oci_core_vcn\u0026#34; \u0026#34;vcn1\u0026#34; { cidr_block = \u0026#34;10.0.0.0/16\u0026#34; dns_label = \u0026#34;vcn1\u0026#34; compartment_id = \u0026#34;${var.compartment_ocid}\u0026#34; display_name = \u0026#34;vcn1\u0026#34; } output \u0026#34;vcn1_ocid\u0026#34; { value = [\u0026#34;${oci_core_vcn.vcn1.id}\u0026#34;] } ","lastmod":"2019-03-29T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/2/","tags":["terraform"],"title":"14.3.2 Terraform Configurations 작성하기"},{"categories":null,"contents":"10.2 VCN 구성 LoadBalancer 테스트용 VCN 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Virtual Cloud Networks 항목으로 이동합니다.\n왼쪽 아래 Compartment에서 Sandbox를 클릭합니다.\nStart VCN Wizard 클릭\nCreate VCN with Internet Connectivity을 선택하고 다시 한번 Start VCN Wizard 클릭\n생성 정보 입력\nBasic Information Name: load-balancer-vcn 입력 Compartment: 사용할 Compartment 선택, 예, oci-hol-xx Configure VCN and Subnets 아래 대역대로 기본 네트워크가 구성됩니다. VCN CIDR Block: 10.0.0.0/16 Public Subnet CIDR Block: 10.0.0.0/24 Private Subnet CIDR Block: 10.0.1.0/24 DNS Resolution: 호스트 이름을 사용하는 기본값을 그대로 사용합니다. 다음으로 이동하여 값을 재확인 후 Create 클릭\n아래와 같이 VCN 자원이 생성되는 것을 볼 수 있습니다.\n","lastmod":"2019-01-23T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/2/","tags":["load balancer","VCN"],"title":"10.2 VCN 구성"},{"categories":null,"contents":"14.1.1.2 리눅스계열에서 OCI CLI 설치하기 OCI Documentation Installing the CLI을 참조하여 설치합니다.\n터미널 실행\n설치된 python 버전을 확인합니다.\n$ python3 --version Python 3.6.8 Oracle Linux 8이고 python 버전이 3.6.x인 경우 삭제후 업그레이드 합니다.\nsudo dnf -y remove python36 sudo dnf -y module enable python39 sudo dnf -y install python39 python3 --version 설치 스크립트 실행을 위해 다음 실행\nbash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34; Python 설치여부: Y 응답 설치 경로에 대한 확인 후 설정 CLI의 새 버전 업데이트 확인시 Y 응답 PATH에 CLI 추가 요청시 Y 응답 설치 예시\n$ bash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34; % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 16926 100 16926 0 0 42104 0 --:--:-- --:--:-- --:--:-- 42000 ****************************************************************************** You have started the OCI CLI Installer in interactive mode. If you do not wish to run this in interactive mode, please include the --accept-all-defaults option. If you have the script locally and would like to know more about input options for this script, then you can run: ./install.sh -h If you would like to know more about input options for this script, refer to: https://github.com/oracle/oci-cli/blob/master/scripts/install/README.rst ****************************************************************************** Downloading Oracle Cloud Infrastructure CLI install script from https://raw.githubusercontent.com/oracle/oci-cli/v3.2.1/scripts/install/install.py to /tmp/oci_cli_install_tmp_KCvI. ############################################################################################################################################################ 100.0% Running install script. python3 /tmp/oci_cli_install_tmp_KCvI -- Verifying Python version. -- Python version 3.6.8 okay. ===\u0026gt; In what directory would you like to place the install? (leave blank to use \u0026#39;/home/opc/lib/oracle-cli\u0026#39;): -- Creating directory \u0026#39;/home/opc/lib/oracle-cli\u0026#39;. -- We will install at \u0026#39;/home/opc/lib/oracle-cli\u0026#39;. ===\u0026gt; In what directory would you like to place the \u0026#39;oci\u0026#39; executable? (leave blank to use \u0026#39;/home/opc/bin\u0026#39;): -- Creating directory \u0026#39;/home/opc/bin\u0026#39;. -- The executable will be in \u0026#39;/home/opc/bin\u0026#39;. ===\u0026gt; In what directory would you like to place the OCI scripts? (leave blank to use \u0026#39;/home/opc/bin/oci-cli-scripts\u0026#39;): -- Creating directory \u0026#39;/home/opc/bin/oci-cli-scripts\u0026#39;. -- The scripts will be in \u0026#39;/home/opc/bin/oci-cli-scripts\u0026#39;. ===\u0026gt; Currently supported optional packages are: [\u0026#39;db (will install cx_Oracle)\u0026#39;] What optional CLI packages would you like to be installed (comma separated names; press enter if you don\u0026#39;t need any optional packages)?: -- The optional packages installed will be \u0026#39;\u0026#39;. -- Trying to use python3 venv. -- Executing: [\u0026#39;/usr/bin/python3\u0026#39;, \u0026#39;-m\u0026#39;, \u0026#39;venv\u0026#39;, \u0026#39;/home/opc/lib/oracle-cli\u0026#39;] -- Executing: [\u0026#39;/home/opc/lib/oracle-cli/bin/pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--upgrade\u0026#39;, \u0026#39;pip\u0026#39;] Collecting pip Downloading https://files.pythonhosted.org/packages/a4/6d/6463d49a933f547439d6b5b98b46af8742cc03ae83543e4d7688c2420f8b/pip-21.3.1-py3-none-any.whl (1.7MB) 100% |████████████████████████████████| 1.7MB 1.0MB/s Installing collected packages: pip Found existing installation: pip 9.0.3 Uninstalling pip-9.0.3: Successfully uninstalled pip-9.0.3 Successfully installed pip-21.3.1 You are using pip version 21.3.1, however version 23.1.2 is available. You should consider upgrading via the \u0026#39;pip install --upgrade pip\u0026#39; command. -- Executing: [\u0026#39;/home/opc/lib/oracle-cli/bin/pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;/tmp/tmpy9ltxtwg\u0026#39;, \u0026#39;wheel\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting wheel Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB) Installing collected packages: wheel Successfully installed wheel-0.37.1 -- Executing: [\u0026#39;/home/opc/lib/oracle-cli/bin/pip\u0026#39;, \u0026#39;install\u0026#39;, \u0026#39;--cache-dir\u0026#39;, \u0026#39;/tmp/tmpy9ltxtwg\u0026#39;, \u0026#39;oci_cli\u0026#39;, \u0026#39;--upgrade\u0026#39;] Collecting oci_cli Downloading oci_cli-3.26.0-py3-none-any.whl (37.7 MB) |████████████████████████████████| 37.7 MB 7.1 MB/s Collecting oci==2.100.0 Downloading oci-2.100.0-py3-none-any.whl (20.4 MB) |████████████████████████████████| 20.4 MB 66.5 MB/s 01 Collecting python-dateutil\u0026lt;3.0.0,\u0026gt;=2.5.3 Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB) |████████████████████████████████| 247 kB 72.8 MB/s Collecting prompt-toolkit==3.0.29 Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB) |████████████████████████████████| 381 kB 65.9 MB/s Collecting cryptography\u0026lt;40.0.0,\u0026gt;=3.2.1 Downloading cryptography-39.0.2-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB) |████████████████████████████████| 4.2 MB 61.2 MB/s Collecting certifi Downloading certifi-2023.5.7-py3-none-any.whl (156 kB) |████████████████████████████████| 156 kB 72.0 MB/s Collecting terminaltables==3.1.0 Downloading terminaltables-3.1.0.tar.gz (12 kB) Preparing metadata (setup.py) ... done Collecting pyOpenSSL\u0026lt;24.0.0,\u0026gt;=17.5.0 Downloading pyOpenSSL-23.1.1-py3-none-any.whl (57 kB) |████████████████████████████████| 57 kB 10.8 MB/s Collecting pytz\u0026gt;=2016.10 Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB) |████████████████████████████████| 502 kB 60.2 MB/s Collecting PyYAML\u0026lt;6,\u0026gt;=5.4 Downloading PyYAML-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (640 kB) |████████████████████████████████| 640 kB 57.0 MB/s Collecting six\u0026gt;=1.15.0 Downloading six-1.16.0-py2.py3-none-any.whl (11 kB) Collecting jmespath==0.10.0 Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB) Collecting click==8.0.4 Downloading click-8.0.4-py3-none-any.whl (97 kB) |████████████████████████████████| 97 kB 13.8 MB/s Collecting arrow\u0026gt;=1.0.0 Downloading arrow-1.2.3-py3-none-any.whl (66 kB) |████████████████████████████████| 66 kB 9.7 MB/s Collecting importlib-metadata Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB) Collecting circuitbreaker\u0026lt;2.0.0,\u0026gt;=1.3.1 Downloading circuitbreaker-1.4.0.tar.gz (9.7 kB) Preparing metadata (setup.py) ... done Collecting wcwidth Downloading wcwidth-0.2.6-py2.py3-none-any.whl (29 kB) Collecting typing-extensions Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB) Collecting cffi\u0026gt;=1.12 Downloading cffi-1.15.1-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (402 kB) |████████████████████████████████| 402 kB 70.8 MB/s Collecting pycparser Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB) |████████████████████████████████| 118 kB 71.5 MB/s Collecting zipp\u0026gt;=0.5 Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB) Building wheels for collected packages: terminaltables, circuitbreaker Building wheel for terminaltables (setup.py) ... done Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15355 sha256=277a3a996dc68625a3b308f49e251b1e433d894e585bdb3fdb435afedfe81073 Stored in directory: /tmp/tmpy9ltxtwg/wheels/86/1b/58/c23af2fe683acd8edc15d5a1268f0242be1ff2cf827fe34737 Building wheel for circuitbreaker (setup.py) ... done Created wheel for circuitbreaker: filename=circuitbreaker-1.4.0-py3-none-any.whl size=7506 sha256=be32c1d739a7e4b2fba9dd4ed7fafb6443ad42082f4a338aebb935506ee2a613 Stored in directory: /tmp/tmpy9ltxtwg/wheels/46/17/98/db2eb826e4a98da672cffe66ec16838182cde0cf19ad2c0c70 Successfully built terminaltables circuitbreaker Installing collected packages: pycparser, cffi, zipp, typing-extensions, six, cryptography, wcwidth, pytz, python-dateutil, pyOpenSSL, importlib-metadata, circuitbreaker, certifi, terminaltables, PyYAML, prompt-toolkit, oci, jmespath, click, arrow, oci-cli Successfully installed PyYAML-5.4.1 arrow-1.2.3 certifi-2023.5.7 cffi-1.15.1 circuitbreaker-1.4.0 click-8.0.4 cryptography-39.0.2 importlib-metadata-4.8.3 jmespath-0.10.0 oci-2.100.0 oci-cli-3.26.0 prompt-toolkit-3.0.29 pyOpenSSL-23.1.1 pycparser-2.21 python-dateutil-2.8.2 pytz-2023.3 six-1.16.0 terminaltables-3.1.0 typing-extensions-4.1.1 wcwidth-0.2.6 zipp-3.6.0 ===\u0026gt; Modify profile to update your $PATH and enable shell/tab completion now? (Y/n): ===\u0026gt; Enter a path to an rc file to update (file will be created if it does not exist) (leave blank to use \u0026#39;/home/opc/.bashrc\u0026#39;): -- Backed up \u0026#39;/home/opc/.bashrc\u0026#39; to \u0026#39;/home/opc/.bashrc.backup\u0026#39; -- Tab completion set up complete. -- If tab completion is not activated, verify that \u0026#39;/home/opc/.bashrc\u0026#39; is sourced by your shell. -- -- ** Run `exec -l $SHELL` to restart your shell. ** -- -- Installation successful. -- Run the CLI with /home/opc/bin/oci --help $ ","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/2/","tags":["linux","CLI"],"title":"14.1.1.2 리눅스계열에서 OCI CLI 설치하기"},{"categories":null,"contents":"5.2 방법 #1. Default Security List 업데이트 하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\n원하는 Instance를 클릭하여 상세 페이지로 이동합니다.\n왼쪽 아래의 Resources \u0026gt; Attached VNICs를 클릭합니다.\n그림과 같이 사용 중인 가상 네트워크 인터페이스 카드, VNIC(Virtual Network Interface Card)이 보입니다. 이 네트워크 카드가 속한 Subnet을 클릭합니다. 물론 VNIC 여러개 인 경우에는, 각각 속한 Subnet 설정에 영향을 받습니다.\n해당 Subnet의 기본 Security List를 클릭합니다.\n현재 설정된 방화벽 규칙을 확인할 수 있습니다. 앞서 설치된 Apache HTTP 서버가 사용하는 수신 포트의 개방하기 위해 Ingress Rule을 수정이 필요합니다.\n추가를 위해 Add Ingress Rules을 클릭합니다.\n아래 규칙을 추가합니다.\nSource CIDR: 0.0.0.0/0, 모든 인터넷에서 오는 요청\nIP Protocol: TCP\nDestination Port Range: 80, 개방할 포트\n아래와 같이 Ingress Rule이 추가되었습니다.\n","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/2/","tags":["security list"],"title":"5.2 Default Security List 업데이트 하기"},{"categories":null,"contents":"3.5.2 Windows에서 PuTTY로 접속하기 PuTTY Key Generator로 생성한 SSH Key는 Save private key를 통해 PuTTY Private Key (PPK) .ppk로 저장하여 PuTTY에서 사용합니다. ssh-keygen로 생성한 PEM(Privacy Enhanced Mail) 파일 형식의 키는 PEM -\u0026gt; PPK 변환을 통해 PuTTY에서 사용 가능합니다.\nPuTTY 설치 PuTTY가 설치되어 있지 않다면, http://www.putty.org/ 에서 putty를 내려 받아 설치합니다.\nWindows에서 Linux 인스턴스 접속하기 putty.exe 를 실행\nCategory에서 Session을 선택하고 다음을 입력\nHost Name (or IP address):\n인스턴스의 Public IP입니다. OCI 콘솔에서 확인할 수 있습니다.\nPort: 22\nConnection type: SSH\nCategory 창에서 Window 하위에 Translation 선택\nRemote character set 드랍 다운 리스트에서 UTF-8을 선택\nLinux 기반 인스턴스는 디폴트 로케일이 UTF-8입니다. 그래서 PuTTY에서도 동일하게 설정합니다.\nCategory 창에서 Connection 하위에 SSH 그리고 Auth 클릭\nBrowse를 클릭하고, 인스턴스 접속을 위한 개인키를 선택\nCategory 창에서 Session으로 이동하여 세션을 저장합니다.\nOpen을 클릭하여 세션을 시작합니다.\n접속하려는 인스턴스에 처음 접속한다면, 서버의 호스트 키가 레지스트리에 캐쉬되지 않았다는 알람 메시지가 뜨게 됩니다. Accept를 클릭합니다.\n디폴트 사용자 로그인\n디폴트 접속 사용자 OS Image Default User Name Oracle Linux, CentOS opc ubuntu ubuntu ","lastmod":"2018-12-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/2/","tags":["windows","ssh","putty"],"title":"3.5.2 Windows에서 PuTTY로 접속하기"},{"categories":null,"contents":"1.2 Fault Domains Fault Domain은 Availability Domain 내에 하드웨어와 인프라스트럭쳐의 그룹입니다. 각 Availability Domain은 3개의 Fault Domain을 가지고 있습니다. Fault Domain은 Compute 인스턴스를 단일 Availability Domain 내에 하나의 물리적 하드웨어에 분배하지 않도록 합니다. 하나의 Fault Domain 내의 하드웨어 장애 또는 하드웨어 유지 보수 작업은 다른 Fault Domain에 영향을 끼치지 않습니다.\n새 Compute 인스턴스를 생성할 때, 사용자는 Fault Domain을 선택할 수 있습니다. 사용자가 선택하지 않으면, 시스템이 자동으로 지정합니다. 인스턴스의 Fault Domain을 변경하기 위해서는 인스턴스를 Terminate 시키고 인스턴스를 다시 생성하여야 하므로 새로 만들 때 잘 선택해야 합니다.\nFault Domains 사용하는 경우\n예상하지 못한 하드웨어 실패에 대비하기 위해 계획된 하드웨어 유지 보수 작업에 대비하기 위해 ","lastmod":"2018-12-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/2/","tags":["fault domain"],"title":"1.2 Fault Domains"},{"categories":null,"contents":"1.5.2.3.2 FluentD로 OKE 로그 전송하기 EFK 구성에서 ElasticSearch와 Kibana 대신 OpenSearch와 OpenSearch Dashboard를 사용할 수 있도록 FluentD를 설정합니다.\nFluentd 구성 Fluentd 설치를 위한 Service Account를 생성하고 관련 권한을 정의합니다.\nconfigmap 추가 설정정의\nFluentd 관련 설정은 컨테이너 내에 /fluentd/etc/ 하위에 .conf 파일로 모두 정의 되어 있습니다. 이 파일들을 재정의 할 수 있습니다. OCI OpenSearch 연결을 위해 꼭 필요한 항목만 남겨두고 나머지는 주석 처리하였습니다. fluentd daemonset 정의\n설정한 configmap 사용을 위해 Fluentd 문서상의 YAML을 일부 변경하셨습니다. configmap으로 재정의한 fluent.conf을 사용하도록 변경하고 있습니다. FluentD 설치 fluentd-daemonset-opensearch.yaml에서 FLUENT_OPENSEARCH_HOST를 대상 OpenSearch 호스트로 변경합니다.\n... env: - name: FLUENT_OPENSEARCH_HOST value: \u0026#34;amaaaaaavsea7yiatk7bvqasj524dqnutj3itigg3tyviql4zppawdjdfspa.opensearch.ap-chuncheon-1.oci.oraclecloud.com\u0026#34; - name: FLUENT_OPENSEARCH_PORT value: \u0026#34;9200\u0026#34; ... 또한 OpenSearch API에 사용할 사용자 인증 정보를 입력합니다. 클러스터 생성시 입력한 Primary Account의 유저, 패스워드를 입력합니다.\n... env: ... - name: FLUENT_OPENSEARCH_USER value: \u0026#34;username\u0026#34; - name: FLUENT_OPENSEARCH_PASSWORD value: \u0026#34;changeme\u0026#34; ... FluentD 설치\nkubectl apply -f fluentd-rbac.yaml kubectl apply -f fluentd-configmap-opensearch.yaml kubectl apply -f fluentd-daemonset-opensearch.yaml OKE 클러스터 Worker Node에서 OpenSearch로 로그를 보낼수 있도록 보안규칙을 추가합니다.\nWorker Node의 보안규칙\n대상: 예) oke-nodeseclist-quick-oke-cluster-1-04cdcc334\nEgress Rules:\n아래 예시는 OpenSearch 위치가 서브넷인 10.0.20.0/24인 경우\nStateless Destination IP Protocol Source Port Range Destination Port Range Description No 10.0.20.0/24 TCP All 9200 OpenSearch API OpenSearch의 보안규칙\n대상: OpenSearch가 속한 서브넷의 Security List\nIngress Rules:\n아래 예시는 Worker Node가 위치가 서브넷인 10.0.10.0/24인 경우\nStateless Source IP Protocol Source Port Range Destination Port Range Description No 10.0.10.0/24 TCP All 9200 OpenSearch API 로그 확인\nfluentd Pod가 기동하면, 로그를 통해 OpenSearch와 연결 오류 없이 정상 동작하는 지 확인합니다.\n$ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE ... fluentd-gls8b 1/1 Running 0 2m15s fluentd-jhpxj 1/1 Running 0 87s fluentd-l9mbn 1/1 Running 0 45s ... $ kubectl logs -n kube-system -f fluentd-gls8b ... 2023-04-21 15:00:05 +0000 [info]: parsing config file is succeeded path=\u0026#34;/fluentd/etc/fluent.conf\u0026#34; ... 2023-04-21 15:00:07 +0000 [info]: adding match pattern=\u0026#34;**\u0026#34; type=\u0026#34;opensearch\u0026#34; ... 2023-04-21 15:00:07 +0000 [info]: #0 starting fluentd worker pid=14 ppid=6 worker=0 2023-04-21 15:00:07 +0000 [info]: #0 [in_tail_container_logs] following tail of /var/log/containers/coredns-f7c884d6f-9kxr5_kube-system_coredns-79f0a4c0099003d963547e7e3f54d78265c84dbe85f26d1adf52ff3dca931c6a.log ... OpenSearch Dashboard 설정 앞서 사용한 방법을 통해 다시 브라우저로 OpenSearch Dashboard에 접속을 확인합니다.\n왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Discover 를 클릭합니다.\nCreate index pattern을 클릭합니다.\n인덱스 패턴을 생성합니다.\n아래쪽에 보이는 소스 중에서 fluentd 소스를 사용합니다.\nIndex pattern name: fluentd* Time field: time 인덱스 패턴이 추가된 결과를 볼 수 있습니다.\n왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Discover 를 클릭합니다.\n생성한 인덱스 패턴을 통해 수집된 로그를 확인할 수 있습니다.\n테스트 앱의 로그를 확인하기 위해 Add filter를 클릭하여 namespace_name=default 로 지정합니다. 테스트 앱을 접속합니다.\n예) http://129.154.xxx.xxx/?opensearch-test 로그 확인\n아래와 같이 OpenSearch Dashboard에서 테스트 앱의 로그를 확인할 수 있습니다.\nFluentd에서 사용할 OpenSearch 유저 만들기 OpenSearch Internal 유저 만들기\nOpenSearch Dashboard에 Primary Account로 로그인합니다.\n왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Security 를 클릭합니다.\nInternal Users \u0026gt; Create internal user를 클릭합니다.\n생성할 유저명과 패스워드를 입력하고, 아래의 Create를 클릭하여 유저를 생성합니다.\nOpenSearch Role 만들기\n메뉴에서 Roles을 선택합니다.\n기본 설치된 Role이 보입니다. fluentd는 logstash와 동일한 권한이 필요하므로, 기본 생성된 logstash Role의 복사본을 만들어 인덱스 패턴만 변경하도록 하겠습니다.\nName: 이름을 fluentd로 변경합니다.\nCluster permissions: 변경없이 그대로 사용합니다.\nIndex permissions: Index에서 기존 패턴(logstatsh*)을 삭제하고, fluentd daemon에서 생성하는 인덱스 패턴을 입력합니다. fluentd*로 입력합니다. Index permission은 변경없이 그대로 사용합니다.\nIndex permissions에서 두번째로 있는 *beat*는 삭제합니다.\n하단 Create를 클릭하여, 설정한 Role을 생성합니다.\n생성한 fluentd Role에 유저를 매핑하기 위해 Mapped users 탭을 클릭하고, Manage mapping을 클릭합니다.\n앞서 생성한 fluentd 유저를 선택하고 매핑합니다.\nfluentd를 위한 유저 및 권한을 위한 Custom Role이 생성 완료되었습니다.\nFluentD 재배포 하기\n앞서 OKE 클러스터에 배포한 fluentd-daemonset-opensearch.yaml에서 사용자 인증 정보를 새로 만든 유저정보로 변경합니다.\n... env: ... - name: FLUENT_OPENSEARCH_USER value: \u0026#34;fluentd\u0026#34; - name: FLUENT_OPENSEARCH_PASSWORD value: \u0026#34;changeme\u0026#34; ... FluentD 재배포\nkubectl apply -f fluentd-daemonset-opensearch.yaml 로그 확인\nfluentd Pod 재기동이 완료되면, 로그를 통해 OpenSearch와 연결 오류 없이 정상 동작하는 지 확인합니다.\n$ kubectl apply -f fluentd-daemonset-opensearch.yaml daemonset.apps/fluentd configured $ kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE ... fluentd-dl74t 1/1 Running 0 22s fluentd-gc6gk 1/1 Running 0 27s fluentd-v8nfj 1/1 Running 0 32s ... $ kubectl logs -n kube-system -f fluentd-dl74t ... 2023-04-22 14:05:57 +0000 [info]: parsing config file is succeeded path=\u0026#34;/fluentd/etc/fluent.conf\u0026#34; ... 2023-04-22 14:05:59 +0000 [info]: adding match pattern=\u0026#34;**\u0026#34; type=\u0026#34;opensearch\u0026#34; ... 2023-04-22 14:05:59 +0000 [info]: #0 starting fluentd worker pid=14 ppid=6 worker=0 2023-04-22 14:05:59 +0000 [info]: #0 [in_tail_container_logs] following tail of /var/log/containers/coredns-f7c884d6f-xdgnw_kube-system_coredns-3195b55827bcaa7cd928bdf90d486ca7de4d5dd467ff63270dcaf132378d2758.log ... ","lastmod":null,"permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/oci-opensearch/2.fluentd-to-oci-opensearch/","tags":["oke","oci opensearch","opensearch"],"title":"1.5.2.3.2 FluentD로 OKE 로그 전송하기"},{"categories":null,"contents":"17.3 Kafka Connect, Debezium로 OCI MySQL Database Service에 CDC 구성하기 OCI Streaming - Kafka Connect, Debezium로 MySQL CDC 구성하기을 먼저 본 것을 전제로 합니다.\n이전 문서에서 debezium에서 제공하는 컨테이너 이미지를 사용하여, Source MySQL -\u0026gt; Debezium Connector -\u0026gt; OCI Streaming -\u0026gt; JDBC Connector -\u0026gt; Target MySQL 구성하고, CDC를 테스트하였습니다.\n여기서는 MySQL을 OCI MySQL Database Service로 만들어진 MySQL을 사용할때, Kafka Connect를 구성시 주의사항이 있는 지 확인해 보도록 하겠습니다.\nSource, Target MySQL Database Service 인스턴스 구성 먼저, Source, Target으로 사용할 MySQL 데이터베이스 인스턴스를 만듭니다.\nDebezium release overview\nDebezium 2.6 릴리즈 문서에서 테스트된 버전이라는 MySQL 8.0.x, 8.2 을 사용합니다. OCI MySQL Database Service로 생성시 해당 버전으로 선택합니다. 2.7 - development 2.6 - latest stable 2.5 Java 11+ 11+ 11+ Kafka Connect 2.x, 3.x 2.x, 3.x 2.x, 3.x MySQL Database: 8.0.x, 8.2 Driver: 8.3.0 Database: 8.0.x, 8.2 Driver: 8.0.33 Database: 8.0.x, 8.2 Driver: 8.0.33 Source MySQL 생성 정보\n왼쪽 상단의 Navigation Menu를 클릭하고 Databases로 이동한 다음 MySQL HeatWave 하위 DB systems를 선택 합니다.\n생성을 위해 Create DB system을 클릭합니다.\nDevelopment or testing 유형을 선택합니다.\nProvide DB system information:\nName: 이름을 입력합니다. 예) mysql-source Create Administrator credentials: 관리자 이름과 암호를 입력합니다.\nUsername: admin\nPassword: 예) Password123!\nAdministrator password must be between 8 and 32 characters, and contain at least 1 uppercase, 1 lowercase, 1 numeric, and 1 special characters.\nOCI MySQL Database Service에서 Administrator는 root user랑은 다르며, root user보다는 제한된 권한을 가지고 있습니다. Default MySQL Privileges을 참조하세요\nDB system 타입: Standalone을 선택합니다.\nConfigure networking: Private IP만을 사용하므로, Kafka Connect와 같은 서브넷 또는 접근 가능한 서브넷을 선택합니다.\nConfigure placement: 기본값을 선택\nConfigure hardware:\nEnable HeatWave: 체크 해제 Shape Details: 기본 선택된 MySQL.2을 사용합니다. Configure backup plan\nEnable automatic backups: 체크 해제 고급 옵션을 엽니다.\nConfiguration MySQL version: Debezium release을 기준으로 8.0.x을 선택합니다. 예, 8.0.37을 선택 Create 버튼을 클릭하여, DB를 생성합니다.\n일단 시작되면 MySQL DB가 프로비저닝됩니다. 위 설정기준으로 Active 상태로 되기까지, 약 10~15분 정도 걸립니다.\nSecurity List Ingress 규칙에 3306 포트를 개방합니다.\nTarget MySQL 생성 정보\n이름만 달리하여, MySQL Database Service 인스턴스를 하나 더 만듭니다. Name: mysql-target Source DB에 설정 확인\nDebezium Setting up MySQL을 기준으로 설정값을 확인해 봅니다.\nAdministrator로 MySQL에 접속\nmysql --host \u0026lt;Source-MySQL-Public-IP\u0026gt; -u admin --password=Password123! Enabling the binlog: ON으로 이미 설정됨\nmysql\u0026gt; SELECT variable_value as \u0026#34;BINARY LOGGING STATUS (log-bin) ::\u0026#34; -\u0026gt; FROM performance_schema.global_variables WHERE variable_name=\u0026#39;log_bin\u0026#39;; +------------------------------------+ | BINARY LOGGING STATUS (log-bin) :: | +------------------------------------+ | ON | +------------------------------------+ 1 row in set (0.00 sec) Enabling GTIDs: ON으로 이미 설정됨\nmysql\u0026gt; show global variables like \u0026#39;%GTID%\u0026#39;; +----------------------------------------------+-------+ | Variable_name | Value | +----------------------------------------------+-------+ ... | enforce_gtid_consistency | ON | ... | gtid_mode | ON | ... +----------------------------------------------+-------+ 9 rows in set (0.00 sec) Enabling query log events: ON으로 이미 설정됨\nmysql\u0026gt; show global variables where variable_name = \u0026#39;binlog_rows_query_log_events\u0026#39;; +------------------------------+-------+ | Variable_name | Value | +------------------------------+-------+ | binlog_rows_query_log_events | ON | +------------------------------+-------+ 1 row in set (0.00 sec) Validating binlog row value options: \u0026quot;\u0026quot; - 설정값이 없어야 하나 PARTIAL_JSON 로 설정된 상태\nmysql\u0026gt; show global variables where variable_name = \u0026#39;binlog_row_value_options\u0026#39;; +--------------------------+--------------+ | Variable_name | Value | +--------------------------+--------------+ | binlog_row_value_options | PARTIAL_JSON | +--------------------------+--------------+ 1 row in set (0.00 sec) PARTIAL_JSON인 경우 빈 값으로 설정해야 하나, Administrator 권한으로 설정할 수 없습니다.\nmysql\u0026gt; set @@global.binlog_row_value_options=\u0026#34;\u0026#34; ; ERROR 1227 (42000): Access denied; you need (at least one of) the SUPER or SYSTEM_VARIABLES_ADMIN privilege(s) for this operation PARTIAL_JSON로 설정된 경우, UPDATE 이벤트를 처리하는 데 실패할 수 있다고 합니다.\n문서 상의 원문: make sure that value is not set to PARTIAL_JSON, since in such case connector might fail to consume UPDATE events. Source DB에 CDC 관련 권한 설정\nAdministrator로 MySQL에 접속\nmysql --host \u0026lt;Source-MySQL-Public-IP\u0026gt; -u admin --password=Password123! 데이터베이스 및 유저 생성\nCREATE DATABASE sourcedb; CREATE USER \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Password123!\u0026#39;; GRANT ALL PRIVILEGES ON sourcedb.* TO \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39;; 복제를 위한 권한 mysqluser 유저에 설정\nGRANT SELECT, RELOAD, SHOW DATABASES, LOCK TABLES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39;; 이 중 RELOAD는 Administrator가 부여할 수 없는 권한으로 제외하고, 나머지 권한을 부여\n### RELOAD는 Administrator가 부여할 수 없는 권한 mysql\u0026gt; GRANT RELOAD ON *.* TO \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39;; ERROR 1045 (28000): Access denied for user \u0026#39;admin\u0026#39;@\u0026#39;%\u0026#39; (using password: YES) ### RELOAD 제외하고 권한 부여 GRANT SELECT, SHOW DATABASES, LOCK TABLES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39;; Debezium에서 설명하는 유저 권한을 snapshot을 실행할 때 이 RELOAD 권한이 필요하다고 합니다. 당장 테스트에는 문제가 없지만, 참고합니다.\nhttps://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-creating-user Keyword Description RELOAD Enables the connector the use of the FLUSH statement to clear or reload internal caches, flush tables, or acquire locks. This is used only when performing a snapshot. 접속을 종료합니다.\nquit Target DB 접속 확인\nAdministrator로 MySQL에 접속\nmysql --host \u0026lt;Target-MySQL-Public-IP\u0026gt; -u admin --password=Password123! 데이터베이스 및 유저 생성\nCREATE DATABASE targetdb; CREATE USER \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;Password123!\u0026#39;; GRANT ALL PRIVILEGES ON targetdb.* TO \u0026#39;mysqluser\u0026#39;@\u0026#39;%\u0026#39;; 접속종료\nquit mysqluser 로 접속\nmysql --host \u0026lt;Target-MySQL-Public-IP\u0026gt; -u mysqluser --password=Password123! OCI Streaming 및 Kafka Connect 설치 및 구성\nOCI Streaming - Kafka Connect, Debezium로 MySQL CDC 구성하기과 동일합니다. Connector에서 MySQL 서버 주소 및 접속 사용자 정보만 변경하면 됩니다.\nCDC 테스트 #1 Source DB에 데이터 변경분 발생\n데이터베이스 접속\nmysql --host \u0026lt;Source-MySQL-Public-IP\u0026gt; -u mysqluser --password=Password123! DATABASE 선택\nuse sourcedb; 샘플 테이블 생성\ncreate table employees ( emp_no int not null, birth_date date not null, first_name varchar(14) not null, last_name varchar(16) not null, gender enum (\u0026#39;M\u0026#39;,\u0026#39;F\u0026#39;) not null, hire_date date not null, primary key (emp_no) ); 새 데이터 삽입\nINSERT INTO employees VALUES (10001,\u0026#39;1953-09-02\u0026#39;,\u0026#39;Georgi\u0026#39;,\u0026#39;Facello\u0026#39;,\u0026#39;M\u0026#39;,\u0026#39;1986-06-26\u0026#39;); 데이터를 확인합니다.\nmysql\u0026gt; select * from employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | +--------+------------+------------+-----------+--------+------------+ 1 row in set (0.00 sec) Target DB에 데이터 확인\n데이터베이스 접속\nmysql --host \u0026lt;Target-MySQL-Public-IP\u0026gt; -u mysqluser --password=Password123! DATABASE 접속\nuse targetdb; 현재 데이터 확인합니다. 테이블이 생성되고 데이터가 들어간 것을 확인할 수 있습니다.\nmysql\u0026gt; select * from employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | +--------+------------+------------+-----------+--------+------------+ 1 rows in set (0.00 sec) Target DB에 동일 테이블이 생성되고 데이터도 동기화되었습니다.\nCDC 테스트 #2 - UPDATE Source DB에 데이터 변경분 발생\nDATABASE 선택\nuse sourcedb; 업데이트 및 새 데이터 삽입\nUPDATE employees SET last_name=\u0026#39;Facello - New\u0026#39; WHERE emp_no=10001; INSERT INTO employees VALUES (10002,\u0026#39;1964-06-02\u0026#39;,\u0026#39;Bezalel\u0026#39;,\u0026#39;Simmel\u0026#39;,\u0026#39;F\u0026#39;,\u0026#39;1985-11-21\u0026#39;); 데이터를 확인합니다.\nmysql\u0026gt; select * from employees; +--------+------------+------------+---------------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+---------------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello - New | M | 1986-06-26 | | 10002 | 1964-06-02 | Bezalel | Simmel | F | 1985-11-21 | +--------+------------+------------+---------------+--------+------------+ 2 rows in set (0.00 sec) Target DB에 데이터 확인\nDATABASE 접속\nuse targetdb; 현재 데이터 확인합니다. 테이블이 생성되고 데이터가 들어간 것을 확인할 수 있습니다.\nmysql\u0026gt; select * from employees; +--------+------------+------------+-----------+--------+------------+ | emp_no | birth_date | first_name | last_name | gender | hire_date | +--------+------------+------------+-----------+--------+------------+ | 10001 | 1953-09-02 | Georgi | Facello | M | 1986-06-26 | | 10002 | 1964-06-02 | Bezalel | Simmel | F | 1985-11-21 | +--------+------------+------------+-----------+--------+------------+ 2 rows in set (0.01 sec) binlog_row_value_options=PARTIAL_JSON 설정 때문인지, 추가 UPDATE, INSERT 변경분에 대해서 INSERT만 반영되고, UPDATE한 last_name 변경분은 Target DB에 반영되지 않은 것을 볼 수 있습니다.\nOCI MySQL Database Service 인스턴스를 Debezium MySQL Connector로 사용시 제한사항 확인결과 필요한 권한 중에서 RELOAD 권한을 Administrator로는 설정할 수 없어, snapshot 관련 기능을 사용하는 데 제한사항이 있습니다. binlog_row_value_options=PARTIAL_JSON 설정값을 Administrator 빈값으로 변경 설정할 수 없어, Source DB 변경분 중에서 UPDATE 변경분을 반영할 수 없었습니다. ","lastmod":"2024-05-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter17/oci-oss-cdc-mds-debezium/","tags":["oci streaming","kafka connect","cdc","debezium"],"title":"17.3 Kafka Connect, Debezium로 OCI MySQL Database Service에 CDC 구성하기"},{"categories":null,"contents":"1.3.3 Cluster Autoscaler Kubernetes Cluster Autoscaler는\n리소스가 요구하는 자원이 있는 Pod에 대해, Pod에게 할당할 자원이 부족하여 Pod를 스케줄할 수 없는 경우, 해당 노드 풀에 Worker Node를 추가합니다. 장시간 동안 Worker Node의 활용도가 낮고 Pod를 다른 노드에 배치할 수 있는 경우, 노드 풀에서 Worker Node를 제거합니다. Kubernetes Cluster Autoscaler는 매뉴얼하게 직접 설치하거나, Cluster Add-On으로 설치할 수 있습니다. 여기서는 직접 설치하는 것을 기준합니다.\nStep 1: Cluster Autoscaler가 노드 풀에 접근할 수 있도록, Instance Principal 설정하기 Cluster Autoscaler가 필요한 OCI 자원을 관리할 수 있도록 권한을 부여합니다. Instance Principal 또는 Workload Identity Principal을 사용할 수 있습니다. Basic Cluster에서 사용할 수 있는 Instance Principal을 여기서는 편의상 사용합니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\nOKE 클러스터가 있는 Compartment의 OCID를 확인합니다.\n좌측 Dynamic Group 메뉴로 이동하여 아래 규칙을 가진 Dynamic Group을 만듭니다.\nName: 예, oke-cluster-autoscaler-dyn-grp instance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39; 좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Policies로 이동합니다.\n아래 규칙을 가진 Policy를 만듭니다\nName: 예, oke-cluster-autoscaler-dyn-grp-policy dynamic-group-name: 앞서 만든 dynamic group 이름, 예, oke-cluster-autoscaler-dyn-grp compartment-name: 대상 OKE Cluster가 위치한 compartment 이름 Allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to manage cluster-node-pools in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to manage instance-family in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use subnets in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to read virtual-network-family in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use vnics in compartment \u0026lt;compartment-name\u0026gt; Allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to inspect compartments in compartment \u0026lt;compartment-name\u0026gt; Step 2: Cluster Autoscaler 설정파일 설정하기 설정 파일 샘플을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/oci/examples/oci-nodepool-cluster-autoscaler-w-principals.yaml -O cluster-autoscaler.yaml 설정 파일 변경\n... --- apiVersion: apps/v1 kind: Deployment metadata: name: cluster-autoscaler namespace: kube-system labels: app: cluster-autoscaler spec: ... template: metadata: ... spec: serviceAccountName: cluster-autoscaler containers: - image: iad.ocir.io/oracle/oci-cluster-autoscaler:{{ image tag }} name: cluster-autoscaler resources: limits: cpu: 100m memory: 300Mi requests: cpu: 100m memory: 300Mi command: - ./cluster-autoscaler - --v=4 - --stderrthreshold=info - --cloud-provider=oci - --max-node-provision-time=25m - --nodes=1:5:{{ node pool ocid 1 }} - --nodes=1:5:{{ node pool ocid 2 }} - --scale-down-delay-after-add=10m - --scale-down-unneeded-time=10m - --unremovable-node-recheck-timeout=5m - --balance-similar-node-groups - --balancing-ignore-label=displayName - --balancing-ignore-label=hostname - --balancing-ignore-label=internal_addr - --balancing-ignore-label=oci.oraclecloud.com/fault-domain imagePullPolicy: \u0026#34;Always\u0026#34; env: - name: OKE_USE_INSTANCE_PRINCIPAL value: \u0026#34;true\u0026#34; - name: OCI_SDK_APPEND_USER_AGENT value: \u0026#34;oci-oke-cluster-autoscaler\u0026#34; - --cloud-provider=oci를 OKE 클러스터 버전이 1.27이 아닌, 1.26, 1.25, 또는 1.24인 경우 oci-oke로 변경합니다.\n- --cloud-provider=oci-oke - image: iad.ocir.io/oracle/oci-cluster-autoscaler:{{ image tag }}\nFrankfurt, London, Ashburn, Phoenix 리전에 있는 이미지 중에서 가까운 곳에 있는 이미지를 사용하거나, 테넌시 OCIR에 별도로 push하여 사용합니다. 여기서는 Ashburn 리전의 이미지를 사용하겠습니다.\nImage Location Kubernetes Version Image Path US East (Ashburn) Kubernetes 1.25 iad.ocir.io/oracle/oci-cluster-autoscaler:1.25.0-6 US East (Ashburn) Kubernetes 1.26 iad.ocir.io/oracle/oci-cluster-autoscaler:1.26.2-11 US East (Ashburn) Kubernetes 1.27 iad.ocir.io/oracle/oci-cluster-autoscaler:1.27.2-9 US East (Ashburn) Kubernetes 1.28 iad.ocir.io/oracle/oci-cluster-autoscaler:1.28.0-5 - --nodes=1:5:{{ node pool ocid 1 }}\n포맷은 아래와 같습니다. nodepool-ocid에 Cluster Autoscaler가 관리할 Node Pool의 OCID를 입력합니다.\n--nodes=\u0026lt;min-nodes\u0026gt;:\u0026lt;max-nodes\u0026gt;:\u0026lt;nodepool-ocid\u0026gt; - --nodes=1:5:{{ node pool ocid 2 }}\n관리할 두번째 Node Pool이 있는 경우, 설정해 사용하고 아닌 경우 해당 줄은 삭제합니다. 셋 이상인 Node Pool을 관리자 하는 경우 같은 형식으로 추가합니다.\n설정파일을 저장합니다.\nStep 3: OKE 클러스터에 Cluster Autoscaler 배포하기 Kubernetes Cluster Autoscaler을 OKE 클러스터에 배포합니다.\nkubectl apply -f cluster-autoscaler.yaml 배포결과를 확인하기 위해 로그를 확인합니다.\nkubectl -n kube-system logs -f deployment.apps/cluster-autoscaler 배포가 성공하면 다음과 같은 로그가 보입니다. Pod 3개중 하나가 리더가 되고, lock을 잡습니다. 관련 로그가 지속적으로 보이는 것을 볼 수 있습니다.\n$ kubectl -n kube-system logs -f deployment.apps/cluster-autoscaler ... Found 3 pods, using pod/cluster-autoscaler-544c49444b-45gdt ... I0128 12:35:14.650472 1 main.go:474] Cluster Autoscaler 1.26.2 I0128 12:35:14.668348 1 leaderelection.go:248] attempting to acquire leader lease kube-system/cluster-autoscaler... ... I0128 12:35:32.071336 1 leaderelection.go:352] lock is held by cluster-autoscaler-544c49444b-jjzk7 and has not yet expired I0128 12:35:32.071357 1 leaderelection.go:253] failed to acquire lease kube-system/cluster-autoscaler I0128 12:35:35.723678 1 leaderelection.go:352] lock is held by cluster-autoscaler-544c49444b-jjzk7 and has not yet expired I0128 12:35:35.724233 1 leaderelection.go:253] failed to acquire lease kube-system/cluster-autoscaler ... Kubernetes Cluster Autoscaler Pod 세 개 중 어느 Pod가 실제 동작하고 있는 지 확인해 봅니다.\n$ kubectl get pod -l app=cluster-autoscaler -n kube-system NAME READY STATUS RESTARTS AGE cluster-autoscaler-544c49444b-45gdt 1/1 Running 0 7m55s cluster-autoscaler-544c49444b-gzx64 1/1 Running 0 7m55s cluster-autoscaler-544c49444b-jjzk7 1/1 Running 0 7m55s $ kubectl -n kube-system get lease cluster-autoscaler NAME HOLDER AGE cluster-autoscaler cluster-autoscaler-544c49444b-jjzk7 34m Kubernetes Cluster Autoscaler의 상태를 확인하기 위해 Config Map을 확인해 봅니다.\nkubectl -n kube-system get cm cluster-autoscaler-status -oyaml Step 4: 클러스터 오토스케일링 동작 확인해 보기 현재 Worker Node 상태를 확인합니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.158 Ready node 10d v1.26.7 10.0.10.42 Ready node 2d5h v1.26.7 10.0.10.43 Ready node 2d5h v1.26.7 샘플 애플리케이션 배포 파일 예시입니다.\nrequests.cpu를 기본 200 밀리코어까지 사용할 수 있게 지정하였습니다. 0.2 코어로 설정한 예시\n# nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 resources: requests: cpu: 200m 샘플을 배포합니다.\nkubectl apply -f nginx.yaml Pod의 수를 늘립니다.\nkubectl scale deployment nginx-deployment --replicas=40 Deployment 상태를 확인합니다. 배포하다가, 자원을 다 쓰고 더 이상 Pod를 생성하지 못하고 멈춰있게 됩니다.\n$ kubectl get deployment nginx-deployment --watch NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 20/40 40 20 60s nginx-deployment 22/40 40 22 60s nginx-deployment 23/40 40 23 61s nginx-deployment 24/40 40 24 61s 이벤트 로그를 확인해 보면, CPU 부족으로 Pod 스케줄링에 실패한 것을 볼 수 있습니다. 이로 인해 Cluster Scale 이벤트가 발생하고, 노드가 3개에서 5개로 늘어납니다.\n$ kubectl get events --sort-by=.metadata.creationTimestamp ... 5m1s Warning FailedScheduling pod/nginx-deployment-694bc9bdb8-bgqww 0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.. ... 40s Normal TriggeredScaleUp pod/nginx-deployment-694bc9bdb8-bgqww pod triggered scale-up: [{ocid1.nodepool.oc1.ap-chuncheon-1.aaaaaaaabtavqjthmpeivjj5dj7i4yttl74y7ncnvoxgxn7kqna6xzvlolbq 3-\u0026gt;5 (max: 5)}] $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.132 NotReady \u0026lt;none\u0026gt; 5s v1.26.7 10.0.10.158 Ready node 10d v1.26.7 10.0.10.167 NotReady node 13s v1.26.7 10.0.10.42 Ready node 2d5h v1.26.7 10.0.10.43 Ready node 2d5h v1.26.7 확장된 Worker Node가 Ready 상태가 되면 나머지 Pod에 대한 스케줄링이 진행됩니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.132 Ready node 3m52s v1.26.7 10.0.10.158 Ready node 10d v1.26.7 10.0.10.167 Ready node 4m v1.26.7 10.0.10.42 Ready node 2d5h v1.26.7 10.0.10.43 Ready node 2d5h v1.26.7 $ kubectl get deployment nginx-deployment --watch NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 24/40 40 24 61s ... ... ... nginx-deployment 25/40 40 25 7m18s ... nginx-deployment 40/40 40 40 7m51s Step 5: 정리 및 Scale In 확인하기 배포한 샘플 애플리케이션을 삭제합니다.\nkubectl delete deployment nginx-deployment scale-down-unneeded-time=10m 설정값에 따라 10분 뒤에 ScaleDown 이벤트가 발생한 것을 확인할 수 있습니다.\n$ kubectl get events --sort-by=.metadata.creationTimestamp -A ... default 12m Normal Killing pod/nginx-deployment-694bc9bdb8-t44jp Stopping container nginx default 105s Normal ScaleDown node/10.0.10.167 marked the node as toBeDeleted/unschedulable ... default 105s Normal ScaleDown node/10.0.10.132 marked the node as toBeDeleted/unschedulable ... default 89s Normal NodeNotSchedulable node/10.0.10.167 Node 10.0.10.167 status is now: NodeNotSchedulable ... default 82s Normal ScaleDown node/10.0.10.132 marked the node as toBeDeleted/unschedulable 노드의 상태를 조회해봅니다. 2개 노드가 스케줄링에서 제외되었습니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.132 Ready,SchedulingDisabled node 22m v1.26.7 10.0.10.158 Ready node 10d v1.26.7 10.0.10.167 NotReady,SchedulingDisabled node 22m v1.26.7 10.0.10.42 Ready node 2d5h v1.26.7 10.0.10.43 Ready node 2d5h v1.26.7 이후 해당 노드가 삭제되고 원래대로 3개의 노드로 남았습니다.\nCluster Autoscaler 배포시 \u0026lt;min-nodes\u0026gt;을 1로 설정해도, Node Pool 생성시 지정한 수가 더 큰 경우, 그 수 만큼은 유지하는 것으로 보입니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.158 Ready node 10d v1.26.7 10.0.10.42 Ready node 2d6h v1.26.7 10.0.10.43 Ready node 2d6h v1.26.7 ","lastmod":"2024-01-28T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/autoscaling/3.cluster-autoscaler/","tags":["oke","autoscaling","cluster-autoscaler"],"title":"1.3.3 Cluster Autoscaler"},{"categories":null,"contents":"4.1.1.3 Spring Boot에서 OCI Cache with Redis + MySQL 쓰기 사전 준비: 테스트용 MySQL 서버 준비 OCI에서는 MySQL 데이터베이스 서비스를 제공하고 있습니다. 여기서는 간단한 테스트이므로, 간단히 컨테이너 이미지를 실행할 수 있는 OCI Container Instance를 통해 MySQL 서버를 만들도록 하겠습니다.\n9.1 Container Instances로 컨테이너 배포하기를 참조하여 배포합니다.\nMySQL 컨테이너로 배포하기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Container Instances 로 이동합니다.\n생성을 위해 Create container instance 버튼을 클릭합니다.\n기본 생성정보를 입력을 입력합니다.\nName: 예) mysql Networking: 여기서는 편의상, 작업 PC에서 접근하기 위해 Public Subnet을 선택하고, Public IP를 할당합니다. 생성할 컨테이너 정보를 입력합니다.\nName: 예) mysql\nImage: External registry 선택\nRegistry hostname: docker.io Repository: mysql Tag: 8.0.35 Registry credentials type: None Environmental variables: 사용하는 MySQL 컨테이너 이미지에서 제공하는 환경변수 값 중에서 데이터베이스 설치하기 위해 필요한 아래변수들을 추가합니다.\nKey Value MYSQL_DATABASE db_example MYSQL_ROOT_PASSWORD ThePassword MYSQL_USER springuser MYSQL_PASSWORD ThePassword Show advanced options을 클릭하여 추가적인 고급 설정을 지정\nStartup Options\nCommand arguments: --default-authentication-plugin=mysql_native_password\nMySQL 8부터 기본 인증방식이 변경지만, 여기서는 편의상 기존 암호방식을 사용하기 위한 설정으로 컨테이너 시작시 사용할 값으로 추가합니다.\n입력한 모든 정보를 확인하고 Create 버튼을 클릭하여 Container Instance를 생성합니다.\nContainer Instance가 생성되면 Public IP를 확인합니다.\nContainer Instance가 사용하는 Subnet의 Security List에, 인터넷에서 3306 포트로 접근할 수 Ingress에 미리 등록해야 합니다.\nStateless Source IP Protocol Source Port Range Destination Port Range Description No 0.0.0.0/0 TCP All 3306 MySQL MySQL 클라이언트(예, MySQL Workbench)를 통해 연결 테스트를 해봅니다.\nSpring Boot에서 MySQL 연동하기 Caching Data with Spring을 참고하여 MySQL, Redis 부분을 추가하였습니다.\nSpring Boot 프로젝트 만들기 Spring Initializr를 사용하여 기본 프로젝트 소스파일을 기반으로 개발을 하게 됩니다.\nSpring Initializr을 통해 프로젝트 파일을 만듭니다.\n방법 1. Spring Initializr를 사용하여 기본 프로젝트 소스파일을 만듭니다.\n아래 그림과 같이 프로젝트 정보를 입력하고 Generate를 클릭하여 소스파일을 생성합니다. Book Model 만들기 src/main/java/com/example/caching/Book.java 파일을 다음과 같이 작성합니다.\n@Entity : 테이블과 링크될 클래스임을 표시합니다. @Id : 해당 테이블의 Primary Key 임을 표시합니다. package com.example.caching; import jakarta.persistence.Entity; import jakarta.persistence.Id; @Entity public class Book { @Id private String isbn; private String title; public Book() { } public Book(String isbn, String title) { this.isbn = isbn; this.title = title; } public String getIsbn() { return isbn; } public void setIsbn(String isbn) { this.isbn = isbn; } public String getTitle() { return title; } public void setTitle(String title) { this.title = title; } @Override public String toString() { return \u0026#34;Book{\u0026#34; + \u0026#34;isbn=\u0026#39;\u0026#34; + isbn + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#34;, title=\u0026#39;\u0026#34; + title + \u0026#39;\\\u0026#39;\u0026#39; + \u0026#39;}\u0026#39;; } } Book Repository 만들기 src/main/java/com/example/caching/BookRepository.java 파일을 다음과 같이 작성합니다.\nJpaRepository 인터페이스를 상속받으면, JPA 관련 메서드를 사용할 수 있습니다. package com.example.caching; import org.springframework.data.jpa.repository.JpaRepository; import org.springframework.stereotype.Repository; public interface BookRepository extends JpaRepository\u0026lt;Book, String\u0026gt; { } JPA, MySQL 사용을 위한 설정 추가하기 dependency에 redis를 추가합니다.\nMaven 기준 pom.xml에 다음이 있는지 확인합니다. 없으면 추가합니다. \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-j\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; src/main/application.properties 파일을 생성하고 다음 항목을 추가합니다.\nxxx.xxx.xxx.xxx는 MySQL 서버 Container Instance의 Public IP로 대체합니다. spring.jpa.hibernate.ddl-auto=update spring.datasource.url=jdbc:mysql://${MYSQL_HOST:xxx.xxx.xxx.xxx}:3306/db_example spring.datasource.username=springuser spring.datasource.password=ThePassword spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver #spring.jpa.show-sql: true Book Service 만들기 src/main/java/com/example/caching/BookService.java 파일을 다음과 같이 작성합니다.\nBook Repository에서 사용할 CRUD 메서드를 만듭니다. package com.example.caching; import org.springframework.stereotype.Service; import org.springframework.beans.factory.annotation.Autowired; import java.util.Optional; @Service public class BookService { @Autowired BookRepository bookRepository; public Book createBook(Book book) { return bookRepository.save(book); } public Book getByIsbn(String isbn) throws Exception { Optional\u0026lt;Book\u0026gt; opt = bookRepository.findById(isbn); return opt.orElseThrow(() -\u0026gt; new Exception(\u0026#34;Book Not Found\u0026#34;)); } public Book updateBook(String isbn, Book book) throws Exception { bookRepository.findById(isbn).orElseThrow(() -\u0026gt; new Exception(\u0026#34;Book Not Found\u0026#34;)); return bookRepository.save(book); }\tpublic void deleteBook(String isbn) throws Exception { Book book = bookRepository.findById(isbn).orElseThrow(() -\u0026gt; new Exception(\u0026#34;Book Not Found\u0026#34;)); bookRepository.delete(book); }\t} 테스트 구성 실행시 src/main/java/com/example/caching/AppRunner.java 파일을 다음과 같이 작성합니다.\n데이블에 데이터를 생성, 조회하는 것을 확인하는 예시입니다. package com.example.caching; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.stereotype.Component; @Component public class AppRunner implements CommandLineRunner { private static final Logger logger = LoggerFactory.getLogger(AppRunner.class); private final BookService bookService; public AppRunner(BookService bookService) { this.bookService = bookService; } @Override public void run(String... args) throws Exception { // Create logger.info(\u0026#34;isbn-1111 --\u0026gt;\u0026#34; + bookService.createBook(new Book(\u0026#34;isbn-1111\u0026#34;, \u0026#34;test-1111\u0026#34;))); logger.info(\u0026#34;.... Fetching books\u0026#34;); logger.info(\u0026#34;isbn-1111 --\u0026gt; \u0026#34;); logger.info(\u0026#34;isbn-1111 --\u0026gt; \u0026#34; + bookService.getByIsbn(\u0026#34;isbn-1111\u0026#34;)); } } 테스트 하기 캐쉬된 데이터를 삭제합니다.\nredis-cli --tls -h localhost localhost:6379\u0026gt; flushall OK localhost:6379\u0026gt; keys * (empty array) localhost:6379\u0026gt; 소스를 빌드하고 실행합니다.\n./mvnw clean package -Dmaven.test.skip java -jar target/caching-0.0.1-SNAPSHOT.jar isbn-1111이 DB에 입력되고, 다시 조회가 오류없이 실행되었습니다.\n2023-11-16T20:03:44.867+09:00 INFO 54451 --- [ main] com.example.caching.AppRunner : isbn-1111 --\u0026gt;Book{isbn=\u0026#39;isbn-1111\u0026#39;, title=\u0026#39;test-1111\u0026#39;} 2023-11-16T20:03:44.867+09:00 INFO 54451 --- [ main] com.example.caching.AppRunner : .... Fetching books 2023-11-16T20:03:45.117+09:00 INFO 54451 --- [ main] com.example.caching.AppRunner : isbn-1111 --\u0026gt;Book{isbn=\u0026#39;isbn-1111\u0026#39;, title=\u0026#39;test-1111\u0026#39;} 2023-11-16T20:03:45.121+09:00 INFO 54451 --- [ionShutdownHook] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit \u0026#39;default\u0026#39; MySQL Workbench를 통해 접속해서 실제 테이블이 생성된 것과 데이터를 확인합니다. 데이터가 잘 입력된 것을 볼 수 있습니다.\nOCI Cache with Redis를 캐쉬 서버로 연동하기 Redis 관련 설정 추가하기 공통사항으로 10.1.2 Spring Boot에서 OCI Cache with Redis로 데이터 캐쉬하기과 동일한 내용이며, 추가적으로 캐쉬된 데이터의 Time To Live(TTL) 설정이 추가합니다.\ndependency에 redis를 추가합니다.\nMaven 기준 pom.xml에 다음 추가 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;\tsrc/main/application.properties 파일을 생성하고 다음 항목을 추가합니다.\n아래 host, port는 개발환경을 기준으로 앞선 실습에서의 Bastion 서비스를 통해 포트포워딩으로 연동하는 예시입니다. spring.redis.host=localhost spring.redis.port=6379 spring.redis.ssl=true Redis 설정을 위한 Config 클래스 파일( src/main/java/com/example/caching/RedisConfig.java)을 만듭니다.\nlettuce 라이브러리를 사용하는 예시입니다. redis-cli에서 --tls 옵션을 사용한 것 처럼 lettuceClientConfigurationBuilder.useSsl().disablePeerVerification()을 꼭 추가합니다.\napplication.properties에 값을 사용하여 host, port 값과 ssl 설정여부 설정하는 예시입니다.\nMySQL 테이블이 원 데이터이고 Redis는 캐쉬이므로, 여기서는 TTL설정이 필요하고 예시에서는 1시간으로 설정하였습니다.\nRedisCacheConfiguration.defaultCacheConfig().entryTtl(Duration.ofHours(1L));\npackage com.example.caching; import org.springframework.context.annotation.Configuration; import org.springframework.context.annotation.Bean; import org.springframework.beans.factory.annotation.Value; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.connection.RedisStandaloneConfiguration; import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory; import org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration; import org.springframework.data.redis.connection.RedisNode; import java.time.Duration; import org.springframework.data.redis.cache.RedisCacheConfiguration; @Configuration public class RedisConfig { @Value(\u0026#34;${spring.redis.host}\u0026#34;) private String host; @Value(\u0026#34;${spring.redis.port}\u0026#34;) private int port; @Value(\u0026#34;${spring.redis.ssl}\u0026#34;) private boolean ssl; @Bean public RedisConnectionFactory redisConnectionFactory() { final RedisNode redisNode = RedisNode.newRedisNode() .listeningAt(host, port) .build(); // Connecting as a Standalone Redis server final RedisStandaloneConfiguration redisStandaloneConfiguration = new RedisStandaloneConfiguration(); redisStandaloneConfiguration.setHostName(host); redisStandaloneConfiguration.setPort(port); final LettuceClientConfiguration.LettuceClientConfigurationBuilder lettuceClientConfigurationBuilder = LettuceClientConfiguration.builder(); if (ssl) { lettuceClientConfigurationBuilder.useSsl().disablePeerVerification(); } final LettuceClientConfiguration lettuceClientConfiguration = lettuceClientConfigurationBuilder.build(); return new LettuceConnectionFactory(redisStandaloneConfiguration, lettuceClientConfiguration); } @Bean public RedisCacheConfiguration defaultRedisCacheConfiguration() { return RedisCacheConfiguration.defaultCacheConfig() .disableCachingNullValues() .entryTtl(Duration.ofHours(1L)); } } 캐쉬설정 src/main/java/com/example/caching/CachingApplication.java 에 @EnableCaching annotation을 추가합니다.\npackage com.example.caching; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cache.annotation.EnableCaching; @SpringBootApplication @EnableCaching public class CachingApplication { public static void main(String[] args) { SpringApplication.run(CachingApplication.class, args); } } 캐쉬되는 데이터 모델( src/main/java/com/example/caching/Book.java)을 Serializable 하도록 설정합니다.\npackage com.example.caching; ... import java.io.Serializable; @Entity public class Book implements Serializable { ... } src/main/java/com/example/caching/BookService.java 에 캐쉬를 사용할 함수에 각각 @Cacheable, @CachePut, @CacheEvict annotation을 추가하여, 각 메서드 호출시 캐쉬에 데이터 입력, 갱신, 삭제하도록 설정합니다.\npackage com.example.caching; import org.springframework.stereotype.Service; import org.springframework.beans.factory.annotation.Autowired; import java.util.Optional; import org.springframework.cache.annotation.Cacheable; import org.springframework.cache.annotation.CachePut; import org.springframework.cache.annotation.CacheEvict; @Service public class BookService { @Autowired BookRepository bookRepository; public Book createBook(Book book) { return bookRepository.save(book); } @Cacheable(value = \u0026#34;books\u0026#34;, key = \u0026#34;#isbn\u0026#34;) public Book getByIsbn(String isbn) throws Exception { Optional\u0026lt;Book\u0026gt; opt = bookRepository.findById(isbn); return opt.orElseThrow(() -\u0026gt; new Exception(\u0026#34;Book Not Found\u0026#34;)); } @CachePut(value = \u0026#34;books\u0026#34;, key = \u0026#34;#isbn\u0026#34;) public Book updateBook(String isbn, Book book) throws Exception { bookRepository.findById(isbn).orElseThrow(() -\u0026gt; new Exception(\u0026#34;Book Not Found\u0026#34;)); return bookRepository.save(book); }\t@CacheEvict(value = \u0026#34;books\u0026#34;, key = \u0026#34;#isbn\u0026#34;) public void deleteBook(String isbn) throws Exception { Book book = bookRepository.findById(isbn).orElseThrow(() -\u0026gt; new Exception(\u0026#34;Book Not Found\u0026#34;)); bookRepository.delete(book); }\t} 테스트 구성 실행시 src/main/java/com/example/caching/AppRunner.java 파일을 다음과 같이 작성합니다.\n캐쉬가 초기화된 상태에서 Cache Miss, Cache Hit를 테스트하는 예시입니다. package com.example.caching; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.boot.CommandLineRunner; import org.springframework.stereotype.Component; @Component public class AppRunner implements CommandLineRunner { private static final Logger logger = LoggerFactory.getLogger(AppRunner.class); private final BookService bookService; public AppRunner(BookService bookService) { this.bookService = bookService; } @Override public void run(String... args) throws Exception { // Create //logger.info(\u0026#34;isbn-1111 --\u0026gt;\u0026#34; + bookService.createBook(new Book(\u0026#34;isbn-1111\u0026#34;, \u0026#34;test-1111\u0026#34;))); logger.info(\u0026#34;.... Fetching books\u0026#34;); logger.info(\u0026#34;isbn-1111 --\u0026gt; \u0026#34;); logger.info(\u0026#34;isbn-1111 --\u0026gt; \u0026#34; + bookService.getByIsbn(\u0026#34;isbn-1111\u0026#34;)); logger.info(\u0026#34;isbn-1111 --\u0026gt; \u0026#34;); logger.info(\u0026#34;isbn-1111 --\u0026gt; \u0026#34; + bookService.getByIsbn(\u0026#34;isbn-1111\u0026#34;)); } } 테스트 로그 확인을 위해 src/main/application.properties 파일에 다음 항목을 추가합니다.\nlogging.level.org.springframework.cache=TRACE spring.jpa.properties.hibernate.show_sql=true logging.level.org.hibernate.type=trace 테스트 하기 MySQL에는 isbn-1111이 book 테이블에 있는 상태입니다.\n캐쉬된 데이터를 삭제합니다.\nredis-cli --tls -h localhost localhost:6379\u0026gt; keys * 1) \u0026#34;books::isbn-1111\u0026#34; localhost:6379\u0026gt; flushall OK localhost:6379\u0026gt; keys * (empty array) localhost:6379\u0026gt; 소스를 빌드하고 실행합니다.\n./mvnw clean package -Dmaven.test.skip java -jar target/caching-0.0.1-SNAPSHOT.jar 실행 로그 확인\n첫번째 Isbn-1111 조회:\n2023-11-19T15:19:28.675+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : No cache entry for key 'isbn-1111' in cache(s) [books] 와 같이 Cache Miss가 되고, 캐쉬되었습니다. 이후 DB에서 조회해 오는 것을 볼수 있습니다. Hibernate: select b1_0.isbn,b1_0.title from book b1_0 where b1_0.isbn=? 두번째 Isbn-1111 조회:\n2023-11-19T15:19:29.057+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : Cache entry for key 'isbn-1111' found in cache 'books' 와 같이 Cache Hit 되어, 캐쉬에 있는 데이터를 사용했습니다. 2023-11-19T15:19:28.230+09:00 INFO 9173 --- [ main] com.example.caching.AppRunner : .... Fetching books 2023-11-19T15:19:28.230+09:00 INFO 9173 --- [ main] com.example.caching.AppRunner : isbn-1111 --\u0026gt; 2023-11-19T15:19:28.242+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-1111\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.BookService.getByIsbn(java.lang.String) throws java.lang.Exception] caches=[books] | key=\u0026#39;#isbn\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-19T15:19:28.675+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : No cache entry for key \u0026#39;isbn-1111\u0026#39; in cache(s) [books] 2023-11-19T15:19:28.675+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-1111\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.BookService.getByIsbn(java.lang.String) throws java.lang.Exception] caches=[books] | key=\u0026#39;#isbn\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; Hibernate: select b1_0.isbn,b1_0.title from book b1_0 where b1_0.isbn=? 2023-11-19T15:19:29.013+09:00 INFO 9173 --- [ main] com.example.caching.AppRunner : isbn-1111 --\u0026gt; Book{isbn=\u0026#39;isbn-1111\u0026#39;, title=\u0026#39;test-1111\u0026#39;} 2023-11-19T15:19:29.013+09:00 INFO 9173 --- [ main] com.example.caching.AppRunner : isbn-1111 --\u0026gt; 2023-11-19T15:19:29.013+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : Computed cache key \u0026#39;isbn-1111\u0026#39; for operation Builder[public com.example.caching.Book com.example.caching.BookService.getByIsbn(java.lang.String) throws java.lang.Exception] caches=[books] | key=\u0026#39;#isbn\u0026#39; | keyGenerator=\u0026#39;\u0026#39; | cacheManager=\u0026#39;\u0026#39; | cacheResolver=\u0026#39;\u0026#39; | condition=\u0026#39;\u0026#39; | unless=\u0026#39;\u0026#39; | sync=\u0026#39;false\u0026#39; 2023-11-19T15:19:29.057+09:00 TRACE 9173 --- [ main] o.s.cache.interceptor.CacheInterceptor : Cache entry for key \u0026#39;isbn-1111\u0026#39; found in cache \u0026#39;books\u0026#39; 2023-11-19T15:19:29.057+09:00 INFO 9173 --- [ main] com.example.caching.AppRunner : isbn-1111 --\u0026gt; Book{isbn=\u0026#39;isbn-1111\u0026#39;, title=\u0026#39;test-1111\u0026#39;} redis-cli로 조회하면, 비어있던 캐쉬가, Cache Miss 후 DB에서 조회되고, 그 결과가 캐쉬된 걸 볼수 있습니다. 또한 추가 설정한 TTL 설정이 적용된 것을 볼 수 있습니다.\nlocalhost:6379\u0026gt; keys * 1) \u0026#34;books::isbn-1111\u0026#34; localhost:6379\u0026gt; ttl books::isbn-1111 (integer) 3274 참고 https://m.blog.naver.com/hj_kim97/222780110215 ","lastmod":"2023-11-19T00:00:02Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/databases/redis/cache-with-redis/3.cache-with-redis-mysql-in-springboot/","tags":["cache","redis","mysql","spring-boot"],"title":"4.1.1.3 Spring Boot에서 OCI Cache with Redis + MySQL 쓰기"},{"categories":null,"contents":"Bastion Service 문제해결 세션 접속용 SSH command 실행시 접속 오류 - Permission denied (publickey) 현상: Bastion 서비스 세션 생성후, SSH command로 접속시 아래와 같이 Permission denied (publickey) 오류 발생함.\n세션 생성시 사용한 SSH Key는 기존 Compute 인스턴스 생성시 사용시 연결 오류 발생하지 않음 =\u0026gt; SSH Key 문제는 아닌 거 같은데..\n세션 생성시 \u0026ldquo;Generate SSH key pair\u0026quot;를 통해 OCI 콘솔에서 생성한 키를 사용해도 아래 오류 발생함 =\u0026gt; SSH Key 문제는 아닌 거 같은데..\n$ ssh -i key -N -L 2022:10.0.0.47:22 -p 22 ocid1.bastionsession.oc1.ap-chuncheon-1.amaaaaaavsea7yiakgcncs7ick6c43y2xrzldehca3man7kykx4otskplqwa@host.bastion.ap-chuncheon-1.oci.oraclecloud.com ocid1.bastionsession.oc1.ap-chuncheon-1.amaaaaaavsea7yiakgcncs7ick6c43y2xrzldehca3man7kykx4otskplqwa@host.bastion.ap-chuncheon-1.oci.oraclecloud.com: Permission denied (publickey). 원인:\nOpenSSH 8.8 릴리즈 정보를 보면 아래와 같이 SHA-1을 사용하는 RSA 서명을 비활성화하였다는 문구가 있습니다.\nhttps://www.openssh.com/txt/release-8.8 OpenSSH 8.8 was released on 2021-09-26. It is available from the mirrors listed at https://www.openssh.com/. ... Potentially-incompatible changes ================================ This release disables RSA signatures using the SHA-1 hash algorithm by default. This change has been made as the SHA-1 hash algorithm is cryptographically broken, and it is possible to create chosen-prefix hash collisions for \u0026lt;USD$50K [1] For most users, this change should be invisible and there is no need to replace ssh-rsa keys. OpenSSH has supported RFC8332 RSA/SHA-256/512 signatures since release 7.2 and existing ssh-rsa keys will automatically use the stronger algorithm where possible. Incompatibility is more likely when connecting to older SSH implementations that have not been upgraded or have not closely tracked improvements in the SSH protocol. For these cases, it may be necessary to selectively re-enable RSA/SHA1 to allow connection and/or user authentication via the HostkeyAlgorithms and PubkeyAcceptedAlgorithms options. For example, the following stanza in ~/.ssh/config will enable RSA/SHA1 for host and user authentication for a single destination host: Host old-host HostkeyAlgorithms +ssh-rsa PubkeyAcceptedAlgorithms +ssh-rsa We recommend enabling RSA/SHA1 only as a stopgap measure until legacy implementations can be upgraded or reconfigured with another key type (such as ECDSA or Ed25519). [1] \u0026#34;SHA-1 is a Shambles: First Chosen-Prefix Collision on SHA-1 and Application to the PGP Web of Trust\u0026#34; Leurent, 일반적인 환경에서는 별다른 오류가 발생하지 않았는데, macOS에서 환경에서 업데이트를 잘 해서 그런지 아래와 같이 클라이언트의 버전이 9.4인 상태에서 OCI Bastion Service를 통해 세션을 연결할 때 위와 같은 오류가 발생하였습니다.\n노트북에 설치된 설치 버전 확인 $ ssh -V OpenSSH_9.4p1, LibreSSL 3.3.6 해결책\nOpenSSH 8.8 릴리즈 노트에서 설명되어 있는 것처럼, ~/.ssh/config 파일의 내용에 아래 설정을 추가합니다.\nHost * HostkeyAlgorithms +ssh-rsa PubkeyAcceptedAlgorithms +ssh-rsa 설정후 다시 연결해 보면, 오류없이 연결되는 것을 알 수 있습니다. $ ssh -i key -N -L 2022:10.0.0.47:22 -p 22 ocid1.bastionsession.oc1.ap-chuncheon-1.amaaaaaavsea7yiakgcncs7ick6c43y2xrzldehca3man7kykx4otskplqwa@host.bastion.ap-chuncheon-1.oci.oraclecloud.com 참고\nUsing Oracle Cloud Infrastructure Bastion with OpenSSH 8.8 or greater ","lastmod":"2023-11-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/troubleshooting/bastion-service/","tags":["bastion service"],"title":"Bastion Service 문제해결"},{"categories":null,"contents":"8.3 로컬 환경에 GraalVM for JDK 설치하기 2023년 11월 기준으로 GraalVM for JDK 17, GraalVM for JDK 21 버전 모두 사용 가능합니다. 두 버전 모두 LTS 버전입니다. 여기서는 17 버전을 사용하도록 하겠습니다.\nGraalVM for JDK 17 설치하기 Getting Started with GraalVM 에서 사용할 OS에 해당 하는 문서 기준으로 설치합니다.\n예) macOS M1에 설치하기\nGraalVM 다운로드 페이지에서 원하는 버전과 플랫폼의 설치파일을 다운로드 받습니다. Java 17, macOS (AArch64)를 선택하고 설치파일을 다운로드 받습니다. 또는 다음 명령으로 직접 다운로드 받습니다.\nwget https://download.oracle.com/graalvm/17/latest/graalvm-jdk-17_macos-aarch64_bin.tar.gz 압축해제 합니다.\ntar -xzf graalvm-jdk-17_macos-aarch64_bin.tar.gz 압축해제된 폴더를 /Library/Java/JavaVirtualMachines 경로로 이동시킵니다. 해당 폴더로 옮기기 위해서는 sudo 실행을 위해 관리자 권한이 필요합니다.\nsudo mv graalvm-jdk-17.0.9+11.1 /Library/Java/JavaVirtualMachines /usr/libexec/java_home -V 명령으로 설치된 JDK를 확인합니다.\n$ /usr/libexec/java_home -V Matching Java Virtual Machines (2): 17.0.9 (arm64) \u0026#34;Oracle Corporation\u0026#34; - \u0026#34;Oracle GraalVM 17.0.9+11.1\u0026#34; /Library/Java/JavaVirtualMachines/graalvm-jdk-17.0.9+11.1/Contents/Home 11.0.16 (arm64) \u0026#34;Oracle Corporation\u0026#34; - \u0026#34;Java SE 11.0.16\u0026#34; /Library/Java/JavaVirtualMachines/jdk-11.0.16.jdk/Contents/Home /Library/Java/JavaVirtualMachines/graalvm-jdk-17.0.9+11.1/Contents/Home 둘 이상의 JDK가 설치된 경우 아래와 같이 설치된 경로를 사용하여 환경 변수를 추가합니다.\nexport PATH=/Library/Java/JavaVirtualMachines/graalvm-jdk-17.0.9+11.1/Contents/Home/bin:$PATH export JAVA_HOME=/Library/Java/JavaVirtualMachines/graalvm-jdk-17.0.9+11.1/Contents/Home java -version을 실행하여, 잘 설치 및 설정되었는 지 확인합니다.\n$ java -version java version \u0026#34;17.0.9\u0026#34; 2023-10-17 LTS Java(TM) SE Runtime Environment Oracle GraalVM 17.0.9+11.1 (build 17.0.9+11-LTS-jvmci-23.0-b21) Java HotSpot(TM) 64-Bit Server VM Oracle GraalVM 17.0.9+11.1 (build 17.0.9+11-LTS-jvmci-23.0-b21, mixed mode, sharing) ","lastmod":"2023-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/appdev/3.install-graal-vm-for-jdk/","tags":["graalvm"],"title":"8.3 로컬 환경에 GraalVM for JDK 설치하기"},{"categories":null,"contents":"1.2.3.3 Kubernetes Volume Snapshot 만들기 OCI Block Volume Backup과 연계하여 Kubernetes Volume Snapshot 기능을 제공합니다. 재해 복구 전략의 일부로 Persistent Volume을 백업하고, 복구시에 사용할 수 있습니다.\nBlock Volume Snapshot 사용 전제조건 OKE 클러스터 Control Plane의 쿠버네티스 버전은 1.24 이상 Worker Node가 AMD 또는 Arm 기반이어야 함 Worker Node는 Oracle Linux 7 또는 Oracle Linux 8 CSI Snapshotter 설치 CSI Snapshotter를 설치합니다.\nkubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/master/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml 테스트 환경 준비 1.2.3.1 Block Volume 사용하기에서 Block Volume을 사용하여, Persistent Volume을 구성한 예제를 그대로 사용합니다.\n현재 상태를 조회하면 다음과 같습니다.\nCSI Volume Plugin을 통해 제공하는 기능으로 반드시 oci-bv StorageClass를 사용하는 Persistent Volume 이어야 합니다. /usr/share/nginx/html 폴더가 PV를 마운트하고 있습니다. 마운트된 폴더에 파일 쓰기를 한 상태로 이후 실습에서 해당내용이 복구되는 지 확인할 예정 $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE csi-bvs-pvc Bound csi-63d20026-6bee-4d93-8642-d7e644ff3e1f 50Gi RWO oci-bv 31m $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-549db9449b-fg82v 1/1 Running 0 2m11s $ kubectl exec -it nginx-bvs-pvc-549db9449b-fg82v -- cat /usr/share/nginx/html/hello_world.txt Hello PV #1. 동적(Dynamically)으로 Volume Snapshot 만들기 현재 사용중인 Persistent Volume 기준으로 Snapshot을 만들 때 사용하는 방법입니다.\nSnapshot을 만들기 전에 먼저 VolumeSnapshotClass를 먼저 정의합니다.\n# csi-bv-snapshotclass.yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: csi-bv-snapshotclass driver: blockvolume.csi.oraclecloud.com parameters: backupType: full deletionPolicy: Delete driver: 사용할 CSI volume plugin으로 blockvolume.csi.oraclecloud.com 지정 parameters.backupType: full, incremental으로 OCI Block Volume Backup과 동일 deletionPolicy: Delete, Retain 옵션이 있음. VolumeSnapshot이 삭제되면, 연계된 block volume backup을 삭제할지 여부를 지정함 VolumeSnapshotClass 생성, 확인합니다.\n$ kubectl apply -f csi-bv-snapshotclass.yaml volumesnapshotclass.snapshot.storage.k8s.io/csi-bv-snapshotclass created $ kubectl get volumesnapshotclasses NAME DRIVER DELETIONPOLICY AGE csi-bv-snapshotclass blockvolume.csi.oraclecloud.com Delete 2m5s VolumeSnapshotClass으로 백업할 PVC에 대한 Snapshot 생성을 YAML을 정의합니다.\n# my-snapshot.yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: my-snapshot namespace: default spec: volumeSnapshotClassName: csi-bv-snapshotclass source: persistentVolumeClaimName: csi-bvs-pvc YAML을 실행하여 VolumeSnapshot을 생성합니다. 생성이 완료되면 READYTOUSE가 true가 됩니다.\n$ kubectl apply -f my-snapshot.yaml volumesnapshot.snapshot.storage.k8s.io/my-snapshot created $ kubectl get volumesnapshot NAME READYTOUSE SOURCEPVC SOURCESNAPSHOTCONTENT RESTORESIZE SNAPSHOTCLASS SNAPSHOTCONTENT CREATIONTIME AGE my-snapshot true csi-bvs-pvc 50Gi csi-bv-snapshotclass snapcontent-b888c55b-d64e-4cbc-bf69-519331230e19 61s 62s VolumeSnapshot이 생성되면, VolumeSnapshotContent가 함께 생성되며 매핑되는 Block Volume Backup 정보를 가지고 있습니다. VolumeSnapshotContent 오브젝트는 임의로 변경하지 않습니다.\n$ kubectl get volumesnapshotcontents NAME READYTOUSE RESTORESIZE DELETIONPOLICY DRIVER VOLUMESNAPSHOTCLASS VOLUMESNAPSHOT VOLUMESNAPSHOTNAMESPACE AGE snapcontent-b888c55b-d64e-4cbc-bf69-519331230e19 true 53687091200 Delete blockvolume.csi.oraclecloud.com csi-bv-snapshotclass my-snapshot default 5m28s OCI 콘솔에서 보면 같은 이름으로 Block Volume Backup이 만들어 진 걸 볼 수 있습니다.\n장애복구를 가정하여, Volume Snapshot으로 새 Volume 만들기\n장애 상황을 가정하여, 현재 배포된 테스트 앱과 PVC를 삭제합니다.\nkubectl delete -f nginx-deployment-bvs-pvc.yaml kubectl delete -f csi-bvs-pvc.yaml Snapshot으로 Persistent Volume을 만들기 위한 PVC YAML 파일을 만듭니다.\nBlock Volume Backup으로 PV용 새 Block Volume을 만들게 됩니다. # pvc-from-snapshot.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-from-snapshot namespace: default spec: storageClassName: oci-bv dataSource: name: my-snapshot kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 50Gi datasource.name: 소스로 사용할 VolumeSnapshot 이름 지정 PVC YAML을 실행합니다.\nkubectl apply -f pvc-from-snapshot.yaml 테스트 앱 복구용 YAML 작성\n1.2.3.1 Block Volume 사용하기에서 사용한 테스트앱에서 persistentVolumeClaim.claimName을 Snapshot에서 만든 pvc-from-snapshot로 지정합니다. # nginx-deployment-bvs-pvc-from-snapshot.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-bvs-pvc name: nginx-bvs-pvc spec: replicas: 1 selector: matchLabels: app: nginx-bvs-pvc template: metadata: labels: app: nginx-bvs-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: pvc-from-snapshot 테스트앱 배포\n테스트 앱에 배포되면, 앞선 PVC에 바인딩되는 Persistent Volume이 만들어 지면서 새 Block Volume이 생성됩니다. kubectl apply -f nginx-deployment-bvs-pvc-from-snapshot.yaml 복구된 앱에서 파일 내용을 확인해 보면, 잘 복구된 것을 알 수 있습니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-666979b5c4-c28bv 1/1 Running 0 77s $ kubectl exec -it nginx-bvs-pvc-666979b5c4-c28bv -- cat /usr/share/nginx/html/hello_world.txt Hello PV #2. 정적(Statically)으로 Volume Snapshot 만들기 수동으로 OCI Block Volume Backup을 받은 경우, 해당 백업을 이용하여, Kubernetes Snapshop을 만들고, Persistent Volume을 만들어 복구하는 방식입니다.\n사용한 Block Volume Backup의 OCID를 확인합니다.\n해당 Backup에 매핑되는 VolumeSnapshotContent을 정의합니다.\n# snapshot-content-from-backup.yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotContent metadata: name: snapshot-content-from-backup spec: deletionPolicy: Retain driver: blockvolume.csi.oraclecloud.com source: snapshotHandle: ocid1.volumebackup.oc1.iad.aaaaaa______xbd volumeSnapshotRef: name: my-static-snapshot namespace: default deletionPolicy: Delete, Retain 옵션이 있음. VolumeSnapshot이 삭제되면, 연계된 block volume backup을 삭제할지 여부를 지정함 driver: 사용할 CSI volume plugin으로 blockvolume.csi.oraclecloud.com 지정 snapshotHandle: 사용할 Block Volume Backup의 OCID를 입력 volumeSnapshotRef.name: 이후 생성할 VolumeSnapshot의 이름 VolumeSnapshotContent을 생성합니다.\nkubectl apply -f snapshot-content-from-backup.yaml VolumeSnapshot을 정의합니다.\n# my-static-snapshot.yaml apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: my-static-snapshot spec: source: volumeSnapshotContentName: snapshot-content-from-backup VolumeSnapshot을 생성합니다.\nkubectl apply -f my-static-snapshot.yaml 동적으로 생성한 것과 정적으로 생성한 것의 차이를 확인할 수 있습니다.\n$ kubectl get volumesnapshotcontent NAME READYTOUSE RESTORESIZE DELETIONPOLICY DRIVER VOLUMESNAPSHOTCLASS VOLUMESNAPSHOT VOLUMESNAPSHOTNAMESPACE AGE snapcontent-b888c55b-d64e-4cbc-bf69-519331230e19 true 53687091200 Delete blockvolume.csi.oraclecloud.com csi-bv-snapshotclass my-snapshot default 18h snapshot-content-from-backup true 0 Retain blockvolume.csi.oraclecloud.com my-static-snapshot default 5m53s $ kubectl get volumesnapshot NAME READYTOUSE SOURCEPVC SOURCESNAPSHOTCONTENT RESTORESIZE SNAPSHOTCLASS SNAPSHOTCONTENT CREATIONTIME AGE my-snapshot true csi-bvs-pvc 50Gi csi-bv-snapshotclass snapcontent-b888c55b-d64e-4cbc-bf69-519331230e19 18h 18h my-static-snapshot true snapshot-content-from-backup 0 snapshot-content-from-backup 4m27s 49s 장애복구를 가정하여, Volume Snapshot으로 새 Volume 만들기\n절차 자체는 동적(Dynamically)으로 Volume Snapshot에서 복구할때랑 동일합니다.\n앞서 배포된 테스트 앱과 PVC가 남아있다면 삭제합니다.\nSnapshot으로 Persistent Volume을 만들기 위한 PVC YAML 파일을 만듭니다.\nBlock Volume Backup으로 PV용 새 Block Volume을 만들게 됩니다. # pvc-from-static-snapshot.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc-from-static-snapshot namespace: default spec: storageClassName: oci-bv dataSource: name: my-static-snapshot kind: VolumeSnapshot apiGroup: snapshot.storage.k8s.io accessModes: - ReadWriteOnce resources: requests: storage: 50Gi datasource.name: 소스로 사용할 VolumeSnapshot 이름 지정 PVC YAML을 실행합니다.\nkubectl apply -f pvc-from-static-snapshot.yaml 테스트 앱 복구용 YAML 작성\n테스트앱에서 persistentVolumeClaim.claimName을 Snapshot에서 만든 pvc-from-static-snapshot로 지정합니다. # nginx-deployment-bvs-pvc-from-static-snapshot.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-bvs-pvc name: nginx-bvs-pvc spec: replicas: 1 selector: matchLabels: app: nginx-bvs-pvc template: metadata: labels: app: nginx-bvs-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: pvc-from-static-snapshot 테스트앱 배포\n테스트 앱에 배포되면, 앞선 PVC에 바인딩되는 Persistent Volume이 만들어 지면서 새 Block Volume이 생성됩니다. kubectl apply -f nginx-deployment-bvs-pvc-from-static-snapshot.yaml 복구된 앱에서 파일 내용을 확인해 보면, 잘 복구된 것을 알 수 있습니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-74cb959b5c-5vd44 1/1 Running 0 91s $ kubectl exec -it nginx-bvs-pvc-74cb959b5c-5vd44 -- cat /usr/share/nginx/html/hello_world.txt Hello PV ","lastmod":"2023-10-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/storage/3.volume-snapshot/","tags":["oke","volume snapshot"],"title":"1.2.3.3 Kubernetes Volume Snapshot 만들기"},{"categories":null,"contents":"1.10.3 Worker Node 생성시 Custom Image 사용하기 OKE에서 Managed Node로 프로비저닝을 할때 지원하는 노드 이미지로 다음 세가지를 제공합니다. 세가지 모두 Oracle Linux입니다.\nPlatform Images: 오라클에서 제공하는 이미지로 Oracle Linux를 포함하는 기본 이미지로, Managed Node로 프로비저닝시 OKE의 Worker Node로써 필요한 소프트웨어 설치 및 구성이 이루어집니다. OKE Images: 오라클에서 제공하는 이미지로 OKE Managed Node로 최적화된 이미지입니다. 필요한 소프트웨어 및 구성이 기본 포함되어 있어, 프로비저닝 시간이 단축됩니다. 대신 범용이미지가 아니라, 특정 쿠버네티스 버전, 특정 CPU 자원에 따라 사용해야 하는 이미지가 달라집니다. Custom Images: 사용자가 만든 이미지로, 위 오라클이 제공하는 Platform Images, OKE Images를 기반으로 만든 커스텀 이미지 이어야 합니다. Node Pool 생성시 사용할 이미지를 지정할 수 있는데, 현재 콘솔 UI에서는 Platform Images와 OKE Images만 나열되어, Custom Image 지정은 API 또는 CLI로만 지정이 가능합니다. 참고문서: OKE Documentation \u0026gt; Supported Images for Managed Nodes\n앞선 예제에서 수행한 내용을 이용하여 Custom Node Image를 만들어 보겠습니다.\nWorker Node 디스크 사이즈 늘리기: 100GB kubelet 옵션 변경 최신 OS 업데이트 Custom Node Image 만들기 다음 명령을 통해서 베이스로 사용할 이미지를 확인합니다.\noci ce node-pool-options get --node-pool-option-id all 실행 예시\nPlatform Images: 예, Oracle-Linux-8.7-2023.05.24-0 OKE Images: 예, Oracle-Linux-8.7-2023.05.24-0-OKE-1.26.2-625 thekoguryo@cloudshell:~ (ap-chuncheon-1)$ oci ce node-pool-options get --node-pool-option-id all { \u0026#34;data\u0026#34;: { \u0026#34;images\u0026#34;: [], \u0026#34;kubernetes-versions\u0026#34;: [ \u0026#34;v1.24.1\u0026#34;, \u0026#34;v1.25.4\u0026#34;, \u0026#34;v1.26.2\u0026#34; ], \u0026#34;shapes\u0026#34;: [ ... ], \u0026#34;sources\u0026#34;: [ ... { \u0026#34;image-id\u0026#34;: \u0026#34;ocid1.image.oc1.ap-chuncheon-1.aaaaaaaao24cp6arg6wnytrswldyb2ihr7fa774iiq3ud4xxnw3kwntjs7wa\u0026#34;, \u0026#34;source-name\u0026#34;: \u0026#34;Oracle-Linux-8.7-2023.05.24-0-OKE-1.26.2-625\u0026#34;, \u0026#34;source-type\u0026#34;: \u0026#34;IMAGE\u0026#34; }, { \u0026#34;image-id\u0026#34;: \u0026#34;ocid1.image.oc1.ap-chuncheon-1.aaaaaaaatg5luufmd4dmdprkre335po2eo5fgyl56gun2pijgt2dyczon6ja\u0026#34;, \u0026#34;source-name\u0026#34;: \u0026#34;Oracle-Linux-8.7-2023.05.24-0-OKE-1.25.4-625\u0026#34;, \u0026#34;source-type\u0026#34;: \u0026#34;IMAGE\u0026#34; }, { \u0026#34;image-id\u0026#34;: \u0026#34;ocid1.image.oc1.ap-chuncheon-1.aaaaaaaaxkw7zh4uqazu2g2aywrqrpc7uyye6pmki735d66mf5r4twcng7vq\u0026#34;, \u0026#34;source-name\u0026#34;: \u0026#34;Oracle-Linux-8.7-2023.05.24-0-OKE-1.24.1-625\u0026#34;, \u0026#34;source-type\u0026#34;: \u0026#34;IMAGE\u0026#34; }, ... { \u0026#34;image-id\u0026#34;: \u0026#34;ocid1.image.oc1.ap-chuncheon-1.aaaaaaaaj3e35k76zk3ub6e2bapuwylcgqhnk6ej3bkmjc7uuahdniaymqma\u0026#34;, \u0026#34;source-name\u0026#34;: \u0026#34;Oracle-Linux-8.7-2023.05.24-0\u0026#34;, \u0026#34;source-type\u0026#34;: \u0026#34;IMAGE\u0026#34; }, ... ] } } 사용할 OKE 클러스터 버전에 맞는 이미지의 image-id를 확인합니다. 여기서는 1.26.버전용 OKE Image인 Oracle-Linux-8.7-2023.05.24-0-OKE-1.26.2-625을 사용하겠습니다.\n클라우드 콘솔에서 Compute 인스턴스를 생성합니다.\nOKE Image는 기본 목록에 없으므로, My images로 이동합니다. Image OCID를 선택 후, 앞서 사용할 OKE Image의 OCID를 입력합니다.\nBoot volume size을 100GB로 늘립니다.\nSSH Key 등을 다른 정보를 입력하여 인스턴스를 생성합니다.\n생성한 인스턴스에 SSH로 접속합니다.\n노드사이즈 증설을 위해 oci-growfs 명령으로 루트 파티션을 늘립니다.\nsudo /usr/libexec/oci-growfs -y OKE Images로, 이미 쿠버네티스 설정 파일이 있는 것을 알 수 있습니다,\n[opc@oke-custom-image ~]$ ls -la /etc/kubernetes/kubelet-config.json -rw-r--r--. 1 root root 1203 Jun 12 16:55 kubelet-config.json root 유저로/etc/kubernetes/kubelet-config.json 파일로 kubelet config를 변경합니다.\n수정전 { ... \u0026#34;serializeImagePulls\u0026#34;: false, ... } 수정후 { ... \u0026#34;serializeImagePulls\u0026#34;: false, \u0026#34;cpuManagerPolicy\u0026#34;: \u0026#34;static\u0026#34;, \u0026#34;kubeReserved\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;500m\u0026#34; }, ... } config 파일이 아닌 kubelet-extra-args 옵션은 cloud-init을 통해 적용합니다. 추가 소프트웨어 설치를 가정하여, OS를 업데이트 합니다.\nsudo dnf update -y 모든 설정이 끝이나면, Custom Image를 생성하기 전에 반드시 먼저 OS 레벨에서 종료합니다.\n[opc@oke-custom-image ~]$ sudo su [root@oke-custom-image opc]# shutdown now Connection to xxx.xx.xx.xx closed by remote host. Connection to xxx.xx.xx.xx closed. 클라우드 콘솔에서 인스턴스를 Stop 합니다.\n인스턴스가 중지되면 More Action \u0026gt; Create custom image를 클릭합니다.\n원하는 이름을 입력후 Custom Image를 생성합니다.\nName: 예) Oracle-Linux-8.7-2023.05.24-0-OKE-1.26.2-625-Custom 이미지 생성이 완료되면, 생성된 Custom Image의 OCID를 복사해 둡니다.\nNode Pool의 Image ID 변경하기 동일한 Worker Node 유형을을 가지는 것이 Node Pool입니다. Node Pool 단위로 사용할 Node Image을 지정할 수 있습니다. 현재 클라우드 콘솔에서는 Platform Images와 OKE Images만 선택 목록에 나오기 때문에, Custom Image를 사용하는 경우 CLI, API를 사용해야 합니다.\n공식 문서에 나온 것 처럼 Node Pool 생성시 image id 옵션으로 지정할 수 있습니다.\noci ce node-pool create \\ --cluster-id ocid1.cluster.oc1.iad.aaaaaaaaaf______jrd \\ --name my-custom-linux-image \\ --node-image-id ocid1.image.oc1.iad.aaaaaaaa6______nha \\ --compartment-id ocid1.compartment.oc1..aaaaaaaay______t6q \\ --kubernetes-version v1.15.7 \\ --node-shape VM.Standard2.1 \\ --placement-configs \u0026#34;[{\\\u0026#34;availability-domain\\\u0026#34;:\\\u0026#34;IqDk:US-ASHBURN-AD-2\\\u0026#34;, \\\u0026#34;capacityReservationId\\\u0026#34;:\\\u0026#34;ocid1.capacityreservation.oc1.iad.anuwcljt2ah______yeq\\\u0026#34;, \\\u0026#34;subnet-id\\\u0026#34;:\\\u0026#34;ocid1.subnet.oc1.iad.aaaaaaaa2xpk______zva\\\u0026#34;, \\\u0026#34;faultDomains\\\u0026#34;:[\\\u0026#34;FAULT-DOMAIN-3\\\u0026#34;, \\\u0026#34;FAULT-DOMAIN-1\\\u0026#34;]}, {\\\u0026#34;availability-domain\\\u0026#34;:\\\u0026#34;IqDk:US-ASHBURN-AD-1\\\u0026#34;, \\\u0026#34;subnet-id\\\u0026#34;:\\\u0026#34;ocid1.subnet.oc1.iad.aaaaaaaauhls______bpq\\\u0026#34;, \\\u0026#34;faultDomains\\\u0026#34;: [\\\u0026#34;FAULT-DOMAIN-1\\\u0026#34;, \\\u0026#34;FAULT-DOMAIN-2\\\u0026#34;]}]\u0026#34; \\ --size 1 \\ --region=us-ashburn-1 Node Pool 생성시는 많은 옵션 설정이 필요하므로, 편의상 더 간편하게, 이미 생성된 Node Pool에 대한 image id만 업데이트 하겠습니다.\n업데이트할 Node Pool의 OCID를 확인합니다.\n사용할 Custom Image의 OCID를 확인합니다.\nNode Pool의 image id를 업데이트 하겠습니다. OCI CLI 명령 형식은 다음과 같습니다.\noci ce node-pool update --node-pool-id $NODE_POOL_OCID --node-source-details \u0026#34;{ \u0026#39;source_type\u0026#39;: \u0026#39;IMAGE\u0026#39;, \u0026#39;image_id\u0026#39;: \u0026#39;$CUSTOM_IMAGE_ID\u0026#39;}\u0026#34; Cloud Shell에서 실행한 예시 thekoguryo@cloudshell:~ (ap-chuncheon-1)$ oci ce node-pool update --node-pool-id ocid1.nodepool.oc1.ap-chuncheon-1.aaaaaaaa3_____2ca --node-source-details \u0026#34;{ \u0026#39;source_type\u0026#39;: \u0026#39;IMAGE\u0026#39;, \u0026#39;image_id\u0026#39;: \u0026#39;ocid1.image.oc1.ap-chuncheon-1.aaaaaaaan5_____dfa\u0026#39;}\u0026#34; WARNING: Updates to initial-node-labels and subnet-ids and node-config-details and node-metadata and node-source-details and node-shape-config and freeform-tags and defined-tags and node-eviction-node-pool-settings and node-pool-cycling-details will replace any existing values. Are you sure you want to continue? [y/N]: y { \u0026#34;opc-work-request-id\u0026#34;: \u0026#34;ocid1.clustersworkrequest.oc1.ap-chuncheon-1.aaaaaaaah_____eba\u0026#34; } 업데이트가 끝나고, 클라우드 콘솔에서 결과를 확인합니다.\nNode Pool에 기존 노드가 있는 경우, 노드를 재생성해야 합니다. 여기서는 현재 0개 이므로, 테스트를 위해 1개로 Node 수를 수정합니다.\n노드가 생성되면 Ready 상태인 것을 확인합니다.\nthekoguryo@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.57 Ready node 6m22s v1.26.2 kubelet 옵션을 확인합니다.\n$ kubectl get --raw \u0026#34;/api/v1/nodes/10.0.10.57/proxy/configz\u0026#34; | jq { \u0026#34;kubeletconfig\u0026#34;: { ... \u0026#34;cpuManagerPolicy\u0026#34;: \u0026#34;static\u0026#34;, ... \u0026#34;serializeImagePulls\u0026#34;: false, ... \u0026#34;kubeReserved\u0026#34;: { \u0026#34;cpu\u0026#34;: \u0026#34;500m\u0026#34; }, ... } } 생성된 Worker Node를 SSH로 접속합니다.\n디스크 사이즈를 확인합니다. Custom Image 작성시 늘린 사이즈 인 것을 알 수 있습니다.\n[opc@oke-caefqtyrfoa-n7fcc7cocrq-sgx6e4vktka-0 ~]$ df -h Filesystem Size Used Avail Use% Mounted on ... /dev/mapper/ocivolume-root 89G 14G 76G 16% / /dev/mapper/ocivolume-oled 10G 119M 9.9G 2% /var/oled ... OS 업데이트 상태를 확인합니다.\n[opc@oke-caefqtyrfoa-n7fcc7cocrq-sgx6e4vktka-0 ~]$ sudo dnf update -y Last metadata expiration check: 11:48:43 ago on Sun 30 Jul 2023 01:43:32 PM UTC. Dependencies resolved. Nothing to do. Complete! 작성된 Custom Image의 설정이 잘 적용되었으며, OKE 클러스터에서 Ready 상태로 잘 동작함을 알 수 있습니다.\n","lastmod":"2023-07-31T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/customize/3.custom-node-image/","tags":["oke","custom node image"],"title":"1.10.3 Worker Node 생성시 Custom Image 사용하기"},{"categories":null,"contents":"4.3 OKE에서 OCI 자원관리를 위한 Service Operator 사용하기 OCI Service Operator for Kubernetes(OSOK)는 OCI 자원을 Kubernetes API를 통해 관리할 수 있도록 도와주는 도구입니다. Autonomous Database 서비스를 Kubernetes API, kubectl을 통해 인스턴스를 생성, 삭제 등을 할 수 있게 해준다고 이해하면 됩니다. Kubernetes에서 사용하는 오픈소스 Operator Framework을 기반으로 작성되었습니다. 관련 참고 사이트는 아래와 같습니다.\nOCI Service Operator for Kubernetes GitHub OCI Service Operator for Kubernetes GitHub Documentation OCI Docs Documentation \u0026gt; Adding OCI Service Operator for Kubernetes to Clusters 현재 v1.1.8 기준 지원하고 있는 OCI 서비스는 다음과 같습니다.\nAutonomous Database 서비스 MySQL Database 서비스 Streaming 서비스 Service Mesh 서비스 OCI Service Operator for Kubernetes를 OKE Cluster에 설치 제품 설치문서를 따라 설치한 내용으로 자세한 사항은 아래 문서를 참고합니다.\noci-service-operator/installation.md at main · oracle/oci-service-operator (github.com) Operator SDK 설치 공식 설치 문서에 따라 설치합니다.\nInstallation the Operator SDK CLI Cloud Shell 기준 설치 명령 예시\n아래 명령어로 설치하여 operator-sdk cli가 정상동작하는 지 확인합니다.\n# Download the release binary export ARCH=$(case $(uname -m) in x86_64) echo -n amd64 ;; aarch64) echo -n arm64 ;; *) echo -n $(uname -m) ;; esac) export OS=$(uname | awk \u0026#39;{print tolower($0)}\u0026#39;) export OPERATOR_SDK_DL_URL=https://github.com/operator-framework/operator-sdk/releases/download/v1.30.0 curl -LO ${OPERATOR_SDK_DL_URL}/operator-sdk_${OS}_${ARCH} # Verify the downloaded binary gpg --keyserver keyserver.ubuntu.com --recv-keys 052996E2A20B5C7E curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt curl -LO ${OPERATOR_SDK_DL_URL}/checksums.txt.asc gpg -u \u0026#34;Operator SDK (release) \u0026lt;cncf-operator-sdk@cncf.io\u0026gt;\u0026#34; --verify checksums.txt.asc grep operator-sdk_${OS}_${ARCH} checksums.txt | sha256sum -c - # Install the release binary in your PATH mkdir -p ~/.local/bin chmod +x operator-sdk_${OS}_${ARCH} \u0026amp;\u0026amp; mv operator-sdk_${OS}_${ARCH} ~/.local/bin/operator-sdk operator-sdk version Operator Lifecycle Manager (OLM) 설치 아래 명령으로 현재 OKE 클러스터에 OLM 자원을 설치 및 확인합니다.\noperator-sdk olm install operator-sdk olm status OCI Service Operator for Kubernetes 설치 권한 설정 OCI Service Operator for Kubernetes가 OCI 서비스에 대한 작업을 위한 권한이 필요합니다. OSOK가 OCI 내부에 있는 OKE에 같이 설치되어 있는 경우, Instance Principal을 통해 권한을 부여할 수 있습니다. OSOK가 OCI 외부 환경에 설치된 경우 또는 OSOK가 사용할 사용자 그룹에 권한을 부여하고자 하는 경우 User Principal을 통해 권한을 부여할 수 있습니다. OKE에서 권한을 설정할 것이므로 여기서는 Instance Principal을 사용합니다.\nUser Principal을 사용하고자 하는 경우 OCI Service Operator 사이트에 있는 Enable User Principal를 참고하여 구성합니다. OKE Worker Node에 대한 Dynamic Group 만들기 OCI 콘솔에 로그인 하여 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동하여 OKE 클러스터가 있는 Compartment의 OCID를 복사합니다. 좌측 Dynamic Group 메뉴로 이동하여 복사한 OCID로 아래 규칙을 가진 Dynamic Group을 만듭니다. Name: 예) oke-nodes-dynamic-group Rule: instance.compartment.id = '\u0026lt;compartment-ocid\u0026gt;' Dynamic Group을 위한 Policy 만들기 좌측 Policies 메뉴로 이동하여 만든 Dynamic Group에 아래와 같이 권한을 부여합니다.\nAllow dynamic-group \u0026lt;DYNAMICGROUP_NAME\u0026gt; to manage \u0026lt;OCI_SERVICE_1\u0026gt; in compartment \u0026lt;COMPARTMENT_NAME\u0026gt; ... 예시\nPolicy Name: oke-osok-policy\nCOMPARTMENT_NAME: oci-hol-xx\nDYNAMICGROUP_NAME: oke-nodes-dynamic-group\nAutonomous Database Service\n참고 - https://github.com/oracle/oci-service-operator/blob/main/docs/adb.md#oci-permission-requirement Allow dynamic-group oke-nodes-dynamic-group to manage autonomous-database-family in compartment oci-hol-xx MySQL DB System Service\n참고 - https://github.com/oracle/oci-service-operator/blob/main/docs/mysql.md#create-policies Service Mesh Service\n참고 - https://github.com/oracle/oci-service-operator/blob/main/docs/service-mesh.md#create-policies Streams Service\n참고 - https://github.com/oracle/oci-service-operator/blob/main/docs/oss.md#create-policies OCI Service Operator for Kubernetes(OSOK) 배포 OSOK가 배포될 namespace를 만듭니다.\nkubectl create ns oci-service-operator-system OSOK Operator 설치\ndocker pull iad.ocir.io/oracle/oci-service-operator-bundle:1.1.8 operator-sdk run bundle iad.ocir.io/oracle/oci-service-operator-bundle:1.1.8 -n oci-service-operator-system --timeout 5m OSOK 설치후 OCI 서비스를 위한 CustomResource가 추가된 것을 알 수 있습니다.\n$ kubectl api-resources | grep oci.oracle.com autonomousdatabases oci.oracle.com/v1beta1 true AutonomousDatabases mysqldbsystems oci.oracle.com/v1beta1 true MySqlDbSystem streams oci.oracle.com/v1beta1 true Stream accesspolicies servicemesh.oci.oracle.com/v1beta1 true AccessPolicy ingressgatewaydeployments servicemesh.oci.oracle.com/v1beta1 true IngressGatewayDeployment ingressgatewayroutetables servicemesh.oci.oracle.com/v1beta1 true IngressGatewayRouteTable ingressgateways servicemesh.oci.oracle.com/v1beta1 true IngressGateway meshes servicemesh.oci.oracle.com/v1beta1 true Mesh virtualdeploymentbindings servicemesh.oci.oracle.com/v1beta1 true VirtualDeploymentBinding virtualdeployments servicemesh.oci.oracle.com/v1beta1 true VirtualDeployment virtualserviceroutetables servicemesh.oci.oracle.com/v1beta1 true VirtualServiceRouteTable virtualservices servicemesh.oci.oracle.com/v1beta1 true VirtualService OSOK로 Autonomous Database Service 관리하기 참고 문서\noci-service-operator/adb.md at main · oracle/oci-service-operator (github.com) ADB(Autonomous Database) Binding OCI 콘솔에서 만든 ADB 인스턴스를 Kubernetes의 자원으로 Binding하는 경우입니다.\nOCI 콘솔에 로그인하여 바인딩할 Autonomous Database의 인스턴스를 생성합니다.\nBinding을 위해 필요한 명세 yaml을 확인하여 작성합니다.\nspec.id: 기 존재하는, 바인딩할 ADB의 OCID를 입력 walletName: 바인딩후에 wallet이 저장될 kubernetes secret의 이름 입력 walletPassword.secret.secretName: wallet에 사용할 암호가 저장된 secret 이름, 바인딩 전에 미리 secret을 생성합니다. The wallet download password should contain at least 1 number or special character apiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: \u0026lt;CR_OBJECT_NAME\u0026gt; spec: id: \u0026lt;AUTONOMOUS_DATABASE_OCID\u0026gt; wallet: walletName: \u0026lt;WALLET_SECRET_NAME\u0026gt; walletPassword: secret: secretName: \u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt; 실행 예시\nwalletPassword 생성\nThe wallet download password should contain at least 1 number or special character kubectl create secret generic ociadb-wallet-password-secret --from-literal=walletPassword=\u0026#39;xxxxxxxxxxxx\u0026#39; YAML 파일 생성\ncat \u0026lt;\u0026lt;EOF \u0026gt; autonomousdatabases-bind.yaml apiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: ociadb spec: id: ocid1.autonomousdatabase.oc1.ap-chuncheon-1.an4w4__________________________________________________7rlbq wallet: walletName: ociadb-wallet-secret walletPassword: secret: secretName: ociadb-wallet-password-secret EOF 실행\nkubectl apply -f autonomousdatabases-bind.yaml 결과 확인\nkubectl describe 명령을 통해 에러없이 바인딩이 성공했는지 확인합니다.\n$ kubectl get autonomousdatabases NAME DBWORKLOAD STATUS AGE ociadb Active 11s $ kubectl describe autonomousdatabases ociadb Name: ociadb ... Kind: AutonomousDatabases ... Status: Status: Conditions: Last Transition Time: 2023-07-06T10:47:58Z Message: AutonomousDatabase Bound success ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Success 32s AutonomousDatabases Finalizer is added to the object Normal Success 29s (x2 over 29s) AutonomousDatabases Create or Update of resource succeeded wallet 확인\n바인딩 결과 ociadb-wallet-secret secret이 생성되며, 내용을 보면 wallet 상에 있는 파일들이 Base64로 인코딩된 형태로 있는 것을 확인할 수 있습니다. 애플리케이션 컨테이너에서 secret을 마운트하여 ADB 연결시 사용하면 됩니다.\n$ kubectl get secret NAME TYPE DATA AGE ociadb-wallet-password-secret Opaque 1 8m18s ociadb-wallet-secret Opaque 9 2m18s $ kubectl get secret ociadb-wallet-secret -o yaml apiVersion: v1 data: README: V2FsbGV0IE... cwallet.sso: ofhONgAAAA... ewallet.p12: MIIZ/AIBAz... keystore.jks: /u3+7QAAA... ojdbc.properties: IyBDb2... sqlnet.ora: V0FMTE... tnsnames.ora: b2NpYWR... truststore.jks: /u3+7QAAAA... kind: Secret metadata: ... name: ociadb-wallet-secret ... type: Opaque ADB(Autonomous Database) Provisioning Provisioning을 위해 필요한 명세 yaml을 확인하여 작성합니다.\nspec.compartmentId: 생성될 ADB가 위치할 Compartment의 OCID를 입력 walletName: 바인딩후에 wallet이 저장될 kubernetes secret의 이름 입력 walletPassword.secret.secretName: wallet에 사용할 암호가 저장된 secret 이름, 바인딩 전에 미리 secret을 생성합니다. 나머지 항목은 OCI 콘솔에서 ADB 생성시 입력하는 것과 동일하게 원하는 값 입력 - 항목 명세 참조 apiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: \u0026lt;CR_OBJECT_NAME\u0026gt; spec: compartmentId: \u0026lt;COMPARTMENT_OCID\u0026gt; displayName: \u0026lt;DISPLAY_NAME\u0026gt; dbName: \u0026lt;DB_NAME\u0026gt; dbWorkload: \u0026lt;OLTP/DW\u0026gt; isDedicated: \u0026lt;false/true\u0026gt; dbVersion: \u0026lt;ORABLE_DB_VERSION\u0026gt; dataStorageSizeInTBs: \u0026lt;SIZE_IN_TBs\u0026gt; cpuCoreCount: \u0026lt;COUNT\u0026gt; adminPassword: secret: secretName: \u0026lt;ADMIN_PASSWORD_SECRET_NAME\u0026gt; isAutoScalingEnabled: \u0026lt;true/false\u0026gt; isFreeTier: \u0026lt;false/true\u0026gt; licenseModel: \u0026lt;BRING_YOUR_OWN_LICENSE/LICENSE_INCLUDEE\u0026gt; wallet: walletName: \u0026lt;WALLET_SECRET_NAME\u0026gt; walletPassword: secret: secretName: \u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt; freeformTags: \u0026lt;KEY1\u0026gt;: \u0026lt;VALUE1\u0026gt; definedTags: \u0026lt;TAGNAMESPACE1\u0026gt;: \u0026lt;KEY1\u0026gt;: \u0026lt;VALUE1\u0026gt; 실행 예시\nadminPassword, walletPassword 생성\nThe wallet download password should contain at least 1 number or special character kubectl create secret generic ociadb-by-osok-admin-password-secret --from-literal=password=\u0026#39;xxxxxxxxxxxx\u0026#39; kubectl create secret generic ociadb-by-osok-wallet-password-secret --from-literal=walletPassword=\u0026#39;xxxxxxxxxxxx\u0026#39; YAML 파일 생성\ncat \u0026lt;\u0026lt;EOF \u0026gt; autonomousdatabases-provision.yaml apiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: ociadbbyosok spec: compartmentId: ocid1.compartment.oc1..aaaaaaaa_______________________________________________toqhq displayName: OCIADBbyOSOK dbName: ociadbbyosok dbWorkload: OLTP isDedicated: false dbVersion: 19c dataStorageSizeInTBs: 1 cpuCoreCount: 1 adminPassword: secret: secretName: ociadb-by-osok-admin-password-secret isAutoScalingEnabled: false isFreeTier: false licenseModel: LICENSE_INCLUDED wallet: walletName: ociadb-by-osok-wallet-secret walletPassword: secret: secretName: ociadb-by-osok-wallet-password-secret EOF 실행\nkubectl apply -f autonomousdatabases-provision.yaml 결과 확인\nkubectl describe 명령을 통해 에러없이 바인딩이 성공했는지 확인합니다.\n$ kubectl get autonomousdatabases NAME DBWORKLOAD STATUS AGE ... ociadbbyosok OLTP Active 92s $ kubectl describe autonomousdatabases ociadbbyosok Name: ociadbbyosok ... Kind: AutonomousDatabases ... Status: Status: Conditions: Last Transition Time: 2023-07-06T10:58:15Z Message: AutonomousDatabase Provisioning Status: True Type: Provisioning Last Transition Time: 2023-07-06T10:59:20Z Message: AutonomousDatabase OCIADBbyOSOK is Active Status: True Type: Active ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Success 99s AutonomousDatabases Finalizer is added to the object Normal Success 29s (x2 over 30s) AutonomousDatabases Create or Update of resource succeeded wallet 확인\n바인딩 결과 ociadb-wallet-secret secret이 생성되며, 내용을 보면 wallet 상에 있는 파일들이 Base64로 인코딩된 형태로 있는 것을 확인할 수 있습니다. 애플리케이션 컨테이너에서 secret을 마운트하여 ADB 연결시 사용하면 됩니다.\n$ kubectl get secret NAME TYPE DATA AGE ociadb-by-osok-admin-password-secret Opaque 1 8m23s ociadb-by-osok-wallet-password-secret Opaque 1 8m20s ociadb-by-osok-wallet-secret Opaque 9 2m23s ... $ kubectl get secret ociadb-by-osok-wallet-secret -o yaml apiVersion: v1 data: README: V2FsbGV0IE... cwallet.sso: ofhONgAAAA... ewallet.p12: MIIZ/AIBAz... keystore.jks: /u3+7QAAA... ojdbc.properties: IyBDb2... sqlnet.ora: V0FMTE... tnsnames.ora: b2NpYWR... truststore.jks: /u3+7QAAAA... kind: Secret metadata: ... name: ociadb-by-osok-wallet-secret ... type: Opaque ADB(Autonomous Database) Update OCI API에서 제공하는 Autonomous Database에 대한 Update 지원 항목내에서 OSOK GitHub 문서의 예시를 참고합니다.\nhttps://github.com/oracle/oci-service-operator/blob/main/docs/adb.md#updating-an-autonomous-database\nGitHub 문서 기준\napiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: \u0026lt;CR_OBJECT_NAME\u0026gt; spec: id: \u0026lt;AUTONOMOUS_DATABASE_OCID\u0026gt; displayName: \u0026lt;DISPLAY_NAME\u0026gt; dbName: \u0026lt;DB_NAME\u0026gt; dbWorkload: \u0026lt;OLTP/DW\u0026gt; isDedicated: \u0026lt;false/true\u0026gt; dbVersion: \u0026lt;ORABLE_DB_VERSION\u0026gt; dataStorageSizeInTBs: \u0026lt;SIZE_IN_TBs\u0026gt; cpuCoreCount: \u0026lt;COUNT\u0026gt; adminPassword: secret: secretName: \u0026lt;ADMIN_PASSWORD_SECRET_NAME\u0026gt; isAutoScalingEnabled: \u0026lt;true/false\u0026gt; isFreeTier: \u0026lt;false/true\u0026gt; licenseModel: \u0026lt;BRING_YOUR_OWN_LICENSE/LICENSE_INCLUDEE\u0026gt; wallet: walletName: \u0026lt;WALLET_SECRET_NAME\u0026gt; walletPassword: secret: secretName: \u0026lt;WALLET_PASSWORD_SECRET_NAME\u0026gt; freeformTags: \u0026lt;KEY1\u0026gt;: \u0026lt;VALUE1\u0026gt; definedTags: \u0026lt;TAGNAMESPACE1\u0026gt;: \u0026lt;KEY1\u0026gt;: \u0026lt;VALUE1\u0026gt; Binding 한 경우 기존 YAML 또는 배포된 YAML에 업데이트 항목을 추가 하여 반영합니다.\n스토리지 증가 예시\n앞선 autonomousdatabases-bind.yaml 파일에 dataStorageSizeInTBs 항목을 추가하여 배포합니다.\napiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: ociadb spec: id: ocid1.autonomousdatabase.oc1.ap-chuncheon-1.an4w4__________________________________________________7rlbq wallet: walletName: ociadb-wallet-secret walletPassword: secret: secretName: ociadb-wallet-password-secret dataStorageSizeInTBs: 2 Provisioning 한 경우 스토리지 증가 예시\n앞선 autonomousdatabases-provision.yaml 파일에 생성된 ADB의 OCID를 spec.id에 추가합니다. 그리고 dataStorageSizeInTBs 값을 변경합니다.\napiVersion: oci.oracle.com/v1beta1 kind: AutonomousDatabases metadata: name: ociadbbyosok spec: id: ocid1.autonomousdatabase.oc1.ap-chuncheon-1.an4w4__________________________________________________eb46a compartmentId: ocid1.compartment.oc1..aaaaaaaa_______________________________________________toqhq displayName: OCIADBbyOSOK dbName: ociadbbyosok dbWorkload: OLTP isDedicated: false dbVersion: 19c dataStorageSizeInTBs: 2 cpuCoreCount: 1 adminPassword: secret: secretName: ociadb-by-osok-admin-password-secret isAutoScalingEnabled: false isFreeTier: false licenseModel: LICENSE_INCLUDED wallet: walletName: ociadb-by-osok-wallet-secret walletPassword: secret: secretName: ociadb-by-osok-wallet-password-secret 업데이트 실행결과\n$ kubectl describe autonomousdatabases ociadbbyosok Name: ociadbbyosok ... Status: Status: Conditions: Last Transition Time: 2023-07-06T10:58:15Z Message: AutonomousDatabase Provisioning Status: True Type: Provisioning Last Transition Time: 2023-07-06T10:59:20Z Message: AutonomousDatabase OCIADBbyOSOK is Active Status: True Type: Active Last Transition Time: 2023-07-06T11:13:46Z Message: AutonomousDatabase Update success Status: True Type: Active ... ADB(Autonomous Database) Delete 현재 버전 기준으로 Delete 기능을 따로 제공하지 않아, OKE 클러스터에서 autonomousdatabases 자원을 kubectl delete 명령으로 삭제해도 실제 ADB 인스턴스가 OCI에서 삭제되지는 않습니다.\n","lastmod":"2023-07-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/databases/oci-services/1.oci-service-operator-1.1.8/","tags":["oke","opensource","osok"],"title":"4.3 OKE에서 OCI 자원관리를 위한 Service Operator"},{"categories":null,"contents":"1.2.2.1.3 OCI Native Ingress Controller에서 HOST 기반 라우팅 DNS에 등록하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 따로 구입한 Domain Name(thekoguryo.xyz)을 사용하였습니다.\nDNS 서비스 설정\n도메인 구입처 또는 OCI DNS 서비스에서 위임하여 서비스하는 경우 OCI DNS 서비스에서 사용한 host를 등록합니다.\n추가할 레코드를 입력하고 제출합니다.\nRecord Type: A - IPv4 Address\nName: *.ingress\n와일드 카드 형식으로 ingress controller가 사용할 서브 Domain Name을 입력합니다. Address: 매핑할 IP, 여기서는 앞서 만든 OCI Native Ingress Controller의 Load Balancer의 IP 입력\n예, GoDaddy DNS 관리화면\nDNS 테스트\nnslookup 툴로 등록한 DNS를 테스트 해봅니다. 잘 등록된 것을 알 수 있습니다.\n$ nslookup *.ingress.thekoguryo.xyz Server: 127.0.0.11 Address: 127.0.0.11#53 Non-authoritative answer: Name: *.ingress.thekoguryo.xyz Address: 158.180.xx.xxx HOST 기반 라우팅 테스트 HOST 이름에 따라 라우팅 서비스를 달리하는 경우입니다.\n테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80 ingress 설정 YAML(native-ic-ingress-host-basic.yaml)을 작성합니다.\nblue.ingress.thekoguryo.ml 요청은 nginx-blue-svc 로 라우팅 green.ingress.thekoguryo.ml 요청은 nginx-green-svc로 라우팅 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: native-ic-ingress-host-basic spec: ingressClassName: native-ic-ingress-class rules: - host: blue.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 - host: green.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 작성한 native-ic-ingress-host-basic.yaml을 배포합니다.\n$ kubectl apply -f native-ic-ingress-host-basic.yaml ingress.networking.k8s.io/native-ic-ingress-host-basic created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE native-ic-ingress-host-basic native-ic-ingress-class blue.ingress.thekoguryo.xyz,green.ingress.thekoguryo.xyz 158.180.xx.xxx 80 2m36s ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\nblue.ingress.thekoguryo.xyz 요청\ngreen.ingress.thekoguryo.xyz 요청\n와일드 카드 주소로 DNS에 등록한 Ingress Controller의 Load Balancer를 거쳐 접속한 host의 FQDN에 따라 대상 서비스에 라우팅 되는 것을 확인할 수 있습니다.\n","lastmod":"2023-06-12T00:00:02Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/oci-native-ingress/3.ingress-host/","tags":["ingress-controller","oci-ingress-controller"],"title":"1.2.2.1.3 HOST 기반 라우팅"},{"categories":null,"contents":"2.3 Virtual Nodes 앱 배포 및 비용 예시 앱 배포 및 Load Balancer 사용하기 1.4 앱 배포 및 Load Balancer 사용하기에서 한 과정을 동일하게 OKE Serverless 클러스터에서 수행해 봅니다.\nDocker Hub 이미지 배포 가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest 배포 결과를 확인합니다.\nVCN-Native Pod Networking을 사용하고 있어, Pod의 IP로 Worker Nodes 서브넷 상의 IP를 사용하고 있습니다. $ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-docker-hub-67c59cc7d5-gkjxg 1/1 Running 0 86s 10.0.10.220 10.0.10.85 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ContainerCreating 상태로 멈춘경우, Policy 적용여부를 확인합니다. 적용후 Pod를 재생성합니다. Load Balancer 타입으로 서비스 만들기 클라이언트 서비스를 위해 LoadBalancer Type으로 서비스를 생성합니다.\nkubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc 서비스 생성 결과를 확인하면 아래와 같이 LoadBalancer 타입으로 생성되어 Public IP가 할당 된 것을 볼 수 있습니다.\n$ kubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc service/nginx-docker-hub-svc exposed $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP,12250/TCP 10m nginx-docker-hub-svc LoadBalancer 10.96.210.192 152.69.xxx.xx 80:31809/TCP 38s 서비스 주소인 Public IP로 접속하면, 다음과 같이 연결 오류가 발생합니다.\n$ curl http://152.69.xxx.xx curl: (56) Recv failure: Connection reset by peer Comparing Virtual Nodes with Managed Nodes에서 Virtual Nodes상의 Load Balancing의 설명을 보면, 기존 OKE 클러스터에서 Load Balancer를 생성하면, 자동으로 Security List에 규칙에 추가가 되었지만, Virtual Nodes에서는 매뉴얼하게 해주어야 합니다.\nVCN-Native Pod Networking을 사용하기 때문에 생성되는 \u0026lt;pod-ip\u0026gt;:\u0026lt;nodeport\u0026gt;에 대한 보안 규칙을 매뉴얼하게 추가합니다.\n또한 생성되는 Load Balancer가 Pod의 대한 Health Check시 사용하는 kube-proxy health port(10256)에 대한 보안 규칙 또한 추가해 줘야 합니다.\nSecurity List를 업데이트합니다.\npod-ip와 nodeport를 확인합니다.\n$ kubectl describe svc nginx-docker-hub Name: nginx-docker-hub-svc Namespace: default ... NodePort: \u0026lt;unset\u0026gt; 31440/TCP Endpoints: 10.0.10.25:80 ... \u0026lt;pod-ip\u0026gt;:\u0026lt;nodeport\u0026gt;, \u0026lt;pod-ip\u0026gt;:10256만 매번 추가하거나, Pod가 속한 Worker Node 서브넷상의 모든 Node Port 범위(30000-32767)와 kube-proxy health port(10256)을 한번에 미리 추가해 놓는 방법이 있습니다. 여기서는 후자를 사용합니다.\nLoad Balancer -\u0026gt; Pod: oke-svclbseclist-~~ Security List 업데이트\nEgress Rules: Stateless Destination IP Protocol Source Port Range Destination Port Range No 10.0.10.0/24 TCP All 30000-32767 No 10.0.10.0/24 TCP All 10256 Load Balancer -\u0026gt; Pod: oke-nodeseclist-~~ Security List 업데이트\nIngress Rules: Stateless Source IP Protocol Source Port Range Destination Port Range No 10.0.20.0/24 TCP All 30000-32767 No 10.0.20.0/24 TCP All 10256 Load Balancer IP로 다시 테스트하면 정상적으로 연결됩니다.\n$ curl http://152.69.xxx.xx \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... \u0026lt;/html\u0026gt; Virtual Nodes에서 Pod에 대한 CPU, 메모리 할당 OCI Documentation \u0026gt; CPU and Memory Resources Allocated to Pods Provisioned by Virtual Nodes에서 pod spec에 정의된 requests와 limits 값을 기준으로 할당하며, 둘 다 없는 경우, 최소값으로 0.125 OCPU, 0.5GB 메모리를 할당합니다.\n사용한 비용 추정\n위 테스트 내용을 기준으로 가격 산정에 들어가는 항목은 아래와 같습니다. 아래 내용에 대해 시간당 비용이 발생할 것으로 추정됩니다.\nEnhanced Cluster: 1개 Virtual Nodes: 3개 1개 Pod 총 CPU: 0.125 OCPU -\u0026gt; Virtual Nodes의 최소 OCPU인 1 OCPU 1개 Pod 총 Memory: 0.5 GB -\u0026gt; Virtual Nodes의 최소 메모리인 1 GB 그외 Load Balancer 비용 등 가격 비교는 OCI Blog \u0026gt; Kubernetes cloud cost comparison: Who provides the best value? 참조\n","lastmod":"2023-05-22T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-virtual-nodes/3.deploy-app-on-virtual-nodes/","tags":["oke","enhanced cluster","virtual nodes"],"title":"2.3 Virtual Nodes 앱 배포 및 비용 예시"},{"categories":null,"contents":"1.9.3 On Demand Node Cycling로 Worker Nodes 업데이트/업그레이드 하기 Worker Nodes의 속성을 변경하거나 OS 버전을 변경하기 위해 Node Pool을 업데이트 하면, 이후 새로 생성되는 Node에 변경된 값이 적용됩니다. 기존 Worker Nodes에는 변경 적용되지 않기 때문에, 적용을 위해서는 Drain 작업을 포함한 재생성 작업이 필요합니다.\n또한 쿠버네티스 버전 업그레이드를 위해 Node Pool의 쿠버네티스 버전을 업그레이드 한 경우에도 Node Pool내에 새로 새로 생성되는 Node는 새 버전으로 생성되지만, 기존 Worker Nodes에는 변경 적용되지 않기 때문에, 적용을 위해서는 Drain 작업을 포함한 업그레이드 작업이 필요합니다.\n관련 내용은 1.7 Kubernetes 지원 버전 및 업그레이드을 참조 바랍니다. 여기서 설명하는 Node Cycling은 Enhanced Cluster에서만 사용 가능하기 때문에, OKE Basic Cluster에서는 기존 방법대로 업그레이드를 수행합니다.\nNode Cycling을 통해 기존 Worker Nodes 업그레이드 기존 Node Pool의 Worker Nodes를 사이클링하는 방식이기 때문에 1.7 Kubernetes 지원 버전 및 업그레이드에서 설명한 내용중 in-place 방식 업그레이드와 동일한 방식입니다.\n예시는 동일한 상황으로, 1.25.4 버전을 사용 중에 새로운 버전이 출시되었다고 가정합니다\nControl Plane 업그레이드 Worker Nodes를 업그레이드 하기전에 Control Plane을 먼저 업그레이드 합니다.\nWorker Node 업그레이드 - in-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다.\nNode Pool 업그레이드\n업그레이드 하려는 Node Pool의 상세 페이지로 이동합니다.\n수정을 위해 Edit를 클릭하면, 오른쪽에 수정 페이지가 뜹니다.\nVersion 항목에, 클러스터 버전과 Node Pool의 버전이 표시되며, 업그레이드 가능한 버전이 표시됩니다.\n클러스터와 동일한 1.26.2로 선택하고 Save Change를 클릭하여 저장합니다.\nResources \u0026gt; Work Requests에 가서 보면, 15초 정도 지난뒤 Node Pool 업그레이드가 완료됩니다.\n아직 실제 Worker Node가 업그레이드 된 것은 아닙니다.\nNode Cycling\nNode Pool 상세화면에서 Cycle nodes를 클릭합니다.\nCycle nodes 방식을 지정할 수 있습니다.\nMaximun surge: 동시에 최대 몇 개의 새 노드를 만들지 지정합니다. Maximun unavailable: 동시에 최대 몇 개 기존 노드를 사용불가 상태로 변경해도 괜찮을 지를 지정합니다. 예시 새 노드 생성후, 기존 노드 제거 방식: 새 노드가 준비되면, 기존 노드를 제거(cordons, drains, terminates) 방식이라, 안정적이지만, 일시적으로 Node Pool에 지정한 노드 수 보다 더 많은 상황이 발생하여, 비용이 추가 발생할 수 있습니다. 설정 예, maxSurge=1, maxUnavailable=0 기존 노드 제거후, 새 노드 생성 방식: 기존 노드를 제거(cordons, drains, terminates)하고 새 노드를 생성하는 방식이라, Node Pool에 지정한 노드 수에서, 즉 비용이 늘어나지 않는 상태에서 업데이트/업그레이드 할 수 있음. 하지만, 기존 노드에서 제거된 Pod들은 나머지 기존 노드에 이관된 상태라, 새 노드가 준비되는 시점에는 해당 노드에는 애플리케이션 Pod가 위치하지 않을 수 도 있음. 설정 예, maxSurge=1, maxUnavailable=1 미입력시, 디폴트 값은 maxSurge=1, maxUnavailable=0 입니다. maxSurge=1, maxUnavailable=0로 Cycle nodes를 수행합니다.\n설정한 규칙에 따라 업그레이드된 버전으로 새 노드가 먼저 생성됩니다.\n새 노드가 준비가 되면, 기존 노드 하나가 스케줄링에서 제외됩니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.143 Ready node 2m v1.26.2 10.0.10.184 Ready,SchedulingDisabled node 25h v1.25.4 10.0.10.191 Ready node 25h v1.25.4 10.0.10.239 Ready node 25h v1.25.4 다음으로 기존 노드 하나가 삭제되기 시작합니다.\n$ kildong@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.143 Ready node 3m22s v1.26.2 10.0.10.191 Ready node 25h v1.25.4 10.0.10.239 Ready node 25h v1.25.4 기존 노드 하나가 삭제되면, 다시 새 노드를 생성합니다.\n새 노드가 준비되면, 다시 스케줄링 제외, 노드 삭제 순으로 동일한 순서로 모든 노드를 업그레이드 할때까지 계속 진행됩니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.142 Ready node 2m12s v1.26.2 10.0.10.143 Ready node 7m6s v1.26.2 10.0.10.191 Ready node 25h v1.25.4 10.0.10.239 Ready,SchedulingDisabled node 25h v1.25.4 업그레이드가 완료되었습니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.142 Ready node 13m v1.26.2 10.0.10.143 Ready node 17m v1.26.2 10.0.10.184 Ready node 7m37s v1.26.2 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-docker-hub-5bfd857f89-djnmx 1/1 Running 0 6m23s 10.244.2.130 10.0.10.184 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-ocir-86bcf7867c-db8xb 1/1 Running 0 11m 10.244.2.2 10.0.10.142 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Node cycling을 이용하면, 매뉴얼로 직접 이관하는 것에 비해 자동화된 업그레이드를 통해 보다 편리하게 업그레이드 할 수 있습니다.\n안정적으로 업그레이드하면서, 노드 수가 많을 때 빠르게 진행하기 위해서는 배포된 애플리케이션의 특성에 따라 설정이 필요합니다. 또한 Disruption Budget for your Application을 통해 애플리케이션 자체이 대해서도 안정적인 이관에 대해 사전에 고려하여 설계하여야 합니다.\nNode Cycling을 통해 기존 Worker Nodes 업데이트 Node Pool 수정 페이지에서 Kubernetes 버전이외에, 노드의 OS Image, 노드 태그, 속성, SSH Key 등록 등 Node Pool의 설정을 변경할 수 있습니다. Node Pool 속성 변경을 Worker Nodes에 적용하는 것도 Node Cycling을 통해 기존 Worker Nodes 업그레이드와 동일한 과정입니다.\n","lastmod":"2023-05-19T00:00:02Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/enhanced-cluster/3.node-cycling/","tags":["oke","enhanced cluster","node cycling"],"title":"1.9.3 Node Cycling로 Worker Nodes 업데이트 하기"},{"categories":null,"contents":"6.3 Private Repository를 위한 image pull secret 사용하기 컨테이너 이미지 레지스트리 중 Private Repository에서 이미지를 가져오기 위해서는 사용자 인증 정보가 필요합니다. 쿠버네티스에서는 imagePullSecrets을 사용합니다. Container Instance에서도 imagePullSecrets를 설정하여, 사용자 인증 정보를 입력합니다.\nOCI 콘솔에서는 인증정보를 Username / Password로 입력하는 것을 지원히지만, OCI Vault 연동은 아직 지원하고 있지 않습니다. 작성일 기준으로는 API, SDK, CLI를 통해서만 가능합니다. 예시에서는 Private Repository로 OCIR 내 Private Repository를 사용합니다. 기본 인증정보 사용하기 1.5 OCIR 이미지 사용하기을 참조하여 OCIR Private Registry에 사용할 이미지를 미리 넣어 둡니다.\nOCI CLI나 REST API를 이용할 때 아래와 같이 imagePullSecrets을 추가할 수 있습니다.\n{ ... \u0026#34;containers\u0026#34;: [ ... ], \u0026#34;imagePullSecrets\u0026#34;: [ { \u0026#34;username\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;registryEndpoint\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;secretType\u0026#34;: \u0026#34;BASIC\u0026#34; } ] } OCIR 접속 username과 password(AuthToken)을 base64로 인코딩합니다.\n예시\n$ echo -n \u0026#34;cnxxxxxxxx/kildong.example.com\u0026#34; | base64 Y254eHh4eHh4eC9raWxkb25nLmV4YW1wbGUuY29t $ echo -n \u0026#34;ps;yps;yps;yps;yps;y\u0026#34; | base64 cHM7eXBzO3lwczt5cHM7eXBzO3k= 이전 테스트에서 사용한 JSON 파일을 업데이트합니다.\n이미지 주소를 OCIR 주소로 변경합니다.\nimagePullSecrets을 추가합니다.\n{ \u0026#34;displayName\u0026#34;: \u0026#34;wordpress\u0026#34;, ... \u0026#34;containers\u0026#34;: [ { \u0026#34;displayName\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;yny.ocir.io/cnxxxxxxxx/mysql:8.0.31\u0026#34;, ... }, { \u0026#34;displayName\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;yny.ocir.io/cnxxxxxxxx/wordpress:latest\u0026#34;, ... } ], \u0026#34;imagePullSecrets\u0026#34;: [ { \u0026#34;registryEndpoint\u0026#34;: \u0026#34;yny.ocir.io\u0026#34;, \u0026#34;secretType\u0026#34;: \u0026#34;BASIC\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;Y254eHh4eHh4eC9raWxkb25nLmV4YW1wbGUuY29t\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;cHM7eXBzO3lwczt5cHM7eXBzO3k=\u0026#34; } ] } 업데이트한 JSON 파일로 Container Instance를 생성합니다.\noci container-instances container-instance create --from-json=file://wordpress-secret-basic.json CLI로 생성한 결과를 보면, 아래와 같이 OCIR상의 Private Registry에 있는 이미지로 가져와서 실행중인 것을 볼 수 있습니다.\nimagePullSecrets 저장소로 OCI Vault 사용하기 OCI Vault 만들기\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Vault로 이동합니다.\nCreate Vault를 클릭합니다.\n위치할 Compartment를 선택하고, 이름을 입력하여 생성합니다.\n생성된 Vault를 클릭합니다.\n마스터 키 만들기\n먼저 마스터키 생성을 위해 Resources \u0026gt; Master Encryption Keys로 이동합니다.\nCreate Key를 클릭합니다.\n마스터 키 생성 정보를 입력합니다.\nCreate in Compartment: 위치할 Compartment 지정 Protection Mode: HSM(Hardware Security Module) 또는 Software 중에 선택, Software는 무료 Name: 원하는 이름 입력 Key Shape Algorithm: Secret 생성시 사용할 것이므로 대칭키인 AES를 선택합니다. Create Key를 클릭하여 키를 생성합니다.\n암호를 저장할 Secret 만들기\nSecret 생성을 위해 Resources \u0026gt; Secrets로 이동합니다.\nCreate Secret을 클릭합니다.\nSecret 생성 정보를 입력합니다.\nName: 이름을 입력합니다. 예, ocir-imagePullSecrets\nEncryption Key: 앞서 생성한 마스터키 선택\nSecret Contents: 다음 JSON 포맷으로 Private Repository에 대한 인증정보를 저장합니다.\n{ \u0026#34;username\u0026#34;: \u0026#34;container-instance-user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;\u0026lt;password\u0026gt;\u0026#34; } Container Instance를 위한 Policy 만들기\n생성될 Container Instance에서 앞서 만든 OCI Vault상에 있는 Secret에 대한 읽기 권한이 필요합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Policies로 이동합니다.\n다음 Policy를 추가합니다. compartment-name은 Secret이 위치한 Compartment의 이름을 사용합니다.\nallow any-user to read secret-bundles in compartment \u0026lt;compartment-name\u0026gt; where all {request.principal.type=\u0026#39;computecontainerinstance\u0026#39;} Container Instance 만들기\n이전 테스트에서 사용한 JSON 파일을 업데이트합니다.\n이미지 주소를 OCIR 주소로 변경합니다.\nimagePullSecrets을 추가합니다.\nOCI Vault 내에 생성한 Secret의 OCID를 입력 { \u0026#34;displayName\u0026#34;: \u0026#34;wordpress\u0026#34;, ... \u0026#34;containers\u0026#34;: [ { \u0026#34;displayName\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;yny.ocir.io/cnxxxxxxxx/mysql:8.0.31\u0026#34;, ... }, { \u0026#34;displayName\u0026#34;: \u0026#34;wordpress\u0026#34;, \u0026#34;imageUrl\u0026#34;: \u0026#34;yny.ocir.io/cnxxxxxxxx/wordpress:latest\u0026#34;, ... } ], \u0026#34;imagePullSecrets\u0026#34;: [ { \u0026#34;registryEndpoint\u0026#34;: \u0026#34;yny.ocir.io\u0026#34;, \u0026#34;secretType\u0026#34;: \u0026#34;VAULT\u0026#34;, \u0026#34;secretId\u0026#34;: \u0026#34;ocid1.vaultsecret.oc1.\u0026lt;region_code\u0026gt;.\u0026lt;unique_ID\u0026gt;\u0026#34; } ] } 업데이트한 JSON 파일로 Container Instance를 생성합니다.\noci container-instances container-instance create --from-json=file://wordpress-secret-vault.json CLI로 생성한 결과를 보면, 아래와 같이 OCIR상의 Private Registry에 있는 이미지로 가져와서 실행중인 것을 볼 수 있습니다.\n","lastmod":"2023-05-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/container-instances/3.image-pull-secrets/","tags":["container-instances","imagePullSecrets","OCI Vault"],"title":"6.3 Private Registry를 위한 image pull secret 사용하기"},{"categories":null,"contents":"1.2.3.2.2 File Storage 사용하기(CSI Driver 기반) - Dynamic Provisioning 앞선 Static Provisioning 방식으로 File Storage 서비스를 사용하는 경우, Persistent Volume 등록전에, File System과 Mount Target을 매번 만들고, Persistent Volume으로 등록하고, PVC를 요청하는 방식이었습니다. 그리고 Persistent Volume을 만들때 spec.csi.volumeHandle에 \u0026lt;FileSystemOCID\u0026gt;:\u0026lt;MountTargetIP\u0026gt;:\u0026lt;path\u0026gt; 형식으로 매번 지정해야 합니다.\napiVersion: v1 kind: PersistentVolume ... csi: driver: fss.csi.oraclecloud.com volumeHandle: ocid1.filesystem.oc1.ap_chuncheon_1.aaaaa_____wtcaaa:10.0.20.12:/oke-fss-system Dynamic Provisioning은 사전에 File System을 사전에 만드는 것이 아니라, OKE에서 요청(PVC)가 있을 때, 동적으로 File System을 만드는 방식입니다. 따라서 OKE Cluster가 File System을 동적으로 만들 수 있도록 File Storage 서비스에 대한 관련 권한 설정이 필요합니다.\nPolicy 설정하기 File Storage 서비스와 OKE 클러스터, Node Pool이 모든 같은 Compartment에 속하는 경우는 다음과 같이 설정하면 됩니다.\n# oke-ffs-dynamic-provisioning-policy ALLOW any-user to manage file-family in compartment \u0026lt;compartment-name\u0026gt; where request.principal.type = \u0026#39;cluster\u0026#39; ALLOW any-user to use virtual-network-family in compartment \u0026lt;compartment-name\u0026gt; where request.principal.type = \u0026#39;cluster\u0026#39; File Storage 서비스를 이용하여 Persistent Volume을 Dynamic Provision 하기 Storage Class를 만듭니다.\n아래 예시와 같은 파라미터를 설정할 수 있습니다. 각 파라미터에 대한 설명은 공식 문서를 참고합니다. Provisioning a PVC on a New File System Using the CSI Volume Plugin kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: \u0026lt;storage-class-name\u0026gt; provisioner: fss.csi.oraclecloud.com parameters: availabilityDomain: \u0026lt;ad-name\u0026gt; mountTargetOcid: \u0026lt;mt-ocid\u0026gt; | mountTargetSubnetOcid: \u0026lt;mt-subnet-ocid\u0026gt; compartmentOcid: \u0026lt;compartment-ocid\u0026gt; kmsKeyOcid: \u0026lt;key-ocid\u0026gt; exportPath: \u0026lt;path\u0026gt; exportOptions: [{\u0026lt;options-in-json-format\u0026gt;}] encryptInTransit: \u0026#34;true\u0026#34;|\u0026#34;false\u0026#34; 여기서는 최소 설정을 사용하겠습니다.\navailabilityDomain: File Storage 서비스의 File System을 만들 AD를 지정\nmountTargetSubnetOcid: 마운트 타겟도 자동으로 만들어지게 마운트 타겟이 만들어질 Subnet을 지정합니다.\n예시에서는 Node가 속한 oke-nodesubnet-\u0026hellip;-regional 서브넷의 OCID 사용 reclaimPolicy: OKE 문서상의 위 예시에는 없지만, Kubernetes Storage Class 유형에서 설정할 수 있는 항목으로, 여기에 설정하면, 이 Storage Class를 사용하는 PV는 이 설정값을 따라갑니다. 설정하지 않는 경우, 기본값은 Delete 이면, 이때는 PVC 삭제시 PV와 함께 자동생성된 Mount Target, File System도 지워집니다. Delete 또는 Retain 선택 가능\n나머지는 기본값을 사용합니다. 이후 PV 생성 요청이 오면, OKE 노드가 속한 동일 Compartment에 File System, Mount Target이 만들어지게 됩니다.\n# fss-dyn-storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: fss-dyn-storage provisioner: fss.csi.oraclecloud.com parameters: availabilityDomain: AP-CHUNCHEON-1-AD-1 mountTargetSubnetOcid: ocid1.subnet.oc1.ap-chuncheon-1.aaaaa_____wwwa reclaimPolicy: Delete 실행예시 $ kubectl apply -f fss-dyn-storage-class.yaml storageclass.storage.k8s.io/fss-dyn-storage created $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE fss-dyn-storage fss.csi.oraclecloud.com Delete Immediate false 13s oci oracle.com/oci Delete Immediate false 18h oci-bv (default) blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 18h Security List 설정\nOKE Worker Nodes -\u0026gt; Mount Target, Mount Target -\u0026gt; OKE Worker Nodes간의 통신을 위해 Security List에 등록할 필요가 있습니다. 둘다 같은 서브넷에 있는 경우 아래와 같이 설정합니다. 서로 다른 경우는 관련 문서를 참조하여 설정합니다.\n10.0.10.0/24는 Quick Create 모드로 생성된 OKE Cluster의 Node용 서브넷 CIDR\nIngress\nEgress\nPersistent Volume Claim(PVC) 만들기\nstorageClassName: 앞서 만든 Storage Class 이름 accessModes: ReadWriteMany storage: 50Gi PersistentVolumeClaim 유형에 필수 요구항목이기 때문에 입력하지만, File Storage는 용량은 자동관리되며 확장되기 때문에, 입력값과 상관없이 새 File System은 기본사이즈로 생성됩니다. # fss-dyn-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: fss-dyn-pvc spec: accessModes: - ReadWriteMany storageClassName: \u0026#34;fss-dyn-storage\u0026#34; resources: requests: storage: 50Gi PVC를 만들면 자동으로 PV가 만들어지는 것을 볼 수 있습니다. File System이 자동으로 생성될때까지 잠시 기다립니다. 그래도 Pending 상태인 경우 Policy를 설정했는지 확인합니다.\n$ kubectl apply -f fss-dyn-pvc.yaml $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE fss-dyn-pvc Bound csi-fss-d62f6ef1-33a1-477d-8f73-ec8eeb1db4ab 50Gi RWX fss-dyn-storage 68s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-fss-d62f6ef1-33a1-477d-8f73-ec8eeb1db4ab 50Gi RWX Delete Bound default/fss-dyn-pvc fss-dyn-storage 50s File Storage 서비스를 콘솔에서 확인해 보면, PV와 같은 이름으로 마운트 타겟과 File System이 자동으로 만들어 진 것을 볼 수 있습니다.\nPVC를 사용하는 POD 배포하기\n생성한 PVC(spec.template.spec.volumes.persistentVolumeClaim.claimName)를 볼륨으로 등록하여 마운트합니다.\nReadWriteMany 접근모드를 사용하므로 앞선 Block Volume을 PV 사용하는 예제와 달리 replica를 복수개로 지정할 수 있습니다.\n# nginx-deployment-fss-dyn-pvc.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-fss-dyn-pvc name: nginx-fss-dyn-pvc spec: replicas: 3 selector: matchLabels: app: nginx-fss-dyn-pvc template: metadata: labels: app: nginx-fss-dyn-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: fss-dyn-pvc 실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\n$ kubectl apply -f nginx-deployment-fss-dyn-pvc.yaml deployment.apps/nginx-fss-dyn-pvc created $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-dyn-pvc-7ccc9b45bb-fphnc 1/1 Running 0 9s 10.244.1.6 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-dyn-pvc-7ccc9b45bb-lc86c 1/1 Running 0 9s 10.244.0.8 10.0.10.107 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-dyn-pvc-7ccc9b45bb-vmv9t 1/1 Running 0 9s 10.244.0.136 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 파일 쓰기 테스트\n아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\n$ kubectl exec -it nginx-fss-dyn-pvc-7ccc9b45bb-fphnc -- bash -c \u0026#39;echo \u0026#34;Hello FSS - 1\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; $ kubectl exec -it nginx-fss-dyn-pvc-7ccc9b45bb-fphnc -- cat /usr/share/nginx/html/hello_world.txt Hello FSS - 1 $ kubectl exec -it nginx-fss-dyn-pvc-7ccc9b45bb-lc86c -- cat /usr/share/nginx/html/hello_world.txt Hello FSS - 1 $ kubectl exec -it nginx-fss-dyn-pvc-7ccc9b45bb-lc86c -- bash -c \u0026#39;echo \u0026#34;Hello FSS - 2\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; $ kubectl exec -it nginx-fss-dyn-pvc-7ccc9b45bb-vmv9t -- cat /usr/share/nginx/html/hello_world.txt Hello FSS - 1 Hello FSS - 2 자동으로 생성된 Persitent Volume은 StorageClass의 RECLAIM POLICY을 상속받으며, 기본값이 Delete입니다. Delete인 경우 PVC를 삭제하는 경우, PV가 삭제되면, 실제 만들어진 File Storage 의 마운트 타겟과 File System은 자동으로 삭제됩니다.\n$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-fss-d4e59f1d-fa2a-4e92-857f-22a32869141b 50Gi RWX Delete Bound default/fss-dyn-pvc fss-dyn-storage 13m RECLAIM POLICY를 Retain으로 변경 적용하면, PVC 삭제시, PV는 Release 되며 삭제되지 않습니다. PV를 삭제하더라도, 실제 마운트 타겟과 File System은 자동으로 삭제되지 않습니다.\n참고 문서 OCI Documentation \u0026gt; Container Engine \u0026gt; Provisioning a PVC on a New File System Using the CSI Volume Plugin ","lastmod":"2023-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/storage/file-storage-service/2.file-storage-dynamic-provisioning/","tags":["oke"],"title":"1.2.3.2.2 File Storage 사용하기 - Dynamic Provisioning"},{"categories":null,"contents":"1.5.3.3.3 FluentBit로 OKE 로그 전송하기 EFK 구성에서 ElasticSearch와 Kibana 대신 OpenSearch와 OpenSearch Dashboard를 사용합니다.\nFluentBit 설치 FluentBit 사이트의 권장에 따라 Helm Chart로 설치합니다.\nhttps://docs.fluentbit.io/manual/installation/kubernetes#installation FluentBit Helm Chart 저장소를 추가합니다.\nhelm repo add fluent https://fluent.github.io/helm-charts OpenSearch로 로그를 포워딩하기 위한 설정값을 작성합니다.\nmyvalues.yaml config: outputs: | [OUTPUT] Name es Match kube.* Host amaaaaaavsea7yia3enl6q6yrwekrd7qpi3yqaphhcvwowtmzftkt45jel7a.opensearch.ap-chuncheon-1.oci.oraclecloud.com Port 9200 tls On tls.verify Off Retry_Limit False Logstash_Format On Trace_Error On [OUTPUT] Name es Match host.* Host amaaaaaavsea7yia3enl6q6yrwekrd7qpi3yqaphhcvwowtmzftkt45jel7a.opensearch.ap-chuncheon-1.oci.oraclecloud.com Port 9200 tls On tls.verify Off Retry_Limit False Logstash_Format On Trace_Error On OKE 클러스터에 FluentBit을 설치합니다.\nkubectl create ns logging helm upgrade --install fluent-bit fluent/fluent-bit -f myvalues.yaml -n logging 테스트\n테스트 앱을 배포합니다.\nkubectl create deployment docker-hello-world --image=scottsbaldwin/docker-hello-world:latest kubectl expose deployment docker-hello-world --name docker-hello-world-svc --port 80 --type LoadBalancer 테스트 앱을 호출합니다.\n테스트 Pod가 배포된 노드에 있는 fluentbit 로그를 확인합니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES docker-hello-world-84ddf58f67-jpvrt 1/1 Running 0 10m 10.244.1.8 10.0.10.196 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl get pod -o wide -n logging NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES fluent-bit-2n42h 1/1 Running 0 8s 10.244.0.8 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluent-bit-4wpl2 1/1 Running 0 13s 10.244.1.9 10.0.10.196 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; fluent-bit-mwgtm 1/1 Running 0 21s 10.244.0.137 10.0.10.233 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 로그 확인 $ kubectl logs -f fluent-bit-4wpl2 -n logging [2022/10/16 10:24:42] [error] [output:es:es.0] error: Output {\u0026#34;took\u0026#34;:2,\u0026#34;errors\u0026#34;:true,\u0026#34;items\u0026#34;:[{\u0026#34;create\u0026#34;:{\u0026#34;_index\u0026#34;:\u0026#34;logstash-2022.10.16\u0026#34;,\u0026#34;_type\u0026#34;:\u0026#34;_doc\u0026#34;,\u0026#34;_id\u0026#34;:\u0026#34;olBT4IMBFcJImGmyDCZo\u0026#34;,\u0026#34;status\u0026#34;:400,\u0026#34;error\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;mapper_parsing_exception\u0026#34;,\u0026#34;reason\u0026#34;:\u0026#34;object mapping for [kubernetes.labels.app] tried to parse field [app] as object, but found a concrete value\u0026#34;}}}]} [2022/10/16 10:24:42] [ warn] [engine] failed to flush chunk \u0026#39;1-1665915881.614422791.flb\u0026#39;, retry in 8 seconds: task_id=0, input=tail.0 \u0026gt; output=es.0 (out_id=0) [2022/10/16 10:24:43] [ info] [input:tail:tail.0] inotify_fs_add(): inode=142531708 watch_fd=11 name=/var/log/containers/fluent-bit-4wpl2_logging_fluent-bit-ef61bf4637f1e7fb66c939894d854ff3c097c771ea0a44ffb9b1d758691eb952.log 다음과 같은 에러가 나는 것을 알수 있습니다.\nobject mapping for [kubernetes.labels.app] tried to parse field [app] as object, but found a concrete value 쿠버네티스에 배포된 앱을 보면, label이 app, app.kubernetes.io와 같이 사용하고 있는데, app을 텍스트 필드로 처리하는데, app 밑에 .kubernetes.io인 label을 처리하는 opensearch와 elasticsearch에서 문제가 있는 것으로 보입니다.\nlabel app을 k8s-app과 같이 바꿔거나, 아래 링크에 있는 것처럼 app_와 같이 변경하는 방법으로 해결할 수 있습니다. 매뉴얼한 변경은 쉽지 않으므로, Replace_Dots On을 사용하여 해결할 수 있습니다.\n참고\nhttps://github.com/fluent/fluent-bit/issues/4386\n재설치\nOpenSearch로 로그를 포워딩하기 위한 설정값을 작성합니다. Replace_Dots On을 추가합니다.\n앞선 테스트로 만들어진 index의 포맷과 차이로 인한 오류를 막기 위해 Prefix로 변경합니다.\n예시) Logstash_Prefix logstash-rd myvalues.yaml\nconfig: outputs: | [OUTPUT] Name es Match kube.* Host amaaaaaavsea7yia3enl6q6yrwekrd7qpi3yqaphhcvwowtmzftkt45jel7a.opensearch.ap-chuncheon-1.oci.oraclecloud.com Port 9200 tls On tls.verify Off Retry_Limit False Logstash_Format On Logstash_Prefix logstash-rd Trace_Error On Replace_Dots On [OUTPUT] Name es Match host.* Host amaaaaaavsea7yia3enl6q6yrwekrd7qpi3yqaphhcvwowtmzftkt45jel7a.opensearch.ap-chuncheon-1.oci.oraclecloud.com Port 9200 tls On tls.verify Off Retry_Limit False Logstash_Format On Logstash_Prefix logstash-rd Trace_Error On Replace_Dots On OKE 클러스터에 FluentBit을 설치합니다.\nhelm upgrade --install fluent-bit fluent/fluent-bit -f myvalues.yaml -n logging 재테스트\n테스트 앱을 다시 호출합니다.\n테스트 Pod가 배포된 노드에 있는 fluentbit 로그를 확인합니다. 이전 처럼 에러가 발생하지 않습니다.\n로그 확인 $ kubectl logs -f fluent-bit-zxz4w Fluent Bit v1.9.9 * Copyright (C) 2015-2022 The Fluent Bit Authors * Fluent Bit is a CNCF sub-project under the umbrella of Fluentd * https://fluentbit.io [2022/10/16 10:55:58] [ info] [fluent bit] version=1.9.9, commit=5c03b2e555, pid=1 ... [2022/10/16 10:56:57] [ info] [input:tail:tail.0] inotify_fs_add(): inode=134676887 watch_fd=10 name=/var/log/containers/fluent-bit-zxz4w_logging_fluent-bit-88634ee26035df508682e483e8f3e27f64a288b9fce37a3b61510bfc4d438181.log OpenSearch Dashboard 설정 SSH 터널링을 통해 jumpbox VM으로 접속하지 않은 경우 다시 접속합니다.\nSSH 터널링이 되어 있으므로, 로컬에서 브라우저로 OpenSearch Dashboard에 접속을 확인합니다.\n접속주소: https://localhost:5601 왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Discover 를 클릭합니다.\nCreate index pattern을 클릭합니다.\n인덱스 패턴을 생성합니다.\n아래쪽에 보이는 소스 중에서 앞서 만든 index prefix로 시작하는 logstash-rd 소스를 사용합니다.\nIndex pattern name: logstash-rd-*\nTime field: timestamp\n인덱스 패턴이 추가된 결과를 볼 수 있습니다.\nlabels의 app.kubernetes.io가 app_kubernetes_io로 필드명이 등록된 것을 볼 수 있습니다. 왼쪽 상단 내비게이션 메뉴에서 OpenSearch Dashboards \u0026gt; Discover 를 클릭합니다.\n테스트 앱을 접속합니다.\n예) http://152.67.xxx.xxx/?opensearch-test 로그 확인\n아래와 같이 OpenSearch Dashboard에서 테스트 앱의 로그를 확인할 수 있습니다.\n","lastmod":"2022-10-16T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/observability/application/oci-opensearch/3.fluentbit-to-oci-opensearch/","tags":["oke","oci opensearch","opensearch"],"title":"1.5.3.3.3 FluentBit로 OKE 로그 전송하기"},{"categories":null,"contents":"15.3 Resource Manager을 위한 Private Git 서버 구성 Resource Manager는 테라폼 설정 저장소로 GitHub, GitLab을 지원합니다. 퍼블릭 인터넷으로 접근이 가능한 환경은 당연히 Resource Manager에서 연결이 가능합니다. 설정 저장소를 퍼블릭 서버가 아닌 내부 서버를 사용하고자 하는 경우, OCI 상에 Private 하게 설치된 서버는 다음 절차에 따라 연결할 수 있습니다.\nPrivate GitLab CE VM 만들기 Compute 인스턴스 만들기 GitLab CE를 설치할 Compute 인스턴스를 만듭니다.\nImage: Oracle Linux 7.9를 선택 (아래 설치 내용은 Oracle Linux 7 기준입니다) Networking: Public IP가 없는 Private Subnet을 선택 예시, Private Subnet-ExampleVCN Add SSH Keys: 사용할 키 쌍의 Public Key를 입력 GitLab CE 인스턴스 접속을 위한 JumpBox Compute 인스턴스를 준비합니다.\nNetworking: Public IP를 가지고 GitLab CE 인스턴스에 접근이 가능하도록 설정합니다. Public Subnet을 선택하고 Public IP를 받도록 설정합니다. 예시, Public Subnet-ExampleVCN Add SSH Keys: 사용할 키 쌍의 Public Key를 입력 Private Key를 사용하여 JumpBox에 SSH로 접속합니다.\nJumpBox에서 다시 GitLab CE에 SSH로 접속합니다.\nPrivate Key를 JumpBox의 홈에 .ssh/id_rsa에 저장하거나, ssh 옵션으로 지정할 수 있습니다.\nPrivate Key를 사용하여 GitLab CE 인스턴스에 SSH로 접속합니다.\n[opc@jumpbox ~]$ ssh opc@10.0.1.149 Last login: Mon Jun 20 04:44:00 2022 from jumpbox.sub06200152300.examplevcn.oraclevcn.com [opc@gitlab-ce ~]$ GitLab CE 설치하기 접속한 GitLab CE Compute 인스턴스에 GitLab CE를 설치합니다.\n필요한 OS 라이브러리를 설치합니다.\nsudo yum install -y curl policycoreutils-python openssh-server perl # Enable OpenSSH server daemon if not enabled: sudo systemctl status sshd sudo systemctl enable sshd sudo systemctl start sshd # Check if opening the firewall is needed with: sudo systemctl status firewalld sudo firewall-cmd --permanent --add-service=http sudo firewall-cmd --permanent --add-service=https sudo systemctl reload firewalld Postfix를 통해 이메일 통지를 할 경우 다음을 설치합니다.\nsudo yum install postfix sudo systemctl enable postfix sudo systemctl start postfix GitLab CE 라파지토리를 설정합니다.\ncurl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ce/script.rpm.sh | sudo bash GitLab CE 패키지를 설치합니다.\nEXTERNAL_URL는 DNS를 통해 접속하는 주소로, HTTPS 인증서 등록시 사용될 주소입니다. 원하는 주소로 입력합니다.\nsudo EXTERNAL_URL=\u0026#34;https://gitlab.example.com\u0026#34; yum install -y gitlab-ce 서버가 속한 Security List에 Ingress Rule 추가합니다.\nSecurity List Name: 예) Security List for Private Subnet-ExampleVCN 대상: 예) VCN(10.0.0.0/16)내의 Source에 대해서 허용하는 경우 Stateless Source IP Protocol Source Port Range Destination Port Range Description No 10.0.0.0/16 TCP All 443 GitLab CE OCI Certificate 서비스에서 Certificate Authority 만들기 Resource Manager에서 HTTPS기반 Private Git Server를 지원합니다. Private Git Server를 HTTPS로 연결시 사용할 Certificate은 OCI Certificate으로 등록되어 있어야 합니다. 별도 공인기관 Certificate인 경우 사전에 임포트 합니다. OCI Certificate 서비스가 Self-Signed Certificate에 대한 임포트를 지원하지 않기 때문에, 별도 공인 Certificate이 없는 경우 OCI Certificate 서비스에서 만들어서 사용합니다. 여기에서는 OCI Certificate 서비스를 통해 CA와 Certificate을 만듭니다. OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security » Dynamic Group을 클릭합니다.\nCA를 위한 Dynamic Group을 만듭니다.\nName: 예시, ca-dynamic-group resource.type=\u0026#39;certificateauthority\u0026#39; Policy를 만듭니다.\nName: 예시, ca-policy Allow dynamic-group \u0026lt;YourDynamicGroupName\u0026gt; to use keys in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group \u0026lt;YourDynamicGroupName\u0026gt; to manage objects in compartment \u0026lt;YourCompartmentName\u0026gt; OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security » Vault를 클릭합니다.\n새 Vault를 만듭니다.\nName: 예시, my-vault 만들어진 Vault로 이동합니다.\n새 Master Encryption Key를 만듭니다.\nOCI Certificate에서 사용하기 위해서는 HSM 모드로, 비대칭 키(여기서는 RSA 기반 키)를 만듭니다. OCI 콘솔에서 내비게이션 메뉴의 Identity \u0026amp; Security » Certificate » Certificate Authorities를 클릭합니다.\nCreate Certificate Authority를 클릭합니다.\nCertificate Authority 생성 정보를 입력합니다.\n기본 정보\nCA 타입: Root Certificate Authority Name: 예시, my-oci-root-ca Subject Information\nCommon Name: 예시, My OCI Root CA Authority Configuration\n앞서 만든 Vault의 Master Encryption Key를 선택합니다. Expire Rule\nMaximum Validity Duration for Certificates (Days): 하위 인증서의 최대 유효 기간으로 기본 90일 Revocation Configuration\nSkip Revocation을 체크합니다. Summary\n설정 내용 확인후 CA를 만듭니다. OCI Certificate 만들기 앞서 만든 CA 화면으로 이동합니다.\nResources \u0026gt; Certificates에서 Issue Certificate을 클릭합니다.\nCertificate 생성 정보를 입력합니다.\n기본 정보\nCA 타입: Issued by internal CA Name: 예시, gitlab.example.com Subject Information\nCommon Name: HTTPS로 접속시 사용할 주소명을 입력합니다. GitLab CE 설정이 EXTERNAL_URL 값과 동일해야 합니다. 예시, gitlab.example.com Certificate Configuration\n만료 날짜(Not Valid After)를 설정된 날짜의 전날로 지정합니다. 그렇치 않으면, 타임존에 의한 약간 시간차로 최대 만료기간(예시, 90일)을 초과하여 생성시 \u0026ldquo;The validity period 7838778727 exceeds the maximum validity period allowed 7776000000.\u0026ldquo;과 같은 오류가 발생합니다. 나머지는 기본설정 그대로 사용 Rules\n그대로 사용 Summary\n설정 내용 확인후 Certificate를 만듭니다. GitLab CE에 OCI Certificate으로 HTTPS 설정하기 HTTPS 설정시에는 Certificate PEM과 Private Key PEM 파일이 필요합니다.\nCertificate PEM 다운로드\n생성한 Certificate 화면 오른쪽 아이콘을 클릭하여 View Content 클릭\n위쪽 Certificate PEM 영역의 값을 복사하여 gitlab.example.com.crt 파일명으로 저장합니다.\nPrivate Key 다운로드\n오라클 클라우드 콘솔에서 직접 다운받을 수 없어 OCI CLI로 다운로드 받을 수 있습니다.\n편의상 OCI CLI가 바로 사용가능한 Cloud Shell로 접속합니다.\nOCI CLI로 다음명령을 수행하여 Private Key PEM 값을 저장합니다. gitlab.example.com.key 파일명으로 저장합니다.\n\u0026ndash;certificate-id는 앞서 생성한 Certificate의 OCID 값을 사용 oci certificates certificate-bundle get --certificate-id=ocid1.certificate.oc1.... --bundle-type=CERTIFICATE_CONTENT_WITH_PRIVATE_KEY --query \u0026#39;data.\u0026#34;private-key-pem\u0026#34;\u0026#39; --raw-output OCI CLI가 설치된 앞서와 동일한 방식으로 JumpBox를 통해 GitLab CE Compute 인스턴스에 SSH로 접속합니다.\n/etc/gitlab/gitlab.rb 파일에서 external_url 값을 확인합니다\n## GitLab URL ... external_url \u0026#39;https://gitlab.example.com\u0026#39; /etc/gitlab/gitlab.rb에서 Let’s Encrypt을 비활성화 합니다.\n################################################################################ # Let\u0026#39;s Encrypt integration ################################################################################ letsencrypt[\u0026#39;enable\u0026#39;] = false /etc/gitlab/ssl 폴더를 생성하고 앞서 다운로드 및 OCI CLI로 받은 Certificate PEM과 Private Key PEM 파일을 복사합니다.\nsudo mkdir -p /etc/gitlab/ssl/bak sudo chmod 755 /etc/gitlab/ssl sudo mv /etc/gitlab/ssl/*.* /etc/gitlab/ssl/bak/ sudo cp gitlab.example.com.key gitlab.example.com.crt /etc/gitlab/ssl/ GitLab 재구성 및 재시작\n최초 설치이후 24시간이 지난후 gitlab-ctl reconfigure를 돌린 경우 초기 root의 암호 파일이 삭제될 수 있습니다. 실행 전에 미리 확인합니다.\nsudo gitlab-ctl reconfigure sudo gitlab-ctl restart GitLab 접속 테스트 GitLab을 웹으로 접속하기 위해서 JumpBox를 통해 GitLab 서버로 SSH 터널링합니다.\nGitLab CE 인스턴스의 Private IP가 10.0.1.149인 경우 ssh opc@jumpbox -L 8443:10.0.1.149:443 로컬에서 웹브라우저로 접속합니다.\n접속주소: https://localhost:8443 맥에서 크롬브라우저 예시입니다. 접속후 인증서를 확인합니다. 인증서 발급자(CA)와 인증서 정보를 확인할 수 있솝니다.\nroot 암호를 확인합니다. 최초 설치이후 24시간이 지난후 gitlab-ctl reconfigure를 돌린 경우 삭제될 수 있습니다.\nsudo cat /etc/gitlab/initial_root_password root 유저로 접속을 확인합니다.\nPrivate Git 서버로 사용할 GitLab CE 서버의 기본 구성이 완료되었습니다.\nResource Manager에서 Private Git 서버 연결하기 Terraform 구성정보를 저장할 GitLab Repository 및 접속 계정 만들기 새 GitLab 프로젝트를 만듭니다.\n새 blank 프로젝트를 만듭니다. 프로젝트 이름: 예) orm-project 메뉴에서 Admin 페이지로 이동합니다.\nResource Manager에서 연결할 유저를 만듭니다.\nAdmin 메뉴에서 Overview \u0026gt; Users 를 클릭합니다. 오른쪽 상단 New user를 클릭합니다. 유저 기본 정보를 입력하여 유저를 생성합니다. 유저 Account 오른쪽 위에 있는 Edit 버튼을 클릭하여 임시 패스워드를 입력합니다. 생성한 프로젝트에 유저를 추가합니다.\n프로젝트 페이지로 이동합니다. Project Information \u0026gt; Members 메뉴로 이동합니다. Invite Member를 통해 생성한 유저를 Maintainer 역할로 추가합니다. 생성한 유저로 로그인합니다.\nResource Manager에서 사용할 Access Token을 만듭니다.\n유저 프로파일을 변경화면으로 이동합니다.\n왼쪽 User Settings 메뉴에서 Access Token을 클릭합니다.\nGitLab의 경우 read_api 권한을 추가하여 생성합니다.\n생성된 Personal Access Token을 복사해 둡니다.\nPrivate Endpoint 만들기 Private Access는 OCI 서비스에서 인터넷을 거치지 않고, 사용자의 Private 환경에 접근하는 것을 뜻합니다. OCI Resource Manager에서도 인터넷을 통하지 않고 사용자의 Private IP를 통해 대상 서버, 여기서는 Private Git Server를 접근하기 위해 Private Subneet에 연결 포인트인 Private Endpoint를 만듭니다.\nOCI 콘솔에서 내비게이션 메뉴의 Developer Services » Resource Manager » Private Endpoints를 클릭합니다.\nCreate private endpoint를 클릭합니다.\nprivate endpoint 생성 정보하여 생성합니다.\nName: 예시, gitlab-private-endpoint VCN, Subnet: GitLab 서버에 접근이 가능한 서브넷을 선택합니다. 예시, Private Subnet-ExampleVCN Resource Manager에서 Source Provider로 설정하기 위해 다음을 체크합니다. Allow this private endpoint to be used with a configuration source provider Private Endpoint를 위한 내부 구성을 하는 데 일정 시간이 소요됩니다. 테스트 환경에서는 Private Endpoint가 Active 상태로 되기까지 5~6 분 정도 소요되었습니다.\nPrivate DNS Zone 만들기 OCI 콘솔에서 내비게이션 메뉴의 Networking » DNS Management » Zones를 클릭합니다.\nPrivate Zones 탭을 클릭합니다.\nVCN 생성시 만들어진 내부 DNS Zone 정보가 보입니다. Create Zone을 클릭하여 새 Zone을 만듭니다.\nPrivate Endpoint를 속한 DNS Private View내에서, Private Git Server를 위한 Zone을 추가합니다.\n생성된 Zone에 Private Git Server를 위한 레코드를 추가합니다.\n앞서 생성한 GitLab CE의 DNS 이름과 Private IP 주소를 아래와 같이 추가합니다.\nPublish Changes를 클릭하여, 변경사항을 반영합니다.\n이제 해당 VCN내에서 gitlab.example.com와 Private IP가 매핑이 됩니다. 같은 VCN 상인 JumpBox에서 아래와 같이 조회됩니다.\n[opc@jumpbox ~]$ nslookup gitlab.example.com Server:\t169.xxx.xxx.xxx Address:\t169.xxx.xxx.xxx#53 Non-authoritative answer: Name:\tgitlab.example.com Address: 10.0.1.149 [opc@jumpbox ~]$ Source Provider 만들기 다시 Resource Manager로 돌아갑니다.\nOCI 콘솔에서 내비게이션 메뉴의 Developer Services » Resource Manager » Configuration Source Providers를 클릭합니다.\nCreate Configuration Source Provider를 클릭합니다.\nSource Provider 정보를 입력하여 만듭니다.\nName: 예시, private-gitlab Private endpoint 유형 선택 Private Endpoints: 앞서 만든 gitlab-private-endpoint 선택 SSL certificate: GitLab CE 서버에서 사용중인 OCI Certificate 선택 Type: GitLab 선택 Server URL: GitLab CE 서버 설치시 EXTERNAL_URL이자, OCI Certificate 생성시 Common Name으로 등록한 DNS Name을 사용한 주소를 입력합니다. 여기서는 https://gitlab.example.com Personal Access Token: Resource Manager에서 사용하기 위해 앞서 GitLab CE에 추가 생성한 유저의 Access Token 입력 만들어진 Source Provider를 클릭합니다.\n연결 테스트를 위해 Validate connection을 클릭합니다.\n연결을 성공하였습니다.\nSource Provider에서 Create Stack을 클릭합니다.\nPrivate Git Repository 조회 테스트\n테라폼 구성의 소스가 앞서 만든 Private Git 서버가 자동으로 선택됩니다. 해당 서버에 있는 Repository가 조회되는 것을 볼 수 있습니다. 여기서는 일단 잘 조회되는지 테스트만 하고 다음 단계로 넘어갑니다.\nPrivate Git Server에 테라폼 설정 만들기 Private Git Server에 접근이 가능한 JumpBox를 SSH로 접속합니다.\n오라클 클라우드 콘솔에서 앞서 만든 OCI Certificate의 상세화면으로 이동합니다.\nCertificate Chain PEM 다운로드\n생성한 Certificate 화면 오른쪽 아이콘을 클릭하여 View Content 클릭\n아래 Certificate Chain PEM 영역의 값을 복사하여 my-oci-root-ca.crt 파일명으로 저장합니다.\nOCI Certificate 서비스에서 만든 Root CA를 JumpBox에 신뢰할 수 있는 CA로 다음과 같이 추가합니다.\nOracle Linux 기준 명령입니다. sudo cp my-oci-root-ca.crt /etc/pki/ca-trust/source/anchors/ sudo update-ca-trust GitLab 접속 브라우저로 돌아가 Repository의 주소를 복사합니다.\nJumpBox에서 git clone 명령을 수행합니다.\n앞서 만든 GitLab 유저와 패스워드를 사용합니다. Root CA를 등록했기 때문에서, SSL 관련 에러 없이 정상적으로 클론에 성공했습니다. [opc@jumpbox ~]$ git clone https://gitlab.example.com/gitlab-instance-ed05d7b9/orm-project.git Cloning into \u0026#39;orm-project\u0026#39;... Username for \u0026#39;https://gitlab.example.com\u0026#39;: orm-user Password for \u0026#39;https://orm-user@gitlab.example.com\u0026#39;: remote: Enumerating objects: 3, done. remote: Counting objects: 100% (3/3), done. remote: Compressing objects: 100% (2/2), done. remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 Receiving objects: 100% (3/3), done. 복제된 프로젝트 폴더로 이동합니다.\n여기에 이전 가이드(15.1 Resource Manager 사용하기)를 참고하여 테라폼 설정 파일을 만듭니다.\n변경된 파일을 반영합니다.\nResource Manager에서 Private Git Server를 통해 자원 생성하기 오라클 클라우드 콘솔에서 내비게이션 메뉴의 Developer Services » Resource Manager » Stack를 클릭합니다.\nCreate Stack을 클릭합니다.\n설정파일 위치를 Source Code Control System로 선택합니다.\n앞서 설정한 private-gitlab Source Provider에 테라폼 설정이 있는 Repository를 선택합니다.\n아래 그림과 같이 Repository에 있는 스키마 정보를 정상적으로 가져오는 것을 볼 수 있습니다.\n나머지 값을 기본 값을 이용해 Stack을 생성합니다.\n만들어진 Stack을 Apply를 클릭하여 테라폼 설정을 적용합니다.\nPrivate Git Server상의 테라폼 설정을 통해 정상적으로 생성된 것을 볼 수 있습니다.\n","lastmod":"2022-06-21T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter15/3/","tags":["resource manage","terraform","git"],"title":"15.3 Resource Manager을 위한 Private Git 서버 구성"},{"categories":null,"contents":"14.1.1.3 토큰 기반 인증 Config File 설정 유저가 OCI CLI의 Config File 설정을 위해서 크게 API singing key 기반 인증과 토큰 기반 인증이 있습니다. (OCI에 배포된 인스턴스와 자원을 위해서는 다른 추가적인 방법도 있습니다.) 토큰 기반 인증은 웹브라우저를 통해 로그인하면 생성되는 세션 토큰을 이용하는 방식입니다. 토큰은 기본 TTL이 1시간으로 만료되기전에 갱신하여 사용할 수 있습니다.\n토큰을 통한 OCI CLI 인증 설정하기 웹브라우저 실행이 가능한 위치에서 다음 명령을 실행합니다.\noci session authenticate 사용할 리전을 선택합니다.\n웹브라우저가 자동 실행되면, OCI 콘솔에 로그인하여 인증합니다.\n인증이 완료되면 $HOME/.oci/config 파일이 생성됩니다.\n실행결과\n$ oci session authenticate Enter a region by index or name(e.g. 1: af-johannesburg-1, 2: ap-chiyoda-1, 3: ap-chuncheon-1, 4: ap-dcc-canberra-1, 5: ap-hyderabad-1, 6: ap-ibaraki-1, 7: ap-melbourne-1, 8: ap-mumbai-1, 9: ap-osaka-1, 10: ap-seoul-1, 11: ap-singapore-1, 12: ap-sydney-1, 13: ap-tokyo-1, 14: ca-montreal-1, 15: ca-toronto-1, 16: eu-amsterdam-1, 17: eu-dcc-dublin-1, 18: eu-dcc-dublin-2, 19: eu-dcc-milan-1, 20: eu-dcc-milan-2, 21: eu-dcc-rating-1, 22: eu-dcc-rating-2, 23: eu-frankfurt-1, 24: eu-jovanovac-1, 25: eu-madrid-1, 26: eu-marseille-1, 27: eu-milan-1, 28: eu-paris-1, 29: eu-stockholm-1, 30: eu-zurich-1, 31: il-jerusalem-1, 32: me-abudhabi-1, 33: me-dcc-muscat-1, 34: me-dubai-1, 35: me-jeddah-1, 36: mx-queretaro-1, 37: sa-santiago-1, 38: sa-saopaulo-1, 39: sa-vinhedo-1, 40: uk-cardiff-1, 41: uk-gov-cardiff-1, 42: uk-gov-london-1, 43: uk-london-1, 44: us-ashburn-1, 45: us-chicago-1, 46: us-gov-ashburn-1, 47: us-gov-chicago-1, 48: us-gov-phoenix-1, 49: us-langley-1, 50: us-luke-1, 51: us-phoenix-1, 52: us-sanjose-1): 3 Please switch to newly opened browser window to log in! You can also open the following URL in a web browser window to continue: https://login.ap-chuncheon-1.oraclecloud.com/v1/oauth2/authorize?action=login\u0026amp;client_id=iaas_console\u0026amp;response_type=token+id_token\u0026amp;nonce=0eef1383-857d-4fa1-8814-074710ef1a76\u0026amp;scope=openid\u0026amp;public_key=eyJrdHkiOiAiUlNBIiwgIm4iOiAid3Z6U0dTd1d6Vk16bWt4XzZaaE5jelAzQmtsQWNYNXJQRDJ0V080X2dkRllPWlBUb2dRTC1JdHU4R2tLWDdGRk5DMC1SOGIxclBYRE9hcDZoc1gtQ1A2dmRyM1N3bDFXZ3ZyWHNOZl9nM2JOMkRvVVAyUGVsUUtMVVNweE1Fa25kUGVQaGVuLU92RVRTMkVfQmg3SHgyczNKcXVGWmJlWjZMa2p5aGVxa1N3cmg2UkxNQjB4dWNDb3FmTE9OOHBTc2lPZmltU2syLTVHbFRZMHNMWE9JeU9Zd01wbXdELVFZMUlaWlhja1FhRUYzVVJqRTd3S055ZTJvcGtqNDVHdGlnZlR3T0QzWFlzT1Joc1o3MjBTejYwSl9aRmlZSnNXb3NMeEo4bXZiVFRDNVltMmY5U2xaLUNIeHVpRlR3dk5fTmd6QURJdUR4eGFpZUFYZnhvSzNRIiwgImUiOiAiQVFBQiIsICJraWQiOiAiSWdub3JlZCJ9\u0026amp;redirect_uri=http%3A%2F%2Flocalhost%3A8181 Completed browser authentication process! Config written to: /Users/kildong/.oci/config Try out your newly created session credentials with the following example command: oci iam region list --config-file /Users/kildong/.oci/config --profile DEFAULT --auth security_token $ 생성된 config 파일을 보면 아래와 같이 토큰 파일을 기반으로 하고 있습니다.\n$ cat .oci/config [DEFAULT] fingerprint=30:31:c9:4a:79:d2:63:93:d8:cc:1e:55:13:ef:21:71 key_file=/Users/kildong/.oci/sessions/DEFAULT/oci_api_key.pem tenancy=ocid1.tenancy.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx region=ap-chuncheon-1 security_token_file=/Users/kildong/.oci/sessions/DEFAULT/token 다음 명령으로 테스트합니다.\noci os ns get --profile DEFAULT --auth security_token 또는 oci os ns get --auth security_token 실행결과\n$ oci os ns get --profile DEFAULT --auth security_token { \u0026#34;data\u0026#34;: \u0026#34;xxxxxxxxxxxx\u0026#34; } 토큰 유효성 검사 다음과 같이 토큰 유효성과 유효시간을 확인할 수 있습니다.\n$ oci session validate --profile DEFAULT --auth security_token 실행 예시\n$ oci session validate --profile DEFAULT --auth security_token Session is valid until 2023-05-10 11:48:52 토큰 리프레쉬 토큰 기본 TTL은 1시간입니다. 다음고 같이 리프레쉬 할 수 있습니다.\noci session refresh --profile DEFAULT 실행 예시\n$ oci session refresh --profile DEFAULT Attempting to refresh token from https://auth.ap-chuncheon-1.oraclecloud.com/v1/authentication/refresh Successfully refreshed token $ oci session validate --profile DEFAULT --auth security_token Session is valid until 2023-05-10 12:13:11 참고 문서\nOracle Cloud Infrastructure Documentation \u0026gt; Token-based Authentication for the CLI OCI Command Line Interface Installation and Configuration on Linux Instance - Part 1 OCI Command Line Interface Installation and Configuration on Windows Instance - Part 2 ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/3/","tags":["CLI","config"],"title":"14.1.1.3 Token 기반 인증 Config File 설정"},{"categories":null,"contents":"14.3.3 Terraform OCI Provider 연결정보 구하기 OCI 콘솔에 로그인합니다.\nTenancy OCID\n오른쪽 위 사용자 Profile에서 Tenancy 클릭 후 Tenancy OCID 확인\nUser OCID\n오른쪽 위 사용자 Profile에서 My profile 클릭 후 사용자 OCID 확인\nregion: 사용할 Region Identifier\nAPI Key - private_key_path, fingerprint\nOCI CLI 설치시 사용한 API Key를 사용합니다. 또는 새로 생성합니다. 3.3 SSH 접속을 위한 Key Pair 만들기를 참고하여 사용할 SSH Key Pair 생성 오른쪽 위 사용자 Profle에서 My profile 클릭 왼쪽 아래 API Keys 선택후 Add Key를 클릭하여 PEM 형식 Public Key 내용을 복사해서 등록 등록후 보이는 Fingerprint 복사 variable \u0026ldquo;compartment_ocid\u0026rdquo;\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments 항목으로 이동합니다.\n원하는 Compartment 클릭후 OCID 확인\nTerraform 변수 파일 생성 예시 환경에 따라 변수를 달리하기 위해 파일명이 다른 경우는 terraform 실행시 -var-file 옵션으로 파일명 지정 가능합니다.\nterraform.tfvars\n# OCI authentication tenancy_ocid = \u0026#34;ocid1.tenancy.oc1..~~~\u0026#34; # 1에서 확인한 Tenancy OCID 사용 user_ocid = \u0026#34;ocid1.user.oc1..~~~\u0026#34; # 2에서 확인한 User OCID 사용 compartment_ocid = \u0026#34;ocid1.compartment.oc1..~~~\u0026#34; # 5에서 확인한 대상 Compartment OCID 사용 private_key_path = \u0026#34;C:\\\\Users\\\\TheKoguryo\\\\.oci\\\\oci_api_key.pem\u0026#34; # 3에서 생성한 SSH Key중 Private Key의 위치 fingerprint = \u0026#34;7b:e7:~~~~\u0026#34; # 3에서 API Key로 등록한 Public Key의 Fingerprint region = \u0026#34;ap-seoul-1\u0026#34; # 4에서 확인한 대상 Region Name ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/3/","tags":["terraform","oci provider"],"title":"14.3.3 Terraform OCI Provider 연결정보 구하기"},{"categories":null,"contents":"13.3 Autoscaling 테스트 Step #5: 부하 발생에 따라 Instance Pool내에 인스턴스 수 증가 - Scale Out 테스트 Instance Pool로 이동해서 현재 있는 Compute 인스턴스의 IP를 확인한 뒤 SSH로 접속합니다.\nstress 툴 설치\nsudo yum-config-manager --enable ol8_developer_EPEL sudo yum install -y stress stress 수행\nsudo stress --cpu N 실행 예시\n사용할 Compute Instance의 CPU에 갯수에 맞춰 조정하여 부하를 계속 줍니다. 오토스케일 설정시 정한 Cool Down을 값을 고려하여 그 시간 이상 부하를 줍니다.\n[opc@web-server-template ~]$ sudo stress --cpu 4 stress: info: [42769] dispatching hogs: 4 cpu, 0 io, 0 vm, 0 hdd 부하발생 중 모니터링 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instance Pools 항목으로 이동합니다.\nAutoscaling 할 Instance Pool의 상세화면으로 이동합니다.\nResources \u0026gt; Metrics 에서 인스턴스 풀에 대한 모니터링을 지원합니다.\nMetric namespace는 oci_computeagent를 선택하면 인스턴스 풀에 속한 VM 들에 대한 메트릭 정보를 볼 수 있습니다. 아래와 같이 CPU 부하가 모니터링 되고 있습니다.\nScale Out 확인 지정한 Cooldown을 초과하여 부하가 계속발생하여 그림과 같이 Autoscaling이 발생합니다.\nInstance Pool 상세 정보에서 서버 추가가 완료된 결과를 볼 수 있습니다.\nLoad Balancer의 Backend Set에서 신규 서버가 추가되는 것을 볼 수 있습니다.\n서버 추가로 인해 Instance Pool의 평균 CPU 부하는 반으로 내려간 것을 볼 수 있습니다.\n브라우저를 통해 LB의 Public IP로 접속합니다.\n브라우저를 리프레쉬합니다. 잘 분배되는 것을 볼 수 있습니다.\nStep #6: 부하 감소에 따라Instance Pool내에 인스턴스 수 감소 - Scale In 테스트 1번 서버 SSH 접속 터미널에서 주던 부하를 중지합니다.\n[opc@web-server-template ~]$ sudo stress --cpu 4 stress: info: [42769] dispatching hogs: 4 cpu, 0 io, 0 vm, 0 hdd ^C [opc@web-server-template ~]$ 모니터링에서 Instance Pool의 평균 CPU 부하가 감소되었습니다.\n부하가 줄어들어 그림과 같이 Autoscaling이 발생합니다.\nWork Requests 상의 로그를 보면 Load Balancer에서 먼저 인스턴스를 백엔드 셋에서 제외하고 종료하는 순으로 Scale In이 됩니다.\nScale In 완료\nLoad Balancer의 Backend Set에서도 삭제된 것이 확인됨. ","lastmod":"2022-01-17T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter13/3/","tags":["autoscaling"],"title":"13.3 Autoscaling 테스트"},{"categories":null,"contents":"8.3 접근 제어를 위한 Export Option Export Option 설정을 통한 권한 제어 앞서 만든 File System의 상세화면으로 이동합니다. 아래쪽에 그림과 같이 Export 리스트에서 생성된 Export를 클릭합니다.\nExport 상세화면 아래쪽에 그림과 같이 NFS Export Options이 보입니다.\n기본 생성된 옵션은 모든 클라이언트(0.0.0.0/0)에서 Read/Write로 접근 가능합니다.\nEdit Options을 클릭합니다.\n기존 옵션 위에 테스트 VM이 있는 Public Subnet(10.0.0.0/24)는 읽기만 되게 옵션을 추가합니다.\n테스트 VM의 IP가 10.0.0.xxx라면 두 옵션의 Source에 모두 해당됩니다. 이런 경우 순서에 따라 먼저 매칭되는 것이 적용되어 실제로 첫번째 조건에 따라 Read Only 접근만 가능합니다.\n설정완료\nFile System에 권한 테스트 앞서 테스트한 VM에 접속합니다.\n기존 파일이 읽기가 되는 지 확인합니다.\nls -la /mnt/fss-system cat /mnt/fss-system/hello1.txt 실행결과\n[opc@examplelinuxinstance ~]$ ls -la /mnt/fss-system total 9 drwxrwxrwx. 2 root root 1 May 8 09:28 . drwxr-xr-x. 4 root root 36 May 8 09:10 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt drwxrwxrwx. 2 root root 0 May 8 09:28 .snapshot [opc@examplelinuxinstance ~]$ cat /mnt/fss-system/hello1.txt Hello File Storage 새 파일 쓰기를 시도합니다.\necho \u0026#39;Hello File Storage\u0026#39; \u0026gt;/mnt/fss-system/hello2.txt 실행결과\n아래와 같이 Public Subnet(10.0.0.0/24)에 해당되어 쓰기시 에러가 발생합니다.\n[opc@examplelinuxinstance ~]$ cat /mnt/fss-system/hello1.txt Hello File Storage [opc@examplelinuxinstance ~]$ hostname -I 10.0.0.208 [opc@examplelinuxinstance ~]$ echo \u0026#39;Hello File Storage\u0026#39; \u0026gt;/mnt/fss-system/hello2.txt -bash: /mnt/fss-system/hello2.txt: Read-only file system Export Option 순서 변경후 권한 재 확인 NFS Client Export Options 변경 화면 다시 이동합니다.\n각 항목 오른쪽 액션메뉴의 Move Up, Down 메뉴를 통해 순서를 조정하여 업데이트 합니다.\n변경 완료\n테스트 VM으로 돌아가 다시 마운트한 파일 시스템에 쓰기를 시도합니다.\n[opc@examplelinuxinstance ~]$ echo \u0026#39;Hello File Storage\u0026#39; \u0026gt;/mnt/fss-system/hello2.txt [opc@examplelinuxinstance ~]$ ls -la /mnt/fss-system/ total 17 drwxrwxrwx. 2 root root 2 May 8 09:31 . drwxr-xr-x. 4 root root 36 May 8 09:10 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt drwxrwxrwx. 2 root root 0 May 8 09:32 .snapshot 우선 매칭되는 0.0.0.0/0 Read/Write 권한에 따라 파일쓰기가 되는 것을 알 수 있습니다.\n","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/3/","tags":["file storage","export option"],"title":"8.3 접근 제어를 위한 Export Option"},{"categories":null,"contents":"7.3 Object 권한 관리 - 사전인증(Pre-Authenticated Requests) Pre-Authenticated Requests는 Bucket 또는 Object에 대해서 인증 없이 사용자가 접근할 수 있도록 설정하는 기능입니다. 지정된 기간까지 인증없이 Bucket 또는 Object에 접근할 수 있는 임시 공유 링크로 생성됩니다. 이 공유 링크를 통해 임의의 사용자가 접근할 수 있게 됩니다.\nObject URL로 접근 Object Details 화면에서 Object의 URL 경로를 확인합니다.\n확인한 URL 경로를 브라우저로 접근해 보면 다음과 같이 에러가 발생합니다.\nObject에 Pre-Authenticated Requests 설정하기 Object의 우측 액션 메뉴에서 Create Pre-Authenticated Request를 클릭합니다.\nPre-Authenticated Request 생성화면입니다.\nTarget: Bucket, Object 단위 또는 Prefix 기준으로 대상을 선택할 수 있습니다. 여기서는 Object로 합니다.\nAccess Type: 읽기, 쓰기에 대해서 권한을 설정할 수 있습니다.\nExpiration: 만료기간을 정할 수 있습니다. 기본값은 일주일입니다.\n설정후 아래 Create Pre-Authenticated Request를 클릭합니다.\n생성된 PAR 공유 링크를 복사합니다. 경고 문구처럼 이후 다시 확인할 수 없습니다.\n브라우저로 PAR 공유 링크를 접근합니다. 그림과 같이 인증없이 잘 접근됩니다.\nPre-Authenticated Requests 삭제하기 Bucket 상세화면의 좌측 Resources 메뉴에서 Pre-Authenticated Requests를 선택합니다. 그러면 현재 생성된 PAR 리스트가 그림과 같이 보이게 됩니다. 삭제할 PAR의 우측 액션 메뉴를 선택하여 Delete를 클릭합니다.\n확인 메시지를 확인후 삭제를 클릭합니다.\nPAR이 삭제된 후 앞서 접속한 PAR 공유 링크로 다시 접근합니다. 그림과 같이 이제 더 이상 접근되지 않습니다.\nBucket에 Pre-Authenticated Requests 설정하기 Bucket도 같은 방법으로 PAR 공유링크를 생성할 수 있습니다.\nEnable Object Listing: API로 접근시 목록을 가져오는 기능을 추가적으로 활성화하는 것이 좋습니다.\n생성된 PAR 공유 링크를 복사합니다. 경고 문구처럼 이후 다시 확인할 수 없습니다.\nEnable Object Listing을 활성화하면 Bucket PAR 공유링크를 접근하면, 아래와 같이 결과 리스트가 JSON 메시지로 보입니다.\n개별 오브젝트에 대한 접근 아래와 같이 Bucket PAR 공유링크에 개별 오브젝트 이름을 추가하면됩니다. Bucket 사전인증이라 속한 오브젝트에 대해서도 추가인증없이 접근할 수 있습니다.\nPrefix에 대한 사전 인증 링크도 동일한 방식으로 생성합니다.\n","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/3/","tags":["object storage","PAR"],"title":"7.3 Object 권한 관리 - 사전인증"},{"categories":null,"contents":"3.7.3 백업을 다른 Region으로 복사하기 Block Volume의 백업은 단일 Region 내에서만 가능합니다. 장애 복구나 다른 용도로 다른 Region에서 필요한 경우에는 Region 간 복사 기능을 통해서 다른 Region으로 복사할 수 있습니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Block Storage \u0026gt; Block Volumes Backups 항목으로 이동합니다.\n전체 백업 목록을 확인할 수 있습니다.\n원하는 백업의 우측 액션 메뉴에서 Copy to Another Region을 클릭합니다.\n복사 설정 화면\nName: 필요시 원하는 이름으로 변경\nDestination Region: 복사할 대상 Region 선택\nSubscribe 중인 Region 중에서 대상을 선택할 수 있습니다. 복사 설정 요약 확인하고 다음으로 넘어갑니다.\n우측 상단의 Region을 대상 Region으로 변경합니다. 그림과 같이 백업본이 복제된 것을 알 수 있습니다.\n증분 백업으로 복사했지만, Backup Size를 통해 전체 백업이 복사된 것을 확인할 수 있습니다.\n","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/3/","tags":["block volume","backup","copy"],"title":"3.7.3 백업을 다른 Region으로 복사하기"},{"categories":null,"contents":"3.3 SSH 접속을 위한 Key Pair 만들기 서버 인스턴스에 접근하기 위해 패스워드 대신 SSH Key Pair를 사용합니다. Key Pair는 개인키와 공개키로 구성되며, 개인키는 사용자가 사용자의 컴퓨터에 보관하며 공개키는 서버 인스턴스를 만들 때 등록해야 합니다.\nKey Pair를 만들기 위해서는 키 생성 도구가 필요하며 없는 경우 설치 후 사용합니다.\n리눅스/유닉스: ssh-keygen 사용, 미 설치시 OpenSSH(http://www.openssh.com/portable.html) 설치 후 사용 Windows 10: ssh-keygen 사용 그외 Windows: PuTTY 사이트에서(http://www.putty.org/) PuTTY Key Generator(puttygen.exe) 설치 후 사용 또는 OCI에서 VM 생성시 자동생성된 것을 다운받아 사용 리눅스/유닉스, Windows 10 환경에서 SSH Key Pair 만들기 터미널 또는 Powershell 실행\nssh-keygen을 통해 Key Pair를 생성합니다.\n명령을 실행하면, 추가적으로 passphrase 값 입력을 요구합니다. 이것은 생성되는 개인 키 파일에 대한 암호로 설정하게 되면, 매번 ssh 접속시 추가적으로 입력을 요구하게 됩니다. 필요 없으면 값을 입력하지 않고 그냥 엔터키를 칩니다.\n$ ssh-keygen -t rsa -b 2048 -C \u0026#34;\u0026lt;comment\u0026gt;\u0026#34; -f \u0026lt;output_keyfile\u0026gt; 개인키와 공개키가 각각 \u0026lt;output_keyfile\u0026gt;, \u0026lt;output_keyfile\u0026gt;.pub 파일로 생성된 걸 확인할 수 있습니다.\nubuntu@NOTEBOOK-WORK:~$ ssh-keygen -t rsa -b 2048 -C \u0026#34;my ssh key\u0026#34; -f mysshkey Generating public/private rsa key pair. Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in mysshkey Your public key has been saved in mysshkey.pub The key fingerprint is: SHA256:iWtp5RtNqW9prdInSQ0rE8PJjivXMdMeY3LuY5EUAkk my ssh key The key\u0026#39;s randomart image is: +---[RSA 2048]----+ | .Eo | | . . . | | o o . | | .*.o. | | .oS=o= | | .=O+@ . | | =o+#.B | | .oo o+% o | | o .*+= | +----[SHA256]-----+ 윈도우즈 환경에서 SSH Key Pair 만들기 내려받아 설치한 puttygen.exe 실행\n키 타입을 기본 RSA로 선택하고, 비트 수를 2048로 설정\nGenerate 클릭\n무작위 데이터를 생성하기 위해 진행 바가 끝까지 갈 때 까지 가운데 빈 화면에서 마우스를 이리저리 움직입니다.\n생성 완료\nKey comment: 필요하면 수정\nKey passphrase: 개인 키 파일에 대한 암호로 ssh 접속시 입력을 요구하게 됩니다. 필요 없으면 값을 입력하지 않고 그냥 엔터키를 칩니다.\nPutty 전용 형식으로 개인 키 저장\nSave private key 클릭, 프롬프트가 뜨면 passphrase 없이 키 저장하도록 예(Y) 선택 후 파일 저장\n이때 저장되는 개인 키는 PuTTY Private Key (PPK) 형식으로 저장되어 PuTTY 에서만 사용 가능합니다.\nPEM 형식으로 개인 키 저장\nSSH를 통해 Compute VM으로 접속시에는 PEM 형식 키를 일반적으로 사용됩니다.\n메뉴에서 Conversions \u0026gt; Export OpenSSH Key 선택합니다. 프롬프트가 뜨면 passphrase 없이 키 저장하도록 예(Y) 선택 후 파일 저장\n예시(mysshkey) -----BEGIN RSA PRIVATE KEY----- MIIEpQIBAAKCAQEArKWHqta/NDy9DsuBpq4SSiS+p3VfUk96la8Q+/LgSJRU+RPI ... HCSSOnUJxQR3xDOnrz4ywSh1bheoxUUjaHI1PtxTQtcNWHW7K2hPblE= -----END RSA PRIVATE KEY----- 공개 키 저장\n표시된 공개키 문자를 모두 복사하여 개인키가 저장된 위치에 파일로 저장합니다. ssh-keygen에서 생성되는 포맷과 동일하게 공개키의 이름은 개인키 이름과 같게 하고, 확장자만 .pub로 변경하여 저장합니다.\nOCI에서는 OpenSSH 형식을 지원하나, \u0026ldquo;Save public key\u0026quot;은 OpenSSH 형식으로 저장되지 않으므로 사용하지 않습니다.\n예시(mysshkey.pub) ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCspYeq1r80PL0Oy4GmrhJKJL6ndV9ST3qVrxD78uBIlFT5E8gV+lbDma+aZN6QpYRTboccyngd5.........qeL5YOmSw2p7Uu5kqflg+45xP3cbm42R1zMLFs81a2+5vHy/nSV523el1 rsa-key-20220110 파일 경로와 파일명을 확인 후 공개키는 Compute Instance 생성시, 개인키는 SSH로 Instance 접속시 사용합니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/3/","tags":["ssh","key pair"],"title":"3.3 SSH 접속을 위한 Key Pair 만들기"},{"categories":null,"contents":"3.5.3 PEM \u0026lt;-\u0026gt; PPK 포맷 변경하기 PEM(Privacy Enhanced Mail) OCI 인스턴스에서 사용하는 Key Pair 포맷으로 앞선 문서 기준으로 ssh-keygen 명령을 통해 생성됨\nPPK(PuTTY Private Key) PuTTY에서 사용하는 키 포맷으로 PuTTY Key Generator를 통해 저장하면 기본적으로 생성되는 포맷\nPEM -\u0026gt; PPK 포맷 변환 PuTTY Key Generator(http://www.putty.org/) 설치 후 사용\nPuTTY Key Generator를 실행합니다.\nConversion \u0026gt; Import Key 명령으로 PEM 파일을 엽니다.\n키 파일이 열리면, Save private key 클릭으로 PPK 형식으로 저장\nPPK -\u0026gt; PEM 포맷 변환 PuTTY Key Generator를 실행합니다.\nFile \u0026gt; Load private key 명령으로 PPK 파일을 엽니다.\n키 파일이 열리면 Export OpenSSH Key 클릭으로 PEM 형식으로 저장\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/3/","tags":["pem","ppk","putty"],"title":"3.5.3 PEM \u003c-\u003e PPK 포맷 변경하기"},{"categories":null,"contents":"3.6.3 Block Volume에 연결하기 Block Volume 장착 후에 iSCSI 연결을 설정해야 합니다. iscsiadm 명령으로 실행되며 실행해야 할 명령은 장착된 Block Volume에서 제공하므로 복사 후 그대로 실행하면 됩니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\n앞서 생성한 대상 Instance의 이름을 클릭합니다.\nInstance 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Attached block volumes을 클릭합니다.\n방금 장착한 Block Volume 옆에 있는 액션 아이콘(우측 점 3개)을 클릭한 다음 iSCSI commands and information을 클릭합니다.\niSCSI commands and information 다이얼로그가 뜹니다. 장착한 Volume에 대한 IP와 Port를 확인할 수 있고, 장착 및 장착해제에 사용될 명령이 보입니다. 명령에는 IP, Port가 이미 입력되어 있으므로 복사해서 그대로 사용하면 됩니다.\n연결하기 위한 Connect 항목에 있는 명령을 복사합니다.\nAttach한 Instance에 접속한 SSH로 접속합니다.\nConnect Commands의 명령을 복사해서 SSH 세션에서 순서대로 하나씩 실행합니다.\n$ ssh -i privateKey opc@144.24.xx.xxx FIPS mode initialized Activate the web console with: systemctl enable --now cockpit.socket Last login: Tue May 2 02:05:11 2023 from 132.145.xx.xx [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -o new -T iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b -p 169.254.2.2:3260 New iSCSI node [tcp:[hw=,ip=,net_if=,iscsi_if=default] 169.254.2.2,3260,-1 iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b] added [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -o update -T iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b -n node.startup -v automatic [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b -p 169.254.2.2:3260 -l Logging in to [iface: default, target: iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b, portal: 169.254.2.2,3260] Login to [iface: default, target: iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b, portal: 169.254.2.2,3260] successful. 장착을 위한 iSCSI 명령을 모두 수행이 끝나면, 이제 Linux에서 하드 디스크처럼 사용할 때처럼 포맷 및 마운트 작업을 수행하면 됩니다. 먼저, 장착 여부를 확인하기 위해 다음 명령을 수행합니다.\nsudo fdisk -l 실행결과\n아래쪽에 보면 Disk /dev/sdb가 장착된 것을 확인 할 수 있습니다. [opc@examplelinuxinstance ~]$ sudo fdisk -l Disk /dev/sda: 46.6 GiB, 50010783744 bytes, 97677312 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 812F8A56-E8ED-417C-BD3D-661E60705756 Device Start End Sectors Size Type /dev/sda1 2048 206847 204800 100M EFI System /dev/sda2 206848 2303999 2097152 1G Linux filesystem /dev/sda3 2304000 97675263 95371264 45.5G Linux LVM Disk /dev/mapper/ocivolume-root: 35.5 GiB, 38088474624 bytes, 74391552 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disk /dev/mapper/ocivolume-oled: 10 GiB, 10737418240 bytes, 20971520 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disk /dev/sdb: 50 GiB, 53687091200 bytes, 104857600 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/3/","tags":["block volume"],"title":"3.6.3 Block Volume에 연결하기"},{"categories":null,"contents":"3.8.3 Boot Volume으로 새 Instance 생성하기 아직 Instance에 사용 중이 아닌 Boot Volume을 이용하여 새 Compute Instance를 만들 수 있습니다.\nBoot Volume으로 새 Instance 생성하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nInstance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\n사용할 이미지 선택\nImage and Shape에서 Edit를 클릭하여 이미지 소스를 변경합니다. Change image를 클릭하여 이미지를 변경합니다.\nBoot volumes을 소스로 선택하면, 아직 Attached 되지 않은 Boot volumes이 보이고, 앞서 복구한 Boot volume이 보입니다. 해당 이미지를 선택합니다.\n추가 생성정보는 새 인스턴스 생성시와 동일하므로 원하는 값을 선택합니다.\n기존 Boot Volume을 사용하는 것으로 이때는 사이즈 변경없이 기존 사이즈를 그대로 사용하는 차이가 있습니다. 인스턴스 생성이 완료되면 지정한 Boot Volume을 사용하는 것을 확인할 수 있습니다. Compute 인스턴스 삭제하기 생성한 Compute 인스턴스 화면으로 이동합니다.\n인스턴스를 종료하기 위해 Terminate를 클릭합니다.\n인스턴스 종료와 함께 Boot Volume을 삭제하기 위해서는, 반드시 Permanently delete the attache boot volume 항목을 체크하고 Terminate 시킵니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/8/3/","tags":["boot volume"],"title":"3.8.3 Boot Volume으로 새 Instance 생성하기"},{"categories":null,"contents":"2.3 OCI Console 사용하기 OCI Console UI 서비스 내비게이션 왼쪽 위의 내비게이션 메뉴를 통해 서비스 및 자원에 대한 작업을 수행할 수 있습니다.\nRegion 관리 현재 Region이 오른쪽 위에 표시되며, 여러 Region을 구독하여 사용하는 경우 해당 메뉴에서 다른 Region으로 전환할 수 있습니다. 또한 다른 Region에 구독하고 싶으면 하위 메뉴에 있는 Region 관리 메뉴를 통해 구독할 수 있습니다.\n오른쪽 위 Region 선택 메뉴에서 Manage Regions를 클릭합니다.\n가능한 Region 중에서 구독하려는 Region을 확인할 수 있습니다.\n다만, Free Tier 계정은 하나의 Region만 사용할 수 있습니다.\n내비게이션 메뉴 \u0026gt; Governance \u0026amp; Administration \u0026gt; Limits, Quota and Usage를 클릭하고 Service를 Regions로 선택합니다. Subscribed region count가 현재 구독 가능한 Region 수입니다.\n","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/3/","tags":["oci console"],"title":"2.3 OCI Console 사용하기"},{"categories":null,"contents":"1.2.2.2.3 NGINX Ingress Controller에서 HOST 기반 라우팅(feat. OCI DNS) Ingress Controller에서 도메인 네임을 기반하여 라우팅하기 위해 OCI DNS를 사용하는 방법을 확인합니다.\nOCI DNS 서비스 사용하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다.\n테스트를 위해 GoDaddy 사이트에서 구매한 도메인 네임(thekoguryo.xyz)을 사용하였습니다.\n11.1 Domain Name과 매핑하기를 통해 사전에 설정된 상태를 기준으로 진행합니다.\nHOST 기반 라우팅 테스트 HOST 이름에 따라 라우팅 서비스를 달리하는 경우입니다.\n테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\n배경 색깔이 다른 두 개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=ghcr.io/thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=ghcr.io/thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80 ingress 설정 YAML(host-basic.yaml)을 작성합니다.\nblue.ingress.thekoguryo.xyz 요청은 nginx-blue-svc 로 라우팅 green.ingress.thekoguryo.xyz 요청은 nginx-green-svc로 라우팅 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-host-basic spec: ingressClassName: nginx rules: - host: blue.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 - host: green.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 작성한 host-basic.yaml을 배포합니다.\n$ kubectl apply -f host-basic.yaml ingress.networking.k8s.io/ingress-host-basic created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-host-basic nginx blue.ingress.thekoguryo.xyz,green.ingress.thekoguryo.xyz 152.xx.xxx.xxx 80 44s OCI DNS 서비스 설정 blue.ingress.thekoguryo.xyz, green.ingress.thekoguryo.xyz 주소가 Ingress Controller의 IP로 가도록 DNS 정보에 추가가 필요합니다. *.ingress.thekoguryo.xyz에 대한 정보를 추가합니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Networking \u0026gt; DNS Management \u0026gt; Zones로 이동합니다.\n생성해둔 Zone으로 이동합니다.\nManage records를 클릭하여 레코드를 추가해 줍니다.\nPublish changes를 클릭하여 반영합니다.\n반영결과\nDNS 테스트 nslookup 툴로 등록한 DNS를 테스트 해봅니다. 잘 등록된 것을 알 수 있습니다.\n$ nslookup *.ingress.thekoguryo.xyz Server: 127.0.0.11 Address: 127.0.0.11#53 Non-authoritative answer: Name: *.ingress.thekoguryo.xyz Address: 152.69.xxx.xxx HOST 기반 라우팅 테스트 ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\nblue.ingress.thekoguryo.xyz 요청\ngreen.ingress.thekoguryo.xyz 요청\n와일드 카드 주소로 DNS에 등록한 Ingress Controller의 Load Balancer를 거쳐 접속한 host의 FQDN에 따라 대상 서비스에 라우팅 되는 것을 확인할 수 있습니다.\n","lastmod":"2024-01-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/nginx-ingress/lb/3.nginx-ingress-host/","tags":["oss","ingress-controller","nginx"],"title":"1.2.2.2.3 HOST 기반 라우팅"},{"categories":null,"contents":"1.4.1.3 Helm Chart Repository로 사용하기 OCIR은 OCI(Open Container Initiative) Registry로 Helm v3.8.0에서 GA(General Availability)가 되어 공식적으로 지원하고 있습니다. 그 이상의 버전을 설치하여 사용합니다.\nhttps://helm.sh/docs/topics/registries/ Helm CLI 환경 준비 helm cli를 사용할 Cloud Shell 또는 작업환경에 접속합니다.\nhelm cli 3.9.4 설치\n작업환경에 helm cli가 설치되지 않은 경우, 편의상 현재 Cloud Shell에 기본 설치된 버전과 동일한 버전을 설치해 사용합니다.\nwget https://get.helm.sh/helm-v3.9.4-linux-amd64.tar.gz tar -zxvf helm-v3.9.4-linux-amd64.tar.gz linux-amd64/helm mv linux-amd64/helm ~/.local/bin/ .bashrc의 PATH에 등록\nPATH=$HOME/.local/bin:$HOME/bin:$PATH Helm Chart 생성후 등록하기 샘플 차트 만들기\nHelm Chart Template Guide 예제를 따라 만든 샘플 차트를 OCIR 등록 해봅니다.\n테스트를 위해 차트를 만듭니다.\n$ helm create mychart Creating mychart 차트 작성\n생성된 차트는 nginx를 배포하는 샘플 차트입니다. 실제 차트 작성을 위해서는 앱에 맞게 수정하겠지만, 지금은 배포 테스트로 수정없이 그냥 사용합니다.\n차트 패키징\nhelm package 명령으로 패키징합니다.\n$ cd mychart $ helm package . Successfully packaged chart and saved it to: /home/kildong/helm/mychart/mychart-0.1.0.tgz $ ls charts Chart.yaml mychart-0.1.0.tgz templates values.yaml OCIR 로그인 및 Helm Chart Push\nOCIR에 docker cli로 로그인 할때와 동일하게 사용자와 Auth Token을 사용해 로그인합니다. 이전 내용을 참고합니다.\n앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 helm cli로 로그인 합니다.\nOCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io region-key: 예, yny region-identifier: 예, ap-chuncheon-1 전체 Region별 OCIR 주소: Availability by Region Username: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다. OCI 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능 Password: 앞서 생성한 로그인할 유저의 Auth Token $ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnxxxxxxxxgq\u0026#34; } $ helm registry login -u cnxxxxxxxxgq/kildong@example.com yny.ocir.io Password: Login Succeeded Helm Chart Push\nOCIR에 생성한 Repository로 Push 하기 위해 아래 형식 Push 하면 됩니다. 그러면 repo-prefix/ 을 포함하여 repository 가 생성됩니다. \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-prefix\u0026gt; 하위 compartment로 push하는 경우 사전에 repository를 만들어야 합니다. 예) helm/mychart repository를 push 전에 만들것 mychart 예시 $ helm push ./mychart-0.1.0.tgz oci://yny.ocir.io/cn8wdnkejjgq/helm Pushed: yny.ocir.io/cn8wdnkejjgq/helm/mychart:0.1.0 Digest: sha256:b32e3d61d4615569065c9852d9eb0c497056a36fda2cb1646467be6c84412aa1 OCIR 확인\nHelm Chart를 OKE 클러스터에 배포하기\nCloud Shell 또는 작업 환경에 접속합니다.\n등록한 Chart로 배포합니다.\nhelm install mychart oci://yny.ocir.io/${tenancy_namespace}/helm/mychart --version 0.1.0 배포 예시\n$ helm install mychart oci://yny.ocir.io/cnxxxxxxxxgq/helm/mychart --version 0.1.0 --set service.type=LoadBalancer NAME: mychart LAST DEPLOYED: Wed May 17 09:53:49 2023 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running \u0026#39;kubectl get --namespace default svc -w mychart\u0026#39; export SERVICE_IP=$(kubectl get svc --namespace default mychart --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) echo http://$SERVICE_IP:80 $ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mychart default 1 2023-05-17 09:53:49.337838208 +0000 UTC deployed mychart-0.1.0 1.16.0 $ kubectl get all NAME READY STATUS RESTARTS AGE pod/mychart-8bc9498f-kgswz 1/1 Running 0 30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mychart LoadBalancer 10.96.9.117 138.2.xxx.x 80:32765/TCP 35s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mychart 1/1 1 1 30s NAME DESIRED CURRENT READY AGE replicaset.apps/mychart-8bc9498f 1 1 1 30s 배포 앱 접속 확인\n","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/container-registry/ocir/3.helm-chart/","tags":["container registry","helm chart"],"title":"1.4.1.3 Helm Chart Repository로 사용하기"},{"categories":null,"contents":"14.2.3 Insomnia - OCI REST API 호출하기 전문 REST 클라이언트 툴인 Insomnia를 사용하면 GUI를 통해 사용하기가 더 편리합니다. 아직 OCI Request Signature를 기본 지원하지 않아, Insomnia에서 OCI REST API를 호출하는 방법을 설명하고자 합니다.\nStep 0. insomnia 다운로드 공식 사이트에서 Free 앱을 다운로드 받습니다.\nhttps://insomnia.rest/download Step 1. insomnia-plugin-oci-request-signature 플러그인 OCI Request Signature를 통한 호출을 하기 위해 Insomina에서 제공하는 플러그인 확장 기능을 통해 OCI용 플러그인을 만들었습니다. 이를 아래와 같이 설치해서 사용하면 됩니다.\n설치 가이드를 따라 플러그인을 설치합니다.\nStep 2. 환경변수 설정 Insomnia 환경변수를 다음 가이드에 따라 설정합니다.\nenvironment variables within Insomnia 필요 환경 변수\nisOracleCloud: \u0026ldquo;true\u0026quot;로 설정, 설정한 플러그인인 글로벌하게 적용되는 것이라, true일때만 세부 스크립트가 동작함 tenancyId: tenancy OCID authUserId: 사용자 OCID keyFingerprint: API Key의 핑거프린트 privateKeyPath: API Key로 등록한 Public Key에 매칭되는 Private Key의 경로 Step 3. 사용자 조회 REST API 실행 아래와 같이 환경변수만 선택하면, 추가작업 없이 REST API를 호출할 수 있습니다.\nAPI Endpoints\n아래 Endpoint 중 IAM은 Home Region의 Endpoint를 사용합니다. Identity and Access Management Service API | Oracle Cloud Infrastructure API Reference and Endpoints ListUsers\nhttps://docs.oracle.com/en-us/iaas/api/#/en/identity/20160918/User/ListUsers Step 4. 사용자 생성 REST API 실행 아래와 같이 환경변수만 선택하면, 추가작업 없이 REST API를 호출할 수 있습니다.\nCreateUser\nhttps://docs.oracle.com/en-us/iaas/api/#/en/identity/20160918/User/CreateUser ","lastmod":"2019-05-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/3/","tags":["rest api","insomnia"],"title":"14.2.3 Insomnia - OCI REST API 호출하기"},{"categories":null,"contents":"10.3 Backend 웹서버 만들기 첫 번째 Linux Instance 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nInstance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\n생성정보 입력\n주요 항목만 필요한 값을 입력하고 나머지는 기본값을 그대로 사용합니다.\n기본 정보\nName: 이름 입력, 예) Web-Server-1 Create in compartment: 위치할 Compartment를 선택, 앞서 생성한 oct-hol-xx 선택 Image and Shape\n기본값이 VM.Standard.E2.1.Micro 자원이 Limit로 인해 부족한지 확인후 부족한 경우 VM.Standard.E2.1로 변경합니다. Networking\n앞서 만든 oci-hol-vcn의 public subnet(10.0.0.0/24)을 선택합니다.\nAdd SSH Keys\n이후 웹서버 설치 작업을 위해 VM에 접속시 사용할 SSH Key를 Public Key를 입력합니다.\n생성 정보 입력후 Create 클릭\n생성 완료\n두 번째 Instance 만들기 동일한 방법으로 두 번째 인스턴스를 만듭니다.\n생성정보 입력\n일부 값만 다르게 하여 생성합니다.\n기본 정보\nName: Web-Server-2 Placement\nShow advanced options을 클릭하여 가용성을 위해 첫번째 인스턴스와 다른 Fault Domain을 선택하면 좋습니다.\nImage and Shape\n기본값이 VM.Standard.E2.1.Micro 자원이 Limit로 인해 부족한지 확인후 부족한 경우 VM.Standard.E2.1로 변경합니다. 생성완료\n첫번째 Apache HTTP Server 설치 5.1 Linux 인스턴스에 Apache HTTP Server 설치와 설치과정은 거의 같습니다.\n생성한 Instance에 SSH 명령을 통해 접속\nApache HTTP Server 설치\n# Apache HTTP Server 설치 sudo yum -y install httpd # OS 방화벽에서 Apache HTTP용 포트, 80 포트 개방 sudo firewall-cmd --permanent --add-port=80/tcp # 방화벽 변경정보 다시 반영 sudo firewall-cmd --reload # Apache 시작 sudo systemctl start httpd sudo systemctl enable httpd 테스트를 위해 서버의 Root Index Document 생성\nsudo bash -c \u0026#39;echo Hello Apache on Web-Server-1 \u0026gt;/var/www/html/index.html\u0026#39; 설치후 테스트 결과\n[opc@web-server-1 ~]$ curl http://127.0.0.1 Hello Apache on Web-Server-1 두번째 Apache HTTP Server 설치 두번째도 동일하게 Apache HTTP Server를 설치합니다. 1. ~ 2.까지는 첫 번째와 동일\n테스트를 위해 서버의 Root Index Document 생성\nsudo bash -c \u0026#39;echo Hello Apache on Web-Server-2 \u0026gt;/var/www/html/index.html\u0026#39; 설치후 테스트 결과\n[opc@web-server-2 ~]$ curl http://127.0.0.1 Hello Apache on Web-Server-2 ","lastmod":"2019-01-23T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/3/","tags":["load balancer","backend"],"title":"10.3 Backend 웹서버 만들기"},{"categories":null,"contents":"6.3 그룹 만들기 Step 1. OCI 그룹 추가 관리자로 OCI 콘솔에 로그인합니다. OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Groups 항목으로 이동합니다. Cloud Account 생성후에 Administrators 그룹하나만 있는 것을 볼 수 있습니다. Create Group 클릭 그룹 정보 입력 Name: \u0026ldquo;SandboxGroup\u0026rdquo; 입력 Description: 설명 입력, 예) 이름과 동일하게 \u0026ldquo;SandboxGroup\u0026rdquo; Create 클릭 Step 2. 그룹(Group)에 사용자 추가하기 생성된 그룹을 클릭합니다.\n그룹에 사용자를 추가하기 위해 Add User to Group을 클릭 앞서 생성한 새로운 사용자를 추가할 사용자로 선택\n그룹에 사용자가 추가된 모습 ","lastmod":"2019-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/3/","tags":["oci group"],"title":"6.3 그룹 만들기"},{"categories":null,"contents":"5.3 방법 #2. Custom Security List 추가하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Virtual Private Networks 항목으로 이동합니다.\n현재 사용중인 VCN(예시, oci-hol-vcn)을 클릭하여 VCN의 상세 정보로 이동합니다.\n왼쪽 아래 Resources \u0026gt; Security List 항목으로 이동합니다.\nVCN 마법사로 생성한 Private Subnet용, Public Subnet용 Security외에 Custom Security List 추가를 위해 Create Security List를 클릭합니다.\n아래 정보로 Security List를 생성합니다.\nSecurity List Name: 새 규칙의 이름 입력, 예) custom-security-list Ingress Rule 설정 : Apache HTTP 서버가 사용하는 수신 포트 추가 Source CIDR: 0.0.0.0/0, 모든 IP에서 오는 요청 IP Protocol: TCP Destination Port Range: 80, 개방할 포트 Egress Rule 설정: 변경하지 않습니다. 하단으로 스크롤하여 Create Securit List를 클릭\n새 Security List를 추가되었습니다.\nSubnet에 새 Security List를 추가하기 위해 좌측 Resources 메뉴에서 Subnets으로 이동합니다.\nApache 서버가 설치된 인스턴스가 위치한 Public Subnet을 클릭합니다.\n현재 Default Security를 변경하지 않고 새 Security List를 추가하기 위해 Add Security List를 클릭합니다.\n앞서 만든 Custom Security List를 추가합니다.\n이제 Public Subnet에 두 개 Security List가 모두 적용됩니다.\n","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/3/","tags":["security list"],"title":"5.3 Custom Security List 추가하기"},{"categories":null,"contents":"1.3 Compartment Compartment는 자원들을 쉽게 관리할 수 있도록 하는 개념으로 폴더 구조라고 생각하면 됩니다. Tenancy가 생성되면 최초로 Root Compartment 하나가 만들어져 있으며, 관리자가 Root Compartment 하위로 새로운 Compartment를 추가할 수 있습니다. 모든 OCI 자원들은 특정 Compartment에 속하게 되며 Compartment 단위로 사용자들의 접근 정책을 관리할 수 있습니다.\nCompartment는 계층 구조로 되어 있으면, 쉽게 폴더 구조라고 생각하면 됩니다. 관리자는 부서, 사용 목적 등을 생각해 Compartment를 구성하고, 그에 따라 사용자 그룹 할당하고, Compartment 별로 IAM Policy를 설정하여, 사용자 및 자원에 대한 권한을 설정할 수 있습니다. ","lastmod":"2018-12-30T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/3/","tags":["compartment"],"title":"1.3 Compartment"},{"categories":null,"contents":"1.3.4 Cluster Autoscaler를 Cluster Add-on으로 설치하기 Kubernetes Cluster Autoscaler는\n리소스가 요구하는 자원이 있는 Pod에 대해, Pod에게 할당할 자원이 부족하여 Pod를 스케줄할 수 없는 경우, 해당 노드 풀에 Worker Node를 추가합니다. 장시간 동안 Worker Node의 활용도가 낮고 Pod를 다른 노드에 배치할 수 있는 경우, 노드 풀에서 Worker Node를 제거합니다. Kubernetes Cluster Autoscaler는 매뉴얼하게 직접 설치하거나, Cluster Add-On으로 설치할 수 있습니다. 여기서는 Cluster Add-On 설치하는 것을 기준합니다. Cluster Add-On은 Enhanced Cluster에서만 지원합니다.\nStep 1: Cluster Autoscaler가 노드 풀에 접근할 수 있도록, Workload Identity Principal 설정하기 Cluster Autoscaler가 필요한 OCI 자원을 관리할 수 있도록 권한을 부여합니다. Instance Principal 또는 Workload Identity Principal을 사용할 수 있습니다. Enhanced Cluster에서 사용할 수 있는 Workload Identity Principal을 여기서는 편의상 사용합니다.\nKubernetes Cluster Autoscaler와 Workload Identity Principal 조합은 Enhanced Cluster 및 Kubernetes Cluster Autoscaler 1.26 이상 버전에서 지원합니다.\nOCI 콘솔에 로그인합니다.\nOKE 클러스터의 OCID를 확인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Policies로 이동합니다.\n아래 규칙을 가진 Policy를 만듭니다\nName: 예, oke-cluster-autoscaler-grp-policy \u0026lt;compartment-name\u0026gt;: 대상 OKE Cluster가 위치한 compartment 이름 \u0026lt;cluster-ocid\u0026gt;: 대상 OKE 클러스터의 OCID Allow any-user to manage cluster-node-pools in compartment \u0026lt;compartment-name\u0026gt; where ALL {request.principal.type=\u0026#39;workload\u0026#39;, request.principal.namespace =\u0026#39;kube-system\u0026#39;, request.principal.service_account = \u0026#39;cluster-autoscaler\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} Allow any-user to manage instance-family in compartment \u0026lt;compartment-name\u0026gt; where ALL {request.principal.type=\u0026#39;workload\u0026#39;, request.principal.namespace =\u0026#39;kube-system\u0026#39;, request.principal.service_account = \u0026#39;cluster-autoscaler\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} Allow any-user to use subnets in compartment \u0026lt;compartment-name\u0026gt; where ALL {request.principal.type=\u0026#39;workload\u0026#39;, request.principal.namespace =\u0026#39;kube-system\u0026#39;, request.principal.service_account = \u0026#39;cluster-autoscaler\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} Allow any-user to read virtual-network-family in compartment \u0026lt;compartment-name\u0026gt; where ALL {request.principal.type=\u0026#39;workload\u0026#39;, request.principal.namespace =\u0026#39;kube-system\u0026#39;, request.principal.service_account = \u0026#39;cluster-autoscaler\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} Allow any-user to use vnics in compartment \u0026lt;compartment-name\u0026gt; where ALL {request.principal.type=\u0026#39;workload\u0026#39;, request.principal.namespace =\u0026#39;kube-system\u0026#39;, request.principal.service_account = \u0026#39;cluster-autoscaler\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} Allow any-user to inspect compartments in compartment \u0026lt;compartment-name\u0026gt; where ALL {request.principal.type=\u0026#39;workload\u0026#39;, request.principal.namespace =\u0026#39;kube-system\u0026#39;, request.principal.service_account = \u0026#39;cluster-autoscaler\u0026#39;, request.principal.cluster_id = \u0026#39;\u0026lt;cluster-ocid\u0026gt;\u0026#39;} Step 2: Cluster Autoscaler Add-on 설정파일 설정하기 설정 파일 만들기 - 파일명 cluster-autoscaler-add-on.json\n{ \u0026#34;addonName\u0026#34;: \u0026#34;ClusterAutoscaler\u0026#34;, \u0026#34;configurations\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;nodes\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;1:5:{{ node pool ocid 1 }}\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;authType\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;workload\u0026#34; } ] } \u0026quot;key\u0026quot;: \u0026quot;nodes\u0026quot;\n포맷은 아래와 같습니다. nodepool-ocid에 Cluster Autoscaler가 관리할 Node Pool의 OCID를 입력합니다.\n\u0026#34;value\u0026#34;: \u0026#34;\u0026lt;min-nodes\u0026gt;:\u0026lt;max-nodes\u0026gt;:\u0026lt;nodepool-ocid\u0026gt;\u0026#34; 2개 이상의 Node Pool을 관리하는 경우는 value 값에 CSV 형식으로 추가합니다.\n\u0026#34;value\u0026#34;: \u0026#34;2:4:ocid1.nodepool.oc1.iad.aaaaaaaaae____ydq, 1:5:ocid1.nodepool.oc1.iad.aaaaaaaaah____bzr\u0026#34; 다른 파라미터 추가 예시\nCluster Autoscaler add-on add-on configuration arguments에서 전체 설정 가능한 파라미터를 확인할 수 있으며, 아래 예시와 같이 설정할 수 있습니다.\n{ \u0026#34;configurations\u0026#34;: [ { \u0026#34;key\u0026#34;: \u0026#34;nodes\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;2:4:ocid1.nodepool.oc1.iad.aaaaaaaaae____ydq, 1:5:ocid1.nodepool.oc1.iad.aaaaaaaaah____bzr\u0026#34; }, { \u0026#34;key\u0026#34;: \u0026#34;numOfReplicas\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;3\u0026#34; # Default: 1 }, { \u0026#34;key\u0026#34;: \u0026#34;maxNodeProvisionTime\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;15m\u0026#34; # Default: 15m }, { \u0026#34;key\u0026#34;: \u0026#34;scaleDownDelayAfterAdd\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;15m\u0026#34; # Default: 10m }, { \u0026#34;key\u0026#34;: \u0026#34;scaleDownUnneededTime\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;10m\u0026#34; # Default: 10m }, { \u0026#34;key\u0026#34;: \u0026#34;annotations\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{\\\u0026#34;prometheus.io/scrape\\\u0026#34;:\\\u0026#34;true\\\u0026#34;,\\\u0026#34;prometheus.io/port\\\u0026#34;:\\\u0026#34;8086\\\u0026#34;}\u0026#34; } ] } 설정파일을 저장합니다.\nStep 3: OKE 클러스터에 Cluster Autoscaler Add-on 배포하기 oci cli 또는 클라우드 콘솔에서를 통해 Add-on을 배포할 수 있습니다. oci cli에서는 Step 2에서 작성한 JSON 파일을 이용해 배포하고, 클라우드 콘솔에서는 UI 설정을 통해 배포할 수 있습니다. 여기서는 클라우드 콘솔에서 설정하는 방법을 사용하도록 하겠습니다.\nOCI 콘솔에 로그인합니다.\nCluster Autoscaler가 관리할 Node Pool의 OCID를 확인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Container Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n대상 클러스터를 선택합니다.\n클러스터 상세정보에서 Resources \u0026gt; Add-ons를 클릭합니다.\nManage add-ons를 클릭합니다.\nClustaer Autoscaler를 클릭합니다.\nCluster Autoscaler Add-on을 설정합니다. 사용을 활성화합니다. 다른 값은 기본값을 사용하고, 관리할 Node Pool 정보, Workload Identity Principal 사용하기, Pod 수만 조정한 예시입니다. Add-on을 사용하면, 그림과 같이 추후 버전이 업데이트 되면 자동으로 업데이트하거나, 사용자가 버전을 선택하여 사용하는 기능을 제공합니다.\n배포결과를 확인하기 위해 로그를 확인합니다.\nkubectl -n kube-system logs -f -l app=cluster-autoscaler 배포가 성공하면 다음과 같은 로그가 보입니다.\n$ kubectl -n kube-system logs -f -l app=cluster-autoscaler I0129 02:48:12.687628 1 leaderelection.go:248] attempting to acquire leader lease kube-system/cluster-autoscaler... I0129 02:48:13.007616 1 leaderelection.go:248] attempting to acquire leader lease kube-system/cluster-autoscaler... I0129 02:48:13.398224 1 oci_manager.go:79] using workload identity I0129 02:48:14.964758 1 oci_manager.go:179] static node spec constructed: \u0026amp;{manager:\u0026lt;nil\u0026gt; kubeClient:\u0026lt;nil\u0026gt; id:ocid1.nodepool.oc1.ap-chuncheon-1.aaaaaaaabtavqjthmpeivjj5dj7i4yttl74y7ncnvoxgxn7kqna6xzvlolbq minSize:1 maxSize:5} I0129 02:48:16.346753 1 oci_manager.go:230] Refreshed NodePool list, next refresh after 2024-01-29 02:49:16.346746352 +0000 UTC m=+64.429249192 I0129 02:48:16.346964 1 node_instances_cache.go:156] Start refreshing cloud provider node instances cache I0129 02:48:16.346987 1 node_instances_cache.go:168] Refresh cloud provider node instances cache finished, refresh took 10.92µs W0129 02:48:26.348084 1 clusterstate.go:429] AcceptableRanges have not been populated yet. Skip checking I0129 02:49:17.520140 1 oci_manager.go:230] Refreshed NodePool list, next refresh after 2024-01-29 02:50:17.520133017 +0000 UTC m=+125.602635857 ... Kubernetes Cluster Autoscaler Pod 세 개 중 어느 Pod가 실제 동작하고 있는 지 확인해 봅니다.\n$ kubectl get pod -l app=cluster-autoscaler -n kube-system NAME READY STATUS RESTARTS AGE cluster-autoscaler-f7d79b66-g5v4d 1/1 Running 0 2m59s cluster-autoscaler-f7d79b66-lpnsd 1/1 Running 0 2m59s cluster-autoscaler-f7d79b66-w49cr 1/1 Running 0 2m59s $ kubectl -n kube-system get lease cluster-autoscaler NAME HOLDER AGE cluster-autoscaler cluster-autoscaler-f7d79b66-lpnsd 3m6s Kubernetes Cluster Autoscaler의 상태를 확인하기 위해 Config Map을 확인해 봅니다\nkubectl -n kube-system get cm cluster-autoscaler-status -oyaml 필요하면, Cluster Add-on을 통해 배포된 Deployment 설정을 확인해 봅니다.\nkubectl get deploy -n kube-system cluster-autoscaler -o yaml Step 4: 클러스터 오토스케일링 동작 확인해 보기 현재 Worker Node 상태를 확인합니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.158 Ready node 10d v1.26.7 10.0.10.42 Ready node 2d5h v1.26.7 10.0.10.43 Ready node 2d5h v1.26.7 샘플 애플리케이션 배포 파일 예시입니다.\nrequests.cpu를 기본 200 밀리코어까지 사용할 수 있게 지정하였습니다. 0.2 코어로 설정한 예시\n# nginx.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment spec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:latest ports: - containerPort: 80 resources: requests: cpu: 200m 샘플을 배포합니다.\nkubectl apply -f nginx.yaml Pod의 수를 늘립니다.\nkubectl scale deployment nginx-deployment --replicas=40 Deployment 상태를 확인합니다. 배포하다가, 자원을 다 쓰고 더 이상 Pod를 생성하지 못하고 멈춰있게 됩니다.\n$ kubectl get deployment nginx-deployment --watch NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 21/40 40 21 19s nginx-deployment 22/40 40 22 19s nginx-deployment 23/40 40 23 19s nginx-deployment 24/40 40 24 20s 이벤트 로그를 확인해 보면, CPU 부족으로 Pod 스케줄링에 실패한 것을 볼 수 있습니다. 이로 인해 Cluster Scale 이벤트가 발생하고, 노드가 3개에서 5개로 늘어납니다.\n$ kubectl get events --sort-by=.metadata.creationTimestamp ... 70s Warning FailedScheduling pod/nginx-deployment-694bc9bdb8-t2b5x 0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.. ... 58s Normal TriggeredScaleUp pod/nginx-deployment-694bc9bdb8-hz89v pod triggered scale-up: [{ocid1.nodepool.oc1.ap-chuncheon-1.aaaaaaaabtavqjthmpeivjj5dj7i4yttl74y7ncnvoxgxn7kqna6xzvlolbq 3-\u0026gt;5 (max: 5)}] $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.108 NotReady \u0026lt;none\u0026gt; 1s v1.26.7 10.0.10.158 Ready node 11d v1.26.7 10.0.10.204 NotReady \u0026lt;none\u0026gt; 18s v1.26.7 10.0.10.42 Ready node 2d19h v1.26.7 10.0.10.43 Ready node 2d19h v1.26.7 확장된 Worker Node가 Ready 상태가 되면 나머지 Pod에 대한 스케줄링이 진행됩니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.108 Ready node 3m1s v1.26.7 10.0.10.158 Ready node 11d v1.26.7 10.0.10.204 Ready node 3m18s v1.26.7 10.0.10.42 Ready node 2d19h v1.26.7 10.0.10.43 Ready node 2d19h v1.26.7 $ kubectl get deployment nginx-deployment --watch NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 24/40 40 24 20s ... ... ... nginx-deployment 24/40 40 24 6m15s ... nginx-deployment 40/40 40 40 6m32s Step 5: 정리 및 Scale In 확인하기 배포한 샘플 애플리케이션을 삭제합니다.\nkubectl delete deployment nginx-deployment scale-down-unneeded-time=10m 설정값에 따라 10분 뒤에 ScaleDown 이벤트가 발생한 것을 확인할 수 있습니다.\n$ kubectl get events --sort-by=.metadata.creationTimestamp -n default ... 16m Normal Killing pod/nginx-deployment-694bc9bdb8-lbz2r Stopping container nginx 6m18s Normal ScaleDown node/10.0.10.108 marked the node as toBeDeleted/unschedulable 5m58s Normal NodeNotSchedulable node/10.0.10.108 Node 10.0.10.108 status is now: NodeNotSchedulable 5m15s Normal NodeNotReady node/10.0.10.108 Node 10.0.10.108 status is now: NodeNotReady 5m12s Normal DeletingNode node/10.0.10.108 Deleting node 10.0.10.108 because it does not exist in the cloud provider 5m10s Normal RemovingNode node/10.0.10.108 Node 10.0.10.108 event: Removing Node 10.0.10.108 from Controller 87s Normal ScaleDown node/10.0.10.204 marked the node as toBeDeleted/unschedulable 56s Normal NodeNotSchedulable node/10.0.10.204 Node 10.0.10.204 status is now: NodeNotSchedulable 15s Normal NodeNotReady node/10.0.10.204 Node 10.0.10.204 status is now: NodeNotReady 12s Normal DeletingNode node/10.0.10.204 Deleting node 10.0.10.204 because it does not exist in the cloud provider 9s Normal RemovingNode node/10.0.10.204 Node 10.0.10.204 event: Removing Node 10.0.10.204 from Controller 노드의 상태를 조회해봅니다. 해당 노드가 삭제되고 원래대로 3개의 노드로 남았습니다.\nCluster Autoscaler 배포시 \u0026lt;min-nodes\u0026gt;을 1로 설정해도, Node Pool 생성시 지정한 수가 더 큰 경우, 그 수 만큼은 유지하는 것으로 보입니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.158 Ready node 11d v1.26.7 10.0.10.42 Ready node 2d20h v1.26.7 10.0.10.43 Ready node 2d20h v1.26.7 ","lastmod":"2024-01-29T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/autoscaling/4.cluster-autoscaler-addon/","tags":["oke","autoscaling","cluster-autoscaler","cluster-add-on"],"title":"1.3.4 Cluster Autoscaler (Cluster Add-on)"},{"categories":null,"contents":"Console 접근을 위한 방화벽에 등록할 주소 찾기 OCI Documentation 기준으로 아래 주소에 대한 등록이 필요합니다.\n방화벽, Proxy Server 등으로 외부 인터넷 접근을 통제하는 곳에서 Console 접근을 위해 아래 주소를 등록요청합니다.\nOCI Documentation - Allowing Access to the Console from Your Network\nOCI Console과 통신은 HTTP (80), HTTPS (443) 사용\n*.oracle.com *.oraclecloud.com *.oracleinfinity.io oracle.112.2o7.net consent.trustarc.com (Console cookie preferences) 문서 내용외 추가로 등록이 필요한 주소가 있는 지 검증 Squid Proxy Server 설치하기\nCompute VM을 생성합니다.\nSquid Proxy Server를 설치합니다.\nsudo yum install -y squid squid.conf 파일 설정\nsudo vi /etc/squid/squid.conf 설치후 기본값\n아래와 같이 변경\n# Example rule allowing access from your local networks. # Adapt localnet in the ACL section to list your (internal) IP networks # from where browsing should be allowed #http_access allow localnet http_access allow localhost # And finally deny all other access to this proxy http_access deny all Squid가 사용하는 포트를 방화벽에서 개방합니다.\nsudo firewall-cmd --permanent --add-port=3128/tcp sudo firewall-cmd --reload Security List에서도 Ingress에 해당 포트를 등록합니다.\n서버 시작 및 활성화\nsudo systemctl start squid sudo systemctl enable squid 테스트 클라이언트(Windows) 설정\nWindow Server로 Compute VM을 생성합니다.\n현재는 모든 웹이 접속이 가능한 상태입니다.\nSetting \u0026gt; Network \u0026amp; Internet \u0026gt; Proxy 으로 이동하여 설정합니다.\nAutomatically detect settings: Off Use a proxy server: On Address: Squid Proxy Server의 IP Port: Squid 기본 포트 3128 다시 브라우저로 접속해 보면 Proxy Server에 의해 막혀서 접속이 되지 않습니다.\nSquid Proxy Server에 Console 접속을 위한 주소 등록 및 테스트\nsquid.conf 파일 설정 (/etc/squid/squid.conf)\n문서 상의 나와있던 주소를 추가해줍니다. ... #http_access allow localnet http_access allow localhost # For OCI Console acl oci_console_acl dstdomain .oracle.com .oraclecloud.com .oracleinfinity.io oracle.112.2o7.net http_access allow oci_console_acl # And finally deny all other access to this proxy http_access deny all ... Squid 재시작\nsudo systemctl restart squid 테스트 클라이언트 상의 브라우저에서 OCI Console에 다시 접속해 봅니다. 잘 접속되는 것을 확인할 수 있습니다.\nConsole에서 사용할 서비스들을 조회해 봅니다.\n그중 OCIR의 경우 아래와 같이 Repository가 조회되지 않고, Cannot read properties of undefiend (reading 'toString') 의 오류가 발생합니다.\n브라우저의 Developer Tools을 실행합니다. 아래 툴 중에서 네트워크 탭을 열어 오류 내역을 보면, https://icn.ocir.io 에 접속시 오류가 발생하는 것을 알 수 있습니다.\n방화벽 등록을 위해 squid.conf 파일에 .ocir.io를 추가합니다. (/etc/squid/squid.conf)\n... #http_access allow localnet http_access allow localhost # For OCI Console acl oci_console_acl dstdomain .oracle.com .oraclecloud.com .oracleinfinity.io oracle.112.2o7.net acl oci_console_additional_acl dstdomain .ocir.io http_access allow oci_console_acl http_access allow oci_console_additional_acl # And finally deny all other access to this proxy http_access deny all ... Squid 재시작\nsudo systemctl restart squid 테스트 클라이언트에서 다시 OCI Console에서 OCIR 화면으로 이동합니다. 이제 아래와 같이 잘 보이는 것을 알 수 있습니다.\n이처럼 방화벽 등록에 필요한 주소가 혹시 추가적으로 필요할 수 있으니, 필요시 위와 같은 방법으로 사전 확인할 수 있습니다.\n","lastmod":"2023-12-15T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/troubleshooting/access-console/","tags":["console","firewall","proxyserver"],"title":"Console 접근을 위한 방화벽에 등록할 주소 찾기"},{"categories":null,"contents":"1.2.3.4 Kubernetes Volume Clone 만들기 운영에 영향 없이 Volume 복제본을 만들어 테스트가 필요한 경우처럼, 사용중인 Persistent Volume을 복제하여 사용할 필요가 있을 때 사용할 수 있습니다.\nBlock Volume Clone 사용 전제조건 OKE 클러스터 Control Plane의 쿠버네티스 버전은 1.25 이상 Worker Node가 AMD 또는 Arm 기반이어야 함 Worker Node는 Oracle Linux 7 또는 Oracle Linux 8 CSI volume plugin(여, oct-bv StorageClass)를 사용하여 이미 OCI Block Volume과 바인딩된 Persistent Volume이 있어야 함 테스트 환경 준비 1.2.3.1 Block Volume 사용하기에서 Block Volume을 사용하여, Persistent Volume을 구성한 예제를 그대로 사용합니다.\n현재 상태를 조회하면 다음과 같습니다.\nCSI Volume Plugin을 통해 제공하는 기능으로 반드시 oci-bv StorageClass를 사용하는 Persistent Volume 이어야 합니다. /usr/share/nginx/html 폴더가 PV를 마운트하고 있습니다. 마운트된 폴더에 파일 쓰기를 한 상태로 이후 실습에서 해당내용이 복구되는 지 확인할 예정 $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE csi-bvs-pvc Bound csi-5385a65d-0cb8-4c65-9f6e-c65e70b997d3 50Gi RWO oci-bv 98s $ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-549db9449b-gbnf8 1/1 Running 0 95s $ kubectl exec -it nginx-bvs-pvc-549db9449b-gbnf8 -- cat /usr/share/nginx/html/hello_world.txt Hello PV 기존 Block Volume을 복제하는 새 PVC 만들기 아래와 같이 PV 요청 yaml을 사용하여 요청합니다. dataSource에서 기존 PVC를 지정합니다.\n# my-clone-pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-clone-pvc spec: storageClassName: \u0026#34;oci-bv\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 50Gi dataSource: kind: PersistentVolumeClaim name: csi-bvs-pvc dataSource.kind: PersistentVolumeClaim dataSource.name: 복제할 원본 PVC 이름을 입력 PVC를 생성합니다.\nkubectl apply -f my-clone-pvc.yaml 테스트 앱 복구용 YAML 작성\n테스트앱에서 persistentVolumeClaim.claimName을 my-clone-pvc로 지정합니다. # nginx-deployment-bvs-pvc-from-clone-pvc.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-bvs-from-clone-pvc name: nginx-bvs-from-clone-pvc spec: replicas: 1 selector: matchLabels: app: nginx-bvs-from-clone-pvc template: metadata: labels: app: nginx-bvs-from-clone-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: my-clone-pvc 테스트앱 배포\n테스트 앱에 배포되면, 앞선 PVC에 바인딩되는 Persistent Volume이 만들어 지면서 새 OCI Block Volume이 생성됩니다. kubectl apply -f nginx-deployment-bvs-pvc-from-clone-pvc.yaml 복제된 PVC, PV가 생성되었습니다.\n$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE csi-bvs-pvc Bound csi-5385a65d-0cb8-4c65-9f6e-c65e70b997d3 50Gi RWO oci-bv 21m my-clone-pvc Bound csi-efe229e2-02a2-402f-b422-fdde520146c4 50Gi RWO oci-bv 13m $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-5385a65d-0cb8-4c65-9f6e-c65e70b997d3 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 21m csi-efe229e2-02a2-402f-b422-fdde520146c4 50Gi RWO Delete Bound default/my-clone-pvc oci-bv 3m35s 복제된 PV를 사용하는 앱에서 파일 내용을 확인합니다. 복제가 잘 된 것을 알 수 있습니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-from-clone-pvc-7f6fd8dfbb-mr88f 1/1 Running 0 4m42s nginx-bvs-pvc-549db9449b-gbnf8 1/1 Running 0 22m donghee_le@cloudshell:tmp (ap-chuncheon-1)$ kubectl exec -it nginx-bvs-from-clone-pvc-7f6fd8dfbb-mr88f -- cat /usr/share/nginx/html/hello_world.txt Hello PV ","lastmod":"2023-10-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/storage/4.volume-clone/","tags":["oke","volume clone"],"title":"1.2.3.4 Kubernetes Volume Clone 사용하기"},{"categories":null,"contents":"1.10.4 Custom Scheduler 사용하기 OKE에서는 default-scheduler를 사용자에게 열어주고 있지 않습니다. 이런 상황에서 스케줄러를 확장하기 위해 쿠버네티스에서 제공하는 multiple schedulers 기능이 잘 동작하는 지 쿠버네티스 문서상의 내용을 따라 확인해 봅니다.\n스케줄러 패키징 여기서는 default-scheduler 원 소스 Kubernetes source code from GitHub를 이용하여 두 번째 스케줄러를 배포합니다. 여기서는 이후 옵션, 설정값만으로 커스터마이징하겠습니다. 필요에 따라 스케줄러 소스를 자체를 변경할 수도 있겠습니다.\n복제한 kube-scheduler 소소를 패키징합니다.\ngit clone --depth 1 --single-branch --branch release-1.26 https://github.com/kubernetes/kubernetes.git cd kubernetes make all WHAT=cmd/kube-scheduler GOFLAGS=-v 빌드후 생성된 바이너리를 확인할 수 있습니다. thekoguryo@cloudshell:kubernetes (ap-chuncheon-1)$ ls -la ./_output/local/bin/linux/amd64/kube-scheduler -rwxr-xr-x. 1 thekoguryo oci 53497856 Aug 1 04:46 ./_output/local/bin/linux/amd64/kube-scheduler 컨테이너 이미지를 생성하기 위해 Dockerfile을 만듭니다.\nFROM busybox ADD ./_output/local/bin/linux/amd64/kube-scheduler /usr/local/bin/kube-scheduler 이미지를 빌드해서 OCIR에 등록합니다.\n1.5 OCIR 이미지 사용하기 참조하여 등록합니다. # 로그인 docker login -u \u0026#39;\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;\u0026#39; yny.ocir.io # 이미지 빌드후 등록: 예, 춘천 리전 docker build -t yny.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/my-kube-scheduler:1.0 . docker push yny.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/my-kube-scheduler:1.0 편의상 OCIR Repository는 Public으로 전환하였습니다. 스케줄러 배포 파일 수정 배포 파일을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/my-scheduler.yaml 배포 파일에서 이미지 주소를 변경합니다.\nimage 주소를 본인이 등록한 OCIR 주소로 변경합니다. 스케줄러 실행 옵션 kube-scheduler 옵션 값을 변경할 수 있습니다. 여기서는 예시로 command에 - --v=4 을 추가하여 로그 레벨을 변경합니다. ... containers: - command: - /usr/local/bin/kube-scheduler - --config=/etc/kubernetes/my-scheduler/my-scheduler-config.yaml - --v=4 image: yny.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/my-kube-scheduler:1.0 ... OKE 클러스터 버전인 1.25 이상에 배포하는 경우, 배포 파일에서 apiVersion의 /v1으로 변경합니다.\n스케줄러 설정 default-scheduler와 소스코드 동일하더라도, 여기 있는 KubeSchedulerConfiguration 설정을 통해 스케줄러 설정을 커스터마이징할 수 있습니다. 예시는 NodeResourcesFit 플러그인의 스코어링 정책을 일부 변경적용한 예입니다. 추가 설정은 KubeSchedulerConfiguration 을 참고합니다. ... --- apiVersion: v1 kind: ConfigMap metadata: name: my-scheduler-config namespace: kube-system data: my-scheduler-config.yaml: | apiVersion: kubescheduler.config.k8s.io/v1 kind: KubeSchedulerConfiguration profiles: - schedulerName: my-scheduler pluginConfig: - name: NodeResourcesFit args: scoringStrategy: resources: - name: cpu weight: 1 type: MostAllocated leaderElection: leaderElect: false ... 클러스터에 두번째 스케줄러 배포 작성한 배포 파일로 배포합니다.\nkubectl apply -f my-scheduler.yaml 배포된 스케줄러를 확인합니다.\nkubectl get pods --namespace=kube-system 결과 $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE ... my-scheduler-5fb44c4fb9-kqwqc 1/1 Running 0 4m17s ... 스케줄러 테스트 스케줄러 이름 지정없이 Pod를 배포합니다.\nkind: Pod metadata: name: no-annotation labels: name: multischeduler-example spec: containers: - name: pod-with-no-annotation-container image: registry.k8s.io/pause:2.0 실행 wget https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/pod1.yaml kubectl apply -f pod1.yaml default-scheduler로 지정 배포합니다.\napiVersion: v1 kind: Pod metadata: name: annotation-default-scheduler labels: name: multischeduler-example spec: schedulerName: default-scheduler containers: - name: pod-with-default-annotation-container image: registry.k8s.io/pause:2.0 실행 wget https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/pod2.yaml kubectl apply -f pod2.yaml my-scheduler로 지정 배포합니다.\napiVersion: v1 kind: Pod metadata: name: annotation-second-scheduler labels: name: multischeduler-example spec: schedulerName: my-scheduler containers: - name: pod-with-second-annotation-container image: registry.k8s.io/pause:2.0 실행 wget https://raw.githubusercontent.com/kubernetes/website/main/content/en/examples/admin/sched/pod3.yaml kubectl apply -f pod3.yaml 결과 확인 첫번째 배포한 pod1은 스케줄러를 지정하지 않은 경우, default-scheduler가 스케줄링한 것을 볼 수 있습니다.\n$ kubectl describe pod no-annotation Name: no-annotation ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4s default-scheduler Successfully assigned default/no-annotation to 10.0.10.112 ... default-scheduler를 지정한 두번째 pod2는, default-scheduler가 스케줄링한 것을 볼 수 있습니다.\n$ kubectl describe pod annotation-default-scheduler Name: annotation-default-scheduler ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 16s default-scheduler Successfully assigned default/annotation-default-scheduler to 10.0.10.112 두번째 스케줄러인 my-scheduler로 지정한 세번째 pod3는, my-scheduler가 스케줄링한 것을 볼 수 있습니다.\n$ kubectl describe pod annotation-second-scheduler Name: annotation-second-scheduler ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 15s my-scheduler Successfully assigned default/annotation-second-scheduler to 10.0.10.112 배포한 스케줄러의 로그를 확인합니다. FLAG: --v=\u0026quot;4\u0026quot; 에서 변경한 로그레벨이 적용된 것을 볼 수 있습니다. 두 번째 pod2(annotation-default-scheduler)에 대해서는 직접 스케줄하지 않았고, 세 번째 pod3(annotation-second-scheduler)는 \u0026ldquo;Attempting to schedule pod\u0026rdquo; 로그에서 보듯이 배포한 두 번째 스케줄러가 스케줄링한 것을 확인할 수 있습니다.\n$ kubectl logs -n kube-system -f my-scheduler-587dd655fb-sf9w5 ... I0801 07:12:36.759716 1 flags.go:64] FLAG: --v=\u0026#34;4\u0026#34; ... I0801 07:13:56.973413 1 eventhandlers.go:197] \u0026#34;Add event for scheduled pod\u0026#34; pod=\u0026#34;default/annotation-default-scheduler\u0026#34; I0801 07:13:56.991814 1 eventhandlers.go:218] \u0026#34;Update event for scheduled pod\u0026#34; pod=\u0026#34;default/annotation-default-scheduler\u0026#34; I0801 07:13:58.711914 1 eventhandlers.go:218] \u0026#34;Update event for scheduled pod\u0026#34; pod=\u0026#34;default/annotation-default-scheduler\u0026#34; ... I0801 07:14:26.843405 1 eventhandlers.go:126] \u0026#34;Add event for unscheduled pod\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; I0801 07:14:26.843517 1 schedule_one.go:80] \u0026#34;About to try and schedule pod\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; I0801 07:14:26.843527 1 schedule_one.go:93] \u0026#34;Attempting to schedule pod\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; I0801 07:14:26.843792 1 default_binder.go:53] \u0026#34;Attempting to bind pod to node\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; node=\u0026#34;10.0.10.112\u0026#34; I0801 07:14:26.854440 1 eventhandlers.go:171] \u0026#34;Delete event for unscheduled pod\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; I0801 07:14:26.854457 1 eventhandlers.go:197] \u0026#34;Add event for scheduled pod\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; I0801 07:14:26.856109 1 schedule_one.go:285] \u0026#34;Successfully bound pod to node\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; node=\u0026#34;10.0.10.112\u0026#34; evaluatedNodes=1 feasibleNodes=1 I0801 07:14:26.878836 1 eventhandlers.go:218] \u0026#34;Update event for scheduled pod\u0026#34; pod=\u0026#34;default/annotation-second-scheduler\u0026#34; 참고 Extending Kubernetes Scheduling extensions Configure Multiple Schedulers ","lastmod":"2023-08-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/customize/4.custom-scheduler/","tags":["oke","scheduler"],"title":"1.10.4 Custom Scheduler 사용하기"},{"categories":null,"contents":"5.4 방법 #3. Security Group 추가하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Virtual Private Networks 항목으로 이동합니다.\n현재 사용중인 VCN(예시, oci-hol-vcn)을 클릭하여 VCN의 상세 정보로 이동합니다.\n왼쪽 아래 Resources \u0026gt; Network Security Group 항목으로 이동합니다.\nCreate Network Security Group를 클릭합니다.\n아래 정보로 Security List를 생성합니다.\nName: 새 규칙의 이름 입력, 예) apache-nsg Rule Direction: Ingress Source CIDR: 0.0.0.0/0, 모든 IP에서 오는 요청 IP Protocol: TCP Destination Port Range: 80, 개방할 포트 하단으로 스크롤하여 Create를 클릭\n새 Security Rule이 추가되었습니다.\n만든 Network Security Group을 적용할 Compute 인스턴스로 이동합니다.\n적용할 Primary VNIC에서 Network Security Group을 수정합니다.\n앞서 만든 Network Security Group을 추가합니다.\nCompute 인스턴스는 추가된 Network Security Group과 Subnet에 적용된 Security List가 함께 적용됩니다.\n","lastmod":"2023-05-04T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/4/","tags":["security group"],"title":"5.4 Network Security Group 추가하기"},{"categories":null,"contents":"3.5.4 Oracle Linux 8 on OCI에 VNC 서버 설정하기 VNC 서버 설정하기 GUI 환경을 설치합니다.\nsudo dnf group install \u0026#34;Server with GUI\u0026#34; VNC 서버를 설치합니다.\nsudo dnf install tigervnc-server 다음 파일을 수정하여 VNC를 사용할 유저를 추가합니다.\nsudo vi /etc/tigervnc/vncserver.users 디스플레이 넘버와 유저명을 추가합니다. :\u0026lt;display_number\u0026gt;=\u0026lt;user\u0026gt; 예시 # TigerVNC User assignment # # This file assigns users to specific VNC display numbers. # The syntax is \u0026lt;display\u0026gt;=\u0026lt;username\u0026gt;. E.g.: # # :2=andrew # :3=lisa :1=opc VNC 패스워드 생성, opc 유저를 다음을 실행합니다.\nvncpasswd 실행예시 [opc@linux8 ~]$ vncpasswd Password: Verify: Would you like to enter a view-only password (y/n)? n A view-only password is not used VNC 서버를 실행합니다.\nsudo systemctl start vncserver@:\u0026lt;display_number\u0026gt; 실행예시 [opc@linux8 ~]$ sudo systemctl start vncserver@:1 VNC 서버 접속하기 SSH 터널링하기\nssh \u0026lt;user\u0026gt;@\u0026lt;server\u0026gt; -L 590\u0026lt;display_number\u0026gt;:localhost:590\u0026lt;display_number\u0026gt; 실행예시 ssh opc@129.213.xxx.xxx -L 5901:localhost:5901 각자 OS 환경에 맞게 VNC 클라이언트를 설치하여 연결합니다.\n아래는 맥 OS 기준, 기본 VNC 클라이언트로 연결하는 예시입니다.\n브라우저에 접속 주소를 입력합니다. vnc://localhost:5901\n화면 공유 열기를 클릭합니다.\nvncpasswd에서 입력한 암호로 로그인합니다.\n접속에 성공하였습니다.\n데스크탑 기본 설치 구성을 합니다.\n화면 해상도를 변경합니다.\n구성완료\n참고\nOracle Linux 8: How To Setup VNC Server ","lastmod":"2023-04-04T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/5/4/","tags":["oraclelinux8","vnc"],"title":"3.5.4 Oracle Linux 8 on OCI에 VNC 서버 설정하기"},{"categories":null,"contents":"1.2.2.2.4 OCI Load Balancer 기반 NGINX Ingress Controller에서 클라이언트 IP 얻기 NGINX Ingress Controller를 사용할 때, 애플리케이션 Pod의 모니터링 등을 위해 애플리케이션 Pod에서 실 Client IP를 알아야 하거나, 로그에 실 Client IP가 출력될 필요가 있습니다. 하지만, 클라이언트의 요청은 NGINX Ingress Controller가 사용하는 OCI Load Balancer를 거쳐 오기 때문에, 실제 Client IP가 아닌, Load Balancer IP가 찍히게 됩니다. 여기서는 실제 Client IP를 얻기 위한 방법을 알아봅니다.\nOCI Load Balancer → Apache HTTP Server on Compute VM 구성에서 확인해 보기 테스트를 위해 일반 Compute VM에 설치된 HTTPD 기준으로 Access 로그에서 어떻게 보이는 지 먼저 확인해 봅니다.\nApache HTTP Server 설치\n5.1 Linux 인스턴스에 Apache HTTP Server 설치와 설치과정은 거의 같습니다.\nCompute VM을 생성합니다.\n생성한 Instance에 SSH 명령을 통해 접속\nApache HTTP Server 설치\n# Apache HTTP Server 설치 sudo yum -y install httpd # OS 방화벽에서 Apache HTTP용 포트, 80 포트 개방 sudo firewall-cmd --permanent --add-port=80/tcp # 방화벽 변경정보 다시 반영 sudo firewall-cmd --reload # Apache 시작 sudo systemctl start httpd 테스트를 위해 서버의 Root Index Document 생성\nsudo su echo \u0026#39;Hello Apache on WebServer #1\u0026#39; \u0026gt;/var/www/html/index.html 설치후 테스트\n[opc@webserver1 ~]$ curl http://127.0.0.1 Hello Apache on WebServer #1 Compute VM이 속한 Security List에서 80 포트를 오픈합니다.\nLoad Balancer 만들기\n10.5 Load Balancer 만들기의 설치과정과 거의 같습니다.\nLoad Balancer 유형으로 Load Balancer를 생성합니다. Backend Server로 설치한 Apache HTTP Server가 있는 Compute VM을 추가합니다. Listener는 HTTP로 설정합니다. 나머지는 기본값을 사용합니다. Load Balancer가 생성되는 Public IP를 확인합니다. Apache 포렌식 로그 설정\n생성한 Compute VM에 SSH 명령을 통해 접속\nHTTPD의 전체 Request Headers 확인하기 위해 포렌식 로그를 설정합니다.\n/etc/httpd/conf/httpd.conf의 마지막에 다음 추가\nLoadModule log_forensic_module /usr/lib64/httpd/modules/mod_log_forensic.so \u0026lt;IfModule log_forensic_module\u0026gt; ForensicLog /var/log/httpd/forensic.log \u0026lt;/IfModule\u0026gt; 재시작\nsudo systemctl restart httpd 테스트\n로그를 확인합니다.\nsudo tail -f /var/log/httpd/forensic.log Google에서 whatsmyip로 검색하여 작업 PC의 IP를 확인합니다.\n예, 220.117.xxx.x 웹 브라우저에서 Load Balancer Public IP로 접속합니다.\n예, http://144.24.xx.xxx/ forensic.log 로그를 확인합니다.\n+Y-jNMn8Yjl32cJJP5KF@vwAAAAI|GET / HTTP/1.1|Host:144.24.xx.xxx|X-Real-IP:220.117.xxx.x|X-Forwarded-For:220.117.xxx.x|X-Forwarded-Proto:http|X-Forwarded-Port:80|X-Forwarded-Host:144.24.xx.xxx%3a80|Upgrade-Insecure-Requests:1|User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36|Accept:text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9|Accept-Encoding:gzip, deflate|Accept-Language:en-US,en;q=0.9,ko-KR;q=0.8,ko;q=0.7|If-None-Match:\u0026#34;1d-5f572e756e170\u0026#34;|If-Modified-Since:Fri, 24 Feb 2023 14%3a25%3a39 GMT Host:144.24.xx.xxx - Load Balancer의 IP가 찍힙니다. X-Real-IP:220.117.xxx.x - 클라이언트 IP가 찍힙니다. X-Forwarded-For:220.117.xxx.x - 클라이언트 IP가 찍힙니다. Load Balancer를 거쳐오는 요청은 HTTP Request Header의 X-Forwarded-For 값으로 확인할 수 있는 것을 알 수 있습니다.\nOCI Load Balancer → NGINX ingress controller on OKE → 애플리케이션 Pod on OKE 구성에서 확인해 보기 기본 설치 상태에서 애플리케이션 로그 확인하기\nNGINX Ingress Controller 디폴트 상태로 설치 합니다.\n1.2.2.2.1.1 NGINX Ingress Controller 설치하기 테스트 앱을 설치합니다.\n1.2.2.2.1.2 NGINX Ingress Controller에서 PATH 기반 라우팅 인그레스 주소를 확인하고, 접속합니다.\n$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-basic \u0026lt;none\u0026gt; * 146.24.xxx.xxx 80 21m 테스트 앱을 브라우저로 접속합니다.\n예) http://152.69.234.225/blue\n애플리케이션 로그를 확인합니다.\n$ kubectl logs -lapp=nginx-blue -f 10.0.10.242 - - [10/Jan/2024:03:47:40 +0000] \u0026#34;GET /blue HTTP/1.1\u0026#34; 200 7271 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\u0026#34; \u0026#34;10.0.20.92\u0026#34; 클라이언트 주소는 10.0.10.242로 나옵니다. ingress controller의 Pod IP입니다.\n$ kubectl get pod -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ... ingress-nginx-controller-85b45cd7db-gcjh8 1/1 Running 0 35m 10.0.10.242 10.0.10.37 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; X-Forwarded-For는 10.0.20.92로 다른 IP가 보입니다. Load Balancer IP(152.69.xxx.xxx), Client IP(220.117.xxx.x)도 아닙니다.\n테스트에서 보듯이, 기본 Nginx Ingress Controller 설치상태에서는 애플리케이션 Pod에서 찍히는 Client IP는 실제 클라이언트의 IP가 아님을 알 수 있습니다.\nNGINX ingress controller 설정 확인하기\nNGINX ingress controller pod에 접속하여 nginx.conf 설정을 확인합니다.\nkubectl exec -it -n ingress-nginx ingress-nginx-controller-85b45cd7db-gcjh8 -- cat /etc/nginx/nginx.conf Load Balancer가 X-Forwarded-For로 넘겨준 값이 들어 있는 $http_x_forwarded_for를 X-Original-Forwarded-For에 넣어서 뒤로 넘겨주는 것을 알 수 있습니다. 애플리케이션 Pod에서 X-Original-Forwarded-For을 이용하면 되겠지만, 그로인해 애플리케이션 또는 모니터링 툴에서 변경이 필요할 것입니다.\nproxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $remote_addr; proxy_set_header X-Forwarded-Host $best_http_host; .. # Pass the original X-Forwarded-For proxy_set_header X-Original-Forwarded-For $http_x_forwarded_for; 클라이언트 IP를 얻기 위해 관련 설정하기\n배포된 NGINX Ingress Controller의 Service 유형 자원을 수정합니다.\nkubectl edit svc ingress-nginx-controller -n ingress-nginx Client IP를 보존하기 위해 다음 설정을 합니다.\nLoad Balancer 타입을 OCI Load Balancer로 설정합니다.\noci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; OCI Load Balancer의 Listener의 프로토콜은 설정합니다.\n80은 HTTP, 443은 HTTPS로 설정해야 하나, 현재는 이 annotation에서는 하나만 설정할 수 있습니다. 기능이 변경되면, 내용을 다시 업데이트 하겠습니다. 여기서는 테스트를 위해 HTTP로 설정합니다. service.beta.kubernetes.io/oci-load-balancer-backend-protocol: HTTP spec.externalTrafficPolicy을 Local로 지정합니다.\nexternalTrafficPolicy: Local 다운받은 NGINX Ingress Controller 설치 파일 변경 예시\n... --- apiVersion: v1 kind: Service metadata: ... name: ingress-nginx-controller namespace: ingress-nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; service.beta.kubernetes.io/oci-load-balancer-backend-protocol: HTTP spec: externalTrafficPolicy: Local ... nginx ingress controller 설정이 ConfigMap을 변경합니다.\nkubectl edit cm ingress-nginx-controller -n ingress-nginx enable-real-ip: \u0026quot;true\u0026quot; 설정\nproxy-real-ip-cidr:\u0026quot;\u0026lt;LB-SUBNET-CIDR-BLOCK\u0026gt; - 생성되는 Load Balancer가 속한 서브넷의 CIDR Block 입력\ningress-nginx-controller ConfigMap 설정 변경 예시\napiVersion: v1 data: allow-snippet-annotations: \u0026#34;true\u0026#34; enable-real-ip: \u0026#34;true\u0026#34; proxy-real-ip-cidr: \u0026#34;10.0.20.0/24\u0026#34; kind: ConfigMap ... ingress-nginx-controller를 재시작합니다.\nkubectl delete pod -lapp.kubernetes.io/name=ingress-nginx -n ingress-nginx 웹브라우저에서 관련 사이트를 통해, 자신의 Public IP를 확인합니다.\n테스트 앱을 브라우저로 다시 접속합니다.\n예) http://152.69.234.225/blue\n애플리케이션 로그를 확인합니다.\n예시에서는 클라이언트 주소가 10.0.10.251로 나옵니다. 재시작 후의 ingress controller의 새 Pod IP입니다.\n$ kubectl get pod -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES ingress-nginx-controller-85b45cd7db-pxfkx 1/1 Running 0 3m44s 10.0.10.251 10.0.10.37 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 예시에서는 X-Forwarded-For가 220.117.xxx.x로 앞서 확인한 나의 작업 PC의 Public IP가 나오는 것을 확인할 수 있습니다.\n$ kubectl logs -lapp=nginx-blue -f 10.0.10.251 - - [10/Jan/2024:04:11:41 +0000] \u0026#34;GET /blue HTTP/1.1\u0026#34; 200 7271 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\u0026#34; \u0026#34;220.117.xxx.x\u0026#34; 테스트 결과와 같이 OCI 문서에 따라 Client IP를 보존하기 위한 관련 설정을 하면, 애플리케이션 Pod에서 실제 Client IP을 확인할 수 있습니다.\n","lastmod":"2023-02-24T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/nginx-ingress/lb/4.nginx-ingress-preserve-client-ip-lb/","tags":["oss","ingress-controller","client-ip"],"title":"1.2.2.2.4 컨테이너에서 클라이언트 IP 얻기"},{"categories":null,"contents":"14.1.1.4 API Key 기반 인증 Config File 설정 유저가 OCI CLI의 Config File 설정을 위해서 크게 API singing key 기반 인증과 토큰 기반 인증이 있습니다. (OCI에 배포된 인스턴스와 자원을 위해서는 다른 추가적인 방법도 있습니다.) API singing key 기반 인증은 OCI 유저의 API Key에 사용할 Private Key, Public Key 키쌍을 생성하고 등록하여, 인증시 사용하는 방법입니다. OCI 유저 정보에 API Key가 등록된 상태에는 리프레쉬없이 지속해서 사용할 수 있습니다.\n필요한 정보\nTenancy OCID\nUser OCID\nAPI Key\nRegion\n필요한 정보 수집하기 OCI 콘솔에 로그인합니다.\nTenancy OCID\n오른쪽 위 사용자 Profile에서 Tenancy 클릭 후 Tenancy OCID 확인\nUser OCID\n오른쪽 위 사용자 Profile에서 My profile 클릭 후 사용자 OCID 확인\nOCI CLI Config 설정하기 OCI CLI로 다음 명령을 실행합니다.\noci setup config 파일 생성 위치: 디폴트 값 사용\nuser OCID: 수집한 값 입력\ntenancy OCID: 수집한 값 입력\nregion: 원하는 대상 리전 선택\nAPI Signing RSA key pair 생성: Y 입력\nAPI Signing RSA key pair 생성 위치 및 이름: 디폴트 값 사용\npassphrase for your private key: N/A 직접 타이핑\n실행 예시\n$ oci setup config This command provides a walkthrough of creating a valid CLI config file. The following links explain where to find the information required by this script: User API Signing Key, OCID and Tenancy OCID: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#Other Region: https://docs.cloud.oracle.com/Content/General/Concepts/regions.htm General config documentation: https://docs.cloud.oracle.com/Content/API/Concepts/sdkconfig.htm Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaa2wn4ujlahfmgz43ewnw5cwd6jprf5tqjvhosngi4klnbdva56nia Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaaclknekr6cdqy47khigggtgn64ylnrwylriccevpolcwfn3cjgajq Enter a region by index or name(e.g. 1: af-johannesburg-1, 2: ap-chiyoda-1, 3: ap-chuncheon-1, 4: ap-dcc-canberra-1, 5: ap-hyderabad-1, 6: ap-ibaraki-1, 7: ap-melbourne-1, 8: ap-mumbai-1, 9: ap-osaka-1, 10: ap-seoul-1, 11: ap-singapore-1, 12: ap-sydney-1, 13: ap-tokyo-1, 14: ca-montreal-1, 15: ca-toronto-1, 16: eu-amsterdam-1, 17: eu-dcc-dublin-1, 18: eu-dcc-dublin-2, 19: eu-dcc-milan-1, 20: eu-dcc-milan-2, 21: eu-dcc-rating-1, 22: eu-dcc-rating-2, 23: eu-frankfurt-1, 24: eu-jovanovac-1, 25: eu-madrid-1, 26: eu-marseille-1, 27: eu-milan-1, 28: eu-paris-1, 29: eu-stockholm-1, 30: eu-zurich-1, 31: il-jerusalem-1, 32: me-abudhabi-1, 33: me-dcc-muscat-1, 34: me-dubai-1, 35: me-jeddah-1, 36: mx-queretaro-1, 37: sa-santiago-1, 38: sa-saopaulo-1, 39: sa-vinhedo-1, 40: uk-cardiff-1, 41: uk-gov-cardiff-1, 42: uk-gov-london-1, 43: uk-london-1, 44: us-ashburn-1, 45: us-chicago-1, 46: us-gov-ashburn-1, 47: us-gov-chicago-1, 48: us-gov-phoenix-1, 49: us-langley-1, 50: us-luke-1, 51: us-phoenix-1, 52: us-sanjose-1): 3 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (\u0026#34;N/A\u0026#34; for no passphrase): Repeat for confirmation: Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: e0:c4:3f:c6:bc:d2:7a:b4:33:a4:c9:90:73:d3:cd:58 Config written to /home/opc/.oci/config If you haven\u0026#39;t already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2 $ 생성된 config 파일을 보면 아래와 같이 생성된 API Signing Key를 기반으로 하고 있습니다.\n$ cat .oci/config [DEFAULT] user=ocid1.user.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx fingerprint=e0:c4:3f:c6:bc:d2:7a:b4:33:a4:c9:90:73:d3:cd:58 key_file=/home/opc/.oci/oci_api_key.pem tenancy=ocid1.tenancy.oc1..aaaaaaaaxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx region=ap-chuncheon-1 .oci 폴더에 config 파일과 Private Key, Public Key가 생성되었습니다.\n$ ls -la ~/.oci/ total 12 drwx------. 2 opc opc 73 May 10 03:09 . drwx------. 7 opc opc 188 May 10 03:08 .. -rw-------. 1 opc opc 302 May 10 03:09 config -rw-------. 1 opc opc 1704 May 10 03:09 oci_api_key.pem -rw-------. 1 opc opc 451 May 10 03:09 oci_api_key_public.pem API Key 등록하기 오른쪽 위 사용자 Profile에서 My profile 클릭합니다.\nResources \u0026gt; API keys를 선택합니다.\nAdd API Key를 클릭합니다.\n생성된 Public Key(~/.oci/oci_api_key_public.pem)의 내용을 복사하여 등록합니다.\nOCI CLI를 위한 API Public Key 등록 완료\n다음 명령으로 테스트합니다.\noci os ns get 실행결과\n$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;xxxxxxxxxxxx\u0026#34; } ","lastmod":"2022-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/1/1/4/","tags":["API Public Key","API Key"],"title":"14.1.1.4 API Key 기반 인증 Config File 설정"},{"categories":null,"contents":"8.4 백업을 위한 스냅샷 실행한 시간 기준으로 File System의 Snapshot을 만들 수 있습니다. 만들어진 Snapshot을 통해 파일 또는 전체를 복구할 수 있습니다.\n스냅샷 #1 테스트한 VM에 접속합니다.\n마운트된 File System에 파일 하나를 복사합니다.\n[opc@examplelinuxinstance ~]$ cd /mnt/fss-system/ [opc@examplelinuxinstance fss-system]$ wget https://github.com/oracle/oci-cli/releases/download/v3.26.0/oci-cli-3.26.0-Oracle-Linux-8-Offline.zip ... 2023-05-08 09:34:54 (18.7 MB/s) - ‘oci-cli-3.26.0-Oracle-Linux-8-Offline.zip’ saved [67470000/67470000] [opc@examplelinuxinstance fss-system]$ ls -la total 67601 drwxrwxrwx. 2 root root 3 May 8 09:34 . drwxr-xr-x. 4 root root 36 May 8 09:10 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt -rw-rw-r--. 1 opc opc 67470000 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-8-Offline.zip drwxrwxrwx. 2 root root 0 May 8 09:35 .snapshot 웹브라우저에서 OCI Console로 이동하여, 앞서 만든 File System의 상세화면으로 이동합니다.\n왼쪽아래 Resources \u0026gt; Snapshots로 이동합니다.\nSnapshot을 만들기 위해 Create Snapshot 을 클릭합니다.\nSnapshot 생성화면\n생성확인\nCompute Instance에서 확인\n마운트된 경로 바로 밑 .snapshot 폴더에 생성된 Snapshot과 파일들이 보입니다.\n[opc@examplelinuxinstance fss-system]$ ls -la .snapshot/ total 2 drwxrwxrwx. 3 root root 1 May 8 09:38 . drwxrwxrwx. 2 root root 3 May 8 09:34 .. drwxrwxrwx. 2 root root 3 May 8 09:34 Snapshot-20230508-0936-28 [opc@examplelinuxinstance fss-system]$ ls -la .snapshot/Snapshot-20230508-0936-28/ total 67601 drwxrwxrwx. 2 root root 3 May 8 09:34 . drwxrwxrwx. 3 root root 1 May 8 09:38 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt -rw-rw-r--. 1 opc opc 67470000 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-8-Offline.zip 스냅샷 #2 접속한 Compute Instance에서 마운트된 File System에 파일을 하나 더 복사합니다.\n[opc@examplelinuxinstance fss-system]$ wget https://github.com/oracle/oci-cli/releases/download/v3.26.0/oci-cli-3.26.0-Oracle-Linux-7.9-Offline.zip ... 2023-05-08 09:39:56 (25.5 MB/s) - ‘oci-cli-3.26.0-Oracle-Linux-7.9-Offline.zip’ saved [66835442/66835442] [opc@examplelinuxinstance fss-system]$ ls -la total 133137 drwxrwxrwx. 2 root root 4 May 8 09:39 . drwxr-xr-x. 4 root root 36 May 8 09:10 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt -rw-rw-r--. 1 opc opc 66835442 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-7.9-Offline.zip -rw-rw-r--. 1 opc opc 67470000 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-8-Offline.zip drwxrwxrwx. 3 root root 1 May 8 09:39 .snapshot 웹브라우저에서 OCI Console로 이동하여, Snapshot을 하나 더 만듭니다.\n두 번째 Snapshot생성 확인\nCompute Instance에서 확인\n.snapshot 폴더에 생성된 Snapshot과 파일들이 보입니다.\n[opc@examplelinuxinstance fss-system]$ ls -la .snapshot/ total 2 drwxrwxrwx. 4 root root 2 May 8 09:42 . drwxrwxrwx. 2 root root 4 May 8 09:39 .. drwxrwxrwx. 2 root root 3 May 8 09:34 Snapshot-20230508-0936-28 drwxrwxrwx. 2 root root 4 May 8 09:39 Snapshot-20230508-0941-33 [opc@examplelinuxinstance fss-system]$ ls -ls .snapshot/Snapshot-20230508-0936-28/ total 67600 8 -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt 8 -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt 67584 -rw-rw-r--. 1 opc opc 67470000 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-8-Offline.zip [opc@examplelinuxinstance fss-system]$ ls -ls .snapshot/Snapshot-20230508-0941-33/ total 133136 8 -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt 8 -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt 65536 -rw-rw-r--. 1 opc opc 66835442 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-7.9-Offline.zip 67584 -rw-rw-r--. 1 opc opc 67470000 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-8-Offline.zip Snapshot은 읽기 전용 폴더입니다.\n[opc@examplelinuxinstance fss-system]$ cd .snapshot/Snapshot-20230508-0941-33/ [opc@examplelinuxinstance Snapshot-20230508-0941-33]$ ls -la total 133137 drwxrwxrwx. 2 root root 4 May 8 09:39 . drwxrwxrwx. 4 root root 2 May 8 09:44 .. -rw-rw-r--. 1 opc opc 19 May 8 09:10 hello1.txt -rw-rw-r--. 1 opc opc 19 May 8 09:31 hello2.txt -rw-rw-r--. 1 opc opc 66835442 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-7.9-Offline.zip -rw-rw-r--. 1 opc opc 67470000 May 4 14:08 oci-cli-3.26.0-Oracle-Linux-8-Offline.zip [opc@examplelinuxinstance Snapshot-20230508-0941-33]$ rm -rf oci-cli-3.26.0-Oracle-Linux-8-Offline.zip rm: cannot remove \u0026#39;oci-cli-3.26.0-Oracle-Linux-8-Offline.zip\u0026#39;: Read-only file system Snapshot에 있는 파일 또는 특정 Snapshot을 전체 복사하는 방법을 통해 복구할 수 있습니다.\n","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter08/4/","tags":["file storage","snapshot"],"title":"8.4 백업을 위한 스냅샷"},{"categories":null,"contents":"9.3 Custom Image Export/Import Custom Image는 Import, Export 기능을 제공합니다. Custom Image는 해당 Region 내 Comparment에 속하게 되며, 다른 Compartment로 복사하거나, 다른 Region 내로 복사하기 위해서는 Object Storage로 Export 받은 다음, 원하는 곳에서 다시 Import 받으면 됩니다.\nObject Storage에 Bucket 생성하기 및 사전인증 부분에 대한 설명은 7. Object Storage 사용하기를 참조하시기 바랍니다.\nCustom Image Export OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Custom Images 항목으로 이동합니다.\nExport를 원하는 이미지를 클릭합니다.\n대상 이미지 우측의 액션 메뉴의 Export Image를 클릭합니다. Export는 두 가지 방식으로 할 수 있으며 형식은 OCI 포맷으로 Export 됩니다.\nObject Storage Bucket: 동일 Tenancy내에 Object Storage Bucket 으로 Export합니다. 대상 Compartment와 Bucket을 선택합니다. Object Storage URL: URL을 통해 Export합니다. 다른 Tenancy로 Export하는 것이 가능하게 됩니다. Export 정보를 입력하고 아래쪽 Export Image를 클릭합니다.\nBucket name: 대상 Object Storage Bucket 이름 선택, 없는 경우 사전에 만듭니다.\nImage name: 예) ExampleLinuxCustomImage\nImage format: .oci 포맷 선택\nCustom Image의 상세화면으로 이동하여 아래쪽 Work requests를 보면 Export 시작, 종료 시간을 확인할 수 있습니다. 8분 정도 걸린 것을 알 수 있습니다.\n대상 Object Storage Bucket에 가면 Export 된 이미지를 확인할 수 있습니다.\n그림에서 보듯 2.93 GiB에 8분 정도 걸린 걸 알 수 있습니다.\n경우 #1 - 동일 Tenancy, 동일 Region Custom Image Import OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Custom Images 항목으로 이동합니다. 오른쪽 위에서 임포트할 Region으로 변경하고 Import Image 클릭 동일 Tenancy, 동일 Region내 Object Storage내 Bucket에 있으므로, 추가 설정없이 해당 이미지를 선택 하여 임포트합니다. 경우 #2 - 다른 Region 또는 다른 Tenancy 에서 Import 하기 다른 Region 또는 다른 Tenancy의 Object Storage에 있는 Custom 이미지를 임포트하기 위해서는 Object Storage URL을 통해 임포트해야 합니다. 대상 Object가 별도 인증없이 접근할 수 있는 사전인증(Pre-Authenticated Request)이 설정되어 있어야 합니다. Public Bucket은 작성일자 기준으로 지원하지 않습니다.\nCustome Image Object의 우측 액션 메뉴에서 Pre-Authenticated Request 생성 명령을 통해 Custom Image Object에 대한 읽기 권한으로 임시 사전 인증 링크를 생성합니다.\n아래 생성된 링크를 복사합니다.\n임포트해서 사용할 다른 Region으로 이동하거나, 다른 Tenancy로 이동합니다.\nCompute \u0026gt; Custom Images 항목으로 이동합니다.\n오른쪽 위에서 임포트할 Region으로 변경하고 Import Image 클릭\n임포트 입력정보\nName: 이미지 이름 입력\nOperating system: Linux 선택\nObject Storage URL: 앞서 복사한 이미지의 Object Storage URL 복사\nImage type: OCI 선택\n정보 입력후 아래쪽의 Import image 클릭\n임포트 중인 Custom Image의 상세화면으로 이동하여 아래쪽 Work requests를 보면 진행상태를 알수 있고, Import가 완료되면 시작, 종료 시간을 확인할 수 있습니다. 예시에서 춘천 리전에 이미지를 서울 리전으로 임포트하는 데 9분 정도 걸린 것을 알 수 있습니다. 실제로는 이미지 사이즈와 당시 네트워크 상황에 따라 차이가 있을 수 있습니다.\n임포트 완료되었습니다.\n","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter09/3/","tags":["os image","custom image","image"],"title":"9.3 Custom Image Export/Import"},{"categories":null,"contents":"7.4 Object 권한 관리 - Public Bucket 생성된 Bucket은 기본적으로 Private 상태입니다. 인증없이는 접근할 수 없는 상태입니다. Bucket을 Public으로 변경하게 되면 별도 인증 없이 접근할 수 있습니다. Pre-Authenticated Requests 처럼 임시 공유 링크가 생기는 것이 아니라, Bucket에 대한 접근이 허용되어 Object의 URL 경로로 바로 접근할 수 있습니다.\nBucket의 상세 정보 화면으로 이동합니다. Visibility가 Private 임을 확인 할 수 있습니다. 변경을 위해 Edit Visibility를 클릭합니다.\nVisibility을 아래와 같이 변경후 저장합니다.\nVisibility: Public으로 변경합니다.\nAllow users to list objects from bucket: 기본적으로 선택되어 있습니다. 선택하면, API로 Object 목록 조회를 허용하게 됩니다.\nObject의 URL 경로를 확인하기 위해 Object의 우측 액션메뉴를 통해 상세정보로 이동합니다. Object 상세 정보에서 URL 경로를 확인합니다. URL 경로 포맷은 다음과 같습니다.\nhttps://objectstorage.\u0026lt;region_name\u0026gt;.oraclecloud.com/n/\u0026lt;object_storage_namespace\u0026gt;/b/\u0026lt;bucket\u0026gt;/o/\u0026lt;object_name\u0026gt; 확인된 Object URL을 브라우저로 접속합니다. 인증없이 바로 접속되는 것을 확인할 수 있습니다. Object 상위 경로로 접근하면 아래와 같이 JSON 형식으로 Object의 목록이 조회됩니다.\nhttps://objectstorage.\u0026lt;region_name\u0026gt;.oraclecloud.com/n/\u0026lt;object_storage_namespace\u0026gt;/b/\u0026lt;bucket\u0026gt;/o/ ","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/4/","tags":["object storage","bucket","public bucket"],"title":"7.4 Object 권한 관리 - Public Bucket"},{"categories":null,"contents":"3.7.4 정책 기반 Block Volume 백업하기 Volume Backup Policies Bronze Policy\n월단위 - 매월 첫날에 월간 증분 백업이 실행됩니다. 12개월 동안 유지됩니다. 매단위 - 매년 1월 1일에 전체 백업이 실행됩니다. 5년간 유지됩니다. Silver Policy\n주단위 - 매주 일요일에 주 단위 증분 백업이 실행됩니다. 4주 동안 보존됩니다. 월단위 - 매월 첫날에 월간 증분 백업이 실행됩니다. 12개월 동안 유지됩니다. 매단위 - 매년 1월 1일에 전체 백업이 실행됩니다. 5년간 유지됩니다. Gold Policy\n일단위 - 일일 증분 백업이 실행됩니다. 7일 동안 보존됩니다. 주단위 - 매주 일요일에 주 단위 증분 백업이 실행됩니다. 4주 동안 보존됩니다. 월단위 - 매월 첫날에 월간 증분 백업이 실행됩니다. 12개월 동안 유지됩니다. 매단위 - 매년 1월 1일에 전체 백업이 실행됩니다. 5년간 유지됩니다. 정책 기반 Block Volume 백업하기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Block Storage \u0026gt; Block Volumes Backups 항목으로 이동합니다.\n설정하려는 Block Volume의 우측 메뉴를 클릭하고, Edit를 클릭합니다.\n백업 정책을 고르고 아래쪽 Save Changes를 클릭합니다.\nBlock Volume 리스트에서도 확인할 수 있으며, 추가 정보로 백업일정을 확인할 수 있습니다.\n백업 정책에 따라 그림과 같이 스케줄에 따라 백업됩니다.\nBlock Volume 백업 정책 삭제/변경하기 설정시와 동일하게 Block Volume의 우측 액션 메뉴에서 Edit를 클릭하여, 정책을 변경, 삭제하면 됩니다. ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/7/4/","tags":["block volume","backup","copy"],"title":"3.7.4 정책 기반 Block Volume 백업하기"},{"categories":null,"contents":"6.4 Policy 만들기 Step 1. 생성된 그룹을 위한 Policy 추가 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Policies 항목으로 이동합니다.\n왼쪽 아래에 대상 Compartment를 Root Compartment로 지정합니다.\nCreate Policy 클릭\nPolicy 정보 입력\nName: \u0026ldquo;SandboxPolicy\u0026rdquo; 입력\nDescription: 설명 입력, 예) 이름과 동일하게 \u0026ldquo;SandboxPolicy\u0026rdquo;\nPolicy Statements:\n\u0026ldquo;Show manual editor\u0026quot;를 선택하여 직접입력합니다. SandboxGroup내 사용자는 Sandbox Compartment에 모든 권한을 부여하도록 다음과 같이 설정합니다. Allow group SandboxGroup to manage all-resources in compartment Sandbox Step 2. 적용된 Policy 확인을 위한 사용자 다시 로그인 기존 사용자에서 로그아웃합니다.\nPolicy가 적용되는 SandboxGroup 그룹에 속하는, 앞서 생성한 새 사용자(예시, sandboxer)로 다시 로그인합니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nRoot Compartment 내의 자원은 권한이 없어서 여전히 보이지 않습니다. Compartment 선택 메뉴에 이전과 달리 Sandbox Compartment가 보이는 것을 알 수 있습니다. 권한이 없는 Production Compartment는 여전히 보이지 않습니다.\nSandbox Compartment를 선택하면 Sandbox 안에 있는 인스턴스가 보입니다. 물론 Sandbox 관리 권한이 있기 때문에 Sandbox 내 다른 모든 작업도 수행할 수 있습니다. ","lastmod":"2022-01-11T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/4/","tags":["policy"],"title":"6.4 Policy 만들기"},{"categories":null,"contents":"3.4 Linux 인스턴스 생성하기 팁 인스턴스를 생성하기 전에 자원이 충분한지 사전에 확인하는 것이 좋습니다. Service Limit 조회를 통해 가용한 자원이 있는지, 특히 가용한 CPU가 있는 지를 인스턴스 생성 전에 확인합니다. OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nInstance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\nCreate Compute Instance 화면이 뜹니다.\n기본 정보\nName: 이름 입력, 예를 들어 \u0026ldquo;ExampleLinuxInstance\u0026rdquo; Create in compartment: 위치할 Compartment를 선택, 앞서 생성한 oci-hol-xx 선택 Placement\n서울, 춘천 리전은 AD가 하나 이기 때문에 Placement를 기본값을 그대로 사용하면 됩니다. Show advanced options을 클릭하면 아래와 같이 상세 옵션을 볼 수 있으며, AD내 Fault Domain을 직접 지정할 수도 있습니다.\n여기서는 기본값인 On-demand capacity을 사용합니다.\nOn-demand capacity: 사용할 때만 컴퓨트 자원에 대한 비용을 지불하는 방식, 일반적인 경우에 사용합니다.\nPreemptible capacity: 선점형 인스턴스는 일반 인스턴스 가격의 50% 가격으로 책정됩니다. 갑자기 자원을 회수당할 수 있습니다. 잠시 사용할 때 유용합니다.\nCapacity reservation: 추후 사용을 위한 용량을 예약하고 사용하는 방식으로, 추후 생성시 필요한 자원에 대해 보장받습니다. 예약된 용량 중에 미사용 용량은 일반 인스턴스 가격의 85% 가격으로 책정됩니다.\nDedicated host: 전용 가상 머신 호스트를 생성하고 해당 호스트에 가상 머신을 생성하는 방식으로, 전용 서버를 다른 고객과 자원을 공유하지 않고 사용합니다. 격리된 자원 요구시 사용할 수 있습니다. 전용 가상 머신 호스트를 생성하면 그에 대한 비용이 발생합니다.\n노트 - OCI는 모든 리전에서 동일한 성능과 가격을 제공하는 일관된 글로벌 가격을 제공합니다.\nImage \u0026amp; Shape - 사용할 이미지와 Shape(CPU, Memory 크기)을 선택합니다.\nChange Image를 선택하면 제공하는 다양한 이미지를 확인할 수 있습니다. 리눅스 및 윈도우를 지원합니다. 리눅스를 사용할 경우 기술지원이 포함된 Oracle Linux를 권장합니다. OS에 따라 버전과 빌드이미지를 날짜로 선택할 수 있습니다.\nChange Shape을 선택하면, 인스턴스 유형(VM, 베어메탈)을 고를 수 있고, AMD, Intel, Arm CPU를 제공합니다. 기본 AMD를 선택합니다.\nFlex Shape을 제공하여, 비용 최적화된 Shape을 사용할 수 있습니다. 고정된 CPU, Memory 중에 고르는 것이 아니라, 원하는 CPU, 원하는 메모리 크기를 직접 고를 수 있습니다. Specialty 항목과 Bare metal machine에서 특별한 요구에 맞는 추가적인 Shape을 제공하고 있습니다. Networking - 앞선 실습에서 만든 VCN내에 Public Subnet을 선택합니다. 생성될 인스턴스에 대한 접속을 위해 Assign a public IPv4 address을 선택하여 Public IP를 할당합니다.\nAdd SSH Keys - SSH 접속을 위한 키를 등록하는 부분입니다.\nVM에 SSH 접속시 사용할 SSH Key의 Public Key를 입력합니다. 이전 장에서 ssh-keygen 또는 PuTTY Key Generator로 만든 Public Key를 붙여 넣기 합니다.\nGenerate a key pair for me: SSH Key를 미리 생성하지 않은 경우 자동으로 생성된 Key를 사용할수도 있습니다. 이 옵션을 사용하는 경우 Private Key, Public Key를 다운받아 보관합니다.\nBoot volume: 사이즈 변경 등이 가능하며, 지금은 별도 설정없이 기본값 그대로 사용합니다.\nAdvanced Options\n고급 옵션에서 아래 설정을 직접 선택할 수 있습니다. 지금은 별도 설정없이 기본값 그대로 사용합니다.\nManagement: cloud-init을 통해 VM 생성시 커스텀 스크립트 추가 가능\nAvailable configuration: Live migration 기본 옵션을 변경 가능\nOracle Cloud Agent: 기본 설치되는 Cloud Agent 변경 가능\n생성 정보 입력후 Create 클릭\n생성 완료\n인스턴스가 PROVISIONING 상태에서 몇 분이 지나고 생성이 완료되면 RUNNING 상태가 됩니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/4/","tags":["linux","compute","vm"],"title":"3.4 Linux 인스턴스 생성하기"},{"categories":null,"contents":"3.6.4 Block Volume 포맷하기 Compute Instance에 SSH로 접속한 후 Block Volume을 포맷합니다. 파일 시스템 타입은 원하는 다른 것으로 변경하여 포맷하셔도 됩니다.\nsudo mkfs -t \u0026lt;fs type\u0026gt; /dev/sdb 실행예시\n아래와 같이 Block Volume을 조회한후 포맷하였습니다. 기존 sda3과 동일한 xfs 파일 시스템으로 포맷하였습니다. [opc@examplelinuxinstance ~]$ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT sda ├─sda1 vfat 9814-8B9E /boot/efi ├─sda2 xfs f7cf63bd-4975-44bb-ba99-b81e037be7e1 /boot └─sda3 LVM2_member yPCycB-EF5X-YrnA-EeXg-5GGp-rlWn-W8ggkL ├─ocivolume-root xfs 8b8d3b19-9faa-4dee-b3ba-765c3515b283 / └─ocivolume-oled xfs 21d53c77-7876-4a1c-b4e1-a8cd74d1938c /var/oled sdb [opc@examplelinuxinstance ~]$ sudo mkfs -t xfs /dev/sdb meta-data=/dev/sdb isize=512 agcount=4, agsize=3276800 blks = sectsz=4096 attr=2, projid32bit=1 = crc=1 finobt=1, sparse=1, rmapbt=0 = reflink=1 bigtime=0 inobtcount=0 data = bsize=4096 blocks=13107200, imaxpct=25 = sunit=0 swidth=0 blks naming =version 2 bsize=4096 ascii-ci=0, ftype=1 log =internal log bsize=4096 blocks=25600, version=2 = sectsz=4096 sunit=1 blks, lazy-count=1 realtime =none extsz=4096 blocks=0, rtextents=0 Discarding blocks...Done. [opc@examplelinuxinstance ~]$ lsblk -f NAME FSTYPE LABEL UUID MOUNTPOINT sda ├─sda1 vfat 9814-8B9E /boot/efi ├─sda2 xfs f7cf63bd-4975-44bb-ba99-b81e037be7e1 /boot └─sda3 LVM2_member yPCycB-EF5X-YrnA-EeXg-5GGp-rlWn-W8ggkL ├─ocivolume-root xfs 8b8d3b19-9faa-4dee-b3ba-765c3515b283 / └─ocivolume-oled xfs 21d53c77-7876-4a1c-b4e1-a8cd74d1938c /var/oled sdb xfs 873e358d-4efe-4fbf-8cc8-bff2d615414b ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/4/","tags":["block volume"],"title":"3.6.4 Block Volume 포맷하기"},{"categories":null,"contents":"2.4 최초 Cloud Account(Tenancy) 구조 Cloud Account 환경이 생성되면 OCI Native IAM에 의해서 사용자가 관리됩니다. OCI Classic의 유물로 기본적으로 Oracle Identity Cloud Service(IDCS)를 Identity Provider로 사용하게 Federation 되어 있습니다. OCI의 Administrator는 OCI Tenancy의 모든 권한을 가집니다. IDCS의 OCI_Administrator 그룹은 OCI의 Administrator 그룹에 매핑되어 있습니다. OCI Console에 SSO로 로그인하면 실제는 OCI_Administrator에 속한 사용자이지만, OCI 상에서는 Administrator 그룹으로 처리되어 관련 권한을 가지게 됩니다. IDCS 상에 유저를 생성하면, 예) neo@example.com 유저 생성되면 자동으로 동기화되어 OCI에 oracleidentitycloudservice/neo@example.com 란 유저가 생성됩니다. OCI 자원에 대한 구획인 Compartment는 최초 Root Compartment만 있으며 필요시 추가 하면 됩니다. ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter02/4/","tags":["oci account","tenancy"],"title":"2.4 최초 Cloud Account(Tenancy) 구조"},{"categories":null,"contents":"14.2.4 Postman - OCI REST API 호출하기 가장 많이 사용하는 전문 REST 클라이언트 툴인 Postman을 사용하면 GUI를 통해 사용하기가 더 편리합니다. 아직 OCI Request Signature를 기본 지원하지 않아, Postman의 pre-script를 사용하여 OCI REST API를 호출하는 방법을 설명하고자 합니다.\nOCI Request Signature를 통한 호출을 하기 위해 Postman에서는 pre-script를 사용해야 합니다. Insomina처럼 플러그인 기능은 제공하지 않습니다. pre-script는 REST 요청단위로 설정할 수 있고, 폴더, Collection 단위로도 설정할 수 있습니다.\n폴더에 pre-script를 설정하면, 폴더하위의 모든 REST 요청전에 pre-script가 먼저 실행됩니다. 관리 편의를 위해 폴더의 pre-script를 사용하겠습니다.\nStep 0. Postman 다운로드 공식 사이트에서 앱을 다운로드 받습니다.\nhttps://www.postman.com/downloads/ Step 1. pre-script에서 쓸 RSA 라이브러리 로딩 ※ 외부 RSA 자바스크립트 라이브러리 로딩 pre-script에서 기본 포함된 Cryto 라이브러리는 OCI Request Signature가 사용하는 RSA 암호화를 지원하지 않습니다. 그래서 다음방식으로 우회하여 외부 암호화 라이브러리를 추가합니다.\nGET 요청 만들기\n대상 URL: http://kjur.github.io/jsrsasign/jsrsasign-latest-all-min.js Test 탭에 다음 복사\npm.globals.set(\u0026#34;jsrsasign-js\u0026#34;, responseBody); 요청 실행\n실행결과\n아래와 같이 실행되면 암호화모듈이 Postman 글로벌 변수(jsrsasign-js)에 저장됩니다.\nStep 2. pre-script 설정 REST 요청을 포함할 폴더를 생성합니다.\n폴더 이름을 우클릭 하고 Edit 클릭\n폴더 설정에서 Pre-request Scripts 탭을 선택합니다.\n다음 자바스크립트을 복사해서 붙여넣습니다.\nvar navigator = {}; //fake a navigator object for the lib var window = {}; //fake a window object for the lib eval(pm.globals.get(\u0026#34;jsrsasign-js\u0026#34;)); //import javascript jsrsasign const isOracleCloud = pm.environment.get(\u0026#34;isOracleCloud\u0026#34;); if (isOracleCloud != \u0026#34;true\u0026#34;) return; const tenancyId = pm.environment.get(\u0026#39;tenancyId\u0026#39;); const authUserId = pm.environment.get(\u0026#39;authUserId\u0026#39;); const keyFingerprint = pm.environment.get(\u0026#39;keyFingerprint\u0026#39;); var privateKey = pm.environment.get(\u0026#34;privateKey\u0026#34;); var signAlgorithm = \u0026#34;RSA-SHA256\u0026#34;; var sigVersion = \u0026#34;1\u0026#34;; var now = new Date().toUTCString(); var host = getHost(request.url.trim()); var target = getTarget(request.url.trim()); var method = request.method; var keyId = tenancyId + \u0026#34;/\u0026#34; + authUserId + \u0026#34;/\u0026#34; + keyFingerprint; var headers = \u0026#34;(request-target) date host\u0026#34;; var request_target=\u0026#34;(request-target): \u0026#34; + method.toLowerCase() + \u0026#34; \u0026#34; + target; var date_header = \u0026#34;date: \u0026#34; + now; var host_header = \u0026#34;host: \u0026#34; + host; var signing_string = request_target + \u0026#34;\\n\u0026#34; + date_header + \u0026#34;\\n\u0026#34; + host_header; var methodsThatRequireExtraHeaders = [\u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;]; if(methodsThatRequireExtraHeaders.indexOf(method.toUpperCase()) !== -1) { var body = request.data; console.log(body); var signatureSign = CryptoJS.SHA256(body); var content_sha256 = signatureSign.toString(CryptoJS.enc.Base64); var content_type = \u0026#34;application/json\u0026#34;; var content_length = body.length; headers = headers + \u0026#34; x-content-sha256 content-type content-length\u0026#34;; var content_sha256_header = \u0026#34;x-content-sha256: \u0026#34; + content_sha256; var content_type_header = \u0026#34;content-type: \u0026#34; + content_type; var content_length_header = \u0026#34;content-length: \u0026#34; + content_length; signing_string = signing_string + \u0026#34;\\n\u0026#34; + content_sha256_header + \u0026#34;\\n\u0026#34; + content_type_header + \u0026#34;\\n\u0026#34; + content_length_header; pm.environment.set(\u0026#34;x-content-sha256_header\u0026#34;, content_sha256); } // RSA signature generation var signatureSign = new KJUR.crypto.Signature({\u0026#34;alg\u0026#34;: \u0026#34;SHA256withRSA\u0026#34;}); signatureSign.init(privateKey); signatureSign.updateString(signing_string); var signedSignatureHex = signatureSign.sign(); var signedSignature = hexToBase64(signedSignatureHex); const authorization = `Signature version=\u0026#34;${sigVersion}\u0026#34;, keyId=\u0026#34;${keyId}\u0026#34;, algorithm=\u0026#34;${signAlgorithm.toLowerCase()}\u0026#34;, headers=\u0026#34;${headers}\u0026#34;, signature=\u0026#34;${signedSignature}\u0026#34;`; pm.environment.set(\u0026#34;date_header\u0026#34;, now); pm.environment.set(\u0026#34;Authorization_header\u0026#34;, authorization); function getHost(url) { // https://identity.us-ashburn-1.oraclecloud.com/20160918/users/ var n1 = url.indexOf(\u0026#34;//\u0026#34;); var n2 = url.indexOf(\u0026#34;/\u0026#34;, n1 + 2); var start = n1 + 2; var length = n2 - start; var host = url.substr(start, length); return host; } function getTarget(url) { // https://identity.us-ashburn-1.oraclecloud.com/20160918/users/ url = url.replace(new RegExp(\u0026#39;^https?://[^/]+/\u0026#39;),\u0026#39;/\u0026#39;); // strip hostname return url; } function hexToBase64(hexstring) { return btoa(hexstring.match(/\\w{2}/g).map(function(a) { return String.fromCharCode(parseInt(a, 16)); }).join(\u0026#34;\u0026#34;)); } Step 3. 환경변수 설정 필요 환경 변수 isOracleCloud: \u0026ldquo;true\u0026quot;로 설정, 설정한 플러그인인 글로벌하게 적용되는 것이라, true일때만 세부 스크립트가 동작함 tenancyId: tenancy OCID authUserId: 사용자 OCID keyFingerprint: API Key의 핑거프린트 privateKey: API Key로 등록한 Public Key에 매칭되는 Private Key의 텍스트를 그대로 복사해 붙입니다. Step 4. 사용자 조회 REST API 실행 REST 요청을 앞서 만든 폴더 밑으로 만듭니다.\n오른쪽 위에 설정한 환경변수를 선택합니다.\nREST 요청의 Header에 그림과 같이 date, Authorization을 추가합니다.\ndate : {{date_header}} Authorization : {{Authorization_header}} date, Authorization의 실제 값은 pre-script가 실행되면서 실행시점에 설정됩니다. POST, PUT을 제외한 HTTP Operation은 위 두 개만 설정합니다. 실행결과 Step 5. 사용자 생성 REST API 실행 REST 요청을 앞서 만든 폴더 밑으로 만듭니다.\n오른쪽 위에 설정한 환경변수를 선택합니다.\nREST 요청의 Header에 그림과 같이 date, Authorization을 추가합니다.\ndate : {{date_header}} Authorization : {{Authorization_header}} Content-Type : application/json x-content-sha256 : {{x-content-sha256_header}} Content-Type을 제외한 세 개의 실제 값은 pre-script가 실행되면서 실행시점에 설정됩니다. POST, PUT은 위 네 개를 설정합니다. 요청 메시지가 있어 요청메시지도 서명에 추가되기 때문입니다. 실행결과 POST, PUT으로 요청하는 경우 위 REST 요청을 복사해서 씁니다.\n참고 문서 Postman의 pre-script에서 외부 RSA 자바스크립트 라이브러리를 사용하는 부분은 다음 링크를 참고하였습니다.\nhttps://github.com/postmanlabs/postman-app-support/issues/1607#issuecomment-401611119 ","lastmod":"2019-05-19T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/2/4/","tags":["rest api","postman"],"title":"14.2.4 Postman - OCI REST API 호출하기"},{"categories":null,"contents":"14.3.4 Terraform Configuration 실행하기 명령창실행\nConfiguration 파일이 있는 경로로 이동\n[opc@bastion-host example_vcn]$ ls -la total 12 drwxrwxr-x. 2 opc opc 63 Jan 18 09:50 . drwxrwxr-x. 3 opc opc 25 Jan 18 09:46 .. -rw-rw-r--. 1 opc opc 359 Jan 18 09:47 provider.tf -rw-rw-r--. 1 opc opc 479 Jan 18 09:50 terraform.tfvars -rw-rw-r--. 1 opc opc 284 Jan 18 09:47 vcn.tf 초기화 실행\n초기화를 수행하면 provider.tf에서 사용하는 OCI Provider를 자동으로 .terraform 폴더 밑으로 다운받습니다.\nterraform init 실행예시\n[opc@bastion-host example_vcn]$ terraform init Initializing the backend... Initializing provider plugins... - Finding latest version of hashicorp/oci... - Installing hashicorp/oci v4.59.0... - Installed hashicorp/oci v4.59.0 (signed by HashiCorp) ... [opc@bastion-host example_vcn]$ ls -la total 20 drwxrwxr-x. 3 opc opc 4096 Jan 18 09:51 . drwxrwxr-x. 3 opc opc 25 Jan 18 09:46 .. drwxr-xr-x. 3 opc opc 23 Jan 18 09:51 .terraform -rw-r--r--. 1 opc opc 1002 Jan 18 09:51 .terraform.lock.hcl -rw-rw-r--. 1 opc opc 359 Jan 18 09:47 provider.tf -rw-rw-r--. 1 opc opc 479 Jan 18 09:50 terraform.tfvars -rw-rw-r--. 1 opc opc 284 Jan 18 09:47 vcn.tf [opc@bastion-host example_vcn]$ ls -la .terraform total 4 drwxr-xr-x. 3 opc opc 23 Jan 18 09:51 . drwxrwxr-x. 3 opc opc 4096 Jan 18 09:51 .. drwxr-xr-x. 3 opc opc 35 Jan 18 09:51 providers 실행 플랜 확인\n플랜 명령을 수행하면, 실제 OCI에 수행되는 계획을 확인할 수 있습니다.\nterraform plan 실행예시\n[opc@bastion-host example_vcn]$ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Note: You didn\u0026#39;t use the -out option to save this plan, so Terraform can\u0026#39;t guarantee to take exactly these actions if you run \u0026#34;terraform apply\u0026#34; now. 실행\n실행하면 실제 OCI에 Terraform 계획이 적용되어 관련된 OCI 자원이 생성됩니다. 또한 실행후 terraform output 명령을 통해 outout 변수를 조회할 수 있습니다.\nterraform apply 실행예시\n[opc@bastion-host example_vcn]$ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes oci_core_virtual_network.vcn1: Creating... oci_core_virtual_network.vcn1: Creation complete after 1s [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, ] 실행결과 확인\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking » Virtual Cloud Networks 항목으로 이동합니다.\nTerraform Configuration시 지정한 Comparment를 선택합니다.\n그림과 같이 생성된 VCN을 확인 할 수 있습니다.\nState 관리\n다시 terraform apply를 실행하면 아래와 앞서 이미 실행되어 자원이 생성되었기 때문에, 변경 적용할 것이 없다고 표시됩니다.\n[opc@bastion-host example_vcn]$ terraform apply oci_core_virtual_network.vcn1: Refreshing state... [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, ] Terraform State 관리 파일\nterraform 설정을 실행하면, 실행후 아래처럼 terraform.tfstate 파일이 생성되고 생성된 자원에 대한 id와 관련 속성 등을 기록하여 자원 상태를 관리하게 됩니다. 그래서 다시 terraform apply를 통해 실행하더라도, tfstate에서 관리되는 id의 자원의 현재 OCI 자원의 현재 상태를 확인(Refresh)한 상태와 적용할 Plan을 비교하여 변경할 사항이 있는 지 확인후 반영됩니다.\n[opc@bastion-host example_vcn]$ ls -la total 24 drwxrwxr-x. 3 opc opc 4096 Jan 18 10:06 . drwxrwxr-x. 3 opc opc 25 Jan 18 09:46 .. drwxr-xr-x. 3 opc opc 23 Jan 18 09:51 .terraform -rw-r--r--. 1 opc opc 1002 Jan 18 09:51 .terraform.lock.hcl -rw-rw-r--. 1 opc opc 359 Jan 18 09:47 provider.tf -rw-rw-r--. 1 opc opc 2152 Jan 18 10:06 terraform.tfstate -rw-rw-r--. 1 opc opc 479 Jan 18 09:50 terraform.tfvars -rw-rw-r--. 1 opc opc 284 Jan 18 09:47 vcn.tf [opc@bastion-host example_vcn]$ cat terraform.tfstate { \u0026#34;version\u0026#34;: 4, \u0026#34;terraform_version\u0026#34;: \u0026#34;1.1.3\u0026#34;, \u0026#34;serial\u0026#34;: 2, \u0026#34;lineage\u0026#34;: \u0026#34;b9cd2a13-5ef9-f1e1-ffc9-ea5b2964955d\u0026#34;, \u0026#34;outputs\u0026#34;: { \u0026#34;vcn1_ocid\u0026#34;: { \u0026#34;value\u0026#34;: [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34; ], \u0026#34;type\u0026#34;: [ \u0026#34;tuple\u0026#34;, [ \u0026#34;string\u0026#34; ] ] } }, \u0026#34;resources\u0026#34;: [ { \u0026#34;mode\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;oci_core_virtual_network\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vcn1\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/oci\\\u0026#34;]\u0026#34;, \u0026#34;instances\u0026#34;: [ { \u0026#34;schema_version\u0026#34;: 0, \u0026#34;attributes\u0026#34;: { \u0026#34;cidr_block\u0026#34;: \u0026#34;10.0.0.0/16\u0026#34;, \u0026#34;cidr_blocks\u0026#34;: [ \u0026#34;10.0.0.0/16\u0026#34; ], \u0026#34;compartment_id\u0026#34;: \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34;, \u0026#34;default_dhcp_options_id\u0026#34;: \u0026#34;ocid1.dhcpoptions.oc1.ap-seoul-1.aaaaaaaaun4ti33kubad7cqexhe4npgwy773lldor7xxfbbu2lu5yz3q6uaa\u0026#34;, \u0026#34;default_route_table_id\u0026#34;: \u0026#34;ocid1.routetable.oc1.ap-seoul-1.aaaaaaaammsdhhl7czlkrxeji7eqot6tkiyfcvrhuqvf4hh3cra3rt3m2e2a\u0026#34;, \u0026#34;default_security_list_id\u0026#34;: \u0026#34;ocid1.securitylist.oc1.ap-seoul-1.aaaaaaaadmiqjfjkiw7k73lxwbilgci2xrzehxud3ezfujmt3gtjenocsoaa\u0026#34;, \u0026#34;defined_tags\u0026#34;: { \u0026#34;Oracle-Tags.CreatedBy\u0026#34;: \u0026#34;sandboxer\u0026#34;, \u0026#34;Oracle-Tags.CreatedOn\u0026#34;: \u0026#34;2022-01-18T10:00:51.322Z\u0026#34; }, \u0026#34;display_name\u0026#34;: \u0026#34;vcn1\u0026#34;, \u0026#34;dns_label\u0026#34;: \u0026#34;vcn1\u0026#34;, \u0026#34;freeform_tags\u0026#34;: {}, \u0026#34;id\u0026#34;: \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, \u0026#34;ipv6cidr_blocks\u0026#34;: [], \u0026#34;is_ipv6enabled\u0026#34;: false, \u0026#34;state\u0026#34;: \u0026#34;AVAILABLE\u0026#34;, \u0026#34;time_created\u0026#34;: \u0026#34;2022-01-18 10:00:51.389 +0000 UTC\u0026#34;, \u0026#34;timeouts\u0026#34;: null, \u0026#34;vcn_domain_name\u0026#34;: \u0026#34;vcn1.oraclevcn.com\u0026#34; }, \u0026#34;sensitive_attributes\u0026#34;: [], \u0026#34;private\u0026#34;: \u0026#34;eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxMjAwMDAwMDAwMDAwLCJkZWxldGUiOjEyMDAwMDAwMDAwMDAsInVwZGF0ZSI6MTIwMDAwMDAwMDAwMH19\u0026#34; } ] } ] } 삭제\n앞서 Terraform으로 생성된 자원을 삭제하려면 다음 명령을 수행하면 됩니다.\nterraform destroy 실행예시\n[opc@bastion-host example_vcn]$ terraform destroy oci_core_virtual_network.vcn1: Refreshing state... [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: - destroy Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be destroyed - resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { - cidr_block = \u0026#34;10.0.0.0/16\u0026#34; -\u0026gt; null - cidr_blocks = [ - \u0026#34;10.0.0.0/16\u0026#34;, ] -\u0026gt; null - compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaaqcgintvsf3giria27rztelhvr4n6gra5wcvtj2bxzm3ukrk7aq7q\u0026#34; -\u0026gt; null - default_dhcp_options_id = \u0026#34;ocid1.dhcpoptions.oc1.ap-seoul-1.aaaaaaaaun4ti33kubad7cqexhe4npgwy773lldor7xxfbbu2lu5yz3q6uaa\u0026#34; -\u0026gt; null - default_route_table_id = \u0026#34;ocid1.routetable.oc1.ap-seoul-1.aaaaaaaammsdhhl7czlkrxeji7eqot6tkiyfcvrhuqvf4hh3cra3rt3m2e2a\u0026#34; -\u0026gt; null - default_security_list_id = \u0026#34;ocid1.securitylist.oc1.ap-seoul-1.aaaaaaaadmiqjfjkiw7k73lxwbilgci2xrzehxud3ezfujmt3gtjenocsoaa\u0026#34; -\u0026gt; null - defined_tags = { - \u0026#34;Oracle-Tags.CreatedBy\u0026#34; = \u0026#34;sandboxer\u0026#34; - \u0026#34;Oracle-Tags.CreatedOn\u0026#34; = \u0026#34;2022-01-18T10:00:51.322Z\u0026#34; } -\u0026gt; null - display_name = \u0026#34;vcn1\u0026#34; -\u0026gt; null - dns_label = \u0026#34;vcn1\u0026#34; -\u0026gt; null - freeform_tags = {} -\u0026gt; null - id = \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34; -\u0026gt; null - ipv6cidr_blocks = [] -\u0026gt; null - is_ipv6enabled = false -\u0026gt; null - state = \u0026#34;AVAILABLE\u0026#34; -\u0026gt; null - time_created = \u0026#34;2022-01-18 10:00:51.389 +0000 UTC\u0026#34; -\u0026gt; null - vcn_domain_name = \u0026#34;vcn1.oraclevcn.com\u0026#34; -\u0026gt; null } Plan: 0 to add, 0 to change, 1 to destroy. Changes to Outputs: - vcn1_ocid = [ - \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q\u0026#34;, ] -\u0026gt; null Do you really want to destroy all resources? Terraform will destroy all your managed infrastructure, as shown above. There is no undo. Only \u0026#39;yes\u0026#39; will be accepted to confirm. Enter a value: yes oci_core_virtual_network.vcn1: Destroying... [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaansfh2iiaduerl4znqr6sb2yng6j7x2yov4kqrsuhi452pcjxl52q] oci_core_virtual_network.vcn1: Destruction complete after 1s Destroy complete! Resources: 1 destroyed. 실행결과\n아래 그림과 같이 vcn1이 없어진 것을 볼 수 있습니다.\n","lastmod":"2019-04-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/4/","tags":["terraform"],"title":"14.3.4 Terraform Configuration 실행하기"},{"categories":null,"contents":"10.4 Load Balancer를 위한 Subnet 만들기 Subnet 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Virtual Cloud Networks 항목으로 이동합니다.\n앞서 만든 VCN인 load-balancer-vcn 클릭\nCreate Subnets 클릭\n생성정보 입력: 진한 글씨 항목만 입력하고 나머지는 기본값을 사용합니다.\nName: 이름 입력, public-lb-subnet Subnet Type: 기본값인 Regional을 선택 CIDR Block: 10.0.2.0/24 Route Table: Default Route Table 선택 Subnet Access: Public Subnet 선택 DHCP Options: Default DHCP Options 선택 Security Lists: Default Security List 선택 최하단으로 스크롤하여 Create Subnet 클릭\npublic-lb-subnet 생성완료\nLB용 Security List 만들기 이후 생성할 Load Balancer를 위한 Security List를 만듭니다. 기본적으로 VCN 생성시 자동으로 만든 Default Security List를 사용할 수 있지만, 별도로 규칙이 없는 비어 있는 Security List를 생성합니다.\n왼쪽 Resources \u0026gt; Security Lists 클릭 Create Security List 클릭 생성정보 입력 Name: 이름 입력, lb-security-list Subnet에 Security List 적용 앞서 만든 public-lb-subnet의 상세 페이지로 이동합니다.\nAdd Security List을 클릭하여 방금 만든 lb-security-list을 public-lb-subnet에 추가합니다.\n","lastmod":"2019-01-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/4/","tags":["load balancer","subnet"],"title":"10.4 Load Balancer를 위한 Subnet 만들기"},{"categories":null,"contents":"5.5 Apache HTTP Server 접속 테스트 Apache HTTP Server가 구동하는 Compute Instance를 통해 접속을 테스트합니다. 그림처럼 잘 연결되는 것을 확인할 수 있습니다.\n","lastmod":"2019-01-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter05/5/","tags":["apache"],"title":"5.5 Apache HTTP Server 접속 테스트"},{"categories":null,"contents":"1.4 Virtual Cloud Network(VCN) OCI를 사용하게 되면 처음 해야 할 작업 중의 하나가 다른 클라우드 자원을 위해 Virtual Cloud Network(VCN)를 만드는 작업입니다. Oracle 데이터 센터에 서버 자원들이 사용할 가상의 네트워크 환경을 구성하는 것으로 네트워크 주소 할당, 방화벽, 라우팅 규칙, 게이트웨이 등을 하는 작업입니다.\n처음 VCN을 만들 때 자동 생성 방식으로 구성할 경우 생성되는 기본 구성은 다음과 같습니다.\nVirtual Cloud Network(VCN) OCI 상의 다른 클라우드 자원을 위한 가상 네트워크 환경입니다. 생성 시 VCN에서 사용할 IP 대역은 CIDR 형식으로 지정합니다. 예) 10.0.0.0/16 VCN과 함께 VCN에 속한 아래 컴포넌트를 구성하여 원하는 가상 네트워크 환경을 구성하게 됩니다. 아래 컴포넌트 이외에도 추가적인 컴포넌트들이 많이 있지만, 여기서는 가장 기본적인 컴포넌트에 대해서만 언급하도록 하겠습니다.\nInternet Gateway 명칭 그대로 인터넷과 연결하려면 통해야 하는 관문입니다. 이름을 부여하고 생성하기만 하면 됩니다.\nSubnet VCN상에 생성되는 서브 그룹으로 하나의 Availability Domain 내에 존재합니다. 사용할 IP 대역은 VCN의 대역에서 일부를 나누어 가집니다. 사용할 IP 대역은 CIDR 형식으로 지정합니다. 예) 10.0.0.0/24\nPublic Subnet과 Private Subnet이 있으며, Private Subnet에 있는 인스턴스는 Public IP를 가지지 못합니다. 반면 Public Subnet에 있는 인스턴스는 Public IP를 할당받게 됩니다.\nPrivate Subnet에 있는 인스턴스는 Public IP를 가지지 못하기 때문에 인터넷상의 외부 시스템에서 들어오지는 못합니다. 대신 추가로 Network Address Translate(NAT) GATEWAY를 설정하면 외부 인터넷으로 나가는 것은 가능합니다.\nRoute Table VCN을 위한 가상의 라우트 테이블로써 Subnet에서 외부 목적지로 가는 규칙을 정하는 Route Table입니다. 자동으로 생성한 디폴트 라우트 테이블은 그림처럼 모든 목적지(0.0.0.0/0)를 Internet Gateway로 라우트 되어 인터넷에 연결되게 설정되었습니다. 커스텀 라우팅 테이블을 구성하여 원하는 규칙을 설정할 수 있습니다.\nSecurity Lists VCN을 위한 가상의 방화벽으로 Subnet에서 들어오고 나가는 트래픽에 각각에 대한 ingress 규칙 및 egress 규칙을 설정할 수 있습니다. 그림에는 편의상 ingress rule만 표현하였으며, SSH 통신을 위해 22번 포트로 들어오는 모든 요청을 열어두었습니다. 그리고 생략되었지만, egress rule로 모든 요청이 나갈 수 있도록 설정되어 있습니다.\nCompute 인스턴스에 WebServer를 설치하거나 별도의 서비스를 위해 포트를 열어야 할 경우 이 Security Lists를 업데이트하면 됩니다. 물론 Security Lists는 VCN상에서 Subnet의 방화벽이며, Compute 인스턴스 자체에 있는 리눅스 방화벽 또는 윈도우 방화벽에 있을 경우에는 자체 방화벽에서 해당 포트도 당연히 열어 두어야 합니다.\n","lastmod":"2018-12-30T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/4/","tags":["virtual cloud network","VCN"],"title":"1.4 Virtual Cloud Network(VCN)"},{"categories":null,"contents":"1.2.2.2.5 NGINX Ingress Controller에서 TLS termination(OCI LB 레벨) SSL 인증서를 secret으로 등록하기 유료 인증서도 등록절차는 동일하니, 여기서는 무료 인증서를 발급 받아 진행합니다.\nSSL For Free를 사용하는 경우\nSSL 인증서를 발급받습니다.\nSSL For Free에서 무료 SSL 인증서 발급받기 압축파일을 풀면 다음 세가지 파일이 포함되어 있습니다.\nkubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\n발급받은 인증서 Zip 파일을 업로드 하고, 압축을 해제합니다.\ningress가 설치될 namespace에 인증서 파일을 이용하여, secret을 만듭니다.\nkubectl create namespace ingress-nginx kubectl create secret tls ssl-certificate-secret-thekoguryo.xyz --key private.key --cert certificate.crt -n ingress-nginx Let’s Encrypt를 사용하는 경우\nSSL 인증서를 발급받습니다.\nLet’s Encrypt로 무료 SSL 인증서 발급받기 아래와 같이 생성된 인증서를 볼 수 있습니다.\n$ ls -la /home/opc/.acme.sh/thekoguryo.xyz total 32 drwxrwxr-x. 2 opc opc 177 Jan 19 06:24 . drwx------. 7 opc opc 153 Jan 19 06:24 .. -rw-rw-r--. 1 opc opc 3751 Jan 19 06:24 ca.cer -rw-rw-r--. 1 opc opc 5536 Jan 19 06:24 fullchain.cer -rw-rw-r--. 1 opc opc 1785 Jan 19 06:24 thekoguryo.xyz.cer -rw-rw-r--. 1 opc opc 618 Jan 19 06:24 thekoguryo.xyz.conf -rw-rw-r--. 1 opc opc 1025 Jan 19 06:24 thekoguryo.xyz.csr -rw-rw-r--. 1 opc opc 208 Jan 19 06:24 thekoguryo.xyz.csr.conf -rw-------. 1 opc opc 1679 Jan 19 06:24 thekoguryo.xyz.key kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\ningress가 설치될 namespace에 인증서 파일을 이용하여, secret을 만듭니다.\nkubectl create namespace ingress-nginx kubectl create secret tls ssl-certificate-secret-thekoguryo.xyz --key thekoguryo.xyz.key --cert thekoguryo.xyz.cer -n ingress-nginx 생성결과를 확인합니다.\n$ kubectl get secret -n ingress-nginx NAME TYPE DATA AGE ssl-certificate-secret-thekoguryo.xyz kubernetes.io/tls 2 6s Ingress Controller 설치 kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\nnginx ingress controller 설치할 파일 deploy.yaml을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml ingress-nginx-controller의 Load Balancer Service 유형에 대한 설정을 annotation으로 추가합니다.\nmetadata 하위에 Load balancer를 위한 annotations 들을 추가합니다.\noci.oraclecloud.com/load-balancer-type: \u0026quot;lb\u0026quot; 을 추가하여 OCI Load Balancer를 사용하고 관련 설정을 추가합니다. service.beta.kubernetes.io/oci-load-balancer-backend-protocol: \u0026quot;HTTP\u0026quot; Listener를 HTTP 프로토콜은 지정합니다. SSL 포트와 SSL 인증서를 등록해 줍니다. spec.ports 하위에 https의 targetPort를 http로 변경합니다.\n--- apiVersion: v1 kind: Service metadata: labels: ... name: ingress-nginx-controller namespace: ingress-nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape: \u0026#34;flexible\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-backend-protocol: \u0026#34;HTTP\u0026#34; service.beta.kubernetes.io/oci-load-balancer-ssl-ports: \u0026#34;443\u0026#34; service.beta.kubernetes.io/oci-load-balancer-tls-secret: ssl-certificate-secret-thekoguryo.xyz spec: ... ports: - appProtocol: http name: http port: 80 protocol: TCP targetPort: http - appProtocol: https name: https port: 443 protocol: TCP targetPort: http ... type: LoadBalancer loadBalancerIP: 152.69.234.225 # Reserved Public IP ... OCI Load Balancer의 설정가능한 annotations 전체 항목\n다음 명령으로 NGINX Ingress Controller를 설치합니다.\nkubectl apply -f deploy.yaml 설치 확인\nkubectl get all -n ingress-nginx OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Networking \u0026gt; Load Balancers로 이동합니다.\nIP 주소를 통해 ingress controller와 연동된 Load Balancer를 확인후 클릭합니다.\n생성된 OCI Load Balancer의 Listeners를 확인합니다.\n프로토콜이 HTTP(80), HTTPS(443)으로 설정되고 HTTPS에는 SSL 인증서가 등록된 것을 볼 수 있습니다. DNS 등록 발급한 SSL 인증서가 와일드카드 인증서가 아닌, 단일 인증서라 편의상 도메인 구입 사이트에서 직접 등록하겠습니다.\n도메인을 구입한 사이트로 이동합니다.\nDNS 관리화면에서 A 타입으로 ingress controller의 Public IP를 등록합니다.\n글로벌하게 DNS 정보가 업데이트되는 것은 시간이 걸릴 수 있습니다.\n테스트 이전 테스트 앱을 아래 yaml 파일로 일부 수정하여 배포합니다.\n4.1.2 Path 기반 테스트앱 설치\nSSL 인증서 검증을 위해 host에 도메인명을 입력합니다.\napiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-path-basic spec: ingressClassName: nginx rules: - host: www.thekoguryo.xyz http: paths: - path: /blue pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 - path: /green pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 https 로 애플리케이션을 접속합니다. 인증서 오류없이 HTTPS로 연결되는 것을 볼 수 있습니다.\n예, https://www.thekoguryo.xyz/blue 인증서가 유효한지 확인합니다. 발급한 SSL 인증서가 정상 등록된 것을 알 수 있습니다.\n","lastmod":"2024-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/nginx-ingress/lb/5.nginx-ingress-tls-termination-lb/","tags":["oss","ingress-controller","tls"],"title":"1.2.2.2.5 TLS termination - OCI LB 레벨"},{"categories":null,"contents":"7.5 Public Bucket을 통한 정적 웹사이트 호스팅 Public Bucket을 생성하면 URL 경로로 접근할 수 있다는 것을 이용하며 점을 이용하여 간단한 정적 웹사이트 호스팅을 해보도록 하겠습니다. Object Storage는 기본적으로 계층구조가 없는 일자 구조입니다. 폴더구조의 가진 웹페이지는 어떻게 처리할 수 있는지에 대해서 테스트 해 봅니다\n현재 버전 기준 Public Bucket은 파일 링크 공유 등으로 사용할 순 있지만, 정적 웹사이트 호스팅을 위해 사용하기에는 제약사항이 있습니다. URL을 Index Document로 보내주는 기능이 현재는 지원하지 않습니다. 예를 들어, http://www.example.com 주소로 웹브라우저로 접속하면, 자동으로 http://www.example.com/index.html로 보내주는 기능이 현재는 없습니다. 루트 페이지뿐만 아니라, 서브 폴더에 대해서 해당 기능이 일반적으로 필요한데, 현재는 명시적으로 html 주소 전체를 브라우저에서 입력 또는 링크를 타고 갈 때 입력해야 합니다.\nAPI Gateway \u0026amp; Function 조합을 통해 처리할 수 있습니다. https://www.ateam-oracle.com/post/creating-a-web-site-with-object-storage-functions-and-the-api-gateway Public Bucket 생성 Bucket을 생성하고 Visibility를 Public으로 변경하여 Public Bucket을 하나 생성합니다.\nBucket 을 생성합니다.\nName: 예) ExampleBucketForWeb Edit Visibility를 클릭하여 Visibility를 Public으로 변경합니다.\n간단한 웹페이지 테스트 간단한 웹페이지를 생성하고 index.html 파일명으로 저장합니다.\n\u0026lt;html\u0026gt; \u0026lt;header\u0026gt;\u0026lt;title\u0026gt;Hello !!!\u0026lt;/title\u0026gt;\u0026lt;/header\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello OCI Object Storage\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; index.html 파일을 Bucket에 올립니다.\n업로드한 index.html의 상세페이지에서 URL 경로를 확인합니다.\n확인된 URL 경로를 웹브라우저로 접속하면, 잘 동작합니다.\n간단한 폴더구조의 웹페이지 테스트 Object Storage는 기본적으로 계층구조가 없는 일자 구조입니다. 그래서 Object 업로드 화면을 보면 폴더에 대한 항목이 없습니다. 하지만, 일반적으로 웹페이지는 폴더구조를 가지게 됩니다. 폴더 구조는 Object의 Name에 경로를 추가하는 방식으로 지원이 됩니다.\n앞서 생성한 index.html 파일을 수정합니다.\n\u0026lt;html\u0026gt; \u0026lt;header\u0026gt;\u0026lt;title\u0026gt;Hello !!!\u0026lt;/title\u0026gt;\u0026lt;/header\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Hello OCI Object Storage\u0026lt;/h1\u0026gt; \u0026lt;img src=\u0026#34;images/icons8-anonymous-mask-100.png\u0026#34;/\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 수정한 index.html 파일을 Bucket에 올립니다. Object Name이 같으면 기존 파일을 덮어쓰게 됩니다.\n이미지 파일을 올립니다. 올릴 때 Object Name을 html 상에 있는 이미지 경로를 포함한 이름으로 변경합니다.\n앞서 확인된 index.html의 URL 경로를 웹브라우저로 접속하면, 잘 동작합니다.\n많은 파일을 업로드할 때 일일이 Name을 경로를 포함한 이름으로 변경하는 것은 당연히 힘듭니다. CLI를 사용하거나, CloudBerry 같은 도구를 사용하면, 로컬 폴더를 그대로 올리면서 Object의 이름이 경로 형태로 자동으로 변경되니, 걱정할 필요는 없습니다.\n","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/5/","tags":["object storage","bucket","public bucket"],"title":"7.5 Public Bucket을 통한 정적 웹사이트 호스팅"},{"categories":null,"contents":"3.6.5 Block Volume 마운트하기 OCI 문서에 따르면 root volume 이외 volume이 둘 이상일 경우 장치 이름으로 마운트 할 경우 서버 재기동 후 장치 이름과 실제 장치의 대응 되는 순서가 달라 질 수도 있다고 합니다. 2019년 1월 10일 기준 서버 재기동 후 장치 이름과 실제 장치의 대응 되는 순서를 보장하기 위해 Consistent Device Path 기능이 출시 되었습니다. 지원되지 않는 이미지 및 인스턴스에서는 전과 동일하게 Volume의 UUID를 기준으로 마운트 방식을 사용합니다.\nConsistent Device Paths for Block Volumes Services: Block Volume Release Date: Jan. 10, 2019 When you attach a block volume to an instance you can now optionally select a device path that will remain consistent between instance reboots. See Connecting to Volumes With Consistent Device Paths for more information. 지원 대상 Oracle-provided Platform 이미지로 만들어진 인스턴스 Linux 기반 이미지 2018년 11월 이후 이미지, 자세한 사항은 Image Release Notes 참고 2019년 1월 11일 이후 생성된 인스턴스 Device Path로 마운트 하기 OCI 콘솔에서 앞서 Attach된 Block Volume의 Device Path(/dev/oracleoci/oraclevdb)를 확인할 수 있습니다.\nCompute Instance에 SSH로 접속한 후 Device Path를 확인합니다.\n[opc@examplelinuxinstance ~]$ ls -la /dev/oracleoci/ total 0 drwxr-xr-x. 2 root root 140 May 4 06:49 . drwxr-xr-x. 21 root root 3300 May 4 06:49 .. lrwxrwxrwx. 1 root root 6 May 4 06:32 oraclevda -\u0026gt; ../sda lrwxrwxrwx. 1 root root 7 May 4 06:32 oraclevda1 -\u0026gt; ../sda1 lrwxrwxrwx. 1 root root 7 May 4 06:32 oraclevda2 -\u0026gt; ../sda2 lrwxrwxrwx. 1 root root 7 May 4 06:32 oraclevda3 -\u0026gt; ../sda3 lrwxrwxrwx. 1 root root 6 May 4 06:54 oraclevdb -\u0026gt; ../sdb 마운트 할 디렉토리 생성\nsudo mkdir /mnt/vol1 /etc/fstab 업데이트\nsudo vi /etc/fstab 명령을 수행하여 확인한 Device Path를 바탕으로 다음 내용을 추가합니다.\nOCI 문서의 권고에 따라 인스턴스 재기동시 장착한 Volume의 장애로 인한 영향을 줄이기 위해 _netdev,nofail 옵션을 반드시 추가합니다.\nUse the _netdev and nofail Options\nsudo vi /etc/fstab 업데이트 예시\n기존 내용의 제일 아래에 마운트 정보를 한 줄 추가합니다.\n# # /etc/fstab # Created by anaconda on Tue Jan 17 19:39:49 2023 ... ## https://docs.us-phoenix-1.oraclecloud.com/Content/Block/Tasks/connectingtoavolume.htm /.swapfile none swap sw,comment=cloudconfig 0 0 /dev/oracleoci/oraclevdb /mnt/vol1 xfs defaults,_netdev,nofail 0 2 마운트\nsudo mount -a 마운트 결과\n/dev/sdb가 /mnt/vol1에 마운트 된걸 알 수 있습니다.\n[opc@examplelinuxinstance ~]$ sudo mount -a [opc@examplelinuxinstance ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 1.8G 0 1.8G 0% /dev tmpfs 1.8G 0 1.8G 0% /dev/shm tmpfs 1.8G 8.7M 1.8G 1% /run tmpfs 1.8G 0 1.8G 0% /sys/fs/cgroup /dev/mapper/ocivolume-root 36G 8.7G 27G 25% / /dev/sda2 1014M 334M 681M 33% /boot /dev/sda1 100M 5.1M 95M 6% /boot/efi /dev/mapper/ocivolume-oled 10G 112M 9.9G 2% /var/oled tmpfs 363M 0 363M 0% /run/user/0 tmpfs 363M 0 363M 0% /run/user/988 tmpfs 363M 0 363M 0% /run/user/1000 /dev/sdb 50G 389M 50G 1% /mnt/vol1 인스턴스 재시작\n/etc/fstab에 추가하였기 때문에 인스턴스 재시작시에도 자동으로 마운트 됩니다. 아래와 같이 이상없이 마운트 되는지 확인해 봅니다. 재시작 후에도 /mnt/vol1으로 마운트 되어 있는 걸 알 수 있습니다.\n[opc@examplelinuxinstance ~]$ sudo reboot Connection to 144.24.81.117 closed by remote host. Connection to 144.24.81.117 closed $ ssh -i privateKey opc@144.24.xx.xxx FIPS mode initialized Activate the web console with: systemctl enable --now cockpit.socket Last login: Thu May 4 06:49:04 2023 from 132.145.xx.xx [opc@examplelinuxinstance ~]$ df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 1.8G 0 1.8G 0% /dev tmpfs 1.8G 0 1.8G 0% /dev/shm tmpfs 1.8G 8.7M 1.8G 1% /run tmpfs 1.8G 0 1.8G 0% /sys/fs/cgroup /dev/mapper/ocivolume-root 36G 8.7G 27G 25% / /dev/sda2 1014M 334M 681M 33% /boot /dev/sda1 100M 5.1M 95M 6% /boot/efi /dev/mapper/ocivolume-oled 10G 112M 9.9G 2% /var/oled /dev/sdb 50G 389M 50G 1% /mnt/vol1 tmpfs 363M 0 363M 0% /run/user/0 tmpfs 363M 0 363M 0% /run/user/1000 ","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/5/","tags":["block volume","mount"],"title":"3.6.5 Block Volume 마운트하기"},{"categories":null,"contents":"14.3.5 다중 환경을 위한 workspace 사용하기 OCI는 여러 Region을 제공하고 있습니다. 필요에 따라 동일 자원을 여러 Region에 한꺼번에 만들어야 하는 경우가 있습니다. 하지만 앞서 실습에서 잠깐 보았겠지만, terraform 설정이 실행되면 기본으로 해당 폴더에 terraform.tfstate로 상태를 관리하게되어 동일 Terraform 설정을 그대로 사용할 경우 상태파일이 충돌나서 원하는 데로 동작하지 않게 됩니다. Terraform 설정 폴더 자체를 통채로 복사해서 대상 Region 별로 사용해도 되지만, 관리가 번거롭게 됩니다. 그래서 Terraform에서는 workspace만 개념을 제공하고, 기본적으로 default workspace를 사용하게 됩니다.\nTerraform Configuration 파일 이전 실습에서 사용한 vcn 만들기 설정을 그대로 사용합니다.\nWorkspace 만들기 terraform workspace 구문\n# workspace 조회하기 terraform workspace list # 새로운 workspace 만들기 terraform workspace new {이름} # 해당 worksapce 삭제하기 terraform workspace delete {이름} # 현재 workspace를 전환하기 terraform workspace select {이름} 새로운 workspace를 만듭니다.\nterraform workspace new ap-seoul-1 terraform workspace new ap-chuncheon-1 예시\n[opc@bastion-host example_vcn]$ terraform workspace list * default [opc@bastion-host example_vcn]$ terraform workspace new ap-seoul-1 Created and switched to workspace \u0026#34;ap-seoul-1\u0026#34;! You\u0026#39;re now on a new, empty workspace. Workspaces isolate their state, so if you run \u0026#34;terraform plan\u0026#34; Terraform will not see any existing state for this configuration. [opc@bastion-host example_vcn]$ terraform workspace new ap-chuncheon-1 Created and switched to workspace \u0026#34;ap-chuncheon-1\u0026#34;! You\u0026#39;re now on a new, empty workspace. Workspaces isolate their state, so if you run \u0026#34;terraform plan\u0026#34; Terraform will not see any existing state for this configuration. [opc@bastion-host example_vcn]$ terraform workspace list default * ap-chuncheon-1 ap-seoul-1 [opc@bastion-host example_vcn]$ 1번째 workspace: ap-seoul-1에서 terraform 실행하기 workspace를 먼저 전환하고 vcn을 만드는 terraform을 실행합니다.\n[opc@bastion-host example_vcn]$ terraform workspace select ap-seoul-1 Switched to workspace \u0026#34;ap-seoul-1\u0026#34;. [opc@bastion-host example_vcn]$ terraform apply -var=\u0026#34;region=ap-seoul-1\u0026#34; Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaa54ryitndueosfezrxvxvcuosutofi2d6f53rbgwz2dpqrgeci7lq\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] Do you want to perform these actions in workspace \u0026#34;ap-seoul-1\u0026#34;? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes oci_core_virtual_network.vcn1: Creating... oci_core_virtual_network.vcn1: Creation complete after 1s [id=ocid1.vcn.oc1.ap-seoul-1.amaaaaaavsea7yiamf4ktoajkby4sh45zx4e52cwscm6xljilibl5o3prpkq] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-seoul-1.amaaaaaavsea7yiamf4ktoajkby4sh45zx4e52cwscm6xljilibl5o3prpkq\u0026#34;, ] [opc@bastion-host example_vcn]$ 결과 확인\nOCI 콘솔에서 서울 Region으로 변경하면 VCN이 생성된 것을 확인할 수 있습니다.\n2번째 workspace: ap-chuncheon-1에서 terraform 실행하기 동일한 위치에서 workspace를 먼저 전환하고 vcn을 만드는 terraform을 동일하게 실행합니다.\n[opc@bastion-host example_vcn]$ terraform workspace select ap-chuncheon-1 Switched to workspace \u0026#34;ap-chuncheon-1\u0026#34;. [opc@bastion-host example_vcn]$ terraform apply -var=\u0026#34;region=ap-chuncheon-1\u0026#34; Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # oci_core_virtual_network.vcn1 will be created + resource \u0026#34;oci_core_virtual_network\u0026#34; \u0026#34;vcn1\u0026#34; { + cidr_block = \u0026#34;10.0.0.0/16\u0026#34; + cidr_blocks = (known after apply) + compartment_id = \u0026#34;ocid1.compartment.oc1..aaaaaaaa54ryitndueosfezrxvxvcuosutofi2d6f53rbgwz2dpqrgeci7lq\u0026#34; + default_dhcp_options_id = (known after apply) + default_route_table_id = (known after apply) + default_security_list_id = (known after apply) + defined_tags = (known after apply) + display_name = \u0026#34;vcn1\u0026#34; + dns_label = \u0026#34;vcn1\u0026#34; + freeform_tags = (known after apply) + id = (known after apply) + ipv6cidr_blocks = (known after apply) + is_ipv6enabled = (known after apply) + state = (known after apply) + time_created = (known after apply) + vcn_domain_name = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + vcn1_ocid = [ + (known after apply), ] Do you want to perform these actions in workspace \u0026#34;ap-chuncheon-1\u0026#34;? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes oci_core_virtual_network.vcn1: Creating... oci_core_virtual_network.vcn1: Creation complete after 0s [id=ocid1.vcn.oc1.ap-chuncheon-1.amaaaaaavsea7yiamplordafqo25ulky46sjoz74r724wxnd5dyplya3ptoq] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: vcn1_ocid = [ \u0026#34;ocid1.vcn.oc1.ap-chuncheon-1.amaaaaaavsea7yiamplordafqo25ulky46sjoz74r724wxnd5dyplya3ptoq\u0026#34;, ] [opc@bastion-host example_vcn]$ 결과 확인\nOCI 콘솔에서 춘천 Region으로 변경하면 VCN이 생성된 것을 확인할 수 있습니다.\nTerraform State 확인\nworkspace를 사용하는 경우 아래와 같이 terraform.tfstate.d 폴더 밑에 workspace 별 폴더 밑에 terrafrom.tfstate 파일이 생성되어 workspace 별로 상태를 관리하는 것을 알 수 있습니다.\n[opc@bastion-host example_vcn]$ ls terraform.tfstate.d/ -la total 4 drwxr-xr-x. 4 opc opc 46 Jan 18 12:39 . drwxrwxr-x. 4 opc opc 4096 Jan 18 12:47 .. drwxr-xr-x. 2 opc opc 31 Jan 18 12:51 ap-chuncheon-1 drwxr-xr-x. 2 opc opc 31 Jan 18 12:47 ap-seoul-1 [opc@bastion-host example_vcn]$ ls terraform.tfstate.d/ap-seoul-1/ -la total 4 drwxr-xr-x. 2 opc opc 31 Jan 18 12:47 . drwxr-xr-x. 4 opc opc 46 Jan 18 12:39 .. -rw-rw-r--. 1 opc opc 2147 Jan 18 12:47 terraform.tfstate [opc@bastion-host example_vcn]$ ls terraform.tfstate.d/ap-chuncheon-1/ -la total 4 drwxr-xr-x. 2 opc opc 31 Jan 18 12:51 . drwxr-xr-x. 4 opc opc 46 Jan 18 12:39 .. -rw-rw-r--. 1 opc opc 2168 Jan 18 12:51 terraform.tfstate [opc@bastion-host example_vcn]$ ","lastmod":"2019-04-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/5/","tags":["terraform","workspace"],"title":"14.3.5 다중 환경을 위한 workspace 사용하기"},{"categories":null,"contents":"10.5 Load Balancer 만들기 Load Balancer 생성 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Load Balancers 항목으로 이동합니다.\nCreate Load Balancer 클릭합니다.\nLoad Balancer 를 타입으로 선정합니다.\nLoad Balancer Type: L7 로드밸런서로 HTTP Listener로 분배할때 사용합니다. Network Load Balancer Type: L4 로드밸런서로 일반 IP, Port로 분배할때 사용합니다. 기본 생성정보 입력\nName: Load Balancer 이름 입력, 예) ExampleLB\nChoose visibility type: 여기서는 Public 선택\nPublic: Public IP를 부여할지 여부 선택 Private: Private IP 만 가집니다. 내부용 로드밸런서인 경우 선택 용 로드밸런인지 설정합니다. Assign a public IP address: Public IP로 외부 서비스를 하는 고정된 Public IP를 사용할지 여부를 지정합니다. 여기서는 테스트이므로 Ephemeral IP Address 선택\nEphemeral IP Address: Load Balancer 생성시 부여된 Public IP를 사용하는 경우 Reserved IP Address: 사용자 예약한 Public IP를 사용하는 경우 Bandwidth: 여기서는 일단 기본값을 사용\nChoose Networking: LB 테스트용으로 앞서 만든 VCN과 Subnet 선택\nVCN: load-balancer-vcn Subnet: public-lb-subnet Backend 설정\nLoad Balancing Policy: 분배규칙을 선택합니다. 여기서는 이후 테스트를 위해 라운드 로빈을 선택합니다.\nWeighted Round Robin: 가중치 기반 라운드 로빈 분배 방식 IP Hash: 클라이언트 IP 주소의 해쉬값을 이용해 분배 Least Connections: Backend의 연결 갯수를 기준으로 적은 쪽으로 분배 Add Backend\n로드밸런서가 부하분산할 대상을 추가하는 부분입니다. 앞서 미리 만들어 둔 WebServer 2개를 추가합니다.\nHealth Check Policy: 현재 WebServer의 기본 포트(80)으로 헬스 체크할 것이므로 기본값을 그대로 사용합니다.\nUse SSL: 지금은 Apache HTTP Server를 HTTP로만 서비스 중이므로 여기서는 체크안함\nAdvanced Option\nSecurity List: 클라이언트-\u0026gt;LB간, LB-\u0026gt;Backend간 각각 서비스 포트를 개방하기 위한 Security List 업데이트 필요합니다. 기본적으로 자동으로 설정되며, 아래 고급옵션에서 자동 여부 및 대상 Security List를 변경할 수 있습니다. 추가로 만든 LB Security List를 선택합니다.\nSession Persistence: 세션 쿠키값에 따른 Persistent를 사용할 경우 방법 설정, 이후 원활한 분배 테스트 확인을 위해서 여기서는 생략\nConfigure Listener\nListener Name: 이름 입력, 예) ExampleLB-Listener Specify the type of traffic your listener handles: HTTP 선택 HTTP, HTTPS, HTTP/2가 선택옵션이나, HTTPS, HTTP/2 선택이 SSL 인증서를 추가설정이 필요합니다. 여기서는 테스트용으로 HTTP를 사용합니다. Manage Logging\n에러 로그와 액세스 로그를 OCI Log 서비스를 사용하도록 설정할 수 있습니다. 일단 여기서는 사용하지 않습니다. Submit을 클릭하여 Load Balancer를 생성합니다.\nLoad Balancer가 생성되고 조금 지나면 Backend 서버의 헬스체크가 성공하고 Overall Health가 OK 상태가 되게 됩니다.\nLoad Balancer 생성 결과 확인 생성된 Load Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n생성시 입력한 정보를 기준으로 Backend Sets과 Listeners가 만들어져 있습니다.\n분배 규칙 및 Backend 서버 추가 등 업데이트를 지원합니다.\n이름 길이는 19자리가 최대인듯 합니다. Listener 또한 생성된 것을 볼 수 있으며, 해당 설정은 업데이트를 지원합니다.\nSecurity List 자동 업데이트 결과 확인 Load Balancer의 Security List인 lb-security-list의 상세 화면으로 이동합니다.\nEgress Rule이 1개 추가 되었습니다. Add Backend Set으로 추가된 WebServer 2대와 서비스 포트 80에 해당되는, 즉 해당 서브넷(10.0.0.0/24)의 80 포트로 개방된 것을 알 수 있습니다.\nBackend인 WebServer 2대가 포함된 서브넷의 Security List인 Default Security List에 public-lb-subnet(10.0.2.0/24)에서 WebServer 포트인 80으로 들어 올 수 있게 마지막에 규칙이 하나 추가되었습니다.\n자동으로 추가된 위 2개의 규칙으로 Load Balancer -\u0026gt; WebServer간 80 포트 통신은 개방되었습니다. 하지만, 실제 클라이언트가 Load Balancer로 올 수 있게 lb-securit-list의 Ingress에는 80 포트가 개방되지 않았습니다.\n다시 lb-securit-list의 상세 화면으로 돌아가 클라이언트 -\u0026gt; Load Balancer간의 개방을 위한 규칙을 설정합니다. Ingress Rule을 아래와 같이 추가 합니다.\n","lastmod":"2019-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/5/","tags":["load balancer"],"title":"10.5 Load Balancer 만들기"},{"categories":null,"contents":"6.5 Policy 개념 잡기 IAM Policy IAM Policy는 OCI 자원에 대한 접근 정책입니다. 하나의 정책은 일반적으로 사용자 그룹에 특정 Compartment에 속한 특정 타입의 자원에 대한 권한을 허용하는 것을 정의하는 것으로 생각하면 하면 됩니다.\nPolicy 문법 모든 Policy는 다음과 같은 문법으로 되어 있습니다.\nAllow \u0026lt;subject\u0026gt; to \u0026lt;verb\u0026gt; \u0026lt;resource-type\u0026gt; in \u0026lt;location\u0026gt; where \u0026lt;conditions\u0026gt; 자세한 문법은 OCI Documentation을 보시기 바랍니다.\nhttps://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm\nPolicy 일반적인 형식 자원에 대한 권한은 아래와 같은 Policy를 통해 정의되면, Tenancy 생성시 관리자에 대해서는 모든 권한이 부여되고, 별도 Policy를 적용하지 않은 일반 사용자, 그룹은 아무런 권한이 일절 없습니다. Policy로 명시적으로 허용한 권한만을 부여합니다.\n많이 사용하는 Policy는 아래와 같이 사용자 그룹이 compartment 내의 특정 자원 유형에 대해서 읽기, 쓰기, 전체 권한 등을 부여하는 형식으로 정의합니다.\nAllow group \u0026lt;group_name\u0026gt; to \u0026lt;verb\u0026gt; \u0026lt;resource-type\u0026gt; in compartment \u0026lt;compartment_name\u0026gt; 일부에 대해서는 compartment가 아닌 tenancy 전체에 대해 적용되는 Policy를 정의하기도 합니다.\nAllow group \u0026lt;group_name\u0026gt; to \u0026lt;verb\u0026gt; \u0026lt;resource-type\u0026gt; in tenancy Subjects 일반적으로 group이 오며, 특정 사용자는 지정할 수 없습니다. 모든 사용자(any-user)는 지정할 수 있습니다.\n참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm#one\nVerbs Verb 설명 대상 사용자 inspect 사용자 정의 메타데이터, 보안 정보를 제외한 자원을 리스트(조회)할 수 있는 권한 조회가 필요한 제3자 사용자 read inspect에 추가하여 사용자 정의 메타데이터도 조회할 수 있는 권한 포함 조회가 필요한 내부 사용자 use read에 추가하여 자원을 사용할 수 있는 권한 포함. 자원 생성, 삭제에 대한 권한은 없음.\n일반적으로 업데이트 권한도 포함하지만, 생성과 동일한 역활을 하는 업데이트 권한은 포함하지 않음 일반 사용자 manage 자원에 대한 모든 권한 포함 관리자 참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Reference/policyreference.htm#Verbs\nResource-Type 사용할 자원을 지칭하여, 하나의 개별 자원 또는 동일한 자원 전체 등을 지칭할 수 있습니다.\nindividual resource-type : 예) vcns, subnets, instances, volumes 등등 family resource-type : 예) virtual-network-family, instance-family, volume-family 등등 all-resources : Compartment 또는 Tenancy에 있는 모든 자원 참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Reference/policyreference.htm#Resource\nLocations 일반적으로 Compartment 또는 tenancy를 지정합니다.\n참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm#four\nConditions 추가적인 상세 조건을 지정할 수 있습니다.\n참고: https://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/policysyntax.htm#five\n계층적 Compartment Compartment가 여러 계층으로 이루어 진 경우, 부모 Compartment에 부여된 Policy는 자식 Compartment가 상속받게 됩니다.\nPolicy 예시 Tenancy 기준으로 특정 그룹에 사용자 관리 권한을 부여하여 Policy 예시\nAllow group HelpDesk to manage users in tenancy Compartment 기준으로 모든 자원을 관리하도록 관리자 그룹을 지정하는 Policy 예시\nAllow group A-Admins to manage all-resources in compartment Project-A Compartment 기준으로 특정 자원 패밀리에만 관리하도록 부분 관리자 그룹을 지정하는 Policy 예시\nAllow group A-Admins to manage instance-family in compartment Project-A Allow group A-Admins to manage volume-family in compartment Project-A Allow group A-Admins to use virtual-network-family in compartment Networks 사용할 만한 Policy 예시들 일반적으로 사용할 만한 Policy에 대해서 Common Policies로 문서에서 제공하고 있습니다. Policy 생성시 참고하시기 바랍니다.\nhttps://docs.cloud.oracle.com/iaas/Content/Identity/Concepts/commonpolicies.htm\n","lastmod":"2019-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter06/5/","tags":["policy"],"title":"6.5 Policy 개념 잡기"},{"categories":null,"contents":"1.5 Bare Metal, Virtual Machine OCI는 Bare Metal과 Virtual Machine 인스턴스를 모두 제공하고 있습니다.\nBare Metal: 물리적인 서버를 단일 사용자에게 전용으로 사용할 수 있는 환경을 제공합니다. 물리적인 서버의 자원을 직접사용하고, 전용으로 사용하기 때문에 서버자원을 성능을 100% 활용하여 높은 성능을 제공합니다. Virtual Machine: 물리적인 서버 위의 가상화를 통해 컴퓨팅 환경을 제공합니다. Hypervisor에 의해 가상화된 레이어를 거치기 때문에 Bare Metal 보다는 상대적으로 낮은 성능을 제공합니다. ","lastmod":"2019-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/5/","tags":["bare metal","virtual machine"],"title":"1.5 Bare Metal, Virtual Machine"},{"categories":null,"contents":"7.6 CloudBerry Explorer를 통한 파일 업로드 Object Storage는 자체 API를 제공합니다. 또한 Amazon S3 호환 API를 제공합니다. OCI CLI를 통해 다량, 대용량 파일을 업로드가 가능합니다. 여기서는 사용 가능한 GUI 툴 중인 CloudBerry Explorer를 사용해 봅니다.\nCloudBerry Explorer를 통한 파일 업로드 Object Storage는 자체 API를 제공합니다. 또한 Amazon S3 호환 API를 제공하여, 기존 S3를 위한 클라이언트들을 그대로 사용할 수 있습니다. 대표적인 Cloud Berry Explorer 툴이 있으며, Freeware Edition 및 PRO 버전을 제공하고 있습니다.\nCloudBerry Explorer Freeware 다운로드 Explorer Free vs. Explorer PRO 기능 비교 Free 버전은 최대 5GB 파일 사이즈까지 지원합니다. CloudBerry Explorer를 통한 파일 업로드 Step 1. Bucket 생성 Object Storage Bucket 을 생성합니다. Name: 예) ExampleBucketForCloudBerry Step 2. API 접근 사용자 생성 API 통한 접근이라 정확한 권한 체크를 위해 별도 유저, 그룹을 만들어 테스트 하였습니다\n관리자로 OCI 콘솔에 로그인합니다.\nTool을 위해 그룹(ObjectStorageToolGroup) 및 API 접근 사용자(objectstoragetool)를 생성하고 해당 그룹에 사용자를 추가합니다.\nPolicy 설정\n예) 이름: ObjectStorageToolPolicy Allow group \u0026lt;group_name\u0026gt; to inspect buckets in compartment \u0026lt;compartment_name\u0026gt; Allow group \u0026lt;group_name\u0026gt; to manage objects in compartment \u0026lt;compartment_name\u0026gt; Step 3. Customer Secret Key 생성 OCI Object Storage에 연결하기 위해서는 Customer Secret Key가 필요합니다. (초기에는 Amazon S3 Compatibility API key라고도 했습니다.)\n해당 유저(objectstoragetool)로 로그인해서 생성하거나, 관리자 계정으로 해당 유저에 대해서 생성합니다.\nOCI 콘솔에서 내비게이션 메뉴를 엽니다. Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Users 항목으로 이동합니다.\n해당 유저(objectstoragetool)의 상세 페이지로 이동합니다.\n왼쪽 아래 Resources \u0026gt; Customer Secret Keys선택\nGenerate Secret Key 클릭\nKey 이름을 입력하고 Generate Secret Key 클릭\n생성된 키를 복사해 둡니다.\n키를 잊어버리면, 재확인 불가하며, 재생성하여야 합니다. 반드시 복사해 둡니다.\nAccess Key도 복사해 둡니다.\nStep 4. CloudBerry Explorer 설치 CloudBerry Explorer Freeware를 내려받아 설치합니다.\nCloudBerry Explorer Freeware 다운로드 테스트 버전: 6.2.1.8 Tenancy 정보 확인\nOCI 콘솔 오른쪽 위의 사용자 프로필일 클릭하여 Tenancy 정보화면으로 이동합니다.\nTenancy 정보에서 Object Storage Namespace를 확인합니다.\nCloudBerry Explorer Freeware를 실행합니다.\nFile \u0026gt; Add New Account 클릭\nS3 Compatible 유형 선택\n연결정보 설정\nDisplay name : 원하는 이름을 입력\nService point : \u0026lt;Object Storage Namespace\u0026gt;.compat.objectstorage.\u0026lt;리전 키\u0026gt;.oraclecloud.com 형식으로 입력\n예) cnzdxxxxu9s8.compat.objectstorage.ap-seoul-1.oraclecloud.com Access Key : 앞서 복사해둔 Customer Secret Key의 Access Key\nSecret Key : Customer Secret Key 생성 후 복사해 둔 Secret Key\nSignature version : \u0026ldquo;4\u0026rdquo; 선택\nr\u0026gt;\nTest Connection을 클릭하여 연결 테스트\n테스트가 완료되면, OK 클릭하여 Account 등록완료\n연결 확인\n오른쪽 창의 Source을 생성한 대상 Account로 선택하면 아래와 같이 Bucket 리스트가 보입니다. 보이지 않는 경우 IAM Policy가 적용되었는지 다시 확인합니다.\nStep 5. CloudBerry Explorer을 사용하여 파일 업로드 우측 화면에서 올릴 대상 Bucket을 클릭합니다.\n좌측 화면의 로컬 디스크에서 올릴 파일을 선택하고, Copy 메뉴를 클릭합니다.\n업로드 내용을 확인하고 Yes 클릭\n업로드가 완료되었습니다.\nStep 6. Object Storage 확인 OCI 콘솔로 돌아가 테스트 중인 Object Storage Bucket으로 이동합니다.\nOCI Console에서 Bucket에 파일이 올라간 것을 확인할 수 있습니다.\n","lastmod":"2022-01-12T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/6/","tags":["object storage","cloudberry","upload"],"title":"7.6 CloudBerry Explorer를 통한 파일 업로드"},{"categories":null,"contents":"3.6.6 Block Volume 장착 해제하기 언마운트 하기 Compute Instance에 SSH로 접속합니다.\n/etc/fstab 업데이트\nsudo vi /etc/fstab 명령을 수행하여 fstab 파일에서 언마운트할 Volume인 /mnt/vol1을 삭제합니다.\nsudo vi /etc/fstab 언마운트\nsudo umount /mnt/vol1 Volume에 연결 끊기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\n앞서 생성한 대상 Instance의 이름을 클릭합니다.\nInstance 상세 페이지에서 왼쪽 아래의 Resources \u0026gt; Attached block volumes을 클릭합니다.\n방금 장착한 Block Volume 옆에 있는 액션 아이콘(우측 점 3개)을 클릭한 다음 iSCSI commands and information을 클릭합니다.\niSCSI Commands and Information 다이얼로그가 뜹니다.\nDisconnect의 명령을 복사해서 Instance에 접속한 SSH 세션에서 순서대로 하나씩 실행합니다.\n[opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -T iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b -p 169.254.2.2:3260 -u Logging out of session [sid: 1, target: iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b, portal: 169.254.2.2,3260] Logout of [sid: 1, target: iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b, portal: 169.254.2.2,3260] successful. [opc@examplelinuxinstance ~]$ sudo iscsiadm -m node -o delete -T iqn.2015-12.com.oracleiaas:90414f1a-9432-47a0-bd3b-def97af8f86b -p 169.254.2.2:3260 장착해제 여부를 확인하기 위해 fdisk -l 명령으로 확인합니다.\n/dev/sdb 디스크가 없어진 것을 확인할 수 있습니다.\n[opc@examplelinuxinstance ~]$ sudo fdisk -l Disk /dev/sda: 46.6 GiB, 50010783744 bytes, 97677312 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disklabel type: gpt Disk identifier: 812F8A56-E8ED-417C-BD3D-661E60705756 Device Start End Sectors Size Type /dev/sda1 2048 206847 204800 100M EFI System /dev/sda2 206848 2303999 2097152 1G Linux filesystem /dev/sda3 2304000 97675263 95371264 45.5G Linux LVM Disk /dev/mapper/ocivolume-root: 35.5 GiB, 38088474624 bytes, 74391552 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Disk /dev/mapper/ocivolume-oled: 10 GiB, 10737418240 bytes, 20971520 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 1048576 bytes Volume 장착 해제 OCI 콘솔에서 장착 해제할 Block Volume 옆에 있는 액션 아이콘을 클릭한 다음 Detach를 클릭합니다.\n경고문을 확인한 후 Detach volume을 클릭합니다. 우리는 /etc/fstab 정리, unmount, iSCSI Disconnect command를 앞서 모두 실행했습니다.\nDetaching 상태로 보입니다. 완료되면 Attached Block Volumes 목록에서 사라지게 됩니다.\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/6/","tags":["block volume"],"title":"3.6.6 Block Volume 장착 해제하기"},{"categories":null,"contents":"1.6 클라우드 서비스 제공업체(Cloud Service Provider) 간 제공 서비스 매핑 IaaS 기준으로 Oracle Cloud Infrastructure에서 제공하는 클라우드 서비스를 이해를 돕고자 타사 Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform의 서비스 이름과 매핑으 Oracle Cloud 사이트에서 확인할 수 있습니다.\nCompare OCI with AWS, Azure, and Google Cloud 사이트 ","lastmod":"2022-01-09T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter01/6/","tags":["service mapping"],"title":"1.6 클라우드 서비스 제공업체(CSP) 간 제공 서비스 매핑"},{"categories":null,"contents":"1.2.2.2.6.3 NGINX Ingress Controller에서 TLS termination(feats. Let\u0026rsquo;s Encrypt) Ingress Controller에서 외부 수신을 SSL로 하기 위한 설정을 확인합니다.\nNGINX Ingress Controller 설치 kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\nnginx ingress controller 설치할 파일 deploy.yaml을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.9.5/deploy/static/provider/cloud/deploy.yaml ingress-nginx-controller의 Load Balancer Service 유형에 대한 설정을 annotation으로 추가합니다.\noci.oraclecloud.com/load-balancer-type: \u0026quot;lb\u0026quot; 을 추가하여 OCI Load Balancer를 사용하고 관련 설정을 추가합니다.\nservice.beta.kubernetes.io/oci-load-balancer-backend-protocol: \u0026quot;TCP\u0026quot; Ingress 레벨에서 tls를 설정할 것이 때문에 여기는 HTTP가 아닌 TCP 로 설정합니다.\n... --- apiVersion: v1 kind: Service metadata: labels: ... name: ingress-nginx-controller namespace: ingress-nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape: \u0026#34;flexible\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: \u0026#34;10\u0026#34; service.beta.kubernetes.io/oci-load-balancer-backend-protocol: \u0026#34;TCP\u0026#34; spec: ... type: LoadBalancer ... 다음 명령으로 NGINX Ingress Controller를 설치합니다.\nkubectl apply -f deploy.yaml 생성된 OCI Load Balancer 확인\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Networking \u0026gt; Load Balancers \u0026gt; Load Balancer로 이동합니다.\n동일한 Public IP로 생성된 Load Balancer를 클릭하여, Listener를 확인합니다. 보면 그림과 같이 HTTP 프로토콜로 80, 443 포트로 수신하고 있습니다.\nDNS 서비스 설정\n이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 따로 구입한 Domain Name(thekoguryo.xyz)을 사용하였습니다. 앞선 실습에서 등록하지 않은 경우 아래와 같이 등록합니다.\n도메인 구입처 또는 OCI DNS 서비스에서 위임하여 서비스하는 경우 OCI DNS 서비스에서 사용한 host를 등록합니다.\n추가할 레코드를 입력하고 제출합니다.\nRecord Type: A - IPv4 Address\nName: *.ingress\n와일드 카드 형식으로 ingress controller가 사용할 서브 Domain Name을 입력합니다. Address: 매핑할 IP, 여기서는 앞서 만든 OCI Native Ingress Controller의 Load Balancer의 IP 입력\n예, GoDaddy DNS 관리화면\nDNS 테스트\nnslookup 툴로 등록한 DNS를 테스트 해봅니다. 잘 등록된 것을 알 수 있습니다.\n$ nslookup *.ingress.thekoguryo.xyz Server: 127.0.0.11 Address: 127.0.0.11#53 Non-authoritative answer: Name: *.ingress.thekoguryo.xyz Address: 158.180.xx.xxx Self-Signed 인증서 사용하기 테스트 목적으로 Self-Signed 인증서를 만들어 사용하는 방법을 확인해 봅니다. 실제 환경에서는 공인 인증기관에서 발급받은 인증서를 사용합니다. Self-Signed 인증서 발급 절차만 대체되어 TLS Secret 등록과정부터는 동일하게 수행됩니다.\n참고 문서\nhttps://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm 인증서 만들기\nCloud Shell 또는 작업환경에서 다음 명령으로 인증서를 생성합니다. 공인 인증기관에서 발급받은 인증서 사용시 하지 않아도 됩니다.\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; TLS Secret을 만듭니다.\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt 실행결과\n$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; Generating a 2048 bit RSA private key *************************************************************************************************************************************************************+++++ ****************************************************************************************************************************+++++ writing new private key to \u0026#39;tls.key\u0026#39; ----- $ kubectl create secret tls tls-secret --key tls.key --cert tls.crt secret/tls-secret created TLS Ingress 자원 배포\n테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\nkubectl create deployment nginx-blue --image=ghcr.io/thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=ghcr.io/thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80 ingress 설정 YAML(tls-termination.yaml)을 작성합니다.\nspec.tls.secretName으로 앞서 생성한 Self-Signed 인증서 이름을 사용합니다. apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-tls-termination spec: ingressClassName: nginx tls: - secretName: tls-secret rules: - host: blue.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 - host: green.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 작성한 tls-termination.yaml을 배포합니다.\n$ kubectl apply -f tls-termination.yaml ingress.networking.k8s.io/ingress-tls-termination created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination nginx blue.ingress.thekoguryo.xyz,green.ingress.thekoguryo.xyz 10.0.20.170,152.69.xxx.xx 80, 443 43s TLS 적용 결과 검증\ningress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서로 인한 경고 메시지가 뜹니다.\n고급을 클릭하고 해당 페이지로 이동을 선택합니다.\n브라우저 주소창 메뉴를 통해 인증서 정보를 확인합니다. Self-Signed 인증서로 루트 인증서가 신뢰할 수 없다는 경고를 확인할 수 있습니다.\nLet\u0026rsquo;s Encrypt \u0026amp; Cert Manager 사용하기 Let\u0026rsquo;s Encrypt는 무료 인증서 발급 사이트로 TLS에 사용할 인증서를 발급 받을 수 있습니다. 대신 90일 동안만 유효하며 만료전에 갱신해야 합니다. Kubernetes에서는 Cert Manager를 통해 자동으로 갱신할 수 있습니다.\nhttps://letsencrypt.org/2015/11/09/why-90-days.html 설치 참고 문서\nhttps://cert-manager.io/docs/tutorials/acme/nginx-ingress/ Cert Manager 배포\nCloud Shell 또는 작업환경에서 Cert Manager를 배포합니다.\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.3/cert-manager.yaml 설치 확인\ncert-manager namespace에 자원들이 정상 실행중인 확인합니다.\n$ kubectl get all -n cert-manager NAME READY STATUS RESTARTS AGE pod/cert-manager-77c645c9cd-w5vk4 1/1 Running 0 30s pod/cert-manager-cainjector-6678d4cbcd-z58zf 1/1 Running 0 31s pod/cert-manager-webhook-996c79df8-bd64t 1/1 Running 0 30s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager ClusterIP 10.96.44.207 \u0026lt;none\u0026gt; 9402/TCP 33s service/cert-manager-webhook ClusterIP 10.96.138.173 \u0026lt;none\u0026gt; 443/TCP 33s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 31s deployment.apps/cert-manager-cainjector 1/1 1 1 32s deployment.apps/cert-manager-webhook 1/1 1 1 31s NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-77c645c9cd 1 1 1 31s replicaset.apps/cert-manager-cainjector-6678d4cbcd 1 1 1 32s replicaset.apps/cert-manager-webhook-996c79df8 1 1 1 31s Let\u0026rsquo;s Encrypt Issuer 구성\n본 예제에서는 Let\u0026rsquo;s Encrypt에서 제공하는 Staging Issuer, Production Issuer을 사용할 수 있습니다. 여기서는 테스트용도로 Staging Issuer를 사용하겠습니다.\nLet\u0026rsquo;s Encrypt Staging Issuer 설정\nhttps://raw.githubusercontent.com/cert-manager/website/master/content/docs/tutorials/acme/example/staging-issuer.yaml 파일에서 email 부분만 본인 것으로 수정하여 반영합니다.\nwget https://raw.githubusercontent.com/cert-manager/website/master/content/docs/tutorials/acme/example/staging-issuer.yaml kind: 네임스페이스에만 사용되는 Issuer가 아닌 전체 쿠버네티스 클러스터에 사용하기 위해 ClusterIssuer 유형으로 변경합니다. spec.acme.email: 본인 이메일로 변경 apiVersion: cert-manager.io/v1 kind: ClusterIssuer metadata: name: letsencrypt-staging spec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider solvers: - http01: ingress: ingressClassName: nginx Let\u0026rsquo;s Encrypt Production Issuer\nProduction Issuer도 https://raw.githubusercontent.com/cert-manager/website/master/content/docs/tutorials/acme/example/production-issuer.yaml 파일을 이용해 동일한 방식으로 설치할 수 있습니다. 다만 사용 limit로 인해 삭제, 생성을 반복할 경우 Rate Limit에 걸릴 수 있습니다.\n설정 적용\nkubectl apply -f staging-issuer.yaml 결과를 확인합니다.\n$ kubectl get ClusterIssuer NAME READY AGE letsencrypt-staging True 23s TLS Ingress 자원 배포\n테스트 앱은 이전 그대로 사용합니다.\ningress 설정 YAML(tls-termination-cert-manager.yaml)을 작성합니다.\ncert-manager.io/cluster-issuer: 방금 생성한 letsencrypt-staging 설정합니다. issuer가 아닌 cluster-issuer 유형을 사용합니다. spec.tls 하위에 tls 저장할 저장소 이름 및 발급받아 사용할 도메인 이름을 지정합니다 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-tls-termination-cert-manager annotations: cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-staging\u0026#34; spec: ingressClassName: nginx tls: - hosts: - green.ingress.thekoguryo.xyz - blue.ingress.thekoguryo.xyz secretName: ingress-thekoguryo-xyz-tls rules: - host: green.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-green-svc port: number: 80 - host: blue.ingress.thekoguryo.xyz http: paths: - path: / pathType: Prefix backend: service: name: nginx-blue-svc port: number: 80 기존 테스트 ingress는 삭제하고, 작성한 tls-termination-cert-manager.yaml을 배포합니다.\n$ kubectl delete -f tls-termination.yaml ingress.networking.k8s.io \u0026#34;ingress-tls-termination\u0026#34; deleted $ kubectl apply -f tls-termination-cert-manager.yaml ingress.networking.k8s.io/ingress-tls-termination-cert-manager created $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination-cert-manager nginx green.ingress.thekoguryo.xyz,blue.ingress.thekoguryo.xyz 152.69.xxx.xxx 80, 443 36s 인증서 발급 확인\n지정한 spec.tls.secretName으로 secret이 만들어지고, certificate 상태(READY)가 True가 되면 정상 발급되었습니다.\n$ kubectl get secret NAME TYPE DATA AGE ingress-thekoguryo-xyz-tls kubernetes.io/tls 2 34s $ kubectl get certificate NAME READY SECRET AGE ingress-thekoguryo-xyz-tls True ingress-thekoguryo-xyz-tls 80s Let\u0026rsquo;s Encrypt Root CA 변경으로 인해 인증오류 해결\nStaging Issuer를 사용할 경우, (STAGING) Doctored Durian Root CA X3 만료로 인해 웹브라우저 접속했을 때 인증 오류가 발생하는 경우가 있습니다. 아래 과정을 통해 클라이언트에서 인증서를 등록해 주면 됩니다.\nProduction Issuer는 해당 문제가 발생하지 않습니다. 필요한 경우 cert manager issuer를 production issuer를 사용하겠습니다. 대신 production issuer는 생성을 반복할 경우 limit에 걸릴 수 있습니다.\n해당 에러가 발생하는 경우 변경된 새 Root CA를 let\u0026rsquo;s encrypt 사이트에서 다운 받아 브라우저에 등록해 줘야 합니다.\nhttps://letsencrypt.org/docs/staging-environment/#root-certificates (STAGING) 루트 인증서 파일 다운로드\nhttps://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem\nWindows 크롬 브라우저 기준: (STAGING) 루트 인증서 추가\n크롬 브라우저 \u0026gt; 설정 \u0026gt; 개인정보 및 보안 \u0026gt; 인증서 관리 로 이동\n인증서 가져오기 클릭\n다운받은 파일 선택\n인증서 설치\n인증서 등록후 확인\n신뢰할 수 있는 루트 인증 기관에 방금 등록한 (STAGING) 인증서가 보임\n인증서가 등록후 다시 앱의 웹페이지를 접속하면 인증오류가 발생하지 않고, 인증 경로가 아래와 같이 보이게 됩니다.\nMac 기준: (STAGING) 루트 인증서 추가\n맥 키체인 접근을 실행합니다.\n시스템 키체인 잠금 해제합니다\n새로운 키체인 항목 생성 아이콘을 클릭합니다.\n다운받은 (STAGING) 인증서 파일 선택\n추가된 인증서를 더블클릭합니다.\nSSL을 항상 신뢰로 변경합니다. 창을 닫으면, 관리자 암호 입력후 저장할 수 있습니다.\n인증서 설정이 완료되었습니다.\n인증서가 등록후 다시 앱의 웹페이지를 접속하면 인증오류가 발생하지 않고, 인증 경로가 아래와 같이 보이게 됩니다. Root CA가 (STAGING) Doctored Durian Root CA X3에서 등록한 (STAGING) Pretend Pear X1으로 변경되었습니다.\n참고 링크\nhttps://github.com/vancluever/terraform-provider-acme/issues/161\nTLS 적용 결과 검증\ningress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서와 달리 경고 없이 유효한 인증서로 표시됩니다.\nDNS 대체 주소에 요청한 host가 모두 등록되어, blue 앱도 인증에러 없이 접속됩니다.\n","lastmod":"2024-01-18T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/ingress/nginx-ingress/lb/6.nginx-ingress-tls-termination-spec-tls-secret/","tags":["oss","ingress-controller","tls"],"title":"1.2.2.2.6 TLS Termination - Ingress 레벨"},{"categories":null,"contents":"14.3.6 예시, list, count를 이용한 유저 만들기 자원을 만들다 보면 Array를 통해 동일한 자원을 여러개 만드는 경우가 있습니다. 실습환경을 구성하기위해 여러 유저를 만든다는 상황을 고려해 list, count를 통해 여러 유저를 한꺼번에 만드는 방법을 알아 보겠습니다.\nUser 생성을 위해 필요한 Policy Allow group \u0026lt;group_name\u0026gt; to manage users in tenancy User 만들기 OCI 유저는 oci_identity_user 구문을 사용하여 만들 수 있습니다.\n예시\nresource \u0026#34;oci_identity_user\u0026#34; \u0026#34;test_user\u0026#34; { #Required compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;${var.user_description}\u0026#34; name = \u0026#34;${var.user_name}\u0026#34; #Optional defined_tags = {\u0026#34;Operations.CostCenter\u0026#34;= \u0026#34;42\u0026#34;} email = \u0026#34;${var.user_email}\u0026#34; freeform_tags = {\u0026#34;Department\u0026#34;= \u0026#34;Finance\u0026#34;} } count를 사용해 여러 유저 만들기 기본적으로 하나의 resource 블럭은 하나의 자원에 해당됩니다. 여기에 count를 추가하게 되면 동일한 유형을 여러개 만들 수 있습니다. 일반적으로 루프 프로그래밍에서 처럼 count로 반복 횟수를 설정하고, count.index로 현재 루프값을 조회하여 사용합니다. 그래서 아래와 같이 count에 만들 자원 수를 지정하고, count.index를 통해 자원이름 등이 충돌하지 않게 사용할 수 있습니다.\ntestuser_01, testuser_02 처럼 원하는 count 수 만큼 유저를 만드는 예제\nresource \u0026#34;oci_identity_user\u0026#34; \u0026#34;test_user\u0026#34; { count = 2 #Required compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; name = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; } users.tf 전체 내용 예시\nprovider.tf, terraform.tfvars는 앞선 실습 파일을 그대로 사용 ### Create Users resource \u0026#34;oci_identity_user\u0026#34; \u0026#34;users_1\u0026#34; { count = 2 compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; name = \u0026#34;testuser_${format(\u0026#34;%02d\u0026#34;, count.index + 1)}\u0026#34; } ### Set User Passwords ### This is one time password. resource \u0026#34;oci_identity_ui_password\u0026#34; \u0026#34;users_1_password\u0026#34; { count = 2 user_id = \u0026#34;${oci_identity_user.users_1.*.id[count.index]}\u0026#34; } ### Outputs output \u0026#34;my_users_1_password\u0026#34; { sensitive = false value = \u0026#34;${concat(oci_identity_user.users_1.*.name, oci_identity_ui_password.users_1_password.*.password)}\u0026#34; } 실행결과 output과 OCI 유저가 지정한 count 갯수 만큼 생성된 것을 볼 수 있습니다.\n[opc@bastion-host example_user_count]$ terraform apply ... Apply complete! Resources: 4 added, 0 changed, 0 destroyed. Outputs: my_users_1_password = [ \u0026#34;testuser_01\u0026#34;, \u0026#34;testuser_02\u0026#34;, \u0026#34;:\u0026lt;7:qa.Z[XqvKa19uL$u\u0026#34;, \u0026#34;s.;sl72P_.v]Eu]V{rxu\u0026#34;, ] [opc@bastion-host example_user_count]$ list, count를 사용해 여러 유저 만들기 앞선 예제와 달리 유저의 이름을 각각 전혀 다른 이름으로 만들고자 할때는 list 타입 배열변수와 count를 조합하여 다른 이름의 여러 유저를 만들 수 있습니다.\n다른 이름의 여러 유저를 만드는 예제\nlist 타입의 변수를 만들고 \u0026quot;${element(LIST, INDEX)}\u0026quot;, **\u0026quot;${length(LIST)}\u0026quot;**를 사용하여, 특정 데이터 및 list 길이를 조회할 수 있습니다.\nvariable \u0026#34;user_names\u0026#34; { type = \u0026#34;list\u0026#34; default = [\u0026#34;oracle\u0026#34;, \u0026#34;neo\u0026#34;] } resource \u0026#34;oci_identity_user\u0026#34; \u0026#34;my_users_2\u0026#34; { count = \u0026#34;${length(var.user_names)}\u0026#34; compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;${element(var.user_names, count.index)}\u0026#34; name = \u0026#34;${element(var.user_names, count.index)}\u0026#34; } users.tf 전체 내용 예시\nprovider.tf, terraform.tfvars는 앞선 실습 파일을 그대로 사용 variable \u0026#34;user_names\u0026#34; { type = list default = [\u0026#34;oracle\u0026#34;, \u0026#34;neo\u0026#34;] } ### Create Users resource \u0026#34;oci_identity_user\u0026#34; \u0026#34;my_users_2\u0026#34; { count = \u0026#34;${length(var.user_names)}\u0026#34; compartment_id = \u0026#34;${var.tenancy_ocid}\u0026#34; description = \u0026#34;${element(var.user_names, count.index)}\u0026#34; name = \u0026#34;${element(var.user_names, count.index)}\u0026#34; } ### Set User Passwords ### This is one time password. resource \u0026#34;oci_identity_ui_password\u0026#34; \u0026#34;my_users_2_password\u0026#34; { count = \u0026#34;${length(var.user_names)}\u0026#34; user_id = \u0026#34;${oci_identity_user.my_users_2.*.id[count.index]}\u0026#34; } ### Outputs output \u0026#34;my_users_2_password\u0026#34; { sensitive = false value = \u0026#34;${concat(oci_identity_user.my_users_2.*.name, oci_identity_ui_password.my_users_2_password.*.password)}\u0026#34; } 실행결과 output과 OCI 유저가 리스트상의 이름대로 생성된 것을 볼 수 있습니다.\n[opc@bastion-host example_user_list]$ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create ... Apply complete! Resources: 4 added, 0 changed, 0 destroyed. Outputs: my_users_2_password = [ \u0026#34;oracle\u0026#34;, \u0026#34;neo\u0026#34;, \u0026#34;W$tC!bS6k\u0026amp;h;mH8R#}9u\u0026#34;, \u0026#34;(I;hZJ-uZq+.LkXGl5#Z\u0026#34;, ] [opc@bastion-host example_user_list]$ ","lastmod":"2019-04-01T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/6/","tags":["terraform","oci user"],"title":"14.3.6 예시, list, count를 이용한 유저 만들기"},{"categories":null,"contents":"10.6 Load Balancer 연결 확인 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Networking \u0026gt; Load Balancers 항목으로 이동합니다.\n앞서 생성한 Load Balancer의 Public IP를 확인합니다.\n브라우저를 통해 LB의 Public IP로 접속합니다.\n브라우저를 리프레쉬합니다.\nLB의 분배규칙을 Round Robin으로 했기 때문에 순서대로 하나씩 가는 것을 알 수 있습니다.\n","lastmod":"2019-02-06T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter10/6/","tags":["load balancer"],"title":"10.6 Load Balancer 연결 확인"},{"categories":null,"contents":"7.7 수명주기를 통한 Archive 설정하기 Object Storage의 사용 사례의 하나로 디스크 이미지 백업, 파일 백업 등의 용도로 많이 사용합니다. 최근 백업을 사용할 가능성이 높지만, 그 외에 백업을 사용할 가능성을 낮아지지만, 보관 기간 등이 장기간 보관해야 하는 경우가 많습니다. 보관 기간 동안 Storage 비용을 조금이라도 줄이고 싶은 요구 사항이 있습니다.\nObject Storage의 Bucket에는 저장된 객체의 수명주기를 관리하는 기능이 있습니다. 객체 생성 후 일정한 기간이 지났을 때 삭제하거나, 좀 더 저렴한 Archive Storage로 이동시킬 수 있는 기능을 제공합니다.\nStep 1. Object Archive 권한 부여 OCI 콘솔에 로그인합니다.\n수명주기를 설정하기 위해서는 Object Storage 서비스에게 사용자의 오브젝트에 대한 권한을 부여해야 합니다.\nObject Archive를 하기 위해서 다음 권한을 추가합니다.\nallow service objectstorage-\u0026lt;region_name\u0026gt; to manage object-family in compartment \u0026lt;compartment_name\u0026gt; 예시: 이름, object-storage-lifecycle-policy\nallow service objectstorage-ap-chuncheon-1 to manage object-family in compartment oci-hol-xx Step 2. 수명 주기 설정하기 OCI 콘솔에 로그인합니다.\n수명 주기를 설정할 Bucket(예시, ExampleBucket)의 상세 페이지로 이동합니다.\n왼쪽 아래의 Resources \u0026gt; Lifecycle Policy Rules를 클릭합니다.\n새로운 규칙을 생성하기 위해 Create Rule을 클릭합니다.\n수명 주기 규칙 생성\nName: 생성할 규칙의 이름을 입력합니다.\nLifecycle Action:\nMove to Archive, Move to Infrequent Access, Delete 중 원하는 것을 선택 Number of Days: 생성 후 며칠 지난 객체 적용할 지 기간 설정\nObject Name Filters: Bucket 상의 대상 객체에 대한 조건을 지정하는 영역입니다. 현재는 아무 설정하지 않습니다.\nimages/oracle.png 처럼 객체 이름에서 슬래쉬 기준 \u0026lsquo;images/\u0026lsquo;이 Prefix입니다. Prefix를 통해 대상 객체에 대한 조건을 지정할 수 있습니다. 설정 후 Create 클릭합니다.\n수명 주기 규칙이 설정되었습니다.\nStep 3. 수명주기 변경결과 확인하기 업로드후 Standard Tier에 있던, 오브젝트를 10일후에 다시 확인합니다.\n설정한 규칙이 적용되어 그림과 같이 Archive Tier로 변경된 것을 확인할 수 있습니다.\nArchived 상태에서는 다운로드가 불가하여 Public Bucket에서 객체에 URL로 접근해도 다음과 같이 오류가 발생합니다. Step 4. 복구 Archived 상태에서 다시 다운로드 받기 위해서는 대상 객체를 Restore 하면 됩니다. 다운로드 가능한 시간은 미입력시 기본적으로 24시간동안이며, 변경코자 하면 1시간에서 240시간 범위내에서 지정할 수 있습니다. 복구후 지정한 시간동안(기본 24시간)만 다운로드 가능하고 그 이후는 다시 Archive 상태가 됩니다. Restore하면 아래와 같이 상태가 표시되면, 완료될때 까지 시간이 좀 걸립니다. 한 번 밖에 테스트 해보진 않았지만, 걸린 시간은 다음과 같습니다.\n요청 시각: 09:32:00\n완료 시각: 10:22:00 (남은 다운로드 시간으로 추정)\n걸린 시간: 약 50분 (추정) 복구가 완료되면 이제 다운로드 받을 수 있으며, Public Bucket인 경우 객체에 URL로 접근도 가능하게 됩니다. 오브젝트의 상세 정보를 보면, 설정한 시간 기준으로 다운로드 가능 남은 시간이 그림과 같이 보입니다. ","lastmod":"2022-01-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter07/7/","tags":["object storage","lifecycle","archive"],"title":"7.7 수명주기를 통한 Archive 설정하기"},{"categories":null,"contents":"3.6.7 Block Volume 삭제하기 주의사항 Block Volume을 Terminate 하면 완전히 삭제됩니다. 복구할 수 없으니 이점 유의합니다. OCI 콘솔에서 내비게이션 메뉴를 엽니다. Storage \u0026gt; Block Storage \u0026gt; Block Volumes 항목으로 이동합니다.\nBlock Volume 옆에 있는 액션 아이콘을 클릭한 다음 Terminate를 클릭합니다.\n재확인\nTerminate되면 복구할 수 없습니다. 그래도 삭제하려면 Terminate를 클릭합니다.\nTerminate 완료\n","lastmod":"2022-01-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter03/6/7/","tags":null,"title":"3.6.7 Block Volume 삭제하기"},{"categories":null,"contents":"1.2.4.1 Kubernetes 버전 업그레이드 OKE 지원 버전 Kubernetes 지원 버전\nKubernetes 버전은 x.y.z로 표현되며, 각각 x는 메이저, y는 마이너, z는 패치 버전을 뜻하며, 오픈소스 Kubernetes도 현재 버전과 그 이전 2개 버전까지를 지원하고 있습니다. OKE 또한 OKE가 지원하는 최신버전 기준으로 그 이전 2개의 마이너 버전까지 지원하고 있습니다. 금일자 기준 지원하는 버전은 다음과 같습니다.\n1.26.2 1.25.4 1.24.1 신규 버전 출시 후에 30일 동안만 그 이전 버전중 제일 낮은 버전을 지원하고 있습니다. 예를 들어 OKE에서 1.26.2 버전이 2023년 4월 26일에 출시되고 기존 버전인 1.23.4는 통상 30일 기준으로, 2023년 6월 1일까지만 지원합니다. 현재 지원 버전은 다음 링크를 참조합니다.\nOCI Documentation \u0026gt; Container Engine \u0026gt; Supported Versions of Kubernetes 버전 관리 단위\nOKE 클러스터는 Control Plane과 Worker Node로 구성되며, Control Plane의 버전이 정해지면, Worker Node는 Control Plane과 같거나 그 이하 버전을 사용할 수 있습니다. 물론 호환되는 버전 안에서 사용할 수 있습니다.\nWorker Node는 Node Pool 단위로 버전을 가질 수 있습니다. 여러 Node Pool을 만들어 각각 다른 버전을 가질 수 있습니다.\nNode Pool 단위 버전 혼합\n테스트를 위해 1.24.1, 1.25.4, 1.26.2 버전이 사용 가능한 상태에서 1.25.4 버전의 OKE 클러스터를 기준으로 테스트를 진행합니다.\n테스트 환경 OKE 클러스터 - oke-cluster-1 버전: 1.25.4 Control Plane 버전: 1.25.4 Node Pool - poo1 버전: 1.25.4 OKE 클러스터 상세화면으로 이동합니다.\n왼쪽 아래 Resources \u0026gt; Node Pools 로 이동합니다.\n현재 pool1이 1.25.4 버전이 있습니다. 추가 Node Pool 생성을 위해 Add Node Pool을 클릭합니다.\n새 Node Pool 생성을 위한 정보를 입력합니다.\nName: 새 Node Pool 이름\nVersion: 일반 버전 혼합을 확인하기 위해 1.24.1 버전은 선택합니다.\n사용 가능한 버전을 보면, 현재 OKE 클러스터 버전 이하만 선택 가능한 걸 알 수 있습니다. 대상 클러스터의 Control Plane의 상위 버전인 1.26.2 버전은 보이지 않습니다. 나머지 항목은 처음 생성시와 비슷하게 생성시 필요한 정보를 입력합니다. 다음은 예시입니다. Placement Configuration: Worker Node가 위치한 AD와 Node용 서브넷 지정 Shape: VM.Standard.E3.Flex 제일 아래 Advanced Options: Add an SSH Key: Node에 SSH로 접근하기 위한 Publich Key 생성을 요청하면 실제 Node VM이 만들어지고, 준비되는 데 까지 앞서 설치시와 같이 약간의 시간이 걸립니다.\nNode Pool을 추가 생성하면 그림과 같이 동일 OKE 클러스터에 두 가지 버전의 혼합을 지원하여, 앞서 Node Pool 추가시 본것 처럼 Pool 단위 Node Shape, 위치(AD, 서브넷)을 달리 할 수 있습니다.\nkubectl로 노드를 조회해도 동일한 결과가 나옵니다.\n$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.107 Ready node 19h v1.25.4 oke-cluster-1 10.0.10.143 Ready node 19h v1.25.4 oke-cluster-1 10.0.10.39 Ready node 19h v1.25.4 oke-cluster-1 10.0.10.120 Ready node 2m9s v1.24.1 pool2 10.0.10.21 Ready node 2m13s v1.24.1 pool2 10.0.10.8 Ready node 2m22s v1.24.1 pool2 OKE 클러스터 버전 업그레이드 업그레이드 절차\nOKE 새 버전이 출시되면 버전 업그레이드는 다음 절차를 따릅니다.\nControl Plane 업그레이드 OCI 서비스 콘솔에서 OKE 클러스터 단위를 업그레이드하면 Control Plane만 업그레이드됨 오라클이 관리하는 영역으로 다운타임 없이 자동으로 업그레이드 됨 OKE 클러스터를 업그레이드 하면, 즉 Control Plane이 업그레이드하면 이전 버전으로 다시 다운그레이드 할 수 없음 Worker Node 업그레이드 OCI 서비스 콘솔에서 Node Pool 단위로 업그레이드 업그레이드 방식 in-place 업그레이드 대상이 되는 기존 Node Pool은 OCI 콘솔에서 버전 업그레이드, 이미 만들어진 기존 Node들이 자동으로 업그레이드 되지 않음 kubectl drain 명령으로 특정 노드를 컨테이너 스케줄에서 제외함 OCI 서비스 콘솔에서 drain한 Node를 종료(Terminate) 시킴 Node 자가치유에 의해 신규 Node가 자동으로 생성되며, 신규노드는 Node Pool에서 지정한 업그레이드 된 버전으로 생성됨 해당 Node Pool에 있는 나머지 기존 노드에 대해서도 동일한 순서대로 모두 진행 out-of-place 업그레이드 신규 버전의 Node Pool 추가 생성 kubectl 명령으로 기존 Node Pool에 있는 Node 제거, Kubernetes에 의해 컨테이너가 모두 이동하면, Node 삭제하는 방식으로 하나씩 진행 기존 Node가 모두 제거되면, 기존 Node Pool 삭제 Control Plane 업그레이드 위와 같이 1.25.4 버전을 사용 중에 새로운 버전이 출시되었다고 가정합니다. 그러면 앞서 설명한 것과 같이 기술지원 정책에 따라 기존 버전은 30일간 지원하기 때문에, 그동안 버전 검증후 업그레이드가 필요합니다.\nOCI Documentation \u0026gt; Container Engine \u0026gt; Supported Versions of Kubernetes 업그레이드가 가능하면, OKE 클러스터 상세 화면에서 New Kubernetes vesion available 버튼이 활성화 됩니다.\n버튼을 클릭하면 다음과 같이 안내 문구와 함께 업그레이드를 시작할 수 있습니다. 최신 버전인 v1.26.2를 선택하도록 하겠습니다.\n버전을 선택하고 아래 Upgrade 버튼을 클릭하여 업그레이드를 시작합니다.\n클러스터 상태가 UPDATING으로 표시되고 업그레이드가 진행됩니다. 오라클이 관리하는 Control Plane에 대한 업그레이드가 시작되며, 내부적으로 순차적으로 진행됩니다. 실제 애플리케이션이 동작하는 Worker Node는 서비스 중지 없이 업그레이드 됩니다.\n테스트 시점에는 10~15분 후에 업그레이드 완료되었습니다.\nWorker Node 업그레이드 - in-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. in-place 업그레이드 방식은 Node Pool 업그레이드 -\u0026gt; 기존 Node Drain \u0026gt; POD 이동 확인 -\u0026gt; 기존 Node VM 종료 순으로 업그레이드 합니다.\nNode Pool 업그레이드\n업그레이드 하려는 Node Pool의 상세 페이지로 이동합니다.\n수정을 위해 Edit를 클릭하면, 오른쪽에 수정 페이지가 뜹니다.\nVersion 항목에, 클러스터 버전과 Node Pool의 버전이 표시되며, 업그레이드 가능한 버전이 표시됩니다.\n클러스터와 동일한 1.26.2로 선택하고 Save Change를 클릭하여 저장합니다.\nResources \u0026gt; Work Requests에 가서 보면, 15초 정도 지난뒤 Node Pool 업그레이드가 완료됩니다.\n아직 실제 Worker Node가 업그레이드 된 것은 아닙니다.\nResources \u0026gt; Nodes에 가서 보면 기존 버전 그대로입니다.\nNode Drain 시키기 - kubectl 명령 사용\nkubectl 명령으로 Worker Node와 배포된 POD를 확인합니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.107 Ready node 19h v1.25.4 10.0.10.143 Ready node 19h v1.25.4 10.0.10.39 Ready node 19h v1.25.4 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-5fd5849fd4-jst2z 1/1 Running 0 17h 10.244.0.133 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-docker-hub-5bfd857f89-9rh8q 1/1 Running 0 18h 10.244.1.3 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-kdh9j 1/1 Running 0 9s 10.244.0.138 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-rgd44 1/1 Running 0 9s 10.244.1.8 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-t5f2b 1/1 Running 0 9s 10.244.0.10 10.0.10.107 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-ocir-56d7b8d55c-65d22 1/1 Running 0 17h 10.244.0.5 10.0.10.107 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets --delete-emptydir-data 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\nemptydir을 사용하는 Pod가 있는 경우 drain시 에러가 발생합니다. emptydir을 임시데이터를 사용하는 용도임으로 \u0026ndash;delete-emptydir-data 옵션을 통해 drain 시킵니다. 테스트시는 elasticsearch가 설치된 상태에서 관련 이슈가 발생하였습니다.\n참고: https://pet2cattle.com/2021/08/cannot-delete-pods-with-local-storage\n$ kubectl drain 10.0.10.107 --ignore-daemonsets --delete-emptydir-data node/10.0.10.107 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-xqf8t, kube-system/kube-flannel-ds-c27k6, kube-system/kube-proxy-6dbvv, kube-system/proxymux-client-p9wrj evicting pod kube-system/kube-dns-autoscaler-59c7bdff6d-x6kj5 evicting pod default/nginx-fss-pvc-56695486fd-t5f2b evicting pod default/nginx-ocir-56d7b8d55c-65d22 evicting pod kube-system/coredns-68db8f8bcc-dg698 pod/kube-dns-autoscaler-59c7bdff6d-x6kj5 evicted pod/nginx-ocir-56d7b8d55c-65d22 evicted pod/nginx-fss-pvc-56695486fd-t5f2b evicted pod/coredns-68db8f8bcc-dg698 evicted node/10.0.10.107 drained 아래와 같이 107번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.107 Ready,SchedulingDisabled node 20h v1.25.4 10.0.10.143 Ready node 20h v1.25.4 10.0.10.39 Ready node 20h v1.25.4 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-5fd5849fd4-jst2z 1/1 Running 0 17h 10.244.0.133 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-docker-hub-5bfd857f89-9rh8q 1/1 Running 0 18h 10.244.1.3 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-kdh9j 1/1 Running 0 106s 10.244.0.138 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-rgd44 1/1 Running 0 106s 10.244.1.8 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-w2l8j 1/1 Running 0 45s 10.244.0.139 10.0.10.143 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-ocir-56d7b8d55c-7kf7k 1/1 Running 0 45s 10.244.1.9 10.0.10.39 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 종료할 Node의 노드 이름을 확인합니다.\n$ kubectl get nodes -L displayName NAME STATUS ROLES AGE VERSION DISPLAYNAME 10.0.10.107 Ready,SchedulingDisabled node 20h v1.25.4 oke-c4vz7x3dqla-n6c3mexun6a-s2b2igkyisa-1 10.0.10.143 Ready node 20h v1.25.4 oke-c4vz7x3dqla-n6c3mexun6a-s2b2igkyisa-2 10.0.10.39 Ready node 20h v1.25.4 oke-c4vz7x3dqla-n6c3mexun6a-s2b2igkyisa-0 Node Pool의 Nodes 목록에서 종료할 노드의 액션 메뉴에서 Delete node를 선택합니다.\n팝업 창이 뜹니다. 노드 삭제후, 신규 노드가 다시 생성되도록 Do not decrese node pool size를 선택한 후 Delete를 클릭합니다.\nNode 자가치유에 의해 신규 Node가 자동으로 생성됩니다. 생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전으로 생성됩니다.\nkubectl 명령으로 노드를 조회하면, 1.26.2 버전으로 신규 노드(.211번 노드)가 생성되었습니다.\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.143 Ready node 20h v1.25.4 10.0.10.143 \u0026lt;none\u0026gt; Oracle Linux Server 8.7 5.15.0-6.80.3.1.el8uek.x86_64 cri-o://1.25.1-111.el8 10.0.10.211 Ready node 2m14s v1.26.2 10.0.10.211 \u0026lt;none\u0026gt; Oracle Linux Server 8.7 5.15.0-6.80.3.1.el8uek.x86_64 cri-o://1.26.2-142.el8 10.0.10.39 Ready node 20h v1.25.4 10.0.10.39 \u0026lt;none\u0026gt; Oracle Linux Server 8.7 5.15.0-6.80.3.1.el8uek.x86_64 cri-o://1.25.1-111.el8 나머지 기존 노드에 대해서 순서대로 모두 진행합니다.\n완료 결과\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.128 Ready node 6m10s v1.20.11 10.0.10.128 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.146 Ready node 35s v1.20.11 10.0.10.146 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.170 Ready node 23m v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 Node Drain 시키기 - OCI 콘솔 UI 사용\nOCI Documentation \u0026gt; Release Notes \u0026gt; Support for worker node deletion, along with new cordon and drain options 기능이 2022년 6월 28일 출시되어 Worker Node 삭제시 Cordon, Drain 옵션이 추가 되었습니다. 앞서 커맨드로 실행했던 작업을 콘솔 UI에서 Node 삭제시 자동으로 함께 수행할 수 있게 되었습니다.\nNode Pool에서 대상 노드에 대해 액션 메뉴에서 Delete node를 선택합니다.\n팝업 창이 뜹니다. 노드 삭제후, 신규 노드가 다시 생성되도록 Do not decrese node pool size를 선택한 후 Delete를 클릭합니다. 창에 표시된 것처럼 Cordon and drain이 자동으로 수행된후 Node가 삭제됩니다.\n아래와 같이 Node가 스케줄에서 제외되는 것을 볼 수 있습니다.\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.211 Ready node 18m v1.26.2 10.0.10.253 Ready node 12m v1.26.2 10.0.10.39 Ready,SchedulingDisabled node 20h v1.25.4 $ ... $ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.211 Ready node 22m v1.26.2 10.0.10.253 Ready node 16m v1.26.2 10.0.10.73 Ready node 82s v1.26.2 커맨드로 노드 Drain을 수작업으로 수행할지, 콘솔 UI에서 수행할 지는 관리 선호도에 따라 선택하여 사용하면 됩니다.\nWorker Node 업그레이드 - out-of-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. out-of-place 업그레이드 방식은 업그레이드 버전의 Node Pool 신규 생성 -\u0026gt; 기존 Node Pool의 모든 노드 Drain -\u0026gt; 기존 Node Pool 삭제 순으로 업그레이드 합니다.\n새 버전의 Node Pool 만들기\nOKE 클러스터 상세 페이지로 이동합니다.\nResources \u0026gt; Node Pools 로 이동합니다.\n그림과 같이 기존 버전의 Node Pool이 있는 상태에서 신규 Node Pool 추가를 위해 Add Node Pool을 클릭합니다.\n신규 Node Pool 정보를 입력하여 생성합니다.\nName Version: 새 버전 선택 Placement Configuration AD Subnet: Worker Node 서브넷 선택 Shape and image: Node VM 유형 Node count: 노드 수 제일 아래 Advanced Options: Add an SSH key: Node VM에 SSH 접속시 사용할 키의 Private Key Add를 클릭하여 Node Pool을 생성합니다.\n신규 Node가 모두 준비 될때 까지 기다립니다.\n기존 Node Pool의 모든 노드 Drain\n구동 중인 앱들이 기존 Node Pool에서 동작하고 있습니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-5fd5849fd4-q5dwd 1/1 Running 0 29m 10.244.1.136 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-docker-hub-5bfd857f89-9pljn 1/1 Running 0 29m 10.244.1.139 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-bdgnl 1/1 Running 0 29m 10.244.1.132 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-zbgg9 1/1 Running 0 29m 10.244.1.131 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-zv2v6 1/1 Running 0 29m 10.244.1.137 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-ocir-56d7b8d55c-jqrz7 1/1 Running 0 29m 10.244.1.130 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.117 Ready node 19m v1.25.4 pool1 10.0.10.126 Ready node 19m v1.25.4 pool1 10.0.10.193 Ready node 19m v1.25.4 pool1 10.0.10.115 Ready node 5m17s v1.26.2 pool2 10.0.10.136 Ready node 5m14s v1.26.2 pool2 10.0.10.25 Ready node 5m23s v1.26.2 pool2 아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets --delete-emptydir-data 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\n$ kubectl drain 10.0.10.117 --ignore-daemonsets --delete-emptydir-data node/10.0.10.117 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-wfzll, kube-system/kube-flannel-ds-95kzs, kube-system/kube-proxy-fmb4d, kube-system/proxymux-client-xvl4p evicting pod kube-system/coredns-68db8f8bcc-l6c2p pod/coredns-68db8f8bcc-l6c2p evicted node/10.0.10.117 drained 아래와 같이 117번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\n$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.117 Ready,SchedulingDisabled node 21m v1.25.4 pool1 10.0.10.126 Ready node 21m v1.25.4 pool1 10.0.10.193 Ready node 21m v1.25.4 pool1 10.0.10.115 Ready node 7m33s v1.26.2 pool2 10.0.10.136 Ready node 7m30s v1.26.2 pool2 10.0.10.25 Ready node 7m39s v1.26.2 pool2 $ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-5fd5849fd4-q5dwd 1/1 Running 0 32m 10.244.1.136 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-docker-hub-5bfd857f89-9pljn 1/1 Running 0 32m 10.244.1.139 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-bdgnl 1/1 Running 0 32m 10.244.1.132 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-zbgg9 1/1 Running 0 32m 10.244.1.131 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-56695486fd-zv2v6 1/1 Running 0 32m 10.244.1.137 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-ocir-56d7b8d55c-jqrz7 1/1 Running 0 32m 10.244.1.130 10.0.10.193 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 나머지 기존 Node Pool에 있는 Node들도 drain합니다.\n$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.117 Ready,SchedulingDisabled node 22m v1.25.4 pool1 10.0.10.126 Ready,SchedulingDisabled node 22m v1.25.4 pool1 10.0.10.193 Ready,SchedulingDisabled node 23m v1.25.4 pool1 10.0.10.115 Ready node 8m49s v1.26.2 pool2 10.0.10.136 Ready node 8m46s v1.26.2 pool2 10.0.10.25 Ready node 8m55s v1.26.2 pool2 기존 Node Pool 삭제\n기존 Node Pool에 있는 모든 Node들이 drain되어 더이상 사용되지 않습니다.\nOCI 서비스 콘솔에서 OKE 클러스터 상세페이지로 이동합니다.\nResources \u0026gt; Node Pools로 이동하여 기존 Node Pool을 삭제합니다.\n팝업 창이 뜹니다. 확인후 Delete를 클릭합니다.\n업그레이드가 완료되었습니다.\n$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.115 Ready node 16m v1.26.2 pool2 10.0.10.136 Ready node 16m v1.26.2 pool2 10.0.10.25 Ready node 16m v1.26.2 pool2 기존 Node Pool의 모든 노드 Drain - OCI 콘솔 UI 사용\nNode Pool 삭제 팝업 창에서 본건 처럼 Node Pool 삭제시에도 신규 기능을 통해 속한 Node에 대해 Cordon, Drain 기능을 제공합니다.\n이 또한 커맨드로 노드 Drain을 수작업으로 수행한 후, 최종적으로 Node Pool을 삭제하는 방식을 사용할지, 단번에 OCI 콘솔에서 Node Pool을 삭제하여 자동으로 Cordon, Drain을 수행하게 할 지는 관리자 선호도에 따라 선택하여 사용하면 됩니다. 업그레이드 시 발생하는 문제 등을 고려하여 관리자에 따라 커맨드를 선호할 수도 있을 것 같아 함께 표시하였습니다.\n","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/fundamentals/management/1.supported-version-and-upgrade/","tags":["oke"],"title":"1.2.4.1 Kubernetes 버전 업그레이드"},{"categories":null,"contents":"14.3.7 OCI Terraform 사용 예제 오라클에서 OCI Github에서 다양한 예제를 제공하고 있습니다. 다음 사이트를 참고하세요.\nterraform-provider-oci/examples ","lastmod":"2019-04-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci/chapter14/3/7/","tags":["terraform","example"],"title":"14.3.7 OCI Terraform 사용 예제"},{"categories":null,"contents":"1.6.1 Secret Store로 OCI Vault 사용하기 OKE에서는 Secret를 etcd에서 관리, 저장하며, 암호화 키를 통해 암호화해서 저장하고 있습니다. 더불어 Secret이 YAML상에 포함되어 사용되는 경우가 많은데, 이것이 보안상의 문제가 우려되는 경우, 외부 Secret 저장소를 사용하여 관리하고자 하는 요구사항이 있습니다. OKE에서는 OCI Secrets Store CSI Driver Provider를 통해 OCI Vault에서 Secret을 저장하고 사용하는 것을 지원하고 있습니다.\n참고 문서 - OKE Documentation \u0026gt; Managing Secrets for Kubernetes Clusters 아래 과정은 OCI Vault을 Secret Store로 사용하기 위해 OCI Secrets Store CSI Driver Provider를 OKE 클러스터에 설치, 구성해서 사용하는 과정입니다. OCI Secrets Store CSI Driver Provider에 대한 추가적인 항목에 대한 설명은 공식 GitHub 저장소를 참조하세요.\nVault에 Secret 생성하기 OCI Vault 만들기\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Vault로 이동합니다.\nCreate Vault를 클릭합니다.\n위치할 Compartment를 선택하고, 이름을 입력하여 생성합니다.\n생성된 Vault를 클릭합니다.\n마스터 키 만들기\n먼저 마스터키 생성을 위해 Resources \u0026gt; Master Encryption Keys로 이동합니다.\nCreate Key를 클릭합니다.\n마스터 키 생성 정보를 입력합니다.\nCreate in Compartment: 위치할 Compartment 지정 Protection Mode: HSM(Hardware Security Module) 또는 Software 중에 선택, Software는 무료 Name: 원하는 이름 입력 Key Shape Algorithm: Secret 생성시 사용할 것이므로 대칭키인 AES를 선택합니다. Create Key를 클릭하여 키를 생성합니다.\n암호를 저장할 Secret 만들기\nSecret 생성을 위해 Resources \u0026gt; Secrets로 이동합니다.\nCreate Secret을 클릭합니다.\nSecret 생성 정보를 입력합니다.\nName: 이름을 입력합니다. 이후 OKE 클러스터에 설치될 Secrets Store Provider에서 이 이름으로 조회하게 됩니다. Encryption Key: 앞서 생성한 마스터키 선택 Secret Contents: 저장할 패스워드를 입력 Create Secret을 클릭하여 생성합니다.\n같은 방법으로 secret2도 생성합니다.\nSecrets Store CSI Driver Provider for OCI Vault 설치 Helm 또는 YAML로 대상 OKE 클러스터에 설치 가능합니다. 여기서는 Helm으로 설치합니다.\nhelm repo add oci-provider https://oracle-samples.github.io/oci-secrets-store-csi-driver-provider/charts helm install oci-provider oci-provider/oci-secrets-store-csi-driver-provider --namespace kube-system 실행예시 $ helm repo add oci-provider https://oracle-samples.github.io/oci-secrets-store-csi-driver-provider/charts \u0026#34;oci-provider\u0026#34; has been added to your repositories $ helm install oci-provider oci-provider/oci-secrets-store-csi-driver-provider --namespace kube-system NAME: oci-provider LAST DEPLOYED: Wed May 3 01:26:51 2023 NAMESPACE: kube-system STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The Secrets Store CSI Driver and the provider are getting deployed to your cluster. Verify DaemonSets readiness: kubectl get daemonset \\ --namespace kube-system \\ --selector=\u0026#39;app.kubernetes.io/name in (oci-secrets-store-csi-driver-provider, secrets-store-csi-driver)\u0026#39; $ kubectl get daemonset \\ \u0026gt; --namespace kube-system \\ \u0026gt; --selector=\u0026#39;app.kubernetes.io/name in (oci-secrets-store-csi-driver-provider, secrets-store-csi-driver)\u0026#39; NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE oci-provider-secrets-store-csi-driver 3 3 3 3 3 kubernetes.io/os=linux 2m3s oci-secrets-store-csi-driver-provider 3 3 3 3 3 \u0026lt;none\u0026gt; 2m3s #1. Instance Principal을 사용하는 경우 OKE 클러스터의 Worker Node인 Compute 인스턴스에 사용할 OCI Vault에 접근 권한을 부여하는 방법입니다.\nPolicy 설정\nOCI 콘솔로 이동합니다.\nDynamic Group을 생성합니다.\nName: 이름을 입력합니다, 예, oke-worker-nodes-dynamic-group Matching Rules: Worker Node가 속한 Compartment의 ID로 변경하여 생성합니다. Any {instance.compartment.id = \u0026#39;ocid1.compartment.oc1..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\u0026#39;} Policy를 생성합니다.\nName: 이름을 입력합니다, 예, oke-worker-nodes-to-vault-policy Policy: 만든 Dynamic Group 이름, OCI Secret이 있는 compartment-name, 사용할 OCI Vault OCID로 변경하여 생성합니다. allow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use secret-family in compartment \u0026lt;compartment-name\u0026gt; where target.vault.id = \u0026#39;ocid1.vault.oc1..aaaaaaaaaaaaaaa\u0026#39; SecretProviderClass 설정\nSecretProviderClass 생성을 위한 파일을 생성합니다.\n파일이름 예, secret-provider-class.yaml 대상 OCI Vault에 있는 secret1과 secret2을 가져오고, secret2는 컨테이너에 app1-db-password 파일이름으로 마운트해 사용하는 예시입니다. apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: test-oci-provider-class spec: provider: oci parameters: # provider-specific parameters secrets: | - name: secret1 # Name of the secret in vault - name: secret2 fileName: app1-db-password # Secret will be mounted with this name instead of secret name authType: instance # possible values are: user, instance vaultId: ocid1.vault.oc1..aaaaaaaaaaaaaaa 작성한 파일을 배포합니다.\nkubectl apply -f secret-provider-class.yaml 테스트 앱 배포\n테스트할 앱을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/oracle-samples/oci-secrets-store-csi-driver-provider/main/deploy/example/app.deployment.yaml 해당 YAML 파일을 보면, 작성한 SecretProviderClass을 아래와 같이 volume으로 추가하고 있습니다.\nvolumes: - name: some-creds csi: driver: \u0026#39;secrets-store.csi.k8s.io\u0026#39; readOnly: true volumeAttributes: secretProviderClass: \u0026#39;test-oci-provider-class\u0026#39; # here we reference particular SecretProviderClass 그리고 컨테이너 Pod의 /mnt/secrets-store 경로에 마운트하고 있습니다.\nvolumeMounts: - name: \u0026#39;some-creds\u0026#39; mountPath: \u0026#39;/mnt/secrets-store\u0026#39; # here are mounted secrets readOnly: true 다운로드 받은 테스트 앱을 배포합니다.\nkubectl apply -f app.deployment.yaml 마운트가 성공하면, Pod가 RUNNING 상태가 됩니다. 실패시 권한 및 OCI Vault에 Secret이 있는지 등을 확인합니다.\n$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-5fd7f7876d-vtpgp 1/1 Running 0 23m 다음 명령으로 컨테이너에 Secret이 잘 마운트 되었는지 확인합니다.\nkubectl exec -it deployment.apps/nginx -- sh; ls /mnt/secrets-store/; cat /mnt/secrets-store/secret1; echo cat /mnt/secrets-store/app1-db-password; echo exit; 실행예시 대상 위치에서 파일로 확인할 수 있으며, 두번째 secret은 이름이 변경되어 마운트된 것을 알 수 있습니다. $ kubectl exec -it deployment.apps/nginx -- sh; / # ls /mnt/secrets-store/; app1-db-password secret1 / # cat /mnt/secrets-store/secret1; echo test-password-1 / # cat /mnt/secrets-store/app1-db-password; echo test-password-2 / # exit; $ #2. User Principal을 사용하는 경우 User 인증을 통해서 OCI Vault에 접근 권한을 부여하는 방법입니다. OKE 클러스터 전체단위가 아닌 부분단위로 개별 유저로 사용할 수 있겠습니다.\nPolicy 설정\nOCI 콘솔로 이동합니다.\nPolicy를 생성합니다.\nName: 이름을 입력합니다, 예, oke-user-to-vault-policy Policy: User OCID, OCI Secret이 있는 compartment-name, 사용할 OCI Vault OCID로 변경하여 생성합니다. allow any-user to use secret-family in compartment \u0026lt;compartment-name\u0026gt; where all { request.user.id = \u0026#39;ocid1.user.oc1..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\u0026#39;, target.vault.id = \u0026#39;ocid1.vault.oc1..aaaaaaaaaaaaaaa\u0026#39; } OKE 클러스터에 User에 대한 Secret 만들기\nUser 인증에 사용할 User에 대한 API Key가 없는 경우에 OCI 콘솔에서 생성합니다.\nResources \u0026gt; API Keys로 이동합니다. Add API Key를 클릭하여 API Key로 사용할 키쌍을 생성합니다. 미리보기로 뜨는 Configuration File Preview을 사용해 인증파일 작성에 사용합니다. API Key 기반 User 인증을 위한 파일을 생성합니다,\n파일이름 예, user-auth-config-example.yaml auth: region: ap-chuncheon-1 tenancy: ocid1.tenancy.oc1..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa user: ocid1.user.oc1..aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa fingerprint: 12:bf:17:7b:5f:e0:7d:13:75:11:d6:39:0d:e2:84:74 설정 파일과 API Key - Private Key를 사용하여 Secret을 만듭니다.\nprivate-key: API Key 경로 입력 namespace: OKE 클러스터상의 컨테이너가 있는 namespace kubectl create secret generic oci-config \\ --from-file=config=user-auth-config-example.yaml \\ --from-file=private-key=./oci_api_key.pem \\ --namespace \u0026lt;workload-namespace\u0026gt; SecretProviderClass 설정\nSecretProviderClass 생성을 위한 파일을 생성합니다.\n파일이름 예, secret-provider-class.yaml 대상 OCI Vault에 있는 secret1과 secret2을 가져오고, secret2는 컨테이너에 app1-db-password 파일이름으로 마운트해 사용하는 예시입니다. authType은 user 로 변경합니다. authSecretName: oci-config 을 새로 추가하여 방금 생성한 Secret으로 지정합니다. apiVersion: secrets-store.csi.x-k8s.io/v1 kind: SecretProviderClass metadata: name: test-oci-provider-class spec: provider: oci parameters: # provider-specific parameters secrets: | - name: secret1 # Name of the secret in vault - name: secret2 fileName: app1-db-password # Secret will be mounted with this name instead of secret name authType: user # possible values are: user, instance authSecretName: oci-config # required only for user authType vaultId: ocid1.vault.oc1..aaaaaaaaaaaaaaa 작성한 파일을 배포합니다.\nkubectl apply -f secret-provider-class.yaml 테스트 앱 배포\n테스트 절차는 #1. Instance Principal과 동일합니다.\n테스트할 앱을 다운로드 받습니다.\nwget https://raw.githubusercontent.com/oracle-samples/oci-secrets-store-csi-driver-provider/main/deploy/example/app.deployment.yaml 다운로드 받은 테스트 앱을 배포합니다. 또는 이미 배포된 경우 Pod를 삭제하고 다시 시작합니다.\nkubectl apply -f app.deployment.yaml kubectl delete pod -l app=nginx 다음 명령으로 컨테이너에 Secret이 잘 마운트 되었는지 확인합니다.\nkubectl exec -it deployment.apps/nginx -- sh; ls /mnt/secrets-store/; cat /mnt/secrets-store/secret1; echo cat /mnt/secrets-store/app1-db-password; echo exit; 실행예시 대상 위치에서 파일로 확인할 수 있으며, 두번째 secret은 이름이 변경되어 마운트된 것을 알 수 있습니다. $ kubectl exec -it deployment.apps/nginx -- sh; / # ls /mnt/secrets-store/; app1-db-password secret1 / # cat /mnt/secrets-store/secret1; echo test-password-1 / # cat /mnt/secrets-store/app1-db-password; echo test-password-2 / # exit; $ 참고 oci-secrets-store-csi-driver-provider on GitHub ","lastmod":"2023-05-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/security/1.secret-store/","tags":["oke","secret store","vault"],"title":"1.6.1 Secret Store로 OCI Vault 사용하기"},{"categories":null,"contents":"Cache with Redis is now available Release Notes - Cache with Redis is now available 업데이트 사항 OCI Cache with Redis 서비스는 오프소스인 Redis를 OCI에서 제공하는 클라우드 서비스입니다.\n자세한 사항은 다음을 참고합니다.\n10.1. OCI Cache with Redis 사용하기 ","lastmod":"2023-10-17T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20231017-cache-with-redis-is-now-available/","tags":["cache","redis"],"title":"Cache with Redis is now available"},{"categories":null,"contents":"Support for Kubernetes volume clones Release Notes - Support for Kubernetes volume clones 업데이트 사항 이제 CSI volume plugin은 Kubernetes 볼륨 복제를 지원합니다. 내부적으로는 OCI Block Volume 서비스의 복제 기능을 활용합니다.\nKubernetes 볼륨 복제는 스토리지 시스템에 있는 기존 영구 볼륨의 정확한 복제본입니다. 기존 쿠버네티스 Persistent Volume을 복제하여 새로운 쿠버네티스 Persistent Volume 할당을 프로비저닝합니다. 새 쿠버네티스 Persistent Volume은 소스 데이터 복사본이지만, 소스 쿠버네티스 Persistent Volume과는 독립적입니다. 볼륨 복제를 사용하면 운영 환경에 영향을 주지 않고 구성 변경 사항을 신속하게 테스트할 수 있습니다.\n자세한 사항은 다음을 참고합니다.\n1.6.5 Kubernetes Volume Clone 만들기 ","lastmod":"2023-10-04T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20231004-support-for-kubernetes-volume-clones/","tags":["oke","volume clone"],"title":"Support for Kubernetes volume clones"},{"categories":null,"contents":"Support for Kubernetes volume snapshots Release Notes - Support for Kubernetes volume snapshots 업데이트 사항 이제 CSI volume plugin은 Kubernetes 볼륨 스냅샷을 지원합니다. 내부적으로는 OCI Block Volume 서비스의 백업 기능을 활용합니다. 볼륨 스냅샷은 재해 복구(DR) 전략의 하나로 사용할 수 있으며, 특정 시점에 기존 볼륨의 백업본을 생성한 다음 해당 백업본을 사용하여 새 볼륨을 프로비저닝할 수 있습니다.\n쿠버네티스 Persistent Volume에 대한 스냅샷을 요청하면, 동적으로 OCI Block Volume의 백업을 통해 스냅샷을 생성할 수 있습니다. 또는 기존 Block Volume 백업을 사용하여 정적으로 Kubernetes 볼륨 스냅샷에 바인딩할수도 있습니다.\n동적이던, 정적이던 볼륨 스냅샷을 생성한 후 볼륨 스냅샷을 쿠버네티스 Persistent Volume 청구에 대한 데이터 소스로 지정하여 새 Persistent Volume을 프로비저닝할 수 있습니다.\n자세한 사항은 다음을 참고합니다.\n1.6.4 Kubernetes Volume Snapshot 만들기 ","lastmod":"2023-06-20T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20230620-support-for-kubernetes-volume-snapshots/","tags":["oke","volume snapshot"],"title":"Support for Kubernetes volume snapshots"},{"categories":null,"contents":"Support to store image pull secrets in OCI Vault Release Notes - Support to store image pull secrets in OCI Vault 업데이트 사항 OCI Container Instance에서 컨테이너 이미지를 Private Repository에서 가져올때 Username, Password가 필요합니다. 이제 OCI Vault에 Username, Password를 저장하고, Container Instance에서는 imagePullSecrets 설정을 통해 OCI Vault내 Secret을 사용해 Private Repository내 컨테이너 이미지를 가져올 수 있습니다.\n자세한 사항은 다음을 참고합니다.\n8.3 Private Repository를 위한 image pull secret 사용하기 ","lastmod":"2023-05-09T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20230509-support-to-store-image-pull-secrets-in-oci-vault/","tags":["container instances","vault"],"title":"Support to store image pull secrets in OCI Vault"},{"categories":null,"contents":"Support for service mesh and performance improvements when using the VCN-Native CNI plugin Release Notes - Support for service mesh and performance improvements when using the VCN-Native CNI plugin 업데이트 사항 Release Notes - Support for VCN-native pod networking 출시 이후 기능이 업데이드되어 VCN-Native Pod Networking CNI 2.0이 출시 되었습니다.\nVCN-Native Pod Networking CNI에서 OCI Service Mesh, Istio, Linkerd 같은 Service Mesh 제품을 이제 지원합니다.\nWorker Node의 이미지를 Oracle Linux 7를 사용해야 합니다. (Oracle Linux 8은 지원 예정) VCN-Native Pod Networking CNI 1.0 대비하여 네트워크 성능 향상\n단, Worker Node의 버전이 Kubernetes 1.26 이상이어야 합니다.\nVCN-Native CNI 2.0에서 Service Mesh 지원 VCN-Native CNI 2.0에서 네트워크 성능 향상 테스트할 경우의 수가 여러가지 있을 수 있지만, 여기에서는 컨테이너 서비스간의 호출하는 경우에서, 두 컨테이너 Pod가 같은 Worker Node에 있을 때랑 서로 다른 Worker Node 상에 있는 CNI간 성능을 비교해 봅니다.\nFlannel CNI 사용시 Flannel CNI를 사용하는 OKE Cluster를 준비합니다.\n테스트용 nginx 컨테이너를 설치합니다.\nkubectl create deploy nginx --image=nginx kubectl scale deploy nginx --replicas=3 kubectl run ol1 --image=oraclelinux:8 --command -- tail -f /dev/null 설치 결과\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-76d6c9b8c-l4tpk 1/1 Running 0 25s 10.244.1.7 10.0.10.134 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-76d6c9b8c-rvcln 1/1 Running 0 28s 10.244.0.4 10.0.10.52 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-76d6c9b8c-s67x2 1/1 Running 0 25s 10.244.0.134 10.0.10.244 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ol1 1/1 Running 0 19s 10.244.0.6 10.0.10.52 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 같은 Worker Node에 있는 Pod간 테스트 - 같은 Node(10.0.10.52)에 있는 Pod(10.244.0.6) -\u0026gt; Pod(10.244.0.4)간 테스트\n$ kubectl exec -it ol1 -- ping -c 10 10.244.0.4 PING 10.244.0.4 (10.244.0.4) 56(84) bytes of data. 64 bytes from 10.244.0.4: icmp_seq=1 ttl=64 time=0.081 ms ... 64 bytes from 10.244.0.4: icmp_seq=10 ttl=64 time=0.065 ms --- 10.244.0.4 ping statistics --- 10 packets transmitted, 10 received, 0% packet loss, time 9227ms rtt min/avg/max/mdev = 0.065/0.069/0.081/0.007 ms 다른 Worker Node에 있는 Pod간 테스트 - Pod(10.244.0.6) on Node(10.0.10.52) -\u0026gt; Pod(10.244.0.134) on Node(10.0.10.244)\n$ kubectl exec -it ol1 -- ping -c 10 10.244.0.134 PING 10.244.0.134 (10.244.0.134) 56(84) bytes of data. 64 bytes from 10.244.0.134: icmp_seq=1 ttl=62 time=0.623 ms ... 64 bytes from 10.244.0.134: icmp_seq=10 ttl=62 time=0.572 ms --- 10.244.0.134 ping statistics --- 10 packets transmitted, 10 received, 0% packet loss, time 9229ms rtt min/avg/max/mdev = 0.461/0.543/0.623/0.052 ms VCN-Native CNI 2.0 사용시 VCN-Native CNI 2.0 사용하는 OKE Cluster v1.26 버전, Node Pool 이미지 Oracle Linux 7를 준비합니다.\n테스트용 nginx 컨테이너를 설치합니다.\nkubectl create deploy nginx --image=nginx kubectl scale deploy nginx --replicas=3 kubectl run ol1 --image=oraclelinux:8 --command -- tail -f /dev/null 설치 결과\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-748c667d99-cddnv 1/1 Running 0 68m 10.0.40.26 10.0.10.213 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-748c667d99-prmcb 1/1 Running 0 68m 10.0.40.152 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-748c667d99-rf47h 1/1 Running 0 68m 10.0.40.207 10.0.10.156 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ol1 1/1 Running 0 36m 10.0.40.227 10.0.10.93 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 같은 Worker Node에 있는 Pod간 테스트 - 같은 Node(10.0.10.93)에 있는 Pod(10.0.40.227) -\u0026gt; Pod(10.0.40.152)간 테스트\n$ kubectl exec -it ol1 -- ping -c 10 10.0.40.152 PING 10.0.40.152 (10.0.40.152) 56(84) bytes of data. 64 bytes from 10.0.40.152: icmp_seq=1 ttl=63 time=0.084 ms ... 64 bytes from 10.0.40.152: icmp_seq=10 ttl=63 time=0.057 ms --- 10.0.40.152 ping statistics --- 10 packets transmitted, 10 received, 0% packet loss, time 9241ms rtt min/avg/max/mdev = 0.039/0.060/0.084/0.014 ms 다른 Worker Node에 있는 Pod간 테스트 - Pod(10.0.40.227) on Node(10.0.10.93) -\u0026gt; Pod(10.0.40.207) on Node(10.0.10.156)\n$ kubectl exec -it ol1 -- ping -c 10 10.0.40.207 PING 10.0.40.207 (10.0.40.207) 56(84) bytes of data. 64 bytes from 10.0.40.207: icmp_seq=1 ttl=62 time=0.355 ms ... 64 bytes from 10.0.40.207: icmp_seq=10 ttl=62 time=0.351 ms --- 10.0.40.207 ping statistics --- 10 packets transmitted, 10 received, 0% packet loss, time 9247ms rtt min/avg/max/mdev = 0.304/0.352/0.373/0.028 ms 테스트 결과 - 10번 단순 테스트라 절대적인 성능 결과가 아닌 참고치로 보시기 바랍니다.\nFlannel CNI VCN-Native 2.0 CNI 같은 Worker Node에 있는 Pod간 평균 Ping 속도 0.069 ms 0.060 ms 다른 Worker Node에 있는 Pod간 평균 Ping 속도 0.543 ms 0.352 ms ","lastmod":"2023-04-26T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20230426-support-for-service-mesh-and-performance-improvements-when-using-the-vcn-native-cni-plugin/","tags":["oke","VCN-Native Pod Networking CNI"],"title":"Support for service mesh and performance improvements when using the VCN-Native CNI plugin"},{"categories":null,"contents":"3.9.1 Burstable Instances 사용하기 Burstable Instance는 Baseline의 기본 CPU 성능을 제공하며, 일시적으로 사용량이 늘어날때에는 더 높은 성능을 제공하는 가상 머신 인스턴스 타입입니다. 비용은 지정한 Baseline OCPU를 기준으로 과금됩니다.\n참고 문서\nhttps://docs.oracle.com/en-us/iaas/Content/Compute/References/burstable-instances.htm\nBurstable 인스턴스 만들기 OCI 콘솔에서 내비게이션 메뉴를 엽니다. Compute \u0026gt; Instances 항목으로 이동합니다.\nInstance를 생성할 Region을 확인하고 Create Instance 클릭합니다.\nShape에서 Change shape을 클릭합니다.\n다음 Shape에서 Burstable Instance을 선택할 수 있습니다.\nVM.Standard3.Flex VM.Standard.E3.Flex VM.Standard.E4.Flex 선택한 Shape 상세항목을 클릭한후, Burstable을 선택합니다.\nBaseline OCPU를 12.5% 또는 50%에서 선택가능합니다. 여기서는 12.5%를 선택합니다. CPU와 Network Bandwith는 부하에 따라 Burst 됩니다. Memory는 Burst 되지 않으므로, 원하는 메모리를 선택합니다.\n생성이 완료되었습니다.\n1 OCPU * 12.5% 이므로 0.125 OCPU 에 대한 과금이 발생합니다. Burst시 사용한 추가 OCPU에 대해서는 비용이 발생하지 않습니다.\nBurst 발생시키기 OCI 문서에서는 Burst 발생상황에 대해서 아래와 같이 설명하고 있습니다. 자세한 조건에 대한 추가적인 설명이 없는데, 이 부분에 대해서 테스트를 진행해 보겠습니다.\n지난 24시간 동안 평균 CPU 사용도가 Baseline 이하일 때, Baseline 이상으로 Burst를 허용합니다. Burst는 short period of time으로 한정되며, Burst Instance는 oversubscribed된 Compute 자원을 나누어 쓰는 것이지 때문에, Burst가 허용되는 상황에서, Burst를 요청하더라도, Burst 된다는 것을 보장하지는 않습니다.\nThe ability to burst depends on the instance\u0026rsquo;s CPU usage pattern and the underlying server resource usage. If the instance\u0026rsquo;s average CPU utilization over the past 24 hours is below the baseline, the system will allow it to burst above the baseline. The burst is limited to a short period of time to ensure that resources are managed fairly. Because burstable instances are oversubscribed compute resources, there is no guarantee that an instance will be able to burst exactly when needed\n조건에 맞추기 위해 Compute VM 생성후 지난 24시간 동안 그냥 두어, Baseline 이하로 CPU로 사용되게 합니다.\nCPU 사용량을 확인해 봅니다.\n최근 1시간 기준으로 user CPU가 평균 0.56%을 사용하고 있습니다.\n[opc@burstable-instance ~]$ sar Linux 5.15.0-6.80.3.1.el8uek.x86_64 (burstable-instance) 04/24/2023 _x86_64_ (2 CPU) 12:00:05 AM CPU %user %nice %system %iowait %steal %idle 12:10:01 AM all 0.61 0.00 0.41 0.00 1.14 97.84 12:20:05 AM all 0.58 0.00 0.40 0.00 1.01 98.01 12:30:05 AM all 0.49 0.00 0.41 0.00 0.97 98.13 12:40:01 AM all 0.59 0.00 0.41 0.00 1.01 97.99 12:50:05 AM all 0.56 0.04 0.41 0.00 1.07 97.92 01:00:05 AM all 0.57 0.00 0.40 0.00 1.06 97.98 01:10:01 AM all 0.52 0.00 0.41 0.00 1.04 98.03 Average: all 0.56 0.01 0.41 0.00 1.04 97.98 1 OCPU, 2 Thread에 맞게 CPU에 부하를 줍니다.\n$ stress --cpu 2 지난 24시간 동안 Baseline이하였기 때문에, 지금은 실제 Burst가 되어 user CPU가 99% 이상을 사용하고 있습니다. 1시간 정도가 지나면, user CPU는 11% 정도로 Baseline 이하로 떨어집니다. 발생하는 부하량은 변경되지 않았지만, steal CPU가 88.45% 정도로 올라가면서, user CPU가 떨진 것을 볼 수 있습니다.\n[opc@burstable-instance ~]$ sar -u 60 Linux 5.15.0-6.80.3.1.el8uek.x86_64 (burstable-instance) 04/24/2023 _x86_64_ (2 CPU) 01:20:22 AM CPU %user %nice %system %iowait %steal %idle 01:21:22 AM all 56.63 0.00 0.38 0.00 0.56 42.43 01:22:22 AM all 99.43 0.00 0.29 0.00 0.27 0.00 01:23:22 AM all 99.32 0.00 0.39 0.00 0.29 0.00 ... 02:20:22 AM all 99.48 0.00 0.26 0.00 0.26 0.00 02:21:22 AM all 54.78 0.00 0.25 0.00 44.97 0.00 02:22:22 AM all 11.23 0.00 0.32 0.00 88.45 0.00 02:23:22 AM all 11.21 0.00 0.35 0.00 88.44 0.00 02:24:22 AM all 11.22 0.00 0.30 0.00 88.48 0.00 02:25:22 AM all 11.21 0.00 0.34 0.00 88.45 0.00 01:21:22 ~ 02:21:22 까지 아래 그래프 처럼 1시간 정도 CPU는 일시적으로 최대한으로 사용하고, 강제로 Baseline이하로 떨어지게 됩니다. Steal time은 위키피디아에서 다음과 같이 설명하고 있습니다. 가상화 환경에서 OS가 CPU를 요청하였지만, 하이퍼바이저가 허용하지 않은 CPU의 시간으로 말하고 있습니다. OCI 내 하이퍼바이저가 사용자가 정의한 Baseline 이하로만 CPU를 허용하는 것을 볼 수 있습니다.\nhttps://en.wikipedia.org/wiki/CPU_time Steal time (for the whole system only), on virtualized hardware, is the amount of time the operating system wanted to execute, but was not allowed to by the hypervisor. This can happen if the physical hardware runs multiple guest operating system and the hypervisor chose to allocate a CPU time slot to another one. 문서에 처럼 24시간 Baseline 이하로 CPU를 사용하였을 때, Burst가 허용되는 상태에서, 실제 CPU 100%에 해당하는 부하가 발생하게 되면, 자원이 가용한 경우에 실제 Burst가 발생하게 됩니다. 이 때 문서에서 말한 short period of time이 테스트 결과에서는 1시간이었습니다.\nBurst 시간 추정하기 Burst 가능여부 및 가능 시간은 서비스 제공자의 구현방식에 따라 달라지겠지만, OCI Burstable Instance는 기본적으로 Credit을 기반으로 합니다.\n앞선 테스트에서 부하가 전혀 없는 휴지기에서 보인 User CPU를 0.56%을 가정합니다. 초당 쌓이는 Credit: Baseline - 0.56 = 12.5% - 0.56% = 11.94% 100이 되는 시간: 11.94% * 9초 = 107.46% 즉, 평균 User CPU 0.56%로 9초 정도 사용하면, User CPU 100%를 1초동안 사용할 수 있는 Credit이 쌓이게 됩니다. Credit이 없는 상태에서 90분간 아무런 부하가 없는 상태에서, 쌓이는 Credit을 계산하여, Burst할 수 있는 시간을 추정해 보면 다음과 같습니다.\n계산 편의상 휴지기에서 User CPU: 1% (Baseline - 1%) * 60초 * 90분 = (12.5 - 1) * 60 * 90 = 62,100 62,100 / 100% = 621초 = 10분 21초 즉, Baseline이 12.5%일때, 평균 User CPU 1%로 90분간 사용하면, User CPU 100%를 10분 21초가 사용할 수 있는 Credit이 쌓입니다. 검증을 테스트를 순서대로 진행합니다.\n90분간 부하를 주지 않습니다.\n부하를 발생시킵니다.\n$ stress --cpu 2 Burst가 끝나고, User CPU가 다시 Baseline 이하로 내려가는 것을 확인합니다.\n테스트 결과\n03:00:53 ~ 04:29:53: 부하 없음 04:30:53 ~ : 부하 발생 04:30:53 ~ 04:39:53 : Steal CPU에 제한없이 User CPU를 99.5% 사용 04:41:53 ~ : Steal CPU가 88.5%로 올라가고, User CPU는 Baseline이하인 11.x%로 내려감. [opc@burstable-instance ~]$ sar -u 60 Linux 5.15.0-6.80.3.1.el8uek.x86_64 (burstable-instance) 04/24/2023 _x86_64_ (2 CPU) 02:58:53 AM CPU %user %nice %system %iowait %steal %idle 02:59:53 AM all 11.25 0.00 0.28 0.00 88.48 0.00 03:00:53 AM all 6.55 0.00 0.46 0.02 51.70 41.27 03:01:53 AM all 0.51 0.00 0.49 0.00 1.20 97.81 ... 04:29:53 AM all 0.48 0.00 0.42 0.01 1.05 98.04 04:30:53 AM all 48.06 0.00 0.38 0.00 0.82 50.73 04:31:53 AM all 99.49 0.00 0.28 0.00 0.23 0.00 04:32:53 AM all 99.50 0.00 0.25 0.00 0.25 0.00 ... 04:39:53 AM all 99.45 0.00 0.30 0.00 0.25 0.00 04:40:53 AM all 49.12 0.00 0.32 0.00 50.56 0.00 04:41:53 AM all 11.14 0.00 0.34 0.00 88.53 0.00 04:42:53 AM all 11.13 0.00 0.34 0.00 88.53 0.00 04:43:53 AM all 11.18 0.00 0.29 0.00 88.54 0.00 ... Baseline이 12.5%일때, 평균 User CPU 1%로 90분간 사용하면, User CPU 100%를 10분 가량을 사용할 수 있습니다. 테스트 결과, Baseline 이하로 사용한 CPU 사용량을 Credit으로 쌓아, Burst가 필요한 상황에 해당 Credit 만큼 사용하는 것을 볼 수 있습니다. 또한 첫번째 테스트에서 보듯이, 24 시간 동안 쌓인 Credit을 사용하는 경우에는 최대 1시간 Burst 되는 것을 확인할 수 있었습니다.\n","lastmod":"2023-04-24T00:00:01Z","permalink":"https://thekoguryo.github.io/oci/chapter03/9/1.burstable-instances/","tags":["burstable"],"title":"3.9.1 Burstable Instances 사용하기"},{"categories":null,"contents":"Logging Analytics: Monitor and manage Kubernetes with our Marketplace app Release Notes - Logging Analytics: Monitor and manage Kubernetes with our Marketplace app 업데이트 사항 GitHub Source를 통해 OKE 클러스터에서 Fluentd를 통해 로그를 수집하여 OCI Logging Analytics로 모니터링하는 툴을 제공하고 있습니다. 이번 업데이트는 이를 Kubernetes Monitoring and Management 란 이름으로 OCI 마켓플레이스에서 제공하게 되었다는 업데이트 입니다. 마켓플레이스로 가면, Resource Manager를 통해 대상 OKE 클러스터에 수집용 모듈을 설치하고, OCI Logging Analytics로 수집되고, 또한 Kubernetes 모니터링용 대쉬보드 추가 제공합니다.\n자세한 사항은 다음을 참고합니다.\n5.2.3 Logging Analytics를 사용한 모니터링 관련 추가 문서\nReference Architecture\nBlogs\n","lastmod":"2023-01-09T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20230109-logging-analytics-monitor-and-manage-kubernetes/","tags":["logging analytics","kubernetes","marketplace"],"title":"Logging Analytics: Monitor and manage Kubernetes with our Marketplace app"},{"categories":null,"contents":"Support for dynamically provisioning PVCs in File Storage service Release Notes - Support for dynamically provisioning Kubernetes Persistent Volume Claims (PVCs) on new file systems in File Storage service 업데이트 사항 OKE에서 Persistent Volume으로 File Storage 서비스를 사용할 수 있습니다. 이전에는 File Mount와 File System을 사전에 구성한 경우, OKE에서 Persistent Volume에서 연동할 File System의 연결 정보를 직접 입력해 주었습니다. 컨테이너 Pod 들이 File System을 여러개 연동할 할 경우 각각 수작업로 Persistent Volume을 등록해 주어야 했습니다. 등록하 PVC를 요청해 Volume을 사용할 수 있었습니다.\n이번 기능은 Storage Class로 File Storage의 기본 정보를 입력하고, 이후 PVC로 요청하면, Storage Class에 등록된 Compartment, 서브넷 정보를 이용하여, 자동으로 File Mount와 File System이 생성되고, PVC 삭제시 생성된 File Mount와 File System이 자동으로 삭제됩니다.\nDynamic Provisioning을 통해 보다 편리하게 File Storage 서비스를 Persistent Volume으로 사용할 수 있게 되었습니다.\n자세한 사항은 다음을 참고합니다.\n1.6.3 File Storage 사용하기(CSI Driver 기반) - Dynamic Provisioning ","lastmod":"2022-12-14T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20221214-support-for-dynamically-provisioning-pvcs-in-file-storage-service/","tags":["dynamic provisioning","pvc","file storage service","kubernetes"],"title":"Support for dynamically provisioning PVCs in File Storage service"},{"categories":null,"contents":"Container Instances is now available Release Notes - Container Instances is now available\nBlog - Developing on Oracle Cloud Infrastructure just got easier with OCI Code Editor!\nBlog - GA announcement\nBlog - Getting started with OCI Container Instances\nVideo - First Principles – Inside OCI Container Instances\nDemo - Introduction and Deep dive(https://youtu.be/uXYhRp7MtcE)\n업데이트 사항 컨테이너 애플리케이션을 사용하기 위해 Container Orchestration으로 일반적으로 쿠버네티스를 많이 사용합니다. 또는 가상머신에 Docker 엔진을 직접 설치하여, 컨테이너를 간단하게 구동시킬 수도 있습니다. 이때 쿠버네티스 또는 Docker 엔진 등을 사용자가 관리해 주어야 합니다. Container Instances는 Container Orchestration을 사용자가 관리하지 않고, 클라우드 서비스 제공자가 관리합니다. 사용자는 간단하게 컨테이너를 배포해서 사용할 수 있고, 컨테이너가 직접 사용하는 자원에 대한 비용만 드는 장점이 있습니다.\n이제 OCI에서도 OCI Container Instances 서비스를 통해 간편하게 컨테이너를 실행할 수 있습니다.\n자세한 사항은 다음을 참고합니다.\n8.1 Container Instances로 컨테이너 배포하기 ","lastmod":"2022-12-06T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20221206-container-instances-is-now-available/","tags":["container-instances","container"],"title":"Container Instances is now available"},{"categories":null,"contents":"ARM 기반 서버 성능 비교: OCI vs. AWS OCI Ampere Altra A1 오라클은 2021년 5월부터 Arm 기반의 Ampere A1 Compute Shape을 가상 머신에서 사용할 수 있습니다.\nRelease Notes - Arm-based Ampere A1 Compute shape now available Ampere A1 Compute 소개 페이지 OCI Ampere A1 성능 측정 참고 문서 최근 API Gateway인 Apache APISIX 블로그에서 클라우드 ARM 기반 서버들의 성능을 비교하였습니다. 해당 내용을 기반으로 OCI에서도 최대한 유사한 환경을 구성하여 성능 측정을 한번 해보겠습니다.\n#1. June 7, 2022, Installation and performance testing of API Gateway Apache APISIX on AWS Graviton3 #2. August 12, 2022, GCP, AWS, and Azure ARM-based server performance comparison Step#1. ARM Ubuntu VM 생성 참고 문서에서 테스트한 환경\nAWS Graviton3: ARM 아키텍처 기반 AWS EC2 c7g.large(2vCPU, 4GiB Memory) Ubuntu 20.04 OCI에서는 다음 환경을 구성합니다.\nOCI Ampere A1: ARM 아키텍처 기반 VM.Standard.A1.Flex: 자유 구성이나, 동일하게 2OCPU, 4GB 메모리 선택 Ubuntu 22.04 Minimal aarch64: 벤치마크 테스트 모듈 설치 오류 최소화를 위해 22.04 선택 Compute 인스턴스 생성 화면에서 Shape 유형을 Ampere로 선택합니다.\nOS를 이미지를 Ubuntu로 변경하고, 기본 22.04 Minimal aarch64로 변경합니다.\n하나 있는 VM.Standard.A1.Flex Shape을 선택합니다. A1이 Arm기반 Ampere A1을 뜻하며, Flex는 CPU, Memory 선택이 자유로운 것을 뜻합니다. 2 OCPU, 4GB 메모리를 선택합니다.\nOS Image와 Arm 서버 Shape이 선택되었습니다.\nNetwork bandwith는 Flex Shape에서 OCPU 갯수에 따라 증가합니다. VM.Standard.A1.Flex의 Network bandwith는 OCPU 당 1 Gbps이고, 최대 40 Gbps 입니다. 참조: Oracle Cloud Infrastructure Documentation \u0026gt; Compute \u0026gt; Compute Shapes 2 OCPU가 선택되어, 그림과 같이 2 Gbps가 되었습니다. 이름, VCN, SSH Key 등 나머지는 원하는 값으로 지정하여 인스턴스를 생성합니다.\n만든 인스턴스에 SSH로 접속합니다.\nssh ubuntu@\u0026lt;PUBLIC-IP-OF-COMPUTE-INSTANCE\u0026gt; Step#2. ARM Ubuntu VM에 APISIX 설치 참고 문서: How to build APISIX in ARM Ubuntu\n요구사항 설치\n소스 코드 복제\nsudo apt-get update sudo apt-get install git git clone https://github.com/apache/apisix.git cd apisix git checkout release/2.15 OpenResty를 설치합니다. Ubuntu 22.04, ARM에 맞게 설치합니다.\n참고: (https://openresty.org/en/linux-packages.html#ubuntu)\nStep 1: we should install some prerequisites needed by adding GPG public keys (could be removed later):\nsudo apt-get -y install --no-install-recommends wget gnupg ca-certificates Step 2: import our GPG key: # For ubuntu 22 wget -O - https://openresty.org/package/pubkey.gpg | sudo gpg --dearmor -o /usr/share/keyrings/openresty.gpg Step 3: then add the our official APT repository. # for arm64 or aarch64 system # For ubuntu 22 or above echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/openresty.gpg] http://openresty.org/package/arm64/ubuntu $(lsb_release -sc) main\u0026#34; | sudo tee /etc/apt/sources.list.d/openresty.list \u0026gt; /dev/null Step 4: update the APT index \u0026amp; install openresty sudo apt-get update sudo apt-get -y install openresty Dependencies 설치\nbash utils/install-dependencies.sh sudo apt install wget sudo unzip sudo apt-get install make gcc curl https://raw.githubusercontent.com/apache/apisix/master/utils/linux-install-luarocks.sh -sL | bash - LUAROCKS_SERVER=https://luarocks.cn make deps APISIX 설치\nsudo make install ETCD 설치\nDocker 설치\nsudo apt-get install docker.io etcd 시작\nsudo docker run -d --name etcd -p 2379:2379 -e ETCD_UNSUPPORTED_ARCH=arm64 -e ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379 -e ETCD_ADVERTISE_CLIENT_URLS=http://0.0.0.0:2379 gcr.io/etcd-development/etcd:v3.5.1-arm64 확인 STATUS가 Up인지 확인합니다.\nsudo docker ps -a APISIX 시작\n소스 코드 위치로 이동\ncd ~/apisix Dependencies 설치\nmake deps sudo make install 설정 추가\necho \u0026#34;ulimit -n 4096\u0026#34; \u0026gt;\u0026gt; ~/.bashrc source ~/.bashrc APISIX 시작\napisix init # start APISIX apisix start 테스트 Route 등록\ncurl \u0026#34;http://127.0.0.1:9080/apisix/admin/routes/1\u0026#34; \\ -H \u0026#34;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#34; -X PUT -d \u0026#39; { \u0026#34;uri\u0026#34;: \u0026#34;/anything/*\u0026#34;, \u0026#34;upstream\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;: { \u0026#34;httpbin.org:80\u0026#34;: 1 } } }\u0026#39; 테스트\ncurl -i http://127.0.0.1:9080/anything/das 테스트 결과\nHTTP/1.1 200 OK ..... Step#3. 성능 측정 성능 테스트는 참고 문서 #1에서 언급하고 있는 Apache APISIX 공식 벤치마크 스크립트을 사용하였습니다.\n성능 측정 준비\n생성한 ubuntu VM에 HTTP 벤치마크 툴인 wrk를 설치합니다.\nsudo apt-get install wrk 측정 시나리오\n아래 2가지 시나리오를 사용하였으며, 사용할 벤치마크 테스트 스크립트 안에 해당 내용이 들어 있습니다. 각 시나리오는 APISIX가 라우팅할 127.0.0.1:1980에서 서비스되는 Nginx 노드가 필요합니다. 테스트 스크립트 안에 그 부분도 포함되어 있습니다.\nScenario 1: Single upstream​\n참고 문서 #1의 첫번째 시나리오로 플러그인 없이, single upstream을 사용합니다. 순 proxy back-to-origin 모드에서 APISIX 성능 테스트 용도라고 합니다. # apisix: 1 worker + 1 upstream + no plugin # register route curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;uri\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;plugins\u0026#34;: { }, \u0026#34;upstream\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;: { \u0026#34;127.0.0.1:1980\u0026#34;:1 } } }\u0026#39; Scenario 2: Single upstream + Two plugins​\n참고 문서 #1의 두번째 시나리오로 플러그인 2개, single upstream을 사용합니다. 2 코어 성능-소비 플러그인으로 limit-count와 prometheus을 사용하여 APISIX 성능 테스트 용도라고 합니다. # apisix: 1 worker + 1 upstream + 2 plugins (limit-count + prometheus) # register route curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;uri\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;plugins\u0026#34;: { \u0026#34;limit-count\u0026#34;: { \u0026#34;count\u0026#34;: 2000000000000, \u0026#34;time_window\u0026#34;: 60, \u0026#34;rejected_code\u0026#34;: 503, \u0026#34;key\u0026#34;: \u0026#34;remote_addr\u0026#34; }, \u0026#34;prometheus\u0026#34;: {} }, \u0026#34;upstream\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;: { \u0026#34;127.0.0.1:1980\u0026#34;:1 } } }\u0026#39; 벤치마크 테스트 스크립트 수행\n소스 코드 위치로 이동\ncd ~/apisix 편집 툴 설치\nsudo apt-get install vim 백엔드 Nginx 기동을 확인하기 위해 ./benchmark/run.sh 스크립트의 103줄에 curl 명령을 추가합니다.\ncurl -i http://127.0.0.1:9080/hello wrk -d 5 -c 16 http://127.0.0.1:9080/hello ./benchmark/run.sh 스크립트에서 테스트를 위해 부하 시간을 기본 5초에서 60초로 늘립니다. -d 5를 -d 60으로 변경합니다.\nwrk -d 60 -c 16 http://127.0.0.1:9080/hello 벤치마크 스크립트를 수행합니다.\n./benchmark/run.sh 실행 결과\nubuntu@oci-arm-ubuntu-c2m4:~/apisix$ ./benchmark/run.sh + \u0026#39;[\u0026#39; -n \u0026#39;\u0026#39; \u0026#39;]\u0026#39; + worker_cnt=1 + \u0026#39;[\u0026#39; -n \u0026#39;\u0026#39; \u0026#39;]\u0026#39; + upstream_cnt=1 + mkdir -p benchmark/server/logs + mkdir -p benchmark/fake-apisix/logs + make init [ info ] init -\u0026gt; [ Start ] ... apisix: 1 worker + 1 upstream + no plugin + curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;uri\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;plugins\u0026#34;: { }, \u0026#34;upstream\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;: { \u0026#34;127.0.0.1:1980\u0026#34;:1 } } }\u0026#39; {\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;\\/apisix\\/routes\\/1\u0026#34;,\u0026#34;value\u0026#34;:{\u0026#34;uri\u0026#34;:\u0026#34;\\/hello\u0026#34;,\u0026#34;update_time\u0026#34;:1664362611,\u0026#34;plugins\u0026#34;:{},\u0026#34;status\u0026#34;:1,\u0026#34;create_time\u0026#34;:1664361560,\u0026#34;priority\u0026#34;:0,\u0026#34;upstream\u0026#34;:{\u0026#34;scheme\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;pass_host\u0026#34;:\u0026#34;pass\u0026#34;,\u0026#34;nodes\u0026#34;:{\u0026#34;127.0.0.1:1980\u0026#34;:1},\u0026#34;hash_on\u0026#34;:\u0026#34;vars\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;roundrobin\u0026#34;},\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;}},\u0026#34;action\u0026#34;:\u0026#34;set\u0026#34;} + sleep 1 + curl -i http://127.0.0.1:9080/hello HTTP/1.1 200 OK Content-Type: text/plain; charset=utf-8 Transfer-Encoding: chunked Connection: keep-alive Date: Wed, 28 Sep 2022 10:56:52 GMT Server: APISIX/2.15.0 1234567890+ wrk -d 60 -c 16 http://127.0.0.1:9080/hello Running 1m test @ http://127.0.0.1:9080/hello 2 threads and 16 connections Thread Stats Avg Stdev Max +/- Stdev Latency 1.02ms 188.43us 13.83ms 97.08% Req/Sec 7.86k 505.69 8.73k 89.58% 938523 requests in 1.00m, 171.84MB read Requests/sec: 15640.38 Transfer/sec: 2.86MB + sleep 1 + wrk -d 60 -c 16 http://127.0.0.1:9080/hello Running 1m test @ http://127.0.0.1:9080/hello 2 threads and 16 connections Thread Stats Avg Stdev Max +/- Stdev Latency 1.03ms 112.62us 10.02ms 93.94% Req/Sec 7.82k 314.04 15.89k 88.01% 934559 requests in 1.00m, 171.12MB read Requests/sec: 15550.03 Transfer/sec: 2.85MB ... apisix: 1 worker + 1 upstream + 2 plugins (limit-count + prometheus) + curl http://127.0.0.1:9080/apisix/admin/routes/1 -H \u0026#39;X-API-KEY: edd1c9f034335f136f87ad84b625c8f1\u0026#39; -X PUT -d \u0026#39; { \u0026#34;uri\u0026#34;: \u0026#34;/hello\u0026#34;, \u0026#34;plugins\u0026#34;: { \u0026#34;limit-count\u0026#34;: { \u0026#34;count\u0026#34;: 2000000000000, \u0026#34;time_window\u0026#34;: 60, \u0026#34;rejected_code\u0026#34;: 503, \u0026#34;key\u0026#34;: \u0026#34;remote_addr\u0026#34; }, \u0026#34;prometheus\u0026#34;: {} }, \u0026#34;upstream\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;roundrobin\u0026#34;, \u0026#34;nodes\u0026#34;: { \u0026#34;127.0.0.1:1980\u0026#34;:1 } } }\u0026#39; {\u0026#34;node\u0026#34;:{\u0026#34;key\u0026#34;:\u0026#34;\\/apisix\\/routes\\/1\u0026#34;,\u0026#34;value\u0026#34;:{\u0026#34;uri\u0026#34;:\u0026#34;\\/hello\u0026#34;,\u0026#34;update_time\u0026#34;:1664362734,\u0026#34;plugins\u0026#34;:{\u0026#34;limit-count\u0026#34;:{\u0026#34;time_window\u0026#34;:60,\u0026#34;key\u0026#34;:\u0026#34;remote_addr\u0026#34;,\u0026#34;rejected_code\u0026#34;:503,\u0026#34;count\u0026#34;:2000000000000,\u0026#34;key_type\u0026#34;:\u0026#34;var\u0026#34;,\u0026#34;policy\u0026#34;:\u0026#34;local\u0026#34;,\u0026#34;allow_degradation\u0026#34;:false,\u0026#34;show_limit_quota_header\u0026#34;:true},\u0026#34;prometheus\u0026#34;:{\u0026#34;prefer_name\u0026#34;:false}},\u0026#34;status\u0026#34;:1,\u0026#34;create_time\u0026#34;:1664361560,\u0026#34;priority\u0026#34;:0,\u0026#34;upstream\u0026#34;:{\u0026#34;scheme\u0026#34;:\u0026#34;http\u0026#34;,\u0026#34;pass_host\u0026#34;:\u0026#34;pass\u0026#34;,\u0026#34;nodes\u0026#34;:{\u0026#34;127.0.0.1:1980\u0026#34;:1},\u0026#34;hash_on\u0026#34;:\u0026#34;vars\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;roundrobin\u0026#34;},\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;}},\u0026#34;action\u0026#34;:\u0026#34;set\u0026#34;} + sleep 3 + wrk -d 60 -c 16 http://127.0.0.1:9080/hello Running 1m test @ http://127.0.0.1:9080/hello 2 threads and 16 connections Thread Stats Avg Stdev Max +/- Stdev Latency 1.31ms 192.72us 17.15ms 95.50% Req/Sec 6.15k 294.98 6.71k 71.83% 734891 requests in 1.00m, 185.02MB read Requests/sec: 12248.00 Transfer/sec: 3.08MB + sleep 1 + wrk -d 60 -c 16 http://127.0.0.1:9080/hello Running 1m test @ http://127.0.0.1:9080/hello 2 threads and 16 connections Thread Stats Avg Stdev Max +/- Stdev Latency 1.30ms 141.40us 12.13ms 93.56% Req/Sec 6.17k 254.42 6.87k 64.83% 736422 requests in 1.00m, 185.41MB read Requests/sec: 12271.99 Transfer/sec: 3.09MB ... 성능 측정 결과\nScenario 1: Single upstream​\napisix: 1 worker + 1 upstream + no plugin 부분 항목 1차 2차 평균 Requests/sec (초당 요청건) 15640.38 15550.03 15595.205 Avg Latency (평균 응답지연) 1.02ms 1.03ms 1.025ms Scenario 2: Single upstream + Two plugins​\napisix: 1 worker + 1 upstream + 2 plugins (limit-count + prometheus) 항목 1차 2차 평균 Requests/sec (초당 요청건) 12248.00 12271.99 12259.995 Avg Latency (평균 응답지연) 1.31ms 1.30ms 1.305ms 테스트시 CPU, Memory 상태\nCPU: 부하로 인해 백엔드 Nginx Worker 프로세스가 CPU를 많이 사용하고 있음 Memory: Nginx 단순 웹페이지라 메모리는 크게 사용하지 않음 비용대비 성능 비교 OCI Arm 기반 Shape\nVM, Bare Metal 모두 Ampere A1을 지원하며, VM에서는 Flex Shape의 하나로 제공 https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm#flexible Shape Maximum OCPUs Minimum Memory Maximum Memory VM.Standard.A1.Flex 80 1 GB 또는 OCPU 갯수, 둘 중 큰 값 OCPU 당 64 GB, 총합 최대 512 GB OCI Arm 기반 가상 머신 비용\nOCI Ampere A1 Compute https://www.oracle.com/cloud/compute/arm/ Arm CPU 아키텍처(Ampere)에서는 1 OCPU = 1 vCPU 입니다. Cloud Account내 Ampere A1 shape의 매달 전체 사용량 중에 첫 3,000 OCPU-시간과 첫 18,000 GB-시간 까지는 무료입니다. Flex Shape으로 OCPU와 Memory를 조정할 수 있어, 각각 단위 비용이 있습니다. OCI는 모든 리전이 가격이 동일합니다. Compute - Virtual Machine Instances Comparison Price ( /vCPU)* Unit Price Unit Compute – Ampere A1 – OCPU $0.01 $0.01 시간당 OCPU Compute – Ampere A1 – Memory - $0.0015 시간당 기가바이트 벤치마크 테스트 관련 비용 비교 비교 조건\nARM 서버 기준입니다. AWS: C7g (US East Ohio) 기준 시간당 비용입니다. https://aws.amazon.com/ec2/pricing/on-demand/ OCI: VM.Standard.A1.Flex 기준 시간당 비용입니다. OCI는 모든 리전이 가격이 동일합니다. 비교를 위해 AWS C7g와 같은 크기의 CPU, Memory로 비교한 가격입니다. VM.Standard.A1.Flex 가격은 CPU 가격과 Memory 가격의 합입니다. 시간당 비용\nVM series / vCPU(Memory) 1 (2G) 2 (4G) 4 (8G) 8 (16G) 16 (32G) 32 (64G) 64 (128G) AWS C7g $0.0361 $0.0723 $0.1445 $0.289 $0.5781 $1.1562 $1.7342 OCI Ampere A1 $0.013 $0.026 $0.052 $0.104 $0.208 $0.416 $0.832 연간 비용: 테스트 시나리오 1 기준\nQPS (queries per second): 초당 요청 처리 건수 AWS c7g.large의 QPS는 참조 문서 #1, #2 결과 기준 참조 문서 #1, #2를 참고하여 최대한 동일한 구성으로 테스트하였으나, 참조 문서의 최종 부하 요청, 그 당시 자원 사용량을 알 수 없으므로, 아래 성능 및 비용 대비 성능은 절대적인 것이 아닌 참고용입니다. OCI Flex Shape은 구성이 자유로우므로 OCPU는 2개 그대로하고 사용량이 적은 Memory를 2GB로 구성하는 것도 가능합니다. 시간당 비용 일년(24*365) 총 시간 Annual Cost QPS Cost Performance(QPS/cost) AWS c7g.large $0.0723 8760 시간 $633.3 23000 36.3 OCI VM.Standard.A1.Flex 2 OCPU, 4 GB Memory $0.026 8760 시간 $227.8 15595.205 68.5 OCI VM.Standard.A1.Flex 2 OCPU, 2 GB Memory $0.023 8760 시간 $201.5 15595.205 77.4 테넌트내 테스트 환경만 사용한다는 가정하에 무료 제공량 제외시\nCloud Account내 Ampere A1 shape의 매달 전체 사용량 중에 첫 3,000 OCPU-시간과 첫 18,000 GB-시간 까지는 무료입니다. 한달 최대 사용 시간: 24시간 * 31일 = 744시간 한달 최대 사용량 무료 제외후 한달 사용량 일년 사용량 Unit Price Annual Cost OCI VM.Standard.A1.Flex 2 OCPU 1,488 OCPU-시간 0 OCPU-시간 0 $0.01 $0 OCI VM.Standard.A1.Flex 4 GB Memory 2,976 GB-시간 0 GB-시간 0 $0.0015 $0 ","lastmod":"2022-09-27T00:00:01Z","permalink":"https://thekoguryo.github.io/blog/arm-based-server-performance/","tags":["arm","performance"],"title":"ARM 기반 서버 성능 비교: OCI vs. AWS"},{"categories":null,"contents":"Code Editor is now available Release Notes - Code Editor is now available Blog - Developing on Oracle Cloud Infrastructure just got easier with OCI Code Editor! 업데이트 사항 오라클 블로그에 언급되어 있는 것 처럼 Cloud Shell에 추가하여 오라클 클라우드 콘솔에서 그림과 같이 Code Editor를 제공합니다.\n제공 기능 Java, Python, Docker, JSON, YAML, and Terraform 포함 15개의 언어를 지원합니다. OCI 서비스와 연계된 기능을 제공하며, 현재 기준 Function, Resource Manager 통합 기능을 제공합니다. Git 통합 기능을 제공하여, 변경사항 추적, Commit, Pull, Push 등과 같은 기능을 제공합니다. Cloud Shell과 통합되어, 사용자 홈에 이는 파일들을 읽고 편집할 수 있습니다. 사용자 설정을 통해 폰트, 컬러 스킴, 레이아웃 등을 변경할 수 있으며, 설정은 저장되어, Code Editor를 재시작하더라도 적용됩니다. 마지막 세션이 저장되어, 창을 닫더라도 재시작시 마지막 페이지가 열립니다. 자세한 사항은 공식 문서 Working with Code Editor와 블로그 Blog - Developing on Oracle Cloud Infrastructure just got easier with OCI Code Editor!를 참고하기 바랍니다.\n","lastmod":"2022-08-02T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220802-code-editor-is-now-available/","tags":["cloudshell","code editor"],"title":"Code Editor is now available"},{"categories":null,"contents":"Cloud Shell now offers Private Access Release Notes - Cloud Shell now offers Private Access Blog - Securely access private resources from OCI Cloud Shell using private network access 업데이트 사항 Cloud Shell에서 Private Access 하기 OKE Private Cluster에 대해서 Cloud Shell의 Private Access를 통해 연결하는 예시입니다.\nCloud Shell에서 새로 생긴 Network 선택 메뉴에서 Private network setup을 선택합니다.\nCloud Shell에서 연결하려는 서브넷을 선택합니다.\nK8S API Endpoint로 연결이 가능하는 Worker Node 서브넷을 선택합니다.\nPrivate Network에 연결이 완료되고, 해당 서브넷의 IP를 추가로 할당받은 것을 확인할 수 있습니다.\nifconfig 없이 IP 확인하기\nawk \u0026#39;/32 host/ { print i } {i=$2}\u0026#39; /proc/net/fib_trie OKE Cluster의 PRIVATE ENDPOINT를 복사합니다.\nCloud Shell에서 OKE Private Cluster에 접속되는 것을 확인할 수 있습니다. Control Plane의 연결 IP가 Private IP 이지만 연결되어 명령이 수행되고 있습니다.\n참고사항 아래 그림에서 보듯이 Cloud Shell은 실제 Home Region에서 구동되고 있습니다. 오라클 클라우드 콘솔에서 현재 Region을 다른 것으로 변경하더라도 Cloud Shell은 Home Region에서 구동됩니다. 그래서 Cloud Shell Private Access의 Private Network Setup 화면에서 조회되는 VCN과 Subnet은 Home Region에 있는 VCN과 Subnet입니다. 따라서 다른 Region의 자원에 접속하기 위해서는 Home Region에 있는 VCN과 대상 VCN을 Remote Peering을 설정하여, Home Region의 VCN을 통해 거쳐가야 합니다.\n","lastmod":"2022-08-02T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220802-cloud-shell-now-offers-private-access/","tags":["cloudshell","private access"],"title":"Cloud Shell now offers Private Access"},{"categories":null,"contents":"CloudShell now offers GraalVM Enterprise JDK 17 and Native Image Release Notes - CloudShell now offers GraalVM Enterprise JDK 17 and Native Image Blog - Announcing GraalVM Enterprise in OCI Code Editor and Cloud Shell 업데이트 사항 다른 글에서 언급한 것 처럼 OCI에서는 Java SE 및 GraalVM 사용에 대한 권한을 제공합니다. OCI 상에서 실행되는 Java에 대한 기술지원을 받을 수 있으면, GraalVM Enterprise로 사용할 수 있습니다.\n7.2 GraalVM 사용하기 이제 CloudShell에서 GraalVM Enterprise JDK 17을 기본 설치제공하여, CloudShell에서 바로 사용할 수 있습니다.\nCloud Shell에 접속해서 보면, 아래와 같이 기본적으로 OpenJDK 1.8을 사용하고 있습니다.\n다음 명령을 통해 현재 설치된 JDK 목록을 확인할 수 있습니다.\ncsruntimectl java list 현재 기준 아래와 같이 JDK가 3가지가 설치되어 있습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ csruntimectl java list graalvmeejdk-17.0.4 /usr/lib64/graalvm/graalvm22-ee-java17 openjdk-11.0.16 /usr/lib/jvm/java-11-openjdk-11.0.16.0.8-1.0.1.el7_9.x86_64 * openjdk-1.8.0.342 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.342.b07-1.el7_9.x86_64 사용할 JDK를 GraalVM Enterprise JDK 17로 변경합니다.\ncsruntimectl java set graalvmeejdk-17.0.4 변경되면 아래 명령으로 자바 버전과, Native Image CLI의 버전을 확인합니다.\njava -version native-image --version 실행 결과 예시\n","lastmod":"2022-07-26T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220726-cloudshell-now-offers-graalvm-enterprise-jdk-17-and-native-image/","tags":["cloudshell","graalvm"],"title":"CloudShell now offers GraalVM Enterprise JDK 17 and Native Image"},{"categories":null,"contents":"Support for VCN-native pod networking Release Notes - Support for VCN-native pod networking Blog - Announcing VCN-native pod networking for Kubernetes in OCI 업데이트 사항 OKE에서는 기존에 CNI로 flannel만 지원하였습니다. VCN내에 Pod를 위한 가상의 overlay 네트워크를 두는 방식입니다. 이제 VCN-Native Networking을 지원하여, Pod가 VCN Subnet을 직접 사용합니다.\nOKE에서 제공하는 CNI 비교\n출처 - Blog - Announcing VCN-native pod networking for Kubernetes in OCI\nFlannel overlay networking\nflannel overlay 네트워크를 사용하면, 그림처럼 가상의 POD Network가 만들어지고, 생성되는 pod들은 해당 대역의 IP를 할당받습니다. 그리고 다른 Worker Node에 있는 pod와 통신하기 위해서는 왼쪽 아래와 같이 Worker Node IP로 감싸져서 대상 Worker Node로 전달후 대상 Pod IP로 전달됩니다. VCN-Native pod networking\nVCN-Native pod 네트워크를 사용하면, 그림처럼, 생성되는 pod들은 지정한 VCN상의 Subnet 상의 IP를 할당받습니다. 그리도 다른 Worker Node에 있는 pod와 통신하기 위해 VCN 네트워크를 직접사용합니다. 그래서 VCN상의 다른 VM에서도 해당 IP로 접근이 가능합니다. 또한 VCN 관련 네트워크 설정, 모니터링 툴을 그대로 사용 가능합니다. VCN-Native Pod Networking 관련 네트워크 자원 만들기 아래 설정은 Quick Create 모드에서 자동으로 생성되는 VCN 관련자원에서 추가되는 부분만 설명합니다.\n예시, CIDR Block State Source Kubernetes API Endpoint 10.0.0.0/28 Worker Nodes 10.0.10.0/24 Load Balancers 10.0.20.0/24 Pods 10.0.40.0/24 Security Rules 추가 및 만들기 전체 필요한 보안 규칙은 공식 문서를 참조합니다.\nhttps://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#securitylistconfig Kubernetes API Endpoint의 기존 Security Rules에 추가 규칙\nIngress Rules\nState Source Protocol/Dest. Port Description Stateful Pods CIDR TCP/6443 Pod to Kubernetes API endpoint communication (when using VCN-native pod networking). Stateful Pods CIDR TCP/12250 Pod to Kubernetes API endpoint communication (when using VCN-native pod networking). Egress Rules\nState Destination Protocol/Dest. Port Description Stateful Pods CIDR ALL/ALL Kubernetes API endpoint to pod communication (when using VCN-native pod networking). Stateful Worker Nodes CIDR TCP/12250 Kubernetes API endpoint to worker node communication (when using VCN-native pod networking). Worker Nodes의 기존 Security Rules에 추가 규칙\nIngress Rules\nState Source Protocol/Dest. Port Description Stateful Pods CIDR ALL/ALL Allow pods on one worker node to communicate with pods on other worker nodes (when using VCN-native pod networking). Stateful Kubernetes API Endpoint CIDR TCP/12250 Kubernetes API endpoint to worker node communication (when using VCN-native pod networking). Egress Rules\nState Destination Protocol/Dest. Port Description Stateful Pods CIDR ALL/ALL Allow worker nodes to communicate with pods on other worker nodes (when using VCN-native pod networking). Pod 서브넷을 위한 Security Rule 신규 생성\nName: 예) oke-pods-seclist\nIngress Rules\nState Source Protocol/Dest. Port Description Stateful Kubernetes API Endpoint CIDR ALL/ALL Kubernetes API endpoint to pod communication (when using VCN-native pod networking). Stateful Worker Nodes CIDR ALL/ALL Allow pods on one worker node to communicate with pods on other worker nodes. Stateful Pods CIDR ALL/ALL Allow pods to communicate with each other. Egress Rules\nState Destination Protocol/Dest. Port Description Stateful Pods CIDR ALL/ALL Allow pods to communicate with each other. Stateful All Services in Oracle Services Network ICMP 3,4 Path Discovery. Stateful All Services in Oracle Services Network TCP/ALL Allow worker nodes to communicate with OCI services. Stateful Kubernetes API Endpoint CIDR TCP/6443 Pod to Kubernetes API endpoint communication (when using VCN-native pod networking). Stateful Kubernetes API Endpoint CIDR TCP/12250 Pod to Kubernetes API endpoint communication (when using VCN-native pod networking). optional egress rules\nState Destination Protocol/Dest. Port Description Stateful 0.0.0.0/0 TCP/ALL (optional) Allow worker nodes to communicate with internet. Pod 서브넷 만들기 아래 정보를 기준으로 서브넷을 추가 생성합니다. Name: oke-pods-subnet Subnet Type: Regional CIDR Block: 10.0.40.0/24 Route Table: oke-private-routetable-~~~ Subnet Access: Private Subnet Dhcp Options: default Security Lists: oke-pods-seclist OKE 클러스터 만들기 오라클 클라우드 콘솔에서 OKE 클러스터 생성 메뉴로 이동합니다.\nCustom create 방식으로 클러스터를 만듭니다.\nNetwork setup\nNetwork Type에서 VCN-native pod networking을 선택합니다. 나머지 항목은 해당 서브넷을 지정합니다. Node pools\nPod communication에서 앞서 새로 만든 Pods 서브넷을 지정합니다.\n나머지는 필요한 값을 지정합니다.\n입력한 정보로 클러스터를 생성합니다.\n결과 테스트 구성 정보 확인 다음 명령으로 VCN-Native Pod Network 정보를 확인합니다.\nkubectl get NativePodNetwork 결과 예시\nWorker Node 갯수 만큼 자원이 보입니다. $ kubectl get NativePodNetwork NAME STATE REASON an4w4ljrvsea7yic6abavw2aef6d6ap3em6ghx6ik7lbbeli5qtazjjqslza SUCCESS COMPLETED an4w4ljrvsea7yic6mnz7upb5pw4vxe7z3a3xdflzq4njtwcogmi2wkz4bcq SUCCESS COMPLETED an4w4ljrvsea7yiclt6upymxrdpnhaaeqtemijovs5r3gkpm66aiidudwzkq SUCCESS COMPLETED 그 중 하나를 조회하면, 관련 정보를 확인할 수 있습니다. Metadata.Owner References.Name: 대상 Worker Node, 예) 10.0.10.149 Status.Vnics.Addresses: 사전 확보한 Pod 서브넷의 31개 IP 목록 Status.Vnics.Subnet Cidr: Pod 서브넷의 CIDR, 예) 10.0.40.0/24 $ $ kubectl describe NativePodNetwork an4w4ljrvsea7yic6abavw2aef6d6ap3em6ghx6ik7lbbeli5qtazjjqslza Name: an4w4ljrvsea7yic6abavw2aef6d6ap3em6ghx6ik7lbbeli5qtazjjqslza Namespace: Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; API Version: oci.oraclecloud.com/v1beta1 Kind: NativePodNetwork Metadata: Creation Timestamp: 2022-08-01T08:23:43Z Generation: 1 Managed Fields: API Version: oci.oraclecloud.com/v1beta1 ... Owner References: API Version: v1 Kind: Node Name: 10.0.10.149 UID: 3abe2f1e-3e7f-4d6d-8720-d7df76f038ef Resource Version: 1291 UID: 55cbf861-04b8-4536-8a41-3ce051f5d482 Spec: Id: ocid1.instance.oc1.ap-chuncheon-1.an4w4ljrvsea7yic6abavw2aef6d6ap3em6ghx6ik7lbbeli5qtazjjqslza Max Pod Count: 31 Pod Subnet Ids: ... Status: Reason: COMPLETED State: SUCCESS Vnics: Addresses: 10.0.40.102 10.0.40.195 10.0.40.23 10.0.40.7 10.0.40.65 10.0.40.133 10.0.40.157 10.0.40.46 10.0.40.122 10.0.40.69 10.0.40.208 10.0.40.80 10.0.40.225 10.0.40.9 10.0.40.116 10.0.40.237 10.0.40.121 10.0.40.146 10.0.40.49 10.0.40.17 10.0.40.45 10.0.40.111 10.0.40.224 10.0.40.196 10.0.40.188 10.0.40.67 10.0.40.26 10.0.40.4 10.0.40.153 10.0.40.231 10.0.40.86 Mac Address: 02:00:17:01:B8:67 Router Ip: 10.0.40.1 Subnet Cidr: 10.0.40.0/24 Vnic Id: ocid1.vnic.oc1.ap-chuncheon-1.ab4w4ljrve4z6xyu2vw62xsax6lwmhnvdtxhpdbrwqypsqfiq4qkezeqt5eq Events: \u0026lt;none\u0026gt; Worker Node 컴퓨트 인스턴스 확인\n인스턴스에 붙은 VNIC을 보면 아래와 같이 2개가 보입니다. Primary VNIC은 Worker Node 서브넷이며, 두번째는 Pods 서브넷입니다. 앞서 OKE 클러스터에서 확인한 MAC 주소가 동일합니다.\n두 번째 VNIC의 상세 정보에 사전 확보한 31개 IP 목록을 동일하게 확인할 수 있습니다.\nPod 배포 테스트 테스트용 앱인 nginx 를 배포합니다.\nkubectl create deploy nginx --image=nginx 결과 확인\nIP가 10.0.40.23으로 Pods 서브넷 상의 IP입니다.\n$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-85b98978db-8prsc 1/1 Running 0 2m11s 10.0.40.23 10.0.10.149 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; Pod IP로 접근 확인\nVCN내 서브넷의 IP를 직접 할당 받았기 때문에, Pod 서브넷에 접근 가능한 네트워크에서 아래와 같이 Pod의 IP로 직접 연결이 가능합니다. 물론 Pod의 재생성에 따라 IP는 유동적이고, 현재 VCN Private Subnet에서 고정 IP를 지원하지 않기 때문에 Pod IP로 직접 연결하는 것은 올바른 방법은 아닙니다.\n[opc@jumpbox ~]$ curl http://10.0.40.23 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; ... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; LoadBalancer 타입의 서비스를 만듭니다.\n$ kubectl scale deploy nginx --replicas=2 deployment.apps/nginx scaled $ kubectl expose deploy nginx --name=nginx-svc --port=80 --type=LoadBalancer service/nginx-svc exposed $ kubectl get pod,svc -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod/nginx-85b98978db-8prsc 1/1 Running 0 44m 10.0.40.23 10.0.10.149 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; pod/nginx-85b98978db-g9w9s 1/1 Running 0 2m26s 10.0.40.47 10.0.10.101 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 74m \u0026lt;none\u0026gt; service/nginx-svc LoadBalancer 10.96.26.147 144.24.xx.xxx 80:32163/TCP 93s app=nginx 생성된 LoadBalancer로 서비스가 되는 것을 알 수 있습니다.\n$ kubectl describe svc nginx-svc Name: nginx-svc ... LoadBalancer Ingress: 144.24.xx.xxx Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP NodePort: \u0026lt;unset\u0026gt; 32163/TCP Endpoints: 10.0.40.23:80,10.0.40.47:80 ... $ curl http://144.24.90.247 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; ... \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; ... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ","lastmod":"2022-07-12T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220712-support-for-vcn-native-pod-networking/","tags":["oke","vcn-native pod"],"title":"Support for VCN-native pod networking"},{"categories":null,"contents":"Support for worker node deletion, along with new cordon and drain options Release Notes - Support for worker node deletion, along with new cordon and drain options 업데이트 사항 이제부터 OKE 클러스터상의 특정 Worker Node를 삭제할 수 있게 되었습니다.\nWorker Node 삭제시\nWorker Node를 삭제할 때 실행중인 업무들의 이관을 위해 Cordon \u0026amp; drain 옵션을 제공합니다.\nEviction grade period\nGrace period를 두는 것으로 기본값은 60분입니다. 새로운 Pod가 대상 Worker Node로 할당되지 않게 스케줄링에서 제외하고, 이미 할당되어 실행중인 Pod들을 다른 Worker Node로 다시 스케줄링하여 실행이 될때까지 준비시간을 두게 됩니다. Advanced Options에서 원하는 시간으로 변경할 수 있으며, 대기 없이 즉시 삭제하려는 경우 시간을 0으로 지정하면 됩니다.\n해당 시간이전에 모두 이관되는 경우 대상 Worker Node는 바로 삭제됩니다.\nForce terminate after grade period\nEviction grade period가 지난후에도 이관되지 못한 자원이 있는 경우, Worker Node를 강제로 Terminate 시킬지 여부를 선택합니다. 이 옵션을 선택하지 않은 경우에는, Eviction grade period가 지난후 대상 Node Pool은 Need attention 상태로 표시됩니다. 확인 후 별도 처리하면 되겠습니다.\n삭제 예시\n실행 중인 컨테이너가 많지 않고, 원활히 이관되어 지정한 시간 이전에 바로 삭제되었습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.109 Ready,SchedulingDisabled node 60d v1.20.11 10.0.10.217 Ready node 60d v1.20.11 10.0.10.44 Ready node 60d v1.20.11 ","lastmod":"2022-06-28T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220628-support-for-worker-node-deletion-along-with-new-cordon-and-drain-options/","tags":["oke","worker node deletion"],"title":"Support for worker node deletion, along with new cordon and drain options"},{"categories":null,"contents":"Private endpoints for Resource Manager Release Notes - Private endpoints for Resource Manager 업데이트 사항 Resource Manager에서 사용자의 VCN내의 서브넷에 접근할 수 있는 Private Endpoint 기능을 제공합니다. Private Endpoint를 통해 사용자의 대상 Subnet에 VNIC을 통해 Private IP를 할당을 받게 됩니다. 이를 통해 Resource Manager에서 Private 환경에 있는 자원에 대한 접근이 가능하게 되어 다음과 같은 작업이 수행 가능합니다.\nResource Manager에서 Private Compute 인스턴스 내의 스크립트를 실행하는 Remote Exec 가능\n테라폼 구성을 기존에는 인터넷 접근이 가능한 GitHub, GitLab만 지원했는데, 이제는 Private Git Server에 있는 테라폼 구성 파일을 Resource Manager에서 인터넷 접근 없이 가져와서 적용할 수 있게 됩니다.\n실제 설정방법은 링크를 참조합니다 15.3 Resource Manager을 위한 Private Git 서버 구성 ","lastmod":"2022-06-08T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220608-private-endpoints-for-resource-manager/","tags":["resource manager","terraform"],"title":"Private endpoints for Resource Manager"},{"categories":null,"contents":"Support for OKE images as worker node base images Worker Node의 베이스 이미지로 범용 이미지가 아닌 OKE 전용 이미지를 사용하므로 이전보다 더 빠르게 클러스터 생성 및 확장이 가능합니다.\nRelease Notes - Support for OKE images as worker node base images 업데이트 사항 Worker Node의 VM 기본 이미지로 전용 OKE 이미지 지원 이제 클러스터 생성시 또는 Node Pool에서 노드 이미지 지정지 OKE 전용 이미지를 선택할 수 있습니다. 클러스터 생성시, 노드 풀 추가 및 노드 추가시 범용이미지에 Worker Node화 하기 위해 모듈을 설치하는 것보다, 필요한 모듈이 이미 설치된 전용 이미지를 사용하므로 클러스터 생성, 노드 풀 추가 및 노드 확장시 보다 빠르게 프로비저닝 할 수 있습니다.\n기능 업데이트 전 기존 OKE 클러스터에서는 Worker Node를 위한 전용 이미지를 사용하지 않고, Platform Images내의 범용 이미지를 사용하였습니다.\n기능 업데이트 후 Custom create 모드로 클러스터를 만들 때 Node Pool 설정에서 Worker Node 이미지를 OKE 전용 이미지를 선택할 수 있습니다. 기본 OKE 이미지를 사용하고, 이미지 선택을 통해 원하는 이미지를 선택할 수 있습니다. OKE 이미지를 선택하면, 아래와 같이 노드 이미지에 Kubernetes 버전이 표시됩니다. 생성후 Node Compute Instance 정보에 들어가면 이미지가 OKE 인 걸 확인할 수 있습니다. Node Pool 단위로 Shape과 Image를 지정할 수 있기 때문에, 기존 Node Pool 설정시 또는 신규 Node Pool 추가 시에 OKE 전용 이미지를 선택할 수 있습니다. ","lastmod":"2022-06-01T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220601-support-for-oke-images-as-worker-node-base-images/","tags":["oke","oke image"],"title":"Support for OKE images as worker node base images"},{"categories":null,"contents":"Support for fault domains in node pool placement configuration Node Pool 생성시 Availability Domain을 설정시 Fault Domain을 사용자가 지정할 수 있습니다.\nRelease Notes - Support for fault domains in node pool placement configuration 업데이트 사항 Node Pool 설정이 Worker Node가 위치할 Fault Domain 사용자 지정 Worker Node에 대한 Node Pool 생성시 Availability Domain을 설정시 Fault Domain을 지정할 수 있습니다. 따로 지정하지 않는 경우, AD내에 Fault Domain내에 균등하게 분배됩니다. 이제 원하는 Fault Domain을 지정할 수 있으며, 하나 이상 지정할 수 있습니다.\n","lastmod":"2022-06-01T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220601-support-for-fault-domains-in-node-pool-placement-configuration/","tags":["oke","node pool","fault domain"],"title":"Support for fault domains in node pool placement configuration"},{"categories":null,"contents":"Support for CSI metrics Persistent Volume에 대한 용량에 대한 모니터링 지원을 위해 Block Volume을 CSI 드라이버로 Persistent Volume Claims (PVCs)을 생성하는 경우 CSI 용량 통계를 위한 메트릭을 지원하게 되었습니다. 신규 클러스터는 물론, 기존 생성된 클러스터도 1.19 버전 이상에서 이미 기능활성화 되어 바로 사용할 수 있습니다.\nRelease Notes - Support for CSI metrics 업데이트 사항 CSI metris 지원 Block Volume을 CSI 드라이버로 Persistent Volume Claims (PVCs)을 생성하는 경우 CSI 용량 통계를 위한 메트릭을 지원합니다. 클러스터 생성시 기본 배포된 storageClassName: \u0026ldquo;oci-bv\u0026quot;을 사용하는 경우에 해당합니다.\n지원 메트릭 kubelet_volume_stats_available_bytes kubelet_volume_stats_capacity_bytes kubelet_volume_stats_inodes kubelet_volume_stats_inodes_free kubelet_volume_stats_inodes_used kubelet_volume_stats_used_bytes Block Volme CSI 드라이버를 통해 Persistent Volume Claims (PVCs)을 생성합니다.\n1.6.1 Block Volume 사용하기 메트릭을 확인하기 위해 Prometheus를 설치합니다.\n4.3.1 Prometheus\u0026amp;Grafana 설치하기 Prometheus UI에 접속합니다.\n편의상 prometheus-kube-prometheus-prometheus의 서비스 타입을 LoadBalancer로 설정하였습니다. 해당 메트릭이 보이는 지 조회하면 아래와 같이 메트릭이 보입니다.\nPVC 생성 시간 확인\n$ kubectl get pvc csi-bvs-pvc -o yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;PersistentVolumeClaim\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;name\u0026#34;:\u0026#34;csi-bvs-pvc\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;monitoring\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;accessModes\u0026#34;:[\u0026#34;ReadWriteOnce\u0026#34;],\u0026#34;resources\u0026#34;:{\u0026#34;requests\u0026#34;:{\u0026#34;storage\u0026#34;:\u0026#34;50Gi\u0026#34;}},\u0026#34;storageClassName\u0026#34;:\u0026#34;oci-bv\u0026#34;}} pv.kubernetes.io/bind-completed: \u0026#34;yes\u0026#34; pv.kubernetes.io/bound-by-controller: \u0026#34;yes\u0026#34; volume.beta.kubernetes.io/storage-provisioner: blockvolume.csi.oraclecloud.com volume.kubernetes.io/selected-node: 10.0.10.234 creationTimestamp: \u0026#34;2022-06-03T09:34:49Z\u0026#34; ... 첫 메트릭 수집시간\nPVC 생성시간: 09:34:49와 첫 메트릭 값 수집시간:09:37:49로 Bitnami Prometheus Helm Chart의 scrape_interval이 기본 15초 또는 Prometheus 기본에 1분 이라고 하는 데 조금 더 지나야 되네요. 정확한 설정은 다음에 확인하는 걸로\u0026hellip;.\n","lastmod":"2022-05-26T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220526-support-for-csi-metrics/","tags":["metric","csi"],"title":"Support for CSI metrics"},{"categories":null,"contents":"Support for Kubernetes version 1.23.4 OKE에서 쿠버네티스 지원 버전이 업데이트 되었습니다. 이제 1.23.4을 새롭게 지원합니다. 기존에 지원하던 1.22.5, 1.21.5은 계속 지원합니다.\nRelease Notes - Support for Kubernetes version 1.23.4 업데이트 사항 1.23.x 지원으로 인해 1.20.11 버전은 2022년 7월 19일 까지 지원합니다.\n지원이 종료하게 되면, 1.20.x버전으로 신규 클러스터 생성이 불가합니다. 지원이 종료하게 되면, 기존 클러스터에 1.20.x버전으로 새 Node Pool 추가가 불가합니다. 현재 지원 버전은 아래를 확인하세요. Supported Versions of Kubernetes 업그레이드 가이드에 따라 종료일 이전에 업그레이드를 권고하고 있습니다.\nUpgrading the Kubernetes Version on Control Plane Nodes in a Cluster Upgrading the Kubernetes Version on Worker Nodes in a Cluster 1.7 Kubernetes 지원 버전 및 업그레이드 ","lastmod":"2022-05-18T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220518-support-for-kubernetes-version-1.23.4/","tags":["Container Engine for Kubernetes","oke"],"title":"Support for Kubernetes version 1.23.4"},{"categories":null,"contents":"Detect vulnerabilities from the DevOps build pipeline Application Dependency Management (ADM) 서비스는 OCI DevOps 빌드 파이프라인 내에서 소스코드 빌드에 사용되는 라이브러리 패키제에 대한 취약점 검사하는 기능을 제공합니다. 빌드 스펙상에서 취약점 검사를 위한 VulnerabilityAudit 유형을 제공합니다.\nOracle Cloud Infrastructure Documentation / Release Notes 취약점 감사(vulnerability audit) 기능 제공 Oracle Application Dependency Management 서비스와 연계하여, DevOps 빌드 파이프라인에서 코드에서 사용하는 라이브러리 패키지들에 취약점이 있는지를 검점하는 기능을 제공합니다.\n실제 설정방법은 링크를 참조합니다\n3.5 DevOps 서비스의 빌드시 패키지 취약점 검사하기 참조 문서\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/scan-code.htm OCI Blog - Security scanning for Maven now available in OCI DevOps ","lastmod":"2022-05-25T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220517-detect-vulnerabilities-from-the-devops-build-pipeline/","tags":["DevOps","ci/cd"],"title":"Detect vulnerabilities from the DevOps build pipeline"},{"categories":null,"contents":"New features for DevOps Service Mesh의 관리형 서비스로 OCI Service Mesh 서비스가 출시되었습니다.\nRelease Notes - New features for DevOps Bitbucket Cloud 지원 외부 저장소로 기존 GitHub, GitLab 외에 추가로 Bitbucket Cloud를 지원합니다.\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/create_connection.htm\nHelm Chart 지원 OCI DevOps에서 이제 Helm Chart를 지원합니다. 배포 파이프라인에서 Install Helm Chart 스테이지를 통해 OKE 클러스터에 Helm Chart를 배포할 수 있습니다.\n실제 설정방법은 링크를 참조합니다\n3.4 DevOps 서비스를 통해 Helm Chart로 배포 자동화하기 참조 문서\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/add-helmchart.htm OCI Blog - Use Helm with OCI DevOps to deploy to Kubernetes clusters 취약점 감사(vulnerability audit) 기능 제공 Oracle Application Dependency Management 서비스와 연계하여, DevOps 빌드 파이프라인에서 코드에서 사용하는 라이브러리 패키지들에 취약점이 있는지를 검점하는 기능을 제공합니다.\n실제 설정방법은 링크를 참조합니다\n3.5 DevOps 서비스의 빌드시 패키지 취약점 검사하기 참조 문서\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/scan-code.htm OCI Blog - Security scanning for Maven now available in OCI DevOps ","lastmod":"2022-05-17T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220517-new-features-for-devops/","tags":["DevOps","ci/cd"],"title":"New features for DevOps"},{"categories":null,"contents":"OCI Search Service with OpenSearch is now available OpenSearch 기반의 관리형 서비스로 OCI Search 서비스가 출시되었습니다.\nOracle Cloud Infrastructure Documentation / Release Notes OpenSearch 기반 OCI Search 서비스 OpenSearch와 OpenDashboards는 2021년에 ElasticSearch와 Cabana에서 포크되어 별도 오픈소스 프로젝트로 운영되고 있습니다. OCI OpenSearch는 OCI에서 제공하는 관리형 서비스입니다.\n실제 설정방법은 링크를 참조합니다. 5.2.2 OpenSearch 기반 OCI Search 서비스를 사용한 로그 모니터링 ","lastmod":"2022-05-10T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220510-oci-search-service-with-opensearch-is-now-available/","tags":["searchservice","opensearch"],"title":"OCI Search Service with OpenSearch is now available"},{"categories":null,"contents":"Accelerate function start-ups using provisioned concurrency 함수 실행을 위한 초기 응답지연을 최소화하기 위해 항상 provisioned concurrency 만큼 가용 환경을 지원합ㄴ다.\nRelease Notes - Accelerate function start-ups using provisioned concurrency Function 호출시 자원에 대한 이해 Function이 배포되고 처음 호출하게 되면(cold start라고 부름), 그때 실행에 필요한 OCI 자원을 프로비저닝하게 됩니다. 그래서 처음 호출시에는 수초의 응답지연이 발생하게 됩니다. 프로비저닝이 되면, 이후 연속된 호출을 대비해 일정 시간(idle time이라 부름)동안 프로비저닝된 자원은 유지됩니다. 이때 호출은 이미 있는 자원을 사용하게 되어 바로 처리됩니다. (hot start라고 부름). 일정 시간(idle time) 동안 호출이 없는 경우 자원은 반환됩니다.\nProvisioned Concurrency 기능 provisioned concurrency 기능을 사용하게 되면 지정된 동시 요청갯수 만큼은 항상 응답지연 없는 hot start가 될 수 있도록 실행 자원을 보유하게 됩니다. 대신 실행 자원을 항상 보유한 상태이므로 이때에도 비용이 추가되게 됩니다. 가격 리스트(https://www.oracle.com/cloud/price-list.html)에서는 \u0026ldquo;Provisioned Concurrency is priced at 25% of the Execution Time when unused\u0026quot;게 설명하고 있습니다. 즉 기존 Function에서는 사용한 만큼만 비용이 산정되었는데, provisioned concurrency을 사용시에는 사용(즉 호출)이 발생하지 않더라고, 사용시 대비 25%의 비용이 산정됩니다.\nProvisioned Concurrency 사용하기 Provisioned Concurrency 사용하지 않는 경우 처음 호출시 또는 cold start에서 호출시만, 응답 지연이 있는 걸 알 수 있습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ time fn invoke helloworld-app hello-java Hello, world! real 0m29.105s user 0m0.492s sys 0m0.034s winter@cloudshell:~ (ap-chuncheon-1)$ time fn invoke helloworld-app hello-java Hello, world! real 0m0.684s user 0m0.320s sys 0m0.026s Provisioned Concurrency 설정하기 oci cli 또는 클라우드 콘솔에서 설정이 가능합니다. 클라우드 콘솔 함수 설정에서 Provisioned concurrency를 체크하고 동시 처리 유닛(PCU)를 설정합니다.\n동시 처리 유닛(PCU)는 최소 40이며, 40의 배수여야 합니다.\nProvisioned concurrency를 설정하면, 처음 호출시에도 실행할 자원이 준비된 hot start 상태로 응답 지연이 있는 걸 알 수 있습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ time fn invoke helloworld-app hello-java Hello, world! real 0m0.528s user 0m0.203s sys 0m0.030s winter@cloudshell:~ (ap-chuncheon-1)$ time fn invoke helloworld-app hello-java Hello, world! real 0m0.503s user 0m0.238s sys 0m0.020s ","lastmod":"2022-05-04T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220504-accelerate-function-start-ups-using-provisioned-concurrency/","tags":["Functions","fn","serverless"],"title":"Accelerate function start-ups using provisioned concurrency"},{"categories":null,"contents":"Support for PVC block volume performance levels Persistent Volume으로 Block Volume으로 사용할 때 Lower Cost, Balanced 또는 Higher Performance로 성능레벨을 지정할 수 있습니다.\nOracle Cloud Infrastructure Documentation / Release Notes Block Volume 성능 레벨 Block Volume의 성능은 volume performance units (VPUs)으로 표현되며, 가능한 옵션은 다음과 같습니다.\nUltra High Performance: 최고 성능을 요구하는, 가장 높은 I/O를 요구하는 워크로드에 권장합니다. 이 옵션을 사용시 매월 GB당 30 - 120 VPU을 구입할 수 있습니다. 볼륨 사이즈에 따른 처리량 및 IOPS 성능 수치는 다음 링크를 참조하세요. Ultra High Performance Higher Performance: Ultra High Performance 성능까지는 아니지만, 높은 I/O를 요구하는 워크로드에 권장합니다. 이 옵션을 사용시 매월 GB당 20 VPU을 구입할 수 있습니다. 볼륨 사이즈에 따른 처리량 및 IOPS 성능 수치는 다음 링크를 참조하세요. High Performance Balanced: Block Volume과 Boot Volume의 기본 성능 레벨입니다. 일반적인 워크로드에 성능, 비용을 고려하여 균형잡힌 선택을 제공합니다. 이 옵션을 사용시 매월 GB당 10 VPU을 구입할 수 있습니다. 볼륨 사이즈에 따른 처리량 및 IOPS 성능 수치는 다음 링크를 참조하세요. Balanced Performance Lower Cost: 스트리밍, 로그 처리, 데이타 웨어하우스 같은 순차처리 I/O을 가진 워크로드에 권장합니다. 스토리지 비용만 있으며, 추가적인 VPU 비용이 없습니다. 이 옵션은 Block Volume에만 가용한 옵션이며, Boot Volume에서는 선택할 수 없습니다. Lower Performance Block Volume을 Persistent Volume으로 사용시 성능 레벨 지정하기 Block Volume CSI 볼륨 플러그인(provisioner: blockvolume.csi.oraclecloud.com)을 사용하는 경우 Storage Class에서 성능레벨을 추가적으로 설정할 수 있습니다.\nLower Cost 성능 레벨: vpusPerGB: 0 Balanced 성능 레벨: vpusPerGB: 10 Higher Performance: 성능 레벨: vpusPerGB: 20 기본 Storage Class 조회 OKE 클러스터를 생성하면, 기본으로 두개의 Storage Class가 만들어져 있습니다. oci-bv가 Block Volume CSI 볼륨 플러그인을 사용하고 있으면, vpusPerGB이 따로 설정되지 않아, 기본값인 vpusPerGB: 10에 해당합니다. $ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 4d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 4d VPU를 설정한 Persistent Volume 사용하기 Storage Class를 생성합니다. 예) vpusPerGB: 20\ncsi-bvs-sc-higher-perf.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: oci-bv-higher-perf provisioner: blockvolume.csi.oraclecloud.com parameters: vpusPerGB: \u0026#34;20\u0026#34; reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true PVC를 생성합니다.\ncsi-bvs-pvc-higher-perf.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-bvs-pvc-higher-perf spec: storageClassName: \u0026#34;oci-bv-higher-perf\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 50Gi 테스트를 위한 앱을 배포합니다.\nnginx-deployment-bvs-pvc-higher-perf.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-bvs-pvc-higher-perf name: nginx-bvs-pvc-higher-perf spec: replicas: 1 selector: matchLabels: app: nginx-bvs-pvc-higher-perf template: metadata: labels: app: nginx-bvs-pvc-higher-perf spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: csi-bvs-pvc-higher-perf 실행 예시\nwinter@cloudshell:~ (ap-chuncheon-1)$kubectl apply -f csi-bvs-sc-higher-perf.yaml storageclass.storage.k8s.io/oci-bv-higher-perf created winter@cloudshell:~ (ap-chuncheon-1)$kubectl apply -f csi-bvs-pvc-higher-perf.yaml persistentvolumeclaim/csi-bvs-pvc-higher-perf created winter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f nginx-deployment-bvs-pvc-higher-perf.yaml deployment.apps/nginx-bvs-pvc-higher-perf created winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 4d1h oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 4d1h oci-bv-higher-perf blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 35s winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE csi-bvs-pvc-higher-perf Bound csi-51c51276-6103-487e-a549-9039de4f8ae4 50Gi RWO oci-bv-higher-perf 29s winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-higher-perf-6495bcdf45-xzc2j 1/1 Running 0 5m23s spring-boot-hello-6865775b89-gzx4w 1/1 Running 0 3d3h 실행결과 생성된 Block Volume 확인\n실제 생성된 Block Volume을 오라클 클라우드 콘솔에서 확인해 보면, VPU: 20의 Higher Performance로 생성된 것을 볼 수 있습니다.\nvpusPerGB:0의 Lower Cost와 vpusPerGB:10인 기본 Storage Class인 oci-bv를 사용해도, 각각 관련 VPU의 Block Volume이 생성되는 것을 알 수 있습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 4d2h oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 4d2h oci-bv-higher-perf blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 29m oci-bv-lower-cost blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 15m winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE csi-bvs-pvc Bound csi-88a5df3c-2351-4554-9e27-57d7ef948025 50Gi RWO oci-bv 17m csi-bvs-pvc-higher-perf Bound csi-51c51276-6103-487e-a549-9039de4f8ae4 50Gi RWO oci-bv-higher-perf 29m csi-bvs-pvc-lower-cost Bound csi-219ba6c6-f3d5-4308-90a8-51431583dd23 50Gi RWO oci-bv-lower-cost 15m ","lastmod":"2022-04-11T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220411-support-for-pvc-block-volume-performance-levels/","tags":["oke","pvc","vpu"],"title":"Support for PVC block volume performance levels"},{"categories":null,"contents":"DevOps now supports deployment to private Kubernetes API endpoints DevOps 서비스가 OKE 쿠버네티스 클러스터의 API의 프라이빗 IP로 연결하는 것을 지원합니다.\nOracle Cloud Infrastructure Documentation / Release Notes OKE 클러스터는 프라이빗 IP가 기본적으로 할당되고, 생성시 선택에 따라 퍼블릭 IP가 추가적으로 할당됩니다. DevOps 서비스에서는 이제 Private Endpoint로 만든 Private Cluster에 배포하는 것을 지원합니다.\nQuick Create 모드로 생성시\nPrivate Endpoint: API 엔드포인트에 프라이빗 IP만 할당됨 Public Endpoint: API 엔드포인트에 퍼블릭 IP 및 프라이빗 IP 모두 할당됨 Custom Create 모드로 생성시\nAPI 엔드포인트에 프라이빗 IP가 기본 할당됨. 옵션 선택시 퍼블릭 IP 할당됨. DevOps 서비스에서 OKE Private Cluster에 배포하기 배포 환경 추가 DevOps 서비스에서 배포를 위해서는 환경(Environment)을 추가해야 합니다. 이때 OKE 유형을 택하면, 생성된 클러스터 중에서 아래 그림과 같이 이제 Private Endpoint를 가진 Private Cluster도 선택할 수 있습니다.\nPrivate Cluster를 선택한 다음, 대상 Private Cluster의 Kubernetes API Endpoint로 접근 가능한 서브넷을 지정합니다. (원문, If you select a private cluster, then select the VCN and subnet available to the Kubernetes API endpoint.)\nQuick Create로 생성된 클러스터 기준, nodesubnet을 서브넷으로 선택하면, 추가 설정없이 배포됩니다.\nQuick Create로 생성된 클러스터 기준, k8sApiEndpoint 서브넷를 선택한 경우에 Security Rule에 자기 서브넷:6443으로 나갈 수 있는 Egress Rule 추가가 필요합니다. 그렇지 않은 경우 연결 오류가 발생합니다.\nSecurity Rule 문서에는 DevOps가 속한 서비스 네트워크에서 들어올수 있는 Ingress Rule을 필요하면 추가하라고 하는데, Quic Create 생성된 클러스터 기준 테스트시 추가 작업없이 동작하였습니다.\nOKE 배포 환경 추가시 nodesubnet을 선택하면, nodesubnet은 당연히 API endpoint가 통신이 되도록 구성되어 있을 테니, 별다른 추가 설정없이 동작하였습니다.\n배포 결과 ","lastmod":"2022-03-29T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220329-devops-now-supports-deployment-to-private-kubernetes-api-endpoints/","tags":["DevOps","ci/cd"],"title":"DevOps now supports deployment to private Kubernetes API endpoints"},{"categories":null,"contents":"DevOps adds Blue-Green and Canary deployment strategies DevOps 서비스는 이제 Blue-Green 배포와 Canary 배포 방식을 제공합니다. 현재 Nginx Ingress Controller를 사용하여 해당 기능을 지원합니다. 반드시 사전에 Nginx Ingress Controller 설치가 필요합니다.\nOracle Cloud Infrastructure Documentation / Release Notes Blue/Green 배포전략 블루/그린 배포 전략은 다운타임을 최소화하고 리스크를 최소화하는 배포 전략으로 블루 환경이 서비스 중인 상태에, 신규 버전인 그린을 블루와 동일한 규모로 배포합니다. 그리고 준비가 되면, 모든 요청을 그린으로 가도록 트래픽 경로를 변경합니다. 문제가 발생하면, 대기중인 블루로 즉시 트래픽을 변경하여 원복할 수 있습니다. 두 배의 자원이 필요하지만, 리스크를 최소화하여 배포할 수 있습니다.\n실제 설정방법은 링크를 참조합니다. 2.2 DevOps 서비스의 Blue/Green 배포 전략으로 배포하기 Canary 배포전략 카나리 배포전략은 카나리아 새를 탄광 작업시 함께 데리고 가서 유독가스 누출 발생시 사람이 느끼는 단계 이전에, 카나리아가 먼저 반응하여 죽기 때문, 누출을 사전에 인지하는 것과 같이, 테스트 버전을 일부 사용자에게 노출하여, 사전에 잠재적인 문제를 확인하여 해결하고 전체로 전환하는 배포 방식입니다.\n실제 설정방법은 링크를 참조합니다. 2.3 DevOps 서비스의 Canary 배포 전략으로 배포하기 ","lastmod":"2022-03-29T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220329-devops-adds-blue-green-and-canary-deployment-strategies/","tags":["DevOps","ci/cd"],"title":"DevOps adds Blue-Green and Canary deployment strategies"},{"categories":null,"contents":"Support for Kubernetes version 1.22.5 OKE에서 쿠버네티스 지원 버전이 업데이트 되었습니다. 이제 1.22.5을 새롭게 지원합니다. 기존에 지원하던 1.21.5, 1.20.11은 계속 지원합니다.\nOracle Cloud Infrastructure Documentation / Release Notes 업데이트 사항 1.22.x 지원으로 인해 1.19.15 버전은 2022년 4월 22일 까지 지원합니다.\n지원이 종료하게 되면, 1.19.x버전으로 신규 클러스터 생성이 불가합니다. 지원이 종료하게 되면, 기존 클러스터에 1.19.x버전으로 새 Node Pool 추가가 불가합니다. 아래 그림은 현재 3월 22일 기준이며 1.22.5 출시로 인한 2022년 4월 22일 이후 변경이 발생하며, 지원 버전은 아래를 확인하세요. Supported Versions of Kubernetes 업그레이드 가이드에 따라 종료일 이전에 업그레이드를 권고하고 있습니다.\nUpgrading the Kubernetes Version on Control Plane Nodes in a Cluster Upgrading the Kubernetes Version on Worker Nodes in a Cluster 1.7 Kubernetes 지원 버전 및 업그레이드 ","lastmod":"2022-03-21T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220321-support-for-kubernetes-version-1.22.5/","tags":["oke"],"title":"Support for Kubernetes version 1.22.5"},{"categories":null,"contents":"Support for OCI Network Load Balancers OKE에서 Service Type을 Load Balancer를 사용할때 이제는 OCI Network Load Balancer을 추가적으로 지원합니다.\nOracle Cloud Infrastructure Documentation / Release Notes 릴리즈 노트를 기준으로 Network Load Balancer를 사용하면 다음과 같은 것이 가능합니다.\nLoad-balance traffic among Kubernetes pods with high throughput and low latency.\n=\u0026gt; OSI layer 3 and layer 4 (TCP/UDP/ICMP)기반 pass-through 방식으로 데이터를 열어보지 않고 분배하게 되어 보다 낮은 응답지연과 높은 처리량을 제공합니다.\nPreserve source and destination IP addresses and ports.\n=\u0026gt; Source, Destination IP와 Port가 그대로 보존됩니다. Pod에서 Client IP를 알 수 있습니다.\nHandle TCP and UDP traffic in the applications you deploy.\n=\u0026gt; TCP 뿐만 아니라, UDP도 지원합니다.\nOCI Load Balancer와 OCI Network Load Balancer의 차이는 다음 블로그를 참고합니다.\nComparing OCI Load Balancers: Quickly and Easily Network Load Balancer 사용하기 OCI Load Balancer 사용하기 테스트를 위해 아래와 같이 nginx를 배포합니다.\nService Type - Load Balancer에서 oci.oraclecloud.com/load-balancer-type를 지정하지 않으면, 기본값으로 OCI Load Balancer를 사용합니다. apiVersion: apps/v1 kind: Deployment metadata: name: my-nginx labels: app: nginx spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: my-nginx-lb labels: app: nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;lb\u0026#34; spec: type: LoadBalancer ports: - port: 80 selector: app: nginx 배포 결과 확인\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 26m my-nginx-lb LoadBalancer 10.96.197.58 129.154.54.161 80:32258/TCP 8m21s $ curl http://129.154.54.161 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; ... OCI Network Load Balancer 사용하기 앞서 배포된 Pod에 Network Load Balancer를 추가합니다.\nService Type - Load Balancer에서 oci.oraclecloud.com/load-balancer-type=\u0026ldquo;nlb\u0026quot;로 지정하면 OCI Network Load Balancer를 사용합니다. apiVersion: v1 kind: Service metadata: name: my-nginx-nlb labels: app: nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;nlb\u0026#34; spec: type: LoadBalancer ports: - port: 80 selector: app: nginx 배포 결과 확인\nOCI Load Balancer 쓸때와는 다르게 curl로 접속이 안 됩니다.. OCI Network Load Balancer를 OKE에서 사용할 때는 Security Rule을 직접 등록을 해줘야 합니다. $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 43m my-nginx-lb LoadBalancer 10.96.197.58 129.154.54.161 80:32258/TCP 24m my-nginx-nlb LoadBalancer 10.96.159.81 10.0.20.59,144.24.68.55 80:30350/TCP 31m $ curl http://138.2.117.137 ... Security Rule 등록\nQuick Create 모드로 생성한 클러스터를 기준으로 아래 규칙 추가가 필요합니다. (현재 my-nginx-nlb가 사용하는 Node Port는 30350 기준 설정입니다~\nsvclb subnet의 Security List 규칙 추가 설정\nIngress\nEgress\nsvclb subnet -\u0026gt; node subnet:port(10.0.10.0/24:30350)으로 나갈 수 있게 추가\nnode subnet의 Security List 규칙 추가 설정\nIngress\nsvclb subnet -\u0026gt; node subnet:port(10.0.10.0/24:30350)로 들어올 수 있게 추가\nSecurity Rule 추가후 다시 테스트하면 잘 동작합니다.\n$ curl http://144.24.68.55 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; 컨테이너에서 Client IP 주소 가져오기 웹브라우저로 NLB를 통해 배포된 nginx를 접속합니다. 기존 nginx 컨테이너 로그에서 Client IP 확인하면 10.x.x.x의 OCI 내부 IP가 보이는 것을 볼 수 있습니다.\n$ kubectl logs -lapp=nginx -f ... 10.244.0.128 - - [21/Mar/2022:06:32:27 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34; Source IP를 유지하는 기능을 활용하여, 컨테이너 상에서 접속한 클라이언트 IP를 알 수 있습니다. externalTrafficPolicy: Local을 아래와 같이 추가하면 됩니다.\n해당 설정 적용을 위해 기존 Service를 삭제하고 다시 적용합니다. apiVersion: v1 kind: Service metadata: name: my-nginx-nlb labels: app: nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;nlb\u0026#34; spec: loadBalancerIP: 144.24.68.55 type: LoadBalancer externalTrafficPolicy: Local ports: - port: 80 selector: app: nginx Security Rule 등록\nsvclb subnet의 Security List 규칙 추가 설정\n서비스 재배포로 인한 NodePort 변경시 변경 적용 또는 Kubernetes NodePort 대역 30000-32767을 사전에 적용 node subnet의 Security List 규칙 추가 설정\nIngress\nsvclb subnet -\u0026gt; node subnet:port 설정한 부분이, 이제 Source IP가 Load Balancer 내부 IP에서 인터넷상에서 오는 실제 Client IP가 그대로 오기 때문에, 이를 허용하기 위해 0.0.0.0/0으로 설정합니다.\n테스트전 자신의 IP를 확인합니다.\n웹브라우저로 NLB를 통해 배포된 nginx를 접속합니다.\n기존 nginx 컨테이너 로그에서 Client IP 확인하면 아래와 같이 Client IP가 정상적으로 보이는 것을 볼 수 있습니다.\n$ kubectl logs -lapp=nginx -f ... 202.45.129.186 - - [21/Mar/2022:07:00:24 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.80 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34; UDP로 서비스 하기 Load Balancer의 기본 프로토콜이 TCP이며, OCI Network Load Balancer를 사용하는 경우 UDP도 지원합니다.\n앞선 설정에서 protocal: UDP만 다음과 같이 추가하면 됩니다.\napiVersion: v1 kind: Service metadata: name: my-nginx-nlb labels: app: nginx annotations: oci.oraclecloud.com/load-balancer-type: \u0026#34;nlb\u0026#34; spec: type: LoadBalancer ports: - port: 80 protocol: UDP selector: app: nginx ","lastmod":"2022-03-15T00:00:02Z","permalink":"https://thekoguryo.github.io/release-notes/20220315-support-for-oci-network-load-balancers/","tags":["oke"],"title":"Support for OCI Network Load Balancers"},{"categories":null,"contents":"Container image scanning using the Console OCIR에서 컨테이너 이미지 스캐닝 기능을 지원하고 있습니다. 이에 추가하여 Vulnerability Scanning 콘솔에서도 할 수 있게 기능이 추가되었습니다.\nOracle Cloud Infrastructure Documentation / Release Notes Container Image Scan Recipe 만들기 IAM 권한 설정 compartment 단위 또는 전체 테넌시 단위로 권한을 지정할 수 있습니다. 여기서는 전체 테넌시에 대해 Vulnerability Scanning 서비스가 이미지 리파지토리 및 compartment에 접근할 수 있는 권한을 부여합니다.\n전체 테넌시\n이름 예) ocir-scanning-images-root-policy\nallow service vulnerability-scanning-service to read repos in tenancy allow service vulnerability-scanning-service to read compartments in tenancy Container Image Scan Recipe 만들기 Oracle Cloud 콘솔에 로그입니다.\nIdentity \u0026amp; Security \u0026gt; Scanning \u0026gt; Scan Recipes 메뉴로 이동합니다.\nCreate를 클릭하여 이미지 스캔 레시피를 만듭니다.\n이름 예) container-image-scan-recipe\nType: Container image\nContainer Image Target 생성 이미지 스캔 레시피 상세화면에서 Create Target을 클릭합니다.\n컨테이너 이미지 타겟을 만듭니다.\n이름 예) container-image-target\n대상 Repository: 테넌시내 모든 Repository가 되도록 선택합니다.\n스캐닝 리포트 확인 내비게이션 메뉴에서 Identity \u0026amp; Security \u0026gt; Scanning \u0026gt; Scanning Reports 메뉴로 이동합니다.\nContainer Image 탭으로 이동합니다.\n스캐닝 된 이미지와 리포트를 볼 수 있습니다.\n원하는 이미지의 이름을 클릭하면, 취약점 분석 결과를 볼 수 있습니다.\n리스트된 취약점 중에 하나를 클릭합니다. 해당 취약점 기준으로 해당 취약점이 있는 호스트, 컨테이너 이미지 내역을 볼 수 있습니다.\nCVE ID를 클릭하면, 취약점을 관리하는 원 데이터베이스에서 상세 정보를 확인할 수 있습니다.\n취약점 리포트 확인 내비게이션 메뉴에서 Identity \u0026amp; Security \u0026gt; Scanning \u0026gt; Vulnerability Reports 메뉴로 이동합니다.\n취약점 ID 기준으로 리포트를 볼수 있습니다. 각 CVE ID를 클릭하면, 스캐닝 리포트와 동일하게 해당 취약점에 대한 상세 내용을 확인할 수 있습니다.\n","lastmod":"2022-03-15T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220315-container-image-scanning-using-the-console/","tags":["Vulnerability Scanning","vss"],"title":"Container image scanning using the Console"},{"categories":null,"contents":"Support for function invocation logs Oracle Function에 대해서 Logging을 활성화 한 경우, Logging 화면에서 호출 수행시간과 에러시 에러 코드를 확인할 수 있게 되었습니다.\nOracle Cloud Infrastructure Documentation / Release Notes Function 배포 배포된 함수가 없는 경우 아래 문서를 참조하여 테스트할 함수를 사전에 배포합니다.\nFunctions QuickStart on Cloud Shell\nLog 활성화하기 배포한 함수의 상세화면으로 이동합니다.\nEnable Log를 토글하여 활성화합니다.\n로그 생성\n생성한 Log Group이 없으면, 자동 생성하도록 설정합니다.\n로그가 활성화되면, 생성된 로그의 이름을 클릭하여 이동합니다.\n로그 테스트 Cloud Shell에서 배포된 함수를 호출합니다.\n$ fn invoke helloworld-app hello-java Hello, world! $ echo -n \u0026#39;John\u0026#39; | fn invoke helloworld-app hello-java Hello, John! 로그 확인\n아래와 같이 호출된 건에 대한 처리 시간을 확인할 수 있습니다.\n","lastmod":"2022-03-03T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220303-support-for-function-invocation-logs/","tags":["Functions","fn","serverless"],"title":"Support for function invocation logs"},{"categories":null,"contents":"Support for PVC block volume expansion OKE에서 Block Volume을 Persistent Volume으로 사용하는 경우, 할당 받은 이후에도 사용 중인 볼륨의 크리를 확장하는 기능을 지원합니다.\nOracle Cloud Infrastructure Documentation / Release Notes Block Volume Expansion 설정 확인 클러스터 생성후 기본 구성된StorageClass를 업데이트 전후로 확인한 내용입니다.\n기능 업데이트 전\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 2d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d 업데이트 후\nALLOWVOLUMEEXPANSION 항목이 기존 false -\u0026gt; true로 변경되었습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 22h oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer true 22h 기본 구성된 oci-bv StorageClass를 사용하면 기본적으로 볼륨 사이즈 확장이 됩니다. 새롭게 Block Volume CSI Driver(provisioner: blockvolume.csi.oraclecloud.com)로 StorageClass를 생성한다고 하면 아래와 같이 allowVolumeExpansion: true 를 추가하면 됩니다.\n$ kubectl get sc oci-bv -o yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: oci-bv provisioner: blockvolume.csi.oraclecloud.com reclaimPolicy: Delete volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true Block Volume Expansion 기능 확인 아래와 같이 PV 요청 yaml을 사용하여 50Gi 사이즈로 요청합니다.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-bvs-pvc spec: storageClassName: \u0026#34;oci-bv\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 50Gi 테스트 앱 배포\n요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱 apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx-bvs-pvc name: nginx-bvs-pvc spec: replicas: 1 selector: matchLabels: app: nginx-bvs-pvc template: metadata: labels: app: nginx-bvs-pvc spec: containers: - name: nginx image: nginx:latest volumeMounts: - name: data mountPath: /usr/share/nginx/html volumes: - name: data persistentVolumeClaim: claimName: csi-bvs-pvc 생성 결과\n아래와 같이 정상적으로 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다. winter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created winter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-f0f36095-34fa-4d57-8868-a2a102a044c7 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 15s winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-lst4d 1/1 Running 0 72s PV를 사용하는 앱이 배포된 상태에서 볼륨 크기를 늘립니다. 50Gi -\u0026gt; 100Gi로 변경후 적용합니다.\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: csi-bvs-pvc spec: storageClassName: \u0026#34;oci-bv\u0026#34; accessModes: - ReadWriteOnce resources: requests: storage: 100Gi 적용\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc configured winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-f0f36095-34fa-4d57-8868-a2a102a044c7 100Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 4m23s 볼륨 확장 결과확인\n이벤트를 확인해 보면, 볼륨 확장이 성공했음을 알 수 있습니다. 컨테이너 재기동도 일어나지 않았습니다. winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get events ... 6m50s Normal Created pod/nginx-bvs-pvc-7b574c9c5c-lst4d Created container nginx 6m50s Normal Started pod/nginx-bvs-pvc-7b574c9c5c-lst4d Started container nginx 3m53s Normal Resizing persistentvolumeclaim/csi-bvs-pvc External resizer is resizing volume csi-f0f36095-34fa-4d57-8868-a2a102a044c7 3m53s Warning ExternalExpanding persistentvolumeclaim/csi-bvs-pvc Ignoring the PVC: didn\u0026#39;t find a plugin capable of expanding the volume; waiting for an external controller to process this PVC. 3m52s Normal FileSystemResizeRequired persistentvolumeclaim/csi-bvs-pvc Require file system resize of volume on node 3m14s Normal FileSystemResizeSuccessful persistentvolumeclaim/csi-bvs-pvc MountVolume.NodeExpandVolume succeeded for volume \u0026#34;csi-f0f36095-34fa-4d57-8868-a2a102a044c7\u0026#34; 3m14s Normal FileSystemResizeSuccessful pod/nginx-bvs-pvc-7b574c9c5c-lst4d MountVolume.NodeExpandVolume succeeded for volume \u0026#34;csi-f0f36095-34fa-4d57-8868-a2a102a044c7\u0026#34; winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-lst4d 1/1 Running 0 4m42s 컨테이너 내부 확인\n컨테이너 내부를 확인해 보면 PV가 마운트된 /usr/share/nginx/html의 사이즈가 99G인 것을 알 수 있습니다. 볼륨이 확장된 것을 알 수 있습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-lst4d -- /bin/bash root@nginx-bvs-pvc-7b574c9c5c-lst4d:/# df -h Filesystem Size Used Avail Use% Mounted on overlay 39G 6.8G 32G 18% / tmpfs 64M 0 64M 0% /dev tmpfs 7.7G 0 7.7G 0% /sys/fs/cgroup shm 64M 0 64M 0% /dev/shm tmpfs 7.7G 28M 7.7G 1% /etc/hostname /dev/sda3 39G 6.8G 32G 18% /etc/hosts /dev/sdb 99G 60M 94G 1% /usr/share/nginx/html tmpfs 7.7G 12K 7.7G 1% /run/secrets/kubernetes.io/serviceaccount tmpfs 7.7G 0 7.7G 0% /proc/acpi tmpfs 7.7G 0 7.7G 0% /proc/scsi tmpfs 7.7G 0 7.7G 0% /sys/firmware ","lastmod":"2022-02-28T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220228-support-for-pvc-block-volume-expansion/","tags":["oke","pvc"],"title":"Support for PVC block volume expansion"},{"categories":null,"contents":"Support for 2 GB functions Oracle Function에 대해서 최대 2GB까지 메모리를 할당할 수 있게 되었습니다.\nOracle Cloud Infrastructure Documentation / Release Notes Function은 호출 건과 사용한 메모리에 따라 가격을 산정합니다. 메모리는 초당 사용한 GB 메모리 기준 400,000 까지 무료이며 그 이상은 비용을 청구합니다. 함수 당 2GB까지 설정할 수 있지만, 비용과 관련이 있으므로 적정량만 할당합니다.\nCloud Function pricing Function 메모리 설정하기 따로 지정하지 않는 경우 기본값으로 128MB 메모리를 할당합니다.\n메모리 설정시 최대 2GB까지 지정할 수 있습니다. 128, 256, 512, 1024, 2048 중에 설정할 수 있습니다.\n함수 소스 루트에 있는 func.yaml 파일에 memory: 2048과 같이 값을 추가합니다.\nschema_version: 20180708 name: hello-java version: 0.0.4 runtime: java build_image: fnproject/fn-java-fdk-build:jdk11-1.0.146 run_image: fnproject/fn-java-fdk:jre11-1.0.146 cmd: com.example.fn.HelloFunction::handleRequest memory: 2048 fn cli로 업데이트 합니다. fn update function --memory 1024 \u0026lt;app-name\u0026gt; \u0026lt;function-name\u0026gt;와 같이 실행합니다.\nfn update function --memory 2048 helloworld-app hello-java Oracle Cloud 콘솔에서 함수 설정화면에서 변경합니다.\n","lastmod":"2022-02-11T00:00:01Z","permalink":"https://thekoguryo.github.io/release-notes/20220211-support-for-2gb-functions/","tags":["Functions","fn","serverless"],"title":"Support for 2 GB functions"},{"categories":null,"contents":"1.7.1.1 DevOps 서비스를 이용한 Spring Boot 앱을 OKE에 배포 자동화하기 DevOps 서비스 사용을 위한 IAM Policy 설정 DevOps 서비스를 사용하기 위해서는 DevOps 자원들에 권한 설정이 필요합니다. 공식 문서를 참조하여 권한 설정을 위한 Dynamic Group 및 Group에 대한 Policy를 설정합니다.\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/devops_iampolicies.htm#build_policies 아래 Dynamic Group 및 Policy는 위 문서의 예제를 기준으로 작성한 내용으로 요구사항에 따라 일부 변경이 될 수 있습니다.\nDynamic Group 만들기\n주어진 Compartment 내에서 DevOps 서비스를 사용할 수 있도록 Compartment에 대한 Dynamic Group을 먼저 생성합니다.\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\nDevOps를 사용할 Compartment로 이동하여 OCID를 복사해 둡니다.\nIdentity \u0026gt; Dynamic Groups로 이동합니다.\nCreate Dynamic Group을 클릭합니다.\n복사해둔 Compartment OCID를 이용해 필요한 Dynamic Group을 만듭니다.\nCoderepoDynamicGroup\nALL {resource.type = \u0026#39;devopsrepository\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;} ConnectionDynamicGroup\nALL {resource.type = \u0026#39;devopsconnection\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;} BuildDynamicGroup\nALL {resource.type = \u0026#39;devopsbuildpipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;} DeployDynamicGroup\nAll {resource.type = \u0026#39;devopsdeploypipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;} DevOps 서비스를 위한 Policy 설정하기\nIdentity \u0026gt; Policies로 이동합니다.\nCreate Policy을 클릭하여 새 Policy를 만듭니다.\nCompartment 레벨로 다음 Policy를 만듭니다.\nName: 예) DevOps-compartment-policy Allow dynamic-group CoderepoDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage repos in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to read secret-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage generic-artifacts in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to use ons-topics in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group CoderepoDynamicGroup to read secret-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group DeployDynamicGroup to manage all-resources in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group ConnectionDynamicGroup to read secret-family in compartment \u0026lt;YourCompartmentName\u0026gt; Root Compartment 레벨로 다음 Policy를 만듭니다.\nName: 예) DevOps-root-policy OCIR에 Repository를 Push하기 전에 미리 생성하지 않으면 기본적으로 Root Compartment에 이미지가 Push됩니다. 이때 권한으로 에러가 발생하며, Root Compartment에도 허용하고자 하면 다음을 추가합니다.\nAllow dynamic-group BuildDynamicGroup to manage repos in tenancy DevOps 서비스를 통한 CI/CD 배포 자동화 하기 Notification Topic 만들기\nDevOps 파이프 라인 실행이 발생하는 주요 이벤트를 알려주기 위한 용도로 Notification Topic 설정이 필요합니다. DevOps 프로젝트 생성시 필수 요구 사항이라 미리 만듭니다\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Application Integration \u0026gt; Notifications으로 이동합니다.\nCreate Topic을 클릭하여 Topic을 생성합니다.\nName: 예) oke-labs-devops-topic Notification을 위해 생성한 Topic 이벤트를 가져갈 Subscrition을 일단 생략합니다. 필요시 구성하시면 됩니다.\nDevOps 프로젝트 만들기\nOCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; DevOps로 이동합니다.\n프로젝트 생성을 위해 Projects로 이동하여 Create DevOps project를 클릭합니다.\n생성 정보를 입력하여 프로젝트를 만듭니다.\nProject name: 예) my-devops-project Notification Topic: 앞서 생성한 Topic 선택 프로젝트 생성완료\nEnable Logging\n프로젝트 생성 직후 Enable Logging 관련 정보가 보이는 것을 볼 수 있습니다. 설명문에서 보는 것 처럼 Logging을 활성화하지 않을 경우, 파이프라인 실행 화면에서 오른쪽에 보이는 실행 로그가 안보입니다. 그래서 Enable Logging은 필수입니다.\nProject Overview에서 Enable Log을 클릭하거나 왼쪽 메뉴에서 Logs를 클릭합니다.\n로그를 활성화 버튼을 토글합니다.\n대상 Compartment에 이미 Log Group이 있는 경우 나열된 것 중에 선택이 가능합니다. 미리 생성된 Log Group이 없는 경우 아래와 같이 자동입력된 정보를 바탕으로 Enable Log 버튼 클릭시 새로 Log Group과 Log가 만들어 지게 됩니다. 필요시 설정을 수정하고 그렇치 않으면, Enable Log 버튼을 클릭합니다.\nCode Repository를 사용하여 애플리케이션 코드 관리하기\n샘플로 Spring Boot Helloworld 앱을 만들어 테스트하겠습니다.\n코드 저장소 생성을 위해 왼쪽 메뉴에서 Code Repositories를 클릭합니다.\nCreate repository를 클릭하여 저장소를 만듭니다.\nRepository name: 예) spring-boot-hello-repo 생성된 코드 저장소 입니다. 일반적인 Git Repository입니다.\n실제 개발 작업은 git 명령을 통해 개발 PC에서 진행하면 됩니다. 저장소 상세정보 위에 있는 Clone 버튼을 하면 Clone 명령어가 아래 그림처럼 뜨게 됩니다. 여기서는 Clone with HTTPS를 사용하겠습니다.\n개발 PC에 복사한 주소를 사용해 git clone 명령어를 통해 복제합니다.\ngit clone \u0026lt;YourClonewithHTTPS URL\u0026gt; 이때 사용자 인증이 필요합니다. HTTPS기반 사용자 인증시 아래 유저명 형식과 AuthToken을 사용합니다.\n인증 유저명\nOracle Identity Cloud Service상의 유저: \u0026lt;tenancy-name\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-name\u0026gt;/\u0026lt;username\u0026gt; 이전 가이드들과 달리 tenancy-namespace가 아닌 tenacy-name인 것에 주의합니다. AuthToken: 생성에 대한 내용은 이전 가이드들을 참고합니다.\nCode Repository의 HTTPS 인증관련 문서\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/clone_repo.htm#https_auth 예시\n$ git clone https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com/namespaces/cnrlxx3w0wgq/projects/my-devops-project/repositories/spring-boot-hello-repo Cloning into \u0026#39;spring-boot-hello-repo\u0026#39;... Username for \u0026#39;https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: thekoguryo/oke-developer Password for \u0026#39;https://oreozz/oke-admin@devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: remote: Counting objects: 2, done remote: Finding sources: 100% (2/2) remote: Getting sizes: 100% (1/1) Unpacking objects: 100% (2/2), done. remote: Total 2 (delta 0), reused 2 (delta 0) 현재 복제된 저장소는 비어 있습니다. 아래 가이드를 통해 spring-boot-hello 샘플 코드를 작성합니다.\nhttps://spring.io/guides/gs/spring-boot-docker/ 작성된 코드를 git 명령어를 통해서 Code Repository에 저장합니다.\n예시\ngit add . git commit -m \u0026#34;init\u0026#34; git push 코드 작성 및 반영 완료\nBuild Pipeline 만들기\nCI/CD 중에 코드를 빌드하여 배포 산출물을 만드는 CI 과정에 해당되는 부분을 Build Pipeline을 통해 구성이 가능합니다.\nmy-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Build Pipelines로 이동합니다.\nCreate build pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) spring-boot-hello-build-pipeline\n생성된 파이프라인을 클릭합니다.\n그림과 같이 Stage를 추가하여 파이프라인 흐름을 구성할 수 있습니다. Add Stage를 클릭합니다.\n제공 Stage\nManaged Build: 빌드스펙에 정의된 내용에 따라 빌드 과정을 실행합니다. Delivery Artifacts: 빌드 산출물(예시, 컨테이너 이미지)를 Artifact Repository에 저장합니다. Trigger Deployment: 빌드가 끝나고 Deployment Pipeline을 호출합니다. Wait: 일정시간 대기합니다. Build Stage 만들기\n빌드를 위해 먼저 Managed Build Stage를 추가합니다.\nManaged Build Stage 설정\nStage name: 예) build-stage Build Spec File Path: 빌드 스크립트 경로를 지정합니다. 기본적으로 소스 루트에 있는 build_spec.yaml을 파일을 사용합니다. Primary Code Repository: 빌드할 메인 소스가 있는 코드 저장소를 지정합니다. Primary Code Repository 설정 화면\n대상 소스 코드가 있는 저장소를 지정합니다. 설정된 Stage를 Add를 클릭하여 추가합니다.\n아래 그림과 같이 build-stage가 추가되었습니다. Start Manual Run을 클릭하면 테스트를 해 볼수 있습니다.\n테스트처럼 소스 코드상의 Build Spec의 정의가 필요합니다.\nBuild Spec은 다음 문서를 참조합니다.\nhttps://docs.oracle.com/en-us/iaas/Content/devops/using/build_specs.htm\n문서에 있는 Example 2 기준 예시\nsteps: 실행할 스크립트를 정의하는 부분입니다. 예제이는 Build Source, Dockerizer 2개의 step이 정의되어 있고 각각 command에서 실행할 스크립트를 정의하고 있습니다. 정의된 순서대로 실행됩니다. env.exportedVariables: 전역으로 선언된 환경변수로 이전 step에서 값을 변경하면 그다음 step에도 적용됩니다. Deployment Pipeline을 호출시에도 전달됩니다. outputArtifacts: 빌드 산출물의 정의하는 부분으로, 이후 Delivery Artifact Stage를 통해 Artifact Storage에 저장할 때 여기서 정의된 이름을 통해 지정 가능합니다. version: 0.1 component: build timeoutInSeconds: 6000 shell: bash env: exportedVariables: - BuildServiceDemoVersion steps: - type: Command name: \u0026#34;Build Source\u0026#34; timeoutInSeconds: 4000 command: | echo $PATH mvn clean install - type: Command timeoutInSeconds: 400 name: \u0026#34;Dockerizer\u0026#34; command: | BuildServiceDemoVersion=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` echo $BuildServiceDemoVersion docker build -t build-service-demo . outputArtifacts: - name: build-service-demo type: DOCKER_IMAGE location: build-service-demo - name: build-service-demo-kube-manifest type: BINARY location: deployment/app.yml Build Spec 정의\n개발한 spring-boot-hello 소스 코드의 root 경로에 build_spec.yaml을 다음과 같이 정의하고 코드 저장소에 저장합니다.\nbuild_spec.yaml\nversion: 0.1 component: build timeoutInSeconds: 6000 shell: bash env: variables: appName: \u0026#34;spring-boot-hello\u0026#34; exportedVariables: - APP_NAME - OCIR_PATH - TAG steps: - type: Command name: \u0026#34;Init exportedVariables\u0026#34; timeoutInSeconds: 4000 command: | APP_NAME=$appName echo $APP_NAME - type: Command name: \u0026#34;Build Source\u0026#34; timeoutInSeconds: 4000 command: | echo \u0026#34;build\u0026#34; mvn clean install - type: Command timeoutInSeconds: 400 name: \u0026#34;Build Source - Post\u0026#34; command: | echo \u0026#34;add dependency\u0026#34; mkdir -p target/dependency \u0026amp;\u0026amp; (cd target/dependency; jar -xf ../*.jar) - type: Command name: \u0026#34;Define Image Tag - Commit ID\u0026#34; timeoutInSeconds: 30 command: | COMMIT_ID=`echo ${OCI_TRIGGER_COMMIT_HASH} | cut -c 1-7` BUILDRUN_HASH=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` [ -z \u0026#34;$COMMIT_ID\u0026#34; ] \u0026amp;\u0026amp; TAG=$BUILDRUN_HASH || TAG=$COMMIT_ID - type: Command name: \u0026#34;Define OCIR Path\u0026#34; timeoutInSeconds: 30 command: | TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` REPO_NAME=$appName OCIR_PATH=$OCI_RESOURCE_PRINCIPAL_REGION.ocir.io/$TENANCY_NAMESPACE/$REPO_NAME - type: Command timeoutInSeconds: 400 name: \u0026#34;Containerize\u0026#34; command: | docker build -t new-generated-image . docker images - type: Command name: \u0026#34;Check exportedVariables\u0026#34; timeoutInSeconds: 30 command: | [ -z \u0026#34;$APP_NAME\u0026#34; ] \u0026amp;\u0026amp; APP_NAME=unknown [ -z \u0026#34;$OCIR_PATH\u0026#34; ] \u0026amp;\u0026amp; OCIR_PATH=unknown [ -z \u0026#34;$TAG\u0026#34; ] \u0026amp;\u0026amp; TAG=unknown echo \u0026#34;APP_NAME: \u0026#34; $APP_NAME echo \u0026#34;OCIR_PATH: \u0026#34; $OCIR_PATH echo \u0026#34;TAG: \u0026#34; $TAG outputArtifacts: - name: output-image type: DOCKER_IMAGE location: new-generated-image Start Manual Run을 통해 다시 실행하면 아래와 같이 스크립트가 수행되는 것을 볼 수 있습니다.\nExportVariables 확인\n실행 결과 화면에서 오른쪽 위쪽 점3개를 클릭하여 상세 화면으로 이동하면 Build Output에서 실행결과로 나온 변수값을 볼 수 있습니다. 이 변수들은 이후 Stage 또는 연결되어 호출된 Deployment Pipeline으로 전달되어 사용할 수 있게 됩니다.\n컨테이너 이미지 OCIR 등록 Stage 만들기\nBuild Pipeline 탭으로 이동합니다.\n플러스 버튼을 클릭하여 build-stage 다음에 stage를 추가합니다.\nDelivery Artifact Stage를 선택합니다.\nstage 이름을 입력하고 Create Artifact를 선택합니다.\nContainer image 유형으로 Artifact 추가합니다.\n이미지 경로: docker tag를 달때 사용하는 이미지 경로입니다. 직접 입력해도 되지만 여기서는 build-stage에서 넘어온 exportedVariable을 사용하여 ${OCIR_PATH}:${TAG} 과 같이 입력합니다. 같은 방식으로 하나 더 추가 합니다.\nName: generated_image_with_latest Image Path: ${OCIR_PATH}:latest Artifact 매핑\nAssociate Artifact에서 방금 추가한 2개의 Artifact에 실제 컨테이너 이미지 파일을 매핑해 줍니다. 앞서 build-stage에서 build_spec.yaml에서 정의한 outputArtifacts의 이름을 입력합니다.\noutputArtifacts: - name: output-image type: DOCKER_IMAGE location: new-generated-image 이제 delivery stage까지 추가하였습니다.\n파이프라인을 다시 실행해 봅니다. 실제 소스코드로 빌드된 컨테이너 이미지가 OCIR에 자동으로 등록됩니다.\nOCIR 리파지토지(예시, spring-boot-hello)가 사전에 없는 경우 Root Compartment에 Private 형태로 만들어집니다. OKE에서 가져오기 위해서는 imagepullsecret을 사전에 생성하거나, 리파지토리를 미리 Public으로 생성합니다.\nDeploy Pipeline 만들기\nCI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\nmy-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\nCreate pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) spring-boot-hello-deployment-pipeline\n생성된 파이프라인을 클릭합니다.\nAdd Stage를 클릭하여 Stage를 추가합니다.\n제공 Stage\nDeploy: OKE, Compute 인스턴스 배포, Oracle Function에 배포 기능을 제공합니다.\nControl: 승인 대기, 트래픽 변경, 대기 등을 지원합니다.\nIntegration: 커스텀 로직 수행을 위한 Oracle Function 실행을 지원합니다.\nKubernetes에 배포할 manifest 파일 준비\nKubernetes에 배포할 Stage 유형을 사용하기 위해서는 사전에 배포할 manifest yaml 파일을 준비해야 합니다.\nmy-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Artifacts로 이동합니다.\nArtifacts로 앞서 빌드 파이프라인 만들때 등록한 2개가 있는 것을 볼수 있습니다. 여기에 등록된 Artifact는 재사용이 가능합니다.\nmanifest 파일을 등록하기 위해 Add artifact를 클릭합니다.\n4 가지 등록 유형을 제공합니다. 이중에 Kubernetes manifest를 선택합니다.\nKubernetes manifest 유형에는 Artifact Source로 2가지 유형을 제고합니다.\nArtifact Registry Repository: Container Registry로 OCIR을 제공하고 있듯시 Artifact Registry를 서비스로 제공하고 있습니다. 그곳에 있는 자원을 참조할 경우에 선택합니다. Inline: 인라인은 현재 DevOps 프로젝트에 있는 여기 Artifact에 직접 입력하는 것을 말합니다. Artifact Source로 Inline 유형으로 다음과 같이 등록합니다.\nName: 예) k8s_spring_boot_deploy_template Value\n앞 서와 같이 build-stage에서 export한 변수값들을 사용할 수 있습니다.\napiVersion: apps/v1 kind: Deployment metadata: labels: app: ${APP_NAME} name: ${APP_NAME} spec: replicas: 1 selector: matchLabels: app: ${APP_NAME} template: metadata: labels: app: ${APP_NAME} spec: containers: - name: ${APP_NAME} image: ${OCIR_PATH}:${TAG} --- apiVersion: v1 kind: Service metadata: name: ${APP_NAME}-service annotations: service.beta.kubernetes.io/oci-load-balancer-shape: \u0026#34;10Mbps\u0026#34; spec: type: LoadBalancer ports: - port: 80 protocol: TCP targetPort: 8080 selector: app: ${APP_NAME} Kubernetes Environment 등록하기\nmy-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Environments로 이동하여 배포할 OKE 환경을 등록합니다.\nOKE 유형을 선택합니다.\n배포할 클러스터를 선택합니다.\nKubernetes manifest 배포 Stage 만들기\n등록한 Deployment Pipeline(spring-boot-hello-deployment-pipeline) 설정 페이지로 이동합니다.\nAdd Stage를 클릭하여 Apply manifest to your Kubernetes cluster Stage를 추가합니다.\n배포할 환경 및 manifest 파일을 선택합니다\n파이프라인 완성\nBuild Pipeline에서 Deployment Pipeline 호출하기\n앞서 만든 Build Pipeline에서 컨테이너 이미지 까지 OCIR에 등록하고 나면, OKE에 배포할 Deployment Pipeline을 기동되어야 전체 빌드에서 배포까지가 완료됩니다. 이제 Deployment Pipeline을 등록하였으므로, Build Pipeline에서 호출할 수 있습니다.\n앞서 만든 Build Pipelines(spring-boot-hello-build-pipeline)으로 이동합니다.\n파이프라인 마지막에 Stage를 추가합니다.\nTrigger Deployment 유형을 선택합니다.\n설정한 Deployment Pipeline을 지정합니다.\n전체 흐름이 완료되었습니다.\nTrigger 설정하기\n지금 까지는 테스트를 하기 위해 Build Pipeline에서 Start Manual Run을 통해 시작하였습니다. 실제로는 개발자가 코드를 코드 저장소에 반영이 될 때 자동으로 빌드, 배포 파이프라인이 동작할 필요가 있습니다. Trigger는 코드 저장소에 발생한 이벤트를 통해 빌드 파이프라인을 시작하게 하는 역할을 하게 됩니다.\nmy-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Trigger로 이동합니다.\nCreate trigger을 클릭합니다.\nTrigger를 설정합니다.\nName: 예) spring-boot-hello-trigger Source Code Repository: OCI Code Repository, GitHub, GitLab 연동을 지원하며, 예제에서는 앞서 만든 OCI Code Repository상의 spring-boot-hello-repo를 선택합니다. Actions: 트리거링 되었을 때 호출하는 액션으로 작성한 빌드 파이프라인인 spring-boot-hello-build-pipeline을 선택합니다. 설정이 완료되었습니다.\n테스트\nTrigger에서 지정한 spring-boot-hello 소스 코드에 임의의 변경사항을 발생시키고 Code Repository에 반영합니다.\n저는 Application.java에 있는 응답메시지를 \u0026ldquo;Hello OCI DevOps\u0026quot;로 변경하고 반영하셨습니다.\n빌드 실행 내역을 보면, 그림과 같이 Trigger 된것은 Commit ID가 함께 보이며, Code Repository와 링크되어 있습니다.\nCommit ID를 클릭하면 Code Repository상의 코드 변경 분을 확인할 수 있습니다.\n빌드 파이프라인이 정상적으로 코드 빌드 부터 컨테이너 이미지 생성, 배포 파이프라인 호출까지 실행되었습니다.\n배포 파이프라인도 정상 실행되었습니다.\nOKE 클러스터를 조회해 보면 정상 배포 되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/spring-boot-hello-54848fcfd5-5jpxh 1/1 Running 0 5m39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16h service/spring-boot-hello-service LoadBalancer 10.96.186.158 146.56.186.172 80:32224/TCP 41m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spring-boot-hello 1/1 1 1 15h NAME DESIRED CURRENT READY AGE replicaset.apps/spring-boot-hello-54557d9c47 0 0 0 41m replicaset.apps/spring-boot-hello-54848fcfd5 1 1 1 5m39s 서비스 주소로 접속시 정상 동작을 확인할 수 있습니다.\n","lastmod":"2021-11-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/cicd/devops/1.deploy-app-on-oke-using-devops/","tags":["devops","ci/cd"],"title":"1.7.1.1 Spring Boot 앱 배포 자동화하기"},{"categories":null,"contents":"Let’s Encrypt로 무료 SSL 인증서 발급받기 아래는 dns-manual-mode로 SSL 인증서를 발급하는 방법입니다.\n프로젝트 클론후 설치합니다.\ngit clone https://github.com/acmesh-official/acme.sh.git cd ./acme.sh ./acme.sh --install -m my@example.com 재접속하거나, 지금 세션에서 아래 명령을 실행합니다.\nsource ~/.bashrc 기본 인증서 발급기관을 letsencrypt로 설정합니다.\nacme.sh --set-default-ca --server letsencrypt 사용할 도메인에 대한 인증서 발급 명령을 실행합니다. RSA2048 키 형식으로 만듭니다.\nacme.sh --issue --force --keylength 2048 -d thekoguryo.xyz -d \u0026#39;*.thekoguryo.xyz\u0026#39; --dns --yes-I-know-dns-manual-mode-enough-go-ahead-please 실행예시\n$ acme.sh --issue --force --keylength 2048 -d thekoguryo.xyz -d \u0026#39;*.thekoguryo.xyz\u0026#39; --dns --yes-I-know-dns-manual-mode-enough-go-ahead-please [Fri Jan 19 02:51:04 GMT 2024] Using CA: https://acme-v02.api.letsencrypt.org/directory [Fri Jan 19 02:51:04 GMT 2024] Create account key ok. [Fri Jan 19 02:51:04 GMT 2024] Registering account: https://acme-v02.api.letsencrypt.org/directory [Fri Jan 19 02:51:07 GMT 2024] Registered [Fri Jan 19 02:51:07 GMT 2024] ACCOUNT_THUMBPRINT=\u0026#39;naWc9dOtc4HQw_Ftb8H4UojHkYIN_GI4EpbIJ7oV1aQ\u0026#39; [Fri Jan 19 02:51:07 GMT 2024] Creating domain key [Fri Jan 19 02:51:07 GMT 2024] The domain key is here: /home/opc/.acme.sh/thekoguryo.xyz/thekoguryo.xyz.key [Fri Jan 19 02:51:07 GMT 2024] Multi domain=\u0026#39;DNS:thekoguryo.xyz,DNS:*.thekoguryo.xyz\u0026#39; [Fri Jan 19 02:51:07 GMT 2024] Getting domain auth token for each domain [Fri Jan 19 02:51:09 GMT 2024] Getting webroot for domain=\u0026#39;thekoguryo.xyz\u0026#39; [Fri Jan 19 02:51:09 GMT 2024] Getting webroot for domain=\u0026#39;*.thekoguryo.xyz\u0026#39; [Fri Jan 19 02:51:09 GMT 2024] Add the following TXT record: [Fri Jan 19 02:51:09 GMT 2024] Domain: \u0026#39;_acme-challenge.thekoguryo.xyz\u0026#39; [Fri Jan 19 02:51:09 GMT 2024] TXT value: \u0026#39;r__JG3oEDK-t39Tz0cbcvpbqUvjn7QaMDyfuMg6W3ek\u0026#39; [Fri Jan 19 02:51:09 GMT 2024] Please be aware that you prepend _acme-challenge. before your domain [Fri Jan 19 02:51:09 GMT 2024] so the resulting subdomain will be: _acme-challenge.thekoguryo.xyz [Fri Jan 19 02:51:09 GMT 2024] Add the following TXT record: [Fri Jan 19 02:51:09 GMT 2024] Domain: \u0026#39;_acme-challenge.thekoguryo.xyz\u0026#39; [Fri Jan 19 02:51:09 GMT 2024] TXT value: \u0026#39;iuyByqkwUUG2IWp33foj_uysPHQg0lD_YXlIM0FtUpo\u0026#39; [Fri Jan 19 02:51:09 GMT 2024] Please be aware that you prepend _acme-challenge. before your domain [Fri Jan 19 02:51:09 GMT 2024] so the resulting subdomain will be: _acme-challenge.thekoguryo.xyz [Fri Jan 19 02:51:09 GMT 2024] Please add the TXT records to the domains, and re-run with --renew. [Fri Jan 19 02:51:09 GMT 2024] Please add \u0026#39;--debug\u0026#39; or \u0026#39;--log\u0026#39; to check more details. [Fri Jan 19 02:51:09 GMT 2024] See: https://github.com/acmesh-official/acme.sh/wiki/How-to-debug-acme.sh DNS 서버에서 TXT value 2개를 등록합니다.\nGoDaddy 기준 설정화면 예시\n반영될 때까지 시간이 걸릴 수 있습니다. 아래와 같이 nslookup으로 잘 조회되는 지 확인합니다.\n$ nslookup -q=TXT _acme-challenge.thekoguryo.xyz 8.8.8.8 Server: 8.8.8.8 Address: 8.8.8.8#53 Non-authoritative answer: _acme-challenge.thekoguryo.xyz text = \u0026#34;r__JG3oEDK-t39Tz0cbcvpbqUvjn7QaMDyfuMg6W3ek\u0026#34; _acme-challenge.thekoguryo.xyz text = \u0026#34;iuyByqkwUUG2IWp33foj_uysPHQg0lD_YXlIM0FtUpo\u0026#34; Authoritative answers can be found from: --renew 옵션으로 변경하여 다시 명령을 실행합니다.\nacme.sh --renew --force --keylength 2048 -d thekoguryo.xyz -d \u0026#39;*.thekoguryo.xyz\u0026#39; --dns --yes-I-know-dns-manual-mode-enough-go-ahead-please 실행예시\n$ acme.sh --renew --force --keylength 2048 -d thekoguryo.xyz -d \u0026#39;*.thekoguryo.xyz\u0026#39; --dns --yes-I-know-dns-manual-mode-enough-go-ahead-please [Fri Jan 19 06:24:31 GMT 2024] Renew: \u0026#39;thekoguryo.xyz\u0026#39; [Fri Jan 19 06:24:31 GMT 2024] Renew to Le_API=https://acme-v02.api.letsencrypt.org/directory [Fri Jan 19 06:24:32 GMT 2024] Using CA: https://acme-v02.api.letsencrypt.org/directory [Fri Jan 19 06:24:32 GMT 2024] Multi domain=\u0026#39;DNS:thekoguryo.xyz,DNS:*.thekoguryo.xyz\u0026#39; [Fri Jan 19 06:24:32 GMT 2024] Getting domain auth token for each domain [Fri Jan 19 06:24:37 GMT 2024] Getting webroot for domain=\u0026#39;thekoguryo.xyz\u0026#39; [Fri Jan 19 06:24:37 GMT 2024] Getting webroot for domain=\u0026#39;*.thekoguryo.xyz\u0026#39; [Fri Jan 19 06:24:37 GMT 2024] thekoguryo.xyz is already verified, skip dns-01. [Fri Jan 19 06:24:37 GMT 2024] *.thekoguryo.xyz is already verified, skip dns-01. [Fri Jan 19 06:24:37 GMT 2024] Verify finished, start to sign. [Fri Jan 19 06:24:37 GMT 2024] Lets finalize the order. [Fri Jan 19 06:24:37 GMT 2024] Le_OrderFinalize=\u0026#39;https://acme-v02.api.letsencrypt.org/acme/finalize/1524070066/237786255586\u0026#39; [Fri Jan 19 06:24:38 GMT 2024] Downloading cert. [Fri Jan 19 06:24:38 GMT 2024] Le_LinkCert=\u0026#39;https://acme-v02.api.letsencrypt.org/acme/cert/03b685e226b8521bdf8977e4cc035dac7422\u0026#39; [Fri Jan 19 06:24:40 GMT 2024] Cert success. -----BEGIN CERTIFICATE----- MIIE+jCCA+KgAwIBAgISA7aF4ia4UhvfiXfkzANdrHQiMA0GCSqGSIb3DQEBCwUA MDIxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MQswCQYDVQQD ... hA7GiPNNlsUstWgukWV34Wd1Mqs8dp4kxgwnAI/mj1BHtmNDd28A1LcQ2f5rxRIe eruG9RUuH660Qlj+QcW17F6xecfSvCYNpFJgcLeU6qzbG7j3LYWCChZmSYoFek8T Te/oNN70XGG4lPSBz/HkNEYyKBRktFGTVkGJzxk0 -----END CERTIFICATE----- [Fri Jan 19 06:24:40 GMT 2024] Your cert is in: /home/opc/.acme.sh/thekoguryo.xyz/thekoguryo.xyz.cer [Fri Jan 19 06:24:40 GMT 2024] Your cert key is in: /home/opc/.acme.sh/thekoguryo.xyz/thekoguryo.xyz.key [Fri Jan 19 06:24:40 GMT 2024] The intermediate CA cert is in: /home/opc/.acme.sh/thekoguryo.xyz/ca.cer [Fri Jan 19 06:24:40 GMT 2024] And the full chain certs is there: /home/opc/.acme.sh/thekoguryo.xyz/fullchain.cer 알려준 위치를 확인해 보면, 아래와 같이 생성된 인증서를 볼 수 있습니다.\n$ ls -la /home/opc/.acme.sh/thekoguryo.xyz total 32 drwxrwxr-x. 2 opc opc 177 Jan 19 06:24 . drwx------. 7 opc opc 153 Jan 19 06:24 .. -rw-rw-r--. 1 opc opc 3751 Jan 19 06:24 ca.cer -rw-rw-r--. 1 opc opc 5536 Jan 19 06:24 fullchain.cer -rw-rw-r--. 1 opc opc 1785 Jan 19 06:24 thekoguryo.xyz.cer -rw-rw-r--. 1 opc opc 618 Jan 19 06:24 thekoguryo.xyz.conf -rw-rw-r--. 1 opc opc 1025 Jan 19 06:24 thekoguryo.xyz.csr -rw-rw-r--. 1 opc opc 208 Jan 19 06:24 thekoguryo.xyz.csr.conf -rw-------. 1 opc opc 1679 Jan 19 06:24 thekoguryo.xyz.key certificate: thekoguryo.xyz.cer private key: thekoguryo.xyz.key intermediate CA cert: ca.cer full chain certs: fullchain.cer ","lastmod":"2024-01-19T00:00:01Z","permalink":"https://thekoguryo.github.io/blog/lets-encrypt-ssl-certificate/","tags":["ssl","certificate"],"title":"Let’s Encrypt로 무료 SSL 인증서 발급받기"},{"categories":null,"contents":"SSL For Frees에서 무료 SSL 인증서 발급받기 다른 글에서 Let\u0026rsquo;s encrypt로 인증서를 툴을 통해 자동으로 발급받는 내용을 확인했습니다. 여기서는 다른 사람에 SSL Certificate을 발급받은 것을 사용할 때를 가정한 테스트하기 위한 예비단계로 SSL Certificate을 발급 받는 과정을 진행해 보겠습니다. 테스트이므로 무료 인증서를 발급 받을 수 있는 사이트 중에 하나인 SSL For Free을 사용하겠습니다.\n해당 사이트는 현재 정책이 변경되어 3번까지만 무료로 발급받을 수 있습니다.\nSSL For Free 사이트로 이동합니다.\n계정이 없는 경우 Register 메뉴를 통해 회원가입을 합니다.\nNew Certificate을 클릭합니다.\nEnter Domain 항목에 가지고 있는 도메인 주소를 입력합니다. 발급될 인증서는 체크표시로 뜨는 주소에 한해서 사용할 수 있습니다. 확인 후 Next Step을 클릭합니다.\n유효기간은 무료로 제공하는 90일짜리로 선택합니다. 확인 후 Next Step을 클릭합니다.\n인증서 생성을 위한 Certificate Signing Request (CSR) 정보(조직, 이메일, 주소 등등)는 테스트 용도이므로, 기본값을 사용합니다. 원하는 경우 직접 입력해도 됩니다. 확인 후 Next Step을 클릭합니다.\n요금 플랜은 무료인 Free를 선택합니다. 확인 후 Next Step을 클릭합니다.\n사용한 도메인의 실 소유자임을 검증하는 단계입니다. 세가지 방법이 있습니다.\n이메일로 검증 DNS(CNAME)으로 검증 HTTP 파일 업로드로 검증 저는 편의상 두번째 DNS(CNAME)을 통해 검증하겠습니다.\n도메인을 구입한 사이트에서 위 정보를 등록합니다. 저는 GoDaddy 사이트에서 아래와 같이 CNAME을 추가하였습니다.\n도메인 검증화면으로 돌아가서 Next Step을 클릭합니다.\n검증을 위해 Verify Domain을 클릭합니다.\n검증이 완료되면, 인증서 생성이 시작됩니다.\n인증서 발급이 완료되었습니다.\n설치할 서버 타입이 있는 경우 선택합니다. 저는 기본 포맷을 선택했습니다.\n인증서 zip 파일을 다운로드 받습니다.\n압축파일을 풀면 다음 세가지 파일이 포함되어 있습니다.\n","lastmod":"2023-02-28T00:00:01Z","permalink":"https://thekoguryo.github.io/blog/free-ssl-certificate/","tags":["ssl","certificate"],"title":"무료 SSL 인증서 발급받기"},{"categories":null,"contents":"1.7.1.2 DevOps 서비스의 Blue/Green 배포 전략으로 배포하기 DevOps 서비스를 통한 CI/CD 배포 자동화 하기 사전 준비 사항\nDevOps 서비스는 NGINX Ingress Controller를 사용하여, Blue/Green 배포를 지원합니다. 먼저 1.2.2.2.1 NGINX Ingress Controller 설치하기을 참조하여 NGINX Ingress Controller를 미리 설치합니다. DevOps 프로젝트 만들기\n이전에 생성한 프로젝트를 그대로 사용하거나, 새롭게 프로젝트를 만듭니다.\n애플리케이션 배포 자동화 하기 Code Repository를 사용하여 애플리케이션 코드 관리하기\n샘플로 사전에 만든 Nginx 기반 웹앱을 사용하여 만들어 테스트하겠습니다.\n코드 저장소 생성을 위해 왼쪽 메뉴에서 Code Repositories를 클릭합니다.\nCreate repository를 클릭하여 저장소를 만듭니다.\nRepository name: 예) oci-devops-oke-webpage 실제 개발 작업은 git 명령을 통해 개발 PC에서 진행하면 됩니다. 저장소 상세정보 위에 있는 Clone 버튼을 하면 Clone 명령어가 아래 그림처럼 뜨게 됩니다. 여기서는 Clone with HTTPS를 사용하겠습니다.\n개발 PC에 복사한 주소를 사용해 git clone 명령어를 통해 복제합니다.\ngit clone \u0026lt;YourClonewithHTTPS URL\u0026gt; 이때 사용자 인증이 필요합니다. HTTPS기반 사용자 인증시 아래 유저명 형식과 AuthToken을 사용합니다.\n이미 개발된 소스를 가져와 Clone한 저장소로 옮깁니다.\nwget https://github.com/TheKoguryo/oci-devops-oke-webpage/archive/refs/tags/v2022.04.tar.gz tar -xvzf v2022.04.tar.gz --strip-components=1 -C oci-devops-oke-webpage/ 코드를 Code Repository에 Push 합니다.\ncd oci-devops-oke-webpage git add . git commit -m \u0026#34;init\u0026#34; git push 코드 작성 및 반영 완료\nDockerfile, default.conf.template, html 폴더: nginx 기반 웹 앱의 이미지를 빌드 소스 파일입니다. build_spec.yaml: DevOps 서비의 빌드 스테이지에서 사용하는 빌드 정의서입니다. oci-oke-deployment.yaml: OKE에 배포하기 위해 정의된 Kubernetes Manifest 파일입니다. 이후 다시 설명하겠습니다. Build Pipeline 만들기\nCI/CD 중에 코드를 빌드하여 배포 산출물을 만드는 CI 과정에 해당되는 부분을 Build Pipeline을 통해 구성이 가능합니다.\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Build Pipelines로 이동합니다.\nCreate build pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) webpage-build-pipeline 생성된 파이프라인을 클릭합니다.\n그림과 같이 Stage를 추가하여 파이프라인 흐름을 구성할 수 있습니다. Add Stage를 클릭합니다.\nBuild Stage 만들기\n빌드를 위해 먼저 Managed Build Stage를 추가합니다.\nManaged Build Stage 설정\nStage name: 예) build-stage\nBuild Spec File Path: 빌드 스크립트 경로를 지정합니다. 기본적으로 소스 루트에 있는 build_spec.yaml을 파일을 사용합니다.\nPrimary Code Repository: 빌드할 메인 소스가 있는 코드 저장소를 지정합니다.\n대상 소스 코드가 있는 저장소를 지정합니다.\n설정된 Stage를 Add를 클릭하여 추가합니다.\n테스트처럼 소스 코드상의 Build Spec의 정의가 필요합니다.\n개발한 oci-devops-oke-webpage 소스 코드의 root 경로에 build_spec.yaml을 이미 정의해 두었습니다. 정의한 내용을 확인합니다.\nbuild_spec.yaml\noutputArtifacts.output-image: 빌드 산출물인 컨테이너 이미지를 지칭합니다. outputArtifacts.output-oci-oke-deployment: 빌드 산출물로 소스코드상에 있는 OKE 배포용 Manifest 파일입니다. exportedVariables IMAGE_PATH: 빌드되는 환경정보와, APP_NAME을 기반으로 OCIR 리파지토리를 계산합니다. TAG: 편의를 위해 코드 Commit ID 또는 BUILD_ID 해쉬값으로 지정합니다. version: 0.1 component: build timeoutInSeconds: 6000 runAs: root shell: bash env: # these are local variables to the build config variables: defaultAppName: \u0026#34;webpage\u0026#34; defaultContextPath: \u0026#34;/webpage\u0026#34; # the value of a vaultVariable is the secret-id (in OCI ID format) stored in the OCI Vault service # you can then access the value of that secret in your build_spec.yaml commands vaultVariables: # exportedVariables are made available to use as parameters in sucessor Build Pipeline stages # For this Build to run, the Build Pipeline needs to have a BUILDRUN_HASH parameter set exportedVariables: - BUILDRUN_HASH - APP_NAME - IMAGE_PATH - TAG - CONTEXT_PATH # Its a native way to fetch artifacts from external or artifact repo or a file path to use before a stage. # More about buildspec formats - https://docs.oracle.com/en-us/iaas/Content/devops/using/build_specs.htm inputArtifacts: steps: - type: Command name: \u0026#34;Init exportedVariables\u0026#34; timeoutInSeconds: 30 command: | APP_NAME=$defaultAppName CONTEXT_PATH=$defaultContextPath echo $APP_NAME echo $CONTEXT_PATH - type: Command name: \u0026#34;Build Source\u0026#34; timeoutInSeconds: 4000 command: | echo no action - type: Command name: \u0026#34;Define Image Tag - Commit ID\u0026#34; timeoutInSeconds: 30 command: | COMMIT_ID=`echo ${OCI_TRIGGER_COMMIT_HASH} | cut -c 1-7` BUILDRUN_HASH=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` [ -z \u0026#34;$COMMIT_ID\u0026#34; ] \u0026amp;\u0026amp; TAG=$BUILDRUN_HASH || TAG=$COMMIT_ID - type: Command name: \u0026#34;Define Image Path\u0026#34; timeoutInSeconds: 30 command: | TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` REPO_NAME=$APP_NAME IMAGE_PATH=$OCI_RESOURCE_PRINCIPAL_REGION.ocir.io/$TENANCY_NAMESPACE/$REPO_NAME - type: Command timeoutInSeconds: 1200 name: \u0026#34;Build Container Image\u0026#34; command: | cd ${OCI_PRIMARY_SOURCE_DIR} docker build --pull --rm -t new-generated-image . --build-arg CONTEXT_PATH=$CONTEXT_PATH - type: Command name: \u0026#34;Check exportedVariables\u0026#34; timeoutInSeconds: 30 command: | [ -z \u0026#34;$APP_NAME\u0026#34; ] \u0026amp;\u0026amp; APP_NAME=unknown [ -z \u0026#34;$IMAGE_PATH\u0026#34; ] \u0026amp;\u0026amp; IMAGE_PATH=unknown [ -z \u0026#34;$TAG\u0026#34; ] \u0026amp;\u0026amp; TAG=unknown echo \u0026#34;APP_NAME: \u0026#34; $APP_NAME echo \u0026#34;IMAGE_PATH: \u0026#34; $IMAGE_PATH echo \u0026#34;TAG: \u0026#34; $TAG outputArtifacts: - name: output-image type: DOCKER_IMAGE # this location tag doesn\u0026#39;t effect the tag used to deliver the container image # to the Container Registry location: new-generated-image - name: output-oci-oke-deployment type: BINARY location: ${OCI_PRIMARY_SOURCE_DIR}/oci-oke-deployment.yaml 빌드 스테이지가 완성되었습니다.\n빌드 산출물을 저장할 Artifact 저장소 만들기\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Artifact Registry로 이동합니다. Create repository를 클릭하여 저장소를 추가합니다. Name: 예) oci-hol-artifact-repository Immutable artifacts: 테스트용으로 체크 해제 빌드 산출물을 OCIR 및 Artifact 저장소에 저장하기\nBuild Pipeline 탭으로 이동합니다.\n플러스 버튼을 클릭하여 build-stage 다음에 stage를 추가합니다.\nDelivery Artifact Stage를 선택합니다.\nstage 이름을 입력하고 Create Artifact를 선택합니다.\nContainer image 유형으로 Artifact 추가합니다.\n이미지 경로: docker tag를 달때 사용하는 이미지 경로입니다. 직접 입력해도 되지만 여기서는 build-stage에서 넘어온 exportedVariable을 사용하여 아래와 같이 입력합니다.\nName: generated_image_with_tag Image Path: ${IMAGE_PATH}:${TAG} 같은 방식으로 하나 더 추가 합니다.\nName: generated_image_with_latest Image Path: ${IMAGE_PATH}:latest Kubernetes Manifest 파일을 앞서 생성한 Artifact Repository에 추가합니다.\nName: 예) oci-oke-deployment.yaml\nArtifact path: oci-devops-oke-webpage/oci-oke-deployment.yaml\nVersion: ${BUILDRUN_HASH}\nArtifact 매핑\nAssociate Artifact에서 방금 추가한 3개의 Artifact에 실제 컨테이너 이미지 파일을 매핑해 줍니다. 앞서 build-stage에서 build_spec.yaml에서 정의한 outputArtifacts의 이름을 입력합니다.\noutputArtifacts: - name: output-image type: DOCKER_IMAGE # this location tag doesn\u0026#39;t effect the tag used to deliver the container image # to the Container Registry location: new-generated-image - name: output-oci-oke-deployment type: BINARY location: ${OCI_PRIMARY_SOURCE_DIR}/oci-oke-deployment.yaml 이제 delivery stage까지 추가하였습니다.\nBlue/Green Deployment Pipeline 만들기\nCI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\nBlue/Green 전략으로 쿠버네티스에 배포하기 위해서는 배포 환경, 배포할 Kubernetes Manifest 파일, 배포에 사용될 Kubernetes Namespace 두 개가 필요합니다.\nKubernetes Environment 등록하기\n앞서 등록한 OKE 환경이 없는 경우, 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Environments로 이동하여 배포할 OKE 환경을 등록합니다. Kubernetes에 배포할 manifest 파일 준비\nKubernetes에 배포할 Stage 유형을 사용하기 위해서는 사전에 배포할 manifest yaml 파일을 준비해야 합니다.\n앞서 실습과 같이 inline 형식으로 하는 방법도 있고, 이번에서 앞서 가져온 소스 프로젝트 파일에 Kubernetes Manifest 파일을 관리하고 사용할 수도 있습니다.\n사용할 샘플 Manifest 파일\nDeployment, Service: 빌드된 컨테이너 앱을 배포할 Deployment와 Cluster IP기반 Service Ingress: Blue/Green 또는 Canary 배포를 위해 필요합니다. 둘다 Nginx Ingress Controller를 기반으로 라우팅 규칙을 조절하여 해당 기능을 제공하고 있습니다. apiVersion: apps/v1 kind: Deployment metadata: name: ${APP_NAME}-deployment spec: selector: matchLabels: app: ${APP_NAME} replicas: 3 template: metadata: labels: app: ${APP_NAME} spec: containers: - name: ${APP_NAME} # enter the path to your image, be sure to include the correct region prefix image: ${IMAGE_PATH}:${TAG} imagePullPolicy: Always ports: - containerPort: 80 protocol: TCP readinessProbe: exec: command: - cat # For demo, see the start status easily about rolling updates. initialDelaySeconds: 5 periodSeconds: 5 livenessProbe: exec: command: - cat initialDelaySeconds: 5 periodSeconds: 5 env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace --- apiVersion: v1 kind: Service metadata: name: ${APP_NAME}-service spec: type: ClusterIP ports: - port: 8080 protocol: TCP targetPort: 80 selector: app: ${APP_NAME} --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ${APP_NAME}-ingress annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 spec: rules: - http: paths: - path: ${CONTEXT_PATH}(/|$)(.*) pathType: Prefix backend: service: name: ${APP_NAME}-service port: number: 8080 Kubernetes에 배포할 네임스페이스 만들기\n블루 앱이 배포될 네임스페이스, 그린 앱이 배포될 네임스페이스 필요합니다. 각 네임스페이스에 동일한 앱이 현재 버전, 신규 버전이 각각 배포되게 되고, 라우팅 설정을 통해 트래픽을 조절합니다.\nCloud Shell을 통해 kubectl 명령이 실행 가능한 환경으로 접속합니다.\n두 개의 네임스페이스를 생성합니다.\n예) ns-blue, ns-green\nkubectl create ns ns-blue kubectl create ns ns-green OCIR에서 이미지 가져오기\nOCIR 리파지토지(예시, webpage)가 사전에 없는 경우 Root Compartment에 Private 형태로 만들어집니다. OKE에서 가져오기 위해서는 각각 네임스페이스에 imagepullsecret을 사전에 생성하거나, 리파지토리를 미리 Public으로 생성합니다.\nBlue/Green 전략으로 Kubernetes 배포 Stage 만들기\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\nCreate pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) webpage-bluegreen-deployment-pipeline 생성된 파이프라인을 클릭하여 Blue/Green Strategy Stage를 추가합니다.\n배포 유형을 OKE로 선택하고, 배포환경 및 manifest 파일을 선택합니다.\nBlue/Green 배포를 위한 추가 설정을 합니다.\nNamespace set: 앞서 만들 배포용 네임스페이스 2개를 입력합니다. 이전 버전이 있는 곳에 배포하고, 현재 버전으로 향한 트래픽을 새 버전이 있는 곳으로 변경하는 방식으로 2개의 순서는 크게 중요하지는 않습니다.\n예) ns-blue, ns-green NGINX ingress namespace: 배포 manifest 파일에서 정의한 배포 앱의 ingress 자원 이름을 입력합니다.\n옵션 설정: Approval controls을 활성화합니다. 배포까지만 자동으로 진행하고, 승인후 트래픽을 전환하기 위해 추가합니다. 트래픽 전환 스위치 정도라고 생각하면 될 것 같습니다.\nApproval controls:\nName: 예) approval-stage 파이프라인 완성\nBuild Pipeline에서 Deployment Pipeline 호출하기\n앞서 만든 Build Pipeline이 끝나고, 배포가 될수 있도록 Deployment Pipeline 호출을 추가합니다.\n앞서 만든 Build Pipelines으로 이동합니다.\n파이프라인 마지막에 Stage를 추가합니다.\nTrigger Deployment 유형을 선택합니다.\n앞서 만든 Blue/Green 배포용 Deployment Pipeline을 지정합니다.\n빌드후 배포하는 전체 흐름이 완료되었습니다.\nTrigger 설정하기\n개발자가 코드를 코드 저장소에 반영이 될 때 자동으로 빌드, 배포 파이프라인이 동작할 필요가 있습니다. Trigger는 코드 저장소에 발생한 이벤트를 통해 빌드 파이프라인을 시작하게 하는 역할을 하게 됩니다.\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Trigger로 이동합니다.\nCreate trigger을 클릭합니다.\nTrigger를 설정합니다.\nName: 예) webpage-build-pipeline-trigger Source Code Repository: 앞서 만든 OCI Code Repository상의 oci-devops-oke-webpage를 선택합니다. Actions: 트리거링 되었을 때 호출하는 액션으로 작성한 빌드 파이프라인인 webpage-pipeline을 선택합니다. 설정이 완료되었습니다.\n테스트 - Blue 배포\nTrigger에서 지정한 oci-devops-oke-webpage 소스 코드에 임의의 변경사항을 발생시키고 Code Repository에 반영합니다.\n작업중인 코드가 있는 Cloud Shell로 이동합니다.\nhtml/index.html을 변경해도 되지만, 편의상 환경변수로 해놓은 Dockerfile을 변경합니다.\n현재 설정 기준으로 변경사항을 발생시키고 코드를 Commit 후 Push 합니다.\nFROM nginx:alpine COPY default.conf.template /etc/nginx/templates/ COPY html/ /usr/share/nginx/html/ ENV POD_NAMESPACE=\u0026#34;default\u0026#34; ENV VERSION=\u0026#34;1.0\u0026#34; ENV MESSAGE=\u0026#34;Hello OCI DevOps\u0026#34; ENV BACKGROUND=\u0026#34;blue\u0026#34; ARG CONTEXT_PATH=\u0026#34;/webpage\u0026#34; ENV CONTEXT_PATH=$CONTEXT_PATH Trigger가 되고 빌드가 시작됩니다.\n빌드가 성공하고, 배포 파이프라인이 실행하다, approval-stage에서 대기합니다.\n쿠버네티스 배포 상태를 확인합니다. ns-blue에만 배포된 것을 알 수 있습니다.\n[opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-blue NAME READY STATUS RESTARTS AGE pod/webpage-deployment-6fbfd769c7-gqprs 1/1 Running 0 9m28s pod/webpage-deployment-6fbfd769c7-m4lk4 1/1 Running 0 9m28s pod/webpage-deployment-6fbfd769c7-z924h 1/1 Running 0 9m28s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.166.161 \u0026lt;none\u0026gt; 8080/TCP 9m28s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 9m28s NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-6fbfd769c7 3 3 3 9m28s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 9m28s [opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-green No resources found in ns-green namespace. 실행중인 배포 파이프라인으로 돌아가 approval-stage에서 승인합니다.\n승인이 되면 Traffic Shift도 진행되어 ns-green에도 ingress가 배포된 것을 볼수 있습니다. 그러나 지금은 ns-blue가 Active이므로 ns-green에 있는 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;0\u0026ldquo;이 되어는 실제는 ns-green으로는 요청이 가지 않습니다.\n[opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-green NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 4m [opc@jumpbox-945115 ~]$ kubectl get ingress webpage-ingress -n ns-green -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-namespaceB nginx.ingress.kubernetes.io/canary-weight: \u0026#34;0\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 ... Ingress 주소를 통해 웹페이지에 접속하면, 현재 Blue 앱이 잘 동작하는 것을 볼수 있습니다.\n주소 예) http://146.56.118.147/webpage 테스트 - Green 배포\n소스로 돌아가서 코드를 변경(예시, Dockerfile의 환경변수 변경)하고 Commit \u0026amp; Push 하여 코드를 반영합니다\nFROM nginx:alpine COPY default.conf.template /etc/nginx/templates/ COPY html/ /usr/share/nginx/html/ ENV POD_NAMESPACE=\u0026#34;default\u0026#34; ENV VERSION=\u0026#34;1.1\u0026#34; ENV MESSAGE=\u0026#34;Hello OCI DevOps\u0026#34; ENV BACKGROUND=\u0026#34;green\u0026#34; ARG CONTEXT_PATH=\u0026#34;/webpage\u0026#34; ENV CONTEXT_PATH=$CONTEXT_PATH 다시 빌드, 배포 파이프라인이 실행되고 approval-stage에서 멈출때까지 기다립니다.\n쿠버네티스 배포 상태를 확인합니다. ns-green에 앱이 배포된 것을 볼 수 있습니다.\n[opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-blue NAME READY STATUS RESTARTS AGE pod/webpage-deployment-6fbfd769c7-gqprs 1/1 Running 0 34m pod/webpage-deployment-6fbfd769c7-m4lk4 1/1 Running 0 34m pod/webpage-deployment-6fbfd769c7-z924h 1/1 Running 0 34m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.166.161 \u0026lt;none\u0026gt; 8080/TCP 34m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 34m NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-6fbfd769c7 3 3 3 34m NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 34m [opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-green NAME READY STATUS RESTARTS AGE pod/webpage-deployment-75c8bb6ffc-5b497 1/1 Running 0 98s pod/webpage-deployment-75c8bb6ffc-m8dl4 1/1 Running 0 98s pod/webpage-deployment-75c8bb6ffc-xctr4 1/1 Running 0 98s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.153.34 \u0026lt;none\u0026gt; 8080/TCP 98s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 98s NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-75c8bb6ffc 3 3 3 98s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 18m 웹페이지에 접속하면, 여전히 Blue 앱으로 접속됩니다.\n실행중인 배포 파이프라인으로 돌아가 approval-stage에서 승인합니다.\n승인이 되면, Traffic Shift가 됩니다. 이제 ns-green이 Active이므로 ns-green내 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;100\u0026ldquo;으로 변경된 걸 볼 수 있습니다. 이제 모든 요청은 ns-green으로만 전달됩니다.\n[opc@jumpbox-945115 ~]$ kubectl get ingress webpage-ingress -n ns-green -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-namespaceB nginx.ingress.kubernetes.io/canary-weight: \u0026#34;100\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 ... 웹페이지에 접속하면, Green 앱으로 접속됩니다. 여러번 반복해서 요청해도 Green 앱으로 접속됩니다.\n테스트 - 추가적인 변경 사항 배포 - Red 배포\n소스로 돌아가서 코드를 변경(예시, Dockerfile의 환경변수 변경)하고 Commit \u0026amp; Push 하여 코드를 반영합니다\n... ENV VERSION=\u0026#34;1.2\u0026#34; ENV MESSAGE=\u0026#34;Hello OCI DevOps\u0026#34; ENV BACKGROUND=\u0026#34;red\u0026#34; ... 다시 빌드, 배포 파이프라인이 실행되고 approval-stage에서 멈출때까지 기다립니다.\n쿠버네티스 배포 상태를 확인합니다. POD의 AGE를 보면 신규 배포가 ns-blue로 된 것을 알 수 있습니다.\n[opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-blue NAME READY STATUS RESTARTS AGE pod/webpage-deployment-7958d6b7b7-pkzmm 1/1 Running 0 115s pod/webpage-deployment-7958d6b7b7-vwx22 1/1 Running 0 2m4s pod/webpage-deployment-7958d6b7b7-wkbjd 1/1 Running 0 107s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.166.161 \u0026lt;none\u0026gt; 8080/TCP 53m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 53m NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-6fbfd769c7 0 0 0 53m replicaset.apps/webpage-deployment-7958d6b7b7 3 3 3 2m4s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 53m [opc@jumpbox-945115 ~]$ kubectl get all,ingress -n ns-green NAME READY STATUS RESTARTS AGE pod/webpage-deployment-75c8bb6ffc-5b497 1/1 Running 0 20m pod/webpage-deployment-75c8bb6ffc-m8dl4 1/1 Running 0 20m pod/webpage-deployment-75c8bb6ffc-xctr4 1/1 Running 0 20m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.153.34 \u0026lt;none\u0026gt; 8080/TCP 20m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 21m NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-75c8bb6ffc 3 3 3 21m NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 37m 웹페이지에 접속하면, 여전히 Green 앱으로 접속됩니다.\n실행중인 배포 파이프라인으로 돌아가 approval-stage에서 승인합니다.\n승인이 되면, Traffic Shift가 됩니다. 이제 ns-blue가 Active입니다. ns-green내 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;0\u0026ldquo;로 다시 변경된 걸 볼 수 있습니다. 이제 모든 요청은 ns-blue으로만 전달됩니다.\n[opc@jumpbox-945115 ~]$ kubectl get ingress webpage-ingress -n ns-green -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-namespaceB nginx.ingress.kubernetes.io/canary-weight: \u0026#34;0\u0026#34; ... 웹페이지에 접속하면, ns-blue 네임스페이스에 새롭게 배포한 Red 앱으로 접속됩니다. 여러번 반복해서 요청해도 Red 앱으로 접속됩니다.\n이렇게 Blue/Green 전략으로 앱을 배포하면, 지정한 2개의 네임스페이스를 이용하여, 현재 서비스 되지 않는 네임스페이스로 신규 버전을 배포하고, 승인이 되면, 신규 버전으로 트래픽을 변경하는 방법으로 지속적인 신규 버전에 대한 배포를 지원합니다.\n테스트 - 롤백\n신규 버전으로 서비스를 하다, 문제가 발생하면 이전 버전으로 롤백이 기능합니다.\n배포 히스토리 중에서 현재 버전을 배포한 배포 파이프라인으로 이동합니다.\nTraffic Shift 메뉴 중에서 Revert traffic shift를 클릭합니다.\n변경내용 확인 후 다시 Revert traffic shift를 클릭합니다.\n수동 롤백 작업이 완료되었습니다.\n배포 변경은 없고 Traffic Shift만 변경된 것으로, 쿠버네티스 배포 상태를 확인해 보면, ns-green내 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;100\u0026ldquo;로 다시 변경된 걸 볼 수 있습니다.\n[opc@jumpbox-945115 ~]$ kubectl get ingress webpage-ingress -n ns-green -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-namespaceB nginx.ingress.kubernetes.io/canary-weight: \u0026#34;100\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 웹페이지에 접속하면, 마지막 배포앱인 Red에서 ns-blue 네임스페이스에 있는 기존 앱인 Green 앱으로 복구된 것을 알 수 있습니다.\n이렇게 DevOps 서비스에서 제공하는 Blue/Green 전략으로 앱을 배포하면, 2개의 네임스페이스, Ingress Controller의 라우팅 규칙 변경을 통해서 Blue/Green 배포를 할 수 있습니다.\n","lastmod":"2022-04-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/cicd/devops/2.bluegreen-stragtegy/","tags":["devops","blue/green","ci/cd"],"title":"1.7.1.2 Blue/Green 배포 전략으로 배포하기"},{"categories":null,"contents":"Oracle Linux 8에 Visual Studio Code 설치하기 설치를 위해 루트 유저로 yum repo를 추가합니다.\nsudo vi /etc/yum.repos.d/vscode.repo 추가할 내용 [vscode] name=vscode baseurl=https://packages.microsoft.com/yumrepos/vscode/ enabled=1 gpgcheck=1 gpgkey=https://packages.microsoft.com/keys/microsoft.asc Visual Studio Code를 설치합니다.\nsudo yum install code 설치예시 [opc@linux8 ~]$ sudo yum install code vscode 57 MB/s | 34 MB 00:00 Last metadata expiration check: 0:00:07 ago on Tue 04 Apr 2023 09:22:03 AM GMT. Dependencies resolved. =================================================================================================================================================== Package Architecture Version Repository Size =================================================================================================================================================== Installing: code x86_64 1.77.0-1680085658.el7 vscode 120 M Transaction Summary =================================================================================================================================================== Install 1 Package Total download size: 120 M Installed size: 331 M Is this ok [y/N]: y Downloading Packages: code-1.77.0-1680085658.el7.x86_64.rpm 63 MB/s | 120 MB 00:01 --------------------------------------------------------------------------------------------------------------------------------------------------- Total 63 MB/s | 120 MB 00:01 vscode 47 kB/s | 983 B 00:00 Importing GPG key 0xBE1229CF: Userid : \u0026#34;Microsoft (Release signing) \u0026lt;gpgsecurity@microsoft.com\u0026gt;\u0026#34; Fingerprint: BC52 8686 B50D 79E3 39D3 721C EB3E 94AD BE12 29CF From : https://packages.microsoft.com/keys/microsoft.asc Is this ok [y/N]: y Key imported successfully Running transaction check Transaction check succeeded. Running transaction test Transaction test succeeded. Running transaction Preparing : 1/1 Installing : code-1.77.0-1680085658.el7.x86_64 1/1 Running scriptlet: code-1.77.0-1680085658.el7.x86_64 1/1 Verifying : code-1.77.0-1680085658.el7.x86_64 1/1 Installed: code-1.77.0-1680085658.el7.x86_64 Complete! [opc@linux8 ~]$ 설치된 VS Code를 실행합니다.\n참고\nInstalling Visual Studio Code on Oracle Linux 7 ","lastmod":"2023-04-04T00:00:01Z","permalink":"https://thekoguryo.github.io/blog/install-vscode-on-oracle-linux-8/","tags":["oraclelinux8","vscode"],"title":"Oracle Linux 8에 Visual Studio Code 설치하기"},{"categories":null,"contents":"1.7.1.3 DevOps 서비스의 Canary 배포 전략으로 배포하기 1.7.1.2 DevOps 서비스의 Blue/Green 배포 전략으로 배포하기에서 만든 Code Repository, Build Pipeline, Trigger 등을 그대로 사용합니다.\n여기서는 배포전략으로 변경이 발생하는 Deployment Pipeline만 추가로 새로 만듭니다.\n그리고 Build Pipeline의 마지막 단계에서 호출하는 Deployment Pipeline을 기존 Blue/Green에서 Canary Deployment Pipeline으로 변경합니다.\nCanary Deployment Pipeline 만들기 CI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\nCanary 전략으로 쿠버네티스에 배포하기 위해서는 배포 환경, 배포할 Kubernets Manifest 파일, 배포에 사용될 Kubernetes Namespace 두 개가 필요합니다.\nKubernetes Environment 등록하기\nBlue/Green 배포 실습과 동일한 환경을 사용합니다.\nKubernetes에 배포할 manifest 파일 준비\nBlue/Green 배포 실습과 동일한 파일을 사용합니다.\nKubernetes에 배포할 네임스페이스 만들기\n테스트 버전과 실 서비스가 배포될 2개의 네임스페이스 필요합니다.\nCloud Shell을 통해 kubectl 명령이 실행 가능한 환경으로 접속합니다.\n두 개의 네임스페이스를 생성합니다.\n예) ns-canary-stage, ns-canary-prod\nkubectl create ns ns-canary-stage kubectl create ns ns-canary-prod 원활한 테스트를 위해 이전 Blue/Green 용 네임스페이스를 미리 삭제합니다.\nkubectl delete ns ns-blue kubectl delete ns ns-green OCIR에서 이미지 가져오기\nOCIR 리파지토지(예시, webpage)가 사전에 없는 경우 Root Compartment에 Private 형태로 만들어집니다. OKE에서 가져오기 위해서는 각각 네임스페이스에 imagepullsecret을 사전에 생성하거나, 리파지토리를 미리 Public으로 생성합니다.\nCanary 전략으로 Kubernetes 배포 Stage 만들기\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\nCreate pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) webpage-canary-deployment-pipeline 생성된 파이프라인을 클릭하여 Canary Strategy Stage를 추가합니다.\n배포 유형을 OKE로 선택하고, 배포환경 및 manifest 파일을 선택합니다.\nCanary 배포를 위한 추가 설정을 합니다.\nCanary namespace: 테스트 버전이 배포될 네임스페이스 이름을 입력합니다.\n예) ns-canary-stage NGINX ingress namespace: 배포 manifest 파일에서 정의한 배포 앱의 ingress 자원 이름을 입력합니다.\nShift traffic: 트래픽 전환용 스테이지를 추가합니다.\nName: 예) shift-traffic-stage Ramp Limit: 전체 요청중 테스트 버전에게 전달할 비중을 퍼센트로 입력합니다. 예) 25 퍼센트 Approval: 배포까지만 자동으로 진행하고, 승인후 트래픽을 전환하기 위해 추가합니다. 트래픽 전환 스위치 정도라고 생각하면 될 것 같습니다.\nName: 예) approval-stage Production canary\nName: 예) switch-to-production-stage Production namespace: 실 서비스 버전이 배포될 네임스페이스 이름을 입력합니다. 예) ns-canary-prod 설정이 완료되면 다음과 같은 파이프라인이 완성됩니다.\nBuild Pipeline에서 Deployment Pipeline 호출하기\n앞서 만든 Build Pipeline이 끝나고, 배포가 될수 있도록 Deployment Pipeline 호출을 추가합니다.\n앞서 만든 Build Pipelines으로 이동합니다.\n파이프라인 마지막에 Trigger Deployment Stage의 오른쪽 점 세개 클릭 \u0026gt; View details를 선택합니다.\n수정을 위해 Edit Stage를 선택합니다.\nSelect deployment pipeline을 클릭하여 새로 만든 Canary 배포용 Deployment Pipeline로 변경 저장합니다.\n빌드후 배포하는 전체 흐름이 완료되었습니다.\n테스트 - 최초 앱 배포 Trigger에서 지정한 oci-devops-oke-webpage 소스 코드에 임의의 변경사항을 발생시키고 Code Repository에 반영합니다.\n작업중인 코드가 있는 Cloud Shell로 이동합니다.\nhtml/index.html을 변경해도 되지만, 편의상 환경변수로 해놓은 Dockerfile을 변경합니다.\n현재 설정 기준으로 변경사항을 발생시키고 코드를 Commit 후 Push 합니다.\n... ENV VERSION=\u0026#34;2.0\u0026#34; ENV MESSAGE=\u0026#34;Hello OCI DevOps Canary\u0026#34; ENV BACKGROUND=\u0026#34;blue\u0026#34; ... Trigger가 되고 빌드가 시작됩니다.\n빌드가 성공하고, 배포 파이프라인이 실행하다, approval-stage에서 대기합니다.\n쿠버네티스 배포 상태를 확인합니다. ns-canary-stage에만 배포된 것을 알 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get all,ingress -n ns-canary-stage NAME READY STATUS RESTARTS AGE pod/webpage-deployment-9bff68bcf-7bjk2 1/1 Running 0 4m24s pod/webpage-deployment-9bff68bcf-d8pwq 1/1 Running 0 4m24s pod/webpage-deployment-9bff68bcf-v76nt 1/1 Running 0 4m24s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.35.238 \u0026lt;none\u0026gt; 8080/TCP 4m24s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 4m24s NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-9bff68bcf 3 3 3 4m24s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 4m23s [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get all,ingress -n ns-canary-prod No resources found in ns-canary-prod namespace. 실행중인 배포 파이프라인으로 돌아가 approval-stage에서 승인합니다.\n승인이 되면 Procution namespace인 ns-canary-prod에도 배포됩니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get all,ingress -n ns-canary-stage NAME READY STATUS RESTARTS AGE pod/webpage-deployment-9bff68bcf-7bjk2 1/1 Running 0 8m2s pod/webpage-deployment-9bff68bcf-d8pwq 1/1 Running 0 8m2s pod/webpage-deployment-9bff68bcf-v76nt 1/1 Running 0 8m2s ... [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get all,ingress -n ns-canary-prod NAME READY STATUS RESTARTS AGE pod/webpage-deployment-9bff68bcf-4v962 1/1 Running 0 2m14s pod/webpage-deployment-9bff68bcf-9w7d5 1/1 Running 0 2m14s pod/webpage-deployment-9bff68bcf-tqcck 1/1 Running 0 2m14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/webpage-service ClusterIP 10.96.132.13 \u0026lt;none\u0026gt; 8080/TCP 2m14s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/webpage-deployment 3/3 3 3 2m14s NAME DESIRED CURRENT READY AGE replicaset.apps/webpage-deployment-9bff68bcf 3 3 3 2m14s NAME CLASS HOSTS ADDRESS PORTS AGE ingress.networking.k8s.io/webpage-ingress \u0026lt;none\u0026gt; * 146.56.118.147 80 2m13s Ingress 주소를 통해 웹페이지에 접속하면, 현재 Blue 앱이 잘 동작하는 것을 볼수 있습니다.\n주소 예) http://146.56.118.147/webpage 테스트 - 새 버전 배포 - Green 버전 소스로 돌아가서 코드를 변경(예시, Dockerfile의 환경변수 변경)하고 Commit \u0026amp; Push 하여 코드를 반영합니다\n... ENV VERSION=\u0026#34;2.1\u0026#34; ENV MESSAGE=\u0026#34;Hello OCI DevOps Canary\u0026#34; ENV BACKGROUND=\u0026#34;green\u0026#34; ... 다시 빌드, 배포 파이프라인이 실행되고 approval-stage에서 멈출때까지 기다립니다.\n쿠버네티스 배포 상태를 확인합니다.\nns-canary-stage에 새 버전이 배포된 것을 알 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get pod -n ns-canary-stage NAME READY STATUS RESTARTS AGE webpage-deployment-6d8b64947b-2hqm6 1/1 Running 0 2m33s webpage-deployment-6d8b64947b-dtl99 1/1 Running 0 2m19s webpage-deployment-6d8b64947b-rqtpg 1/1 Running 0 2m26s [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get pod -n ns-canary-prod NAME READY STATUS RESTARTS AGE webpage-deployment-9bff68bcf-4v962 1/1 Running 0 13m webpage-deployment-9bff68bcf-9w7d5 1/1 Running 0 13m webpage-deployment-9bff68bcf-tqcck 1/1 Running 0 13m ns-canary-stage내의 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;25\u0026ldquo;로 변경된 걸 볼 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get ingress webpage-ingress -n ns-canary-stage -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-canary nginx.ingress.kubernetes.io/canary-weight: \u0026#34;25\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 ... [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get ingress webpage-ingress -n ns-canary-prod -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$2 ... 웹페이지에 접속합니다. 25%의 확률로 ns-canary-stage에 있는 새 버전으로 요청됩니다.\n실행중인 배포 파이프라인으로 돌아가 approval-stage에서 승인합니다.\n쿠버네티스 배포 상태를 다시 확인합니다.\nns-canary-stage는 그대로 이고, ns-canary-prod로 새 버전이 배포된 것을 알 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get pod -n ns-canary-stage NAME READY STATUS RESTARTS AGE webpage-deployment-6d8b64947b-2hqm6 1/1 Running 0 19m webpage-deployment-6d8b64947b-dtl99 1/1 Running 0 18m webpage-deployment-6d8b64947b-rqtpg 1/1 Running 0 19m [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get pod -n ns-canary-prod NAME READY STATUS RESTARTS AGE webpage-deployment-6d8b64947b-fxnf8 1/1 Running 0 117s webpage-deployment-6d8b64947b-lm44m 1/1 Running 0 2m4s webpage-deployment-6d8b64947b-zv8p8 1/1 Running 0 111 ns-canary-stage내의 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;0\u0026ldquo;로 변경된 걸 볼 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get ingress webpage-ingress -n ns-canary-stage -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-canary nginx.ingress.kubernetes.io/canary-weight: \u0026#34;0\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 ... [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get ingress webpage-ingress -n ns-canary-prod -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/rewrite-target: /$2 ... 웹페이지에 접속합니다. 이제 새 버전이 배포된 ns-canary-prod 쪽으로만 요청됩니다.\nns-canary-stage 쪽에는 액세스 로그로 확인해도 요청사항이 없습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl logs -lapp=webpage -n ns-canary-stage -f ... 테스트 - 추가적인 변경 사항 배포 - Red 배포 소스로 돌아가서 코드를 변경(예시, Dockerfile의 환경변수 변경)하고 Commit \u0026amp; Push 하여 코드를 반영합니다\n... ENV VERSION=\u0026#34;2.2\u0026#34; ENV MESSAGE=\u0026#34;Hello OCI DevOps Canary\u0026#34; ENV BACKGROUND=\u0026#34;red\u0026#34; ... 다시 빌드, 배포 파이프라인이 실행되고 approval-stage에서 멈출때까지 기다립니다.\n쿠버네티스 배포 상태를 확인합니다.\nns-canary-stage에 새 버전이 배포된 것을 알 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get pod -n ns-canary-stage NAME READY STATUS RESTARTS AGE webpage-deployment-79fd4f869b-2kvn4 1/1 Running 0 98s webpage-deployment-79fd4f869b-khbvk 1/1 Running 0 110s webpage-deployment-79fd4f869b-vwzmn 1/1 Running 0 91s [opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get pod -n ns-canary-prod NAME READY STATUS RESTARTS AGE webpage-deployment-6d8b64947b-fxnf8 1/1 Running 0 23m webpage-deployment-6d8b64947b-lm44m 1/1 Running 0 23m webpage-deployment-6d8b64947b-zv8p8 1/1 Running 0 23m ns-canary-stage내의 webpage-ingress의 annotations에서 보면 canary-weight: \u0026ldquo;25\u0026ldquo;로 변경된 걸 볼 수 있습니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get ingress webpage-ingress -n ns-canary-stage -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-canary nginx.ingress.kubernetes.io/canary-weight: \u0026#34;25\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 ... 웹페이지에 접속합니다. 25%의 확률로 ns-canary-stage에 있는 새 버전으로 요청됩니다.\napproval-stage에서 승인을 하면, 이전과 동일한 방식으로, ns-canary-prod로 새 버전이 배포되고, 모든 요청은 새 버전으로 전달됩니다.\napproval-stage에서 거부하면, ns-canary-stage쪽 가중치가 canary-weight: \u0026ldquo;0\u0026ldquo;으로 변경되어, 테스트 버전으로 요청은 중지됩니다.\n[opc@jumpbox-945115 oci-devops-oke-webpage]$ kubectl get ingress webpage-ingress -n ns-canary-stage -o yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/canary: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/canary-by-header: redirect-to-canary nginx.ingress.kubernetes.io/canary-weight: \u0026#34;0\u0026#34; nginx.ingress.kubernetes.io/rewrite-target: /$2 ... 테스트 - 롤백 신규 버전으로 서비스를 하다, 문제가 발생하면 이전 버전으로 롤백이 기능합니다.\n배포 히스토리 중에서 현재 버전을 배포한 배포 파이프라인으로 이동합니다.\nTraffic Shift 메뉴 중에서 Revert traffic shift를 클릭합니다.\n히스토리 중에 복구할 것을 선택합니다.\n히스토리 중에서 선택\n뭐가 뭔지 알고 고르나 싶지만, 일단 고르자.\n수동 롤백 작업이 완료되었습니다.\n웹페이지에 접속하면, 이전 앱으로 복귀된 것을 볼 수 있습니다.\n","lastmod":"2022-04-20T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/cicd/devops/3.canary-stragtegy/","tags":["devops","canary","ci/cd"],"title":"1.7.1.3 Canary 배포 전략으로 배포하기"},{"categories":null,"contents":"1.7.1.4 DevOps 서비스를 통해 Helm Chart로 배포 자동화하기 DevOps 프로젝트 만들기 이전에 생성한 프로젝트를 그대로 사용하거나, 새롭게 프로젝트를 만듭니다.\nCode Repository를 사용하여 애플리케이션 코드 관리하기 코드 저장소 생성을 위해 왼쪽 메뉴에서 Code Repositories를 클릭합니다.\nCreate repository를 클릭하여 저장소를 만듭니다.\nRepository name: 예) oci-devops-mychart 실제 개발 작업은 git 명령을 통해 개발 PC에서 진행하면 됩니다. 저장소 상세정보 위에 있는 Clone 버튼을 하면 Clone 명령어가 아래 그림처럼 뜨게 됩니다. 여기서는 Clone with HTTPS를 사용하겠습니다.\n개발 PC에 복사한 주소를 사용해 git clone 명령어를 통해 복제합니다.\ngit clone \u0026lt;YourClonewithHTTPS URL\u0026gt; 이때 사용자 인증이 필요합니다. HTTPS기반 사용자 인증시 아래 유저명 형식과 AuthToken을 사용합니다.\n샘플 Helm 차트 만들기 1.4.1.3 Helm Chart Repository로 사용하기를 참고하여 배포할 chart를 작성합니다.\n복사한 코드 저장소로 이동하여, Helm 차트를 만듭니다.\n$ cd oci-devops-mychart/ $ helm create mychart Creating mychart 차트 작성\n생성된 차트는 nginx를 배포하는 샘플 차트입니다. 실제 차트 작성을 위해서는 앱에 맞게 수정하겠지만, 지금은 배포 테스트로 수정없이 그냥 사용합니다.\nHelm Chart를 기본 구성 파일이 만들어 집니다. 생성된 차트는 기본적으로 nginx를 배포하게 구성되어 있습니다.\n코드를 Code Repository에 Push 합니다.\ngit add . git commit -m \u0026#34;init\u0026#34; git push 코드 작성 및 반영 완료\nBuild Pipeline 만들기 CI/CD 중에 코드를 빌드하여 배포 산출물을 만드는 CI 과정에 해당되는 부분을 Build Pipeline을 통해 구성이 가능합니다.\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Build Pipelines로 이동합니다.\nCreate build pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) mychart-build-pipeline 생성된 파이프라인을 클릭합니다.\n그림과 같이 Stage를 추가하여 파이프라인 흐름을 구성할 수 있습니다. Add Stage를 클릭합니다.\nBuild Stage 만들기\n빌드를 위해 먼저 Managed Build Stage를 추가합니다.\nManaged Build Stage 설정\nStage name: 예) build-stage\nBuild Spec File Path: 빌드 스크립트 경로를 지정합니다. 기본적으로 소스 루트에 있는 build_spec.yaml을 파일을 사용합니다.\nPrimary Code Repository: 빌드할 메인 소스가 있는 코드 저장소를 지정합니다.\n대상 소스 코드가 있는 저장소를 지정합니다.\n설정된 Stage를 Add를 클릭하여 추가합니다.\n테스트처럼 소스 코드상의 Build Spec의 정의가 필요합니다.\n소스 코드의 root 경로에 build_spec.yaml을 정의합니다.\nbuild_spec.yaml\nenv.variables.ocir_username: OCIR 로그인시 사용할 유저명 env.vaultVariables.ocir_authtoken: helm cli로 OCIR에 로그인시 사용할 AuthToken이 저장된 Vault Secret env.exportedVariables CHART_VERSION: 이후 배포시 사용하기 위해 차트의 값을 동적으로 가져옵니다. steps Helm Chart 단계: OCIR에 helm cli를 통해 Helm Chart를 배포합니다. 배포전 로그인 인증 패스워드로 사용할 유저의 AuthToken은 보안을 위해 Vault Secret을 사용합니다. version: 0.1 component: build timeoutInSeconds: 6000 runAs: root shell: bash env: # these are local variables to the build config variables: ocir_username: \u0026#34;winter\u0026#34; # the value of a vaultVariable is the secret-id (in OCI ID format) stored in the OCI Vault service # you can then access the value of that secret in your build_spec.yaml commands vaultVariables: ocir_authtoken: \u0026#34;ocid1.vaultsecret...\u0026#34; # exportedVariables are made available to use as parameters in sucessor Build Pipeline stages # For this Build to run, the Build Pipeline needs to have a BUILDRUN_HASH parameter set exportedVariables: - CHART_NAME - CHART_VERSION - TENANCY_NAMESPACE - REPO_NAME # Its a native way to fetch artifacts from external or artifact repo or a file path to use before a stage. # More about buildspec formats - https://docs.oracle.com/en-us/iaas/Content/devops/using/build_specs.htm inputArtifacts: steps: - type: Command timeoutInSeconds: 1200 name: \u0026#34;Install Tools\u0026#34; command: | mkdir -p ~/.local/bin wget https://github.com/mikefarah/yq/releases/download/v4.25.1/yq_linux_amd64 -O ~/.local/bin/yq chmod +x ~/.local/bin/yq export PATH=$HOME/.local/bin:$PATH - type: Command name: \u0026#34;Build Source\u0026#34; timeoutInSeconds: 4000 command: | echo no action echo no action - type: Command timeoutInSeconds: 1200 name: \u0026#34;Helm Chart\u0026#34; command: | TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` cd ${OCI_PRIMARY_SOURCE_DIR} cd mychart helm package . helm registry login -u $TENANCY_NAMESPACE/$ocir_username ${OCI_RESOURCE_PRINCIPAL_REGION}.ocir.io -p $ocir_authtoken export HELM_EXPERIMENTAL_OCI=1 helm push ./*.tgz oci://${OCI_RESOURCE_PRINCIPAL_REGION}.ocir.io/$TENANCY_NAMESPACE/helm - type: Command timeoutInSeconds: 1200 name: \u0026#34;Check exportedValues\u0026#34; command: | cd ${OCI_PRIMARY_SOURCE_DIR}/mychart CHART_NAME=`yq eval \u0026#39;.name\u0026#39; Chart.yaml` CHART_VERSION=`yq eval \u0026#39;.version\u0026#39; Chart.yaml` REPO_NAME=helm/$CHART_NAME outputArtifacts: 빌드 스테이지가 완성되었습니다.\nAuthToken 저장을 위한 Vault Secret 만들기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Vault로 이동합니다.\nCreate Vault를 클릭하여 새 Vault를 만듭니다.\nName: 예) myVault 생성된 Vault로 이동하여 Master Encryption Key를 만듭니다. Secret 생성을 위해 암호화 방식은 AES를 선택합니다.\n왼쪽 메뉴에서 Resources \u0026gt; Secrets로 이동하여 새 Secret을 만듭니다. 앞서 만든 암호화 키를 선택하고, Secret Contents에 OCIR에 접속할 유저의 AuthToken을 입력합니다.\n만든 Secret의 OCID를 복사합니다.\n소스로 돌아가서, 앞서 만든 build_spec.yaml의 env.vaultVariables.ocir_authtoken에 복사한 Secret의 OCID를 추가합니다.\nbuild_spec.yaml env: # these are local variables to the build config variables: ocir_username: \u0026#34;winter\u0026#34; # the value of a vaultVariable is the secret-id (in OCI ID format) stored in the OCI Vault service # you can then access the value of that secret in your build_spec.yaml commands vaultVariables: ocir_authtoken: \u0026#34;ocid1.vaultsecret...\u0026#34; 소스코드를 Commit \u0026amp; Push 하여 코드를 반영합니다.\ngit add build_spec.yaml git commit -m \u0026#34;add build_spec\u0026#34; git push Deployment Pipeline 만들기 CI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\nDevOps 서비스를 위한 Policy 설정하기\n앞서 만든 Policy에 Deployment Pipeline에서 OCIR에 접근할 수 있도록 권한을 추가합니다.\nCompartment 레벨로 다음 Policy를 만듭니다.\nName: 예) DevOps-compartment-policy Allow dynamic-group DeployDynamicGroup to read repos in compartment \u0026lt;YourCompartmentName\u0026gt; Root Compartment 레벨로 다음 Policy를 만듭니다.\nName: 예) DevOps-root-policy OCIR에 Repository를 Push하기 전에 미리 생성하지 않으면 기본적으로 Root Compartment에 이미지가 Push됩니다. 이때 권한으로 에러가 발생하며, Root Compartment에도 허용하고자 하면 다음을 추가합니다.\nAllow dynamic-group BuildDynamicGroup to read repos in tenancy Kubernetes Environment 등록하기\n앞서 등록한 OKE 환경이 없는 경우, 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Environments로 이동하여 배포할 OKE 환경을 등록합니다. Kubernetes에 배포할 Helm Chart Artifact 만들기\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Artifacts로 이동합니다.\nAdd artifact를 클릭하여 배포할 Helm Chart 자원을 추가합니다.\nType을 Helm Chart로 선택합니다. Helm Chart URL은 oci://\u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt; 형식으로 OCIR에 위치한 Helm Chart 주소를 입력합니다. Version은 OCIR에서 가져올 차트 버전이자, Tag로 그림과 같이 파라미터를 사용할 수 있습니다. 아래에서는 빌드 파이프라인의 exportedVariables 중 하나인 CHART_VERSION를 사용하였습니다. 필요하면, Add artifact를 클릭하여 배포할 Helm Chart 배포시 사용할 value 자원도 추가합니다.\nType: General artifact 선택\nArtifact source: 여기서는 Inline을 선택합니다.\nValue: 기본 Chart 설정에서 재정의하여 사용할 Chart Value를 입력합니다. 여기서는 테스트용도로 Service Type을 ClusterIP에서 LoadBalancer로 변경하겠습니다.\nservice: type: LoadBalancer Deploy Pipeline 만들기\n프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\nCreate pipeline을 클릭하여 파이프라인을 생성합니다.\nName: 예) mychart-deployment-pipeline 생성된 파이프라인을 클릭합니다.\nAdd Stage를 클릭하여 Install helm chart to Kubernetes cluster를 선택합니다.\n앞서 만든 자원들을 사용하여 Helm Chart 배포 설정을 합니다.\nEnvironment: 배포할 OKE 클러스터 선택 Release name: 차트 배포시 사용할 이름 Helm Chart: 앞서 등록한 배포용 Helm Chart Artifact Value: 앞서 등록한 차트 배포시 사용할 Value Artifact Build Pipeline에서 Deployment Pipeline 호출하기 앞서 만든 Build Pipeline이 끝나고, 배포가 될수 있도록 Deployment Pipeline 호출을 추가합니다.\n앞서 만든 Build Pipelines으로 이동합니다. 파이프라인 마지막에 Trigger Deployment Stage의 오른쪽 점 세개 클릭 \u0026gt; View details를 선택합니다. 수정을 위해 Edit Stage를 선택합니다. Select deployment pipeline을 클릭하여 새로 만든 Helm Chart 배포용 Deployment Pipeline로 변경 저장합니다. 빌드후 배포하는 전체 흐름이 완료되었습니다. 테스트 앞서와 동일하게 Trigger를 설정하여 테스트할 수 있습니다. 여기서는 빠른 테스트르 위해 Start Manual Run을 이용하여 테스트합니다.\n앞서 만든 Build Pipelines으로 이동합니다.\n우측 상단에 있는 Start Manual Run을 클릭하여 실행합니다.\n빌드 파이프라인 실행이 완료되면, OCIR에서 만들어진 차트를 아래와 같이 확인할 수 있습니다.\nDeployment Pipeline에서 그림과 같이 배포 성공합니다.\nOKE 클러스터에서도 Helm Chart 배포된 것을 볼 수 있습니다. 또는 value.yaml 파일이 반영되어 LoadBalancer 서비스 타입이 만들어 졌습니다.\nwinter@cloudshell:~ (ap-chuncheon-1)$ helm list NAME NAMESPACE REVISION UPDATED STATUS CHART APP VERSION mychart default 1 2022-05-25 04:09:15.518487366 +0000 UTC deployed mychart-0.1.0 1.16.0 winter@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/mychart-75c4d695c4-gqgn8 1/1 Running 0 26m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 13d service/mychart LoadBalancer 10.96.251.103 129.xxx.xxx.xxx 80:32461/TCP 26m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mychart 1/1 1 1 26m NAME DESIRED CURRENT READY AGE replicaset.apps/mychart-75c4d695c4 1 1 1 26m ","lastmod":"2022-05-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/cicd/devops/4.deploy-app-with-helm-chart/","tags":["devops","ci/cd"],"title":"1.7.1.4 Helm Chart로 배포 자동화하기"},{"categories":null,"contents":"1.7.1.5 DevOps 서비스의 빌드시 패키지 취약점 검사하기 취약점 검사를 위한 소스 코드 스캐닝 Application Dependency Management (ADM) 서비스는 OCI DevOps 빌드 파이프라인 내에서 소스코드 빌드에 사용되는 라이브러리 패키지에 대한 취약점 검사하는 기능을 제공합니다. 빌드 스펙상에서 취약점 검사를 위한 VulnerabilityAudit 유형을 제공합니다.\n현재는 Maven 패키지에 대한 스캐닝만 지원합니다.\n취약점 검사 Build Step 아래와 같이 빌드 스펙에 VulnerabilityAudit 유형을 추가하고, 관련 설정을 합니다. 이를 사용하기 위해서는 사전에 Application Dependency Management (ADM) 서비스에 Knowledge Base를 생성해야 합니다.\nVulnerabilityAudit 예시\nsteps: - type: VulnerabilityAudit name: \u0026#34;Vulnerability Audit Step\u0026#34; configuration: buildType: maven pomFilePath: ${OCI_PRIMARY_SOURCE_DIR}/pom.xml packagesToIgnore: - com.oracle.* - org.apache.logging maxPermissibleCvssV2Score: 10.0 maxPermissibleCvssV3Score: 10.0 knowledgeBaseId: ocid1.admknowledgebase.oc1.iad.restoftheocid vulnerabilityAuditCompartmentId: ocid1.compartment.oc1..restoftheocid vulnerabilityAuditName: testReport configuration/buildType: 현재는 maven만 지원합니다.\nconfiguration/packagesToIgnore: 취약점 검사에서 제외할 패키지를 지정합니다.\nconfiguration/maxPermissibleCvssV2Score: 지정한 스코어를 초과하는 경우 빌드를 Failed로 표시하고 중지합니다.\nconfiguration/maxPermissibleCvssV3Score: 지정한 스코어를 초과하는 경우 빌드를 Failed로 표시하고 중지합니다.\nknowledgeBaseId: Application Dependency Management (ADM) 서비스에 Knowledge Base의 OCID\n취약도 점수\nhttps://nvd.nist.gov/vuln-metrics/cvss\nApplication Dependency Management (ADM) 서비스에서 Knowledge Base 만들기 OCI 콘솔에 로그인합니다.\n좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; App Dependency Management \u0026gt; Knowledge Bases로 이동합니다.\n대상 Compartment를 선택하고 Create Knowledge Base를 클릭하여 생성합니다.\nName: 예) oci-hol-knowledge-base 생성된 Knowledge Base 상세정보로 가서 OCID를 복사해 둡니다.\nDevOps 서비스에서 ADM 권한관련 Policy 추가하기 DevOps를 위해 만든 Policy에 다음을 추가합니다.\nAllow dynamic-group BuildDynamicGroup to use adm-knowledge-bases in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group BuildDynamicGroup to manage adm-vulnerability-audits in compartment \u0026lt;YourCompartmentName\u0026gt; DevOps 프로젝트 만들기 진행을 위해 1.7.1.1 DevOps 서비스를 이용한 Spring Boot 앱을 OKE에 배포 자동화하기을 따라 소스코드를 생성하고, Build Pipeline의 Build Stage까지 만든 상태에서 다음 과정을 추가합니다.\n취약점 발생을 위한 변경작업 최신 Spring Boot 버전으로 프로젝트를 생성할 경우, 간단한 코드로 인해 취약점이 없을 수 있습니다. 강제 발생을 위해 아래와 같이 오래된 이전 버전으로 낮춥니다.\n소스 코드의 pom.xml 상의 Spring Boot 버전을 낮춥니다. 예) 2.6.5\n... \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.5\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt; \u0026lt;/parent\u0026gt; ... Build Step에 취약점 검사 추가하기 Build Spec 정의\n개발한 spring-boot-hello 소스 코드의 root 경로에 build_spec.yaml을 다음과 같이 정의합니다.\nbuild_spec.yaml\n기존 빌드스펙에, 코드를 빌드하기 전에 취약점 검사를 위한 Vulnerability Audit Step 단계를 추가합니다. 취약점 문제로 빌드를 중지하기 위해 스코어를 낮게 지정하였습니다. version: 0.1 component: build timeoutInSeconds: 6000 shell: bash env: variables: appName: \u0026#34;spring-boot-hello\u0026#34; exportedVariables: - APP_NAME - OCIR_PATH - TAG steps: - type: Command name: \u0026#34;Init exportedVariables\u0026#34; timeoutInSeconds: 4000 command: | APP_NAME=$appName echo $APP_NAME - type: VulnerabilityAudit name: \u0026#34;Vulnerability Audit Step\u0026#34; configuration: buildType: maven pomFilePath: ${OCI_PRIMARY_SOURCE_DIR}/pom.xml packagesToIgnore: - com.oracle.* maxPermissibleCvssV2Score: 9.0 maxPermissibleCvssV3Score: 9.0 knowledgeBaseId: ocid1.admknowledgebase.oc1.ap-chuncheon-1... vulnerabilityAuditName: spring-boot-hello-vulnerability-audit - type: Command name: \u0026#34;Build Source\u0026#34; timeoutInSeconds: 4000 command: | echo \u0026#34;build\u0026#34; mvn clean install - type: Command timeoutInSeconds: 400 name: \u0026#34;Build Source - Post\u0026#34; command: | echo \u0026#34;add dependency\u0026#34; mkdir -p target/dependency \u0026amp;\u0026amp; (cd target/dependency; jar -xf ../*.jar) - type: Command name: \u0026#34;Define Image Tag - Commit ID\u0026#34; timeoutInSeconds: 30 command: | COMMIT_ID=`echo ${OCI_TRIGGER_COMMIT_HASH} | cut -c 1-7` BUILDRUN_HASH=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` [ -z \u0026#34;$COMMIT_ID\u0026#34; ] \u0026amp;\u0026amp; TAG=$BUILDRUN_HASH || TAG=$COMMIT_ID - type: Command name: \u0026#34;Define OCIR Path\u0026#34; timeoutInSeconds: 30 command: | TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` REPO_NAME=$appName OCIR_PATH=$OCI_RESOURCE_PRINCIPAL_REGION.ocir.io/$TENANCY_NAMESPACE/$REPO_NAME - type: Command timeoutInSeconds: 400 name: \u0026#34;Containerize\u0026#34; command: | docker build -t new-generated-image . docker images - type: Command name: \u0026#34;Check exportedVariables\u0026#34; timeoutInSeconds: 30 command: | [ -z \u0026#34;$APP_NAME\u0026#34; ] \u0026amp;\u0026amp; APP_NAME=unknown [ -z \u0026#34;$OCIR_PATH\u0026#34; ] \u0026amp;\u0026amp; OCIR_PATH=unknown [ -z \u0026#34;$TAG\u0026#34; ] \u0026amp;\u0026amp; TAG=unknown echo \u0026#34;APP_NAME: \u0026#34; $APP_NAME echo \u0026#34;OCIR_PATH: \u0026#34; $OCIR_PATH echo \u0026#34;TAG: \u0026#34; $TAG outputArtifacts: - name: output-image type: DOCKER_IMAGE location: new-generated-image 작성된 코드를 git 명령어를 통해서 Code Repository에 저장합니다.\ngit add . git commit -m \u0026#34;build spec\u0026#34; git push 테스트 작성한 빌드 파이프라인으로 이동하여 Start Manual Run을 통해 빌드 파이프라인을 실행합니다.\n빌드과정중에 취약점 검사에서 실패하는 것을 볼수 있습니다.\n상세내역을 확인하기 위해 오른쪽 상단 점 세개를 클릭하여 View Details를 클릭합니다.\n상세내역에서 취약점 검사 결과를 요약 정보를 볼 수 있습니다.\n리포트 이름을 클릭하면, 전체 리포트를 볼 수 있습니다.\n취약점 코드 링크를 취약점 데이터 베이스에 등록된 상세 정보를 확인할 수 있습니다.\n","lastmod":"2022-05-26T00:00:00Z","permalink":"https://thekoguryo.github.io/oracle-cloudnative/oke-managed-nodes/cicd/devops/5.detect-vulnerabilities-from-build-pipeline/","tags":["devops","ci/cd"],"title":"1.7.1.5 빌드시 패키지 취약점 검사하기"},{"categories":null,"contents":"참조할만한 사이트 OCI Documentation - Oracle OCI 공식 문서 사이트입니다. Oracle Learning - 오라클에서 OU에서 OCI를 학습할 수 있는 사이트로 동영상과 실습교재를 제공합니다. 프로모션 기간 동안 무료로 사용할 수 있습니다. Oracle Cloud Overview Oracle Cloud Infrastructure foundations associate Oracle Cloud Data Management foundations associate Cloud application development explorer Cloud native development explorer OCI for AWS architects OCI for Azure architects Oracle LiveLabs - 오라클에서 제공하는 OCI 실습 가이드를 제공하는 사이트입니다. OCI 기초 OCI 관리형 쿠버네티스 - OKE 기초 OCI 서버리스 함수 기초 ","lastmod":null,"permalink":"https://thekoguryo.github.io/link/","tags":null,"title":""},{"categories":null,"contents":"https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/terraformUsingObjectStore.htm#http\nBucket 생성 tf-example-tfstate\nPAR(Pre-Authenticated Request) 생성 =\u0026gt; 근데 PAR URL로 접근해야 하면, 이건 인터넷에서 인증 없이 접근됨. Object Storage가 Private Bucket이라 하더라도 해당 PAR URL로는 인증없이 접근됨 par-bucket-tf-example-tfstate Permit object reads and writes\nURL https://objectstorage.ap-seoul-1.oraclecloud.com/p/k5TQIRpJrotyyCVwzQLDw3CqWrI1feMzvPtz7YEwm9I5IdvZwBxDUhGGmAxboJvf/n/cn8wdnkejjgq/b/tf-example-tfstate/o/\nTerraform Backend로 HTTP 구성 terraform { backend \u0026ldquo;http\u0026rdquo; { address = \u0026ldquo;https://objectstorage.ap-seoul-1.oraclecloud.com/p/k5TQIRpJrotyyCVwzQLDw3CqWrI1feMzvPtz7YEwm9I5IdvZwBxDUhGGmAxboJvf/n/cn8wdnkejjgq/b/tf-example-tfstate/o/terraform.tfstate\u0026quot; update_method = \u0026ldquo;PUT\u0026rdquo; } }\nterraform init\nterraform apply\nObject Storage 가 보면 tfstate 파일 생성되어 있음.\nlock 설정\nterraform { backend \u0026ldquo;http\u0026rdquo; { address = \u0026ldquo;https://objectstorage.ap-seoul-1.oraclecloud.com/p/k5TQIRpJrotyyCVwzQLDw3CqWrI1feMzvPtz7YEwm9I5IdvZwBxDUhGGmAxboJvf/n/cn8wdnkejjgq/b/tf-example-tfstate/o/terraform.tfstate\u0026quot; update_method = \u0026ldquo;PUT\u0026rdquo; lock_address = \u0026ldquo;https://objectstorage.ap-seoul-1.oraclecloud.com/p/k5TQIRpJrotyyCVwzQLDw3CqWrI1feMzvPtz7YEwm9I5IdvZwBxDUhGGmAxboJvf/n/cn8wdnkejjgq/b/tf-example-tfstate/o/lock\u0026quot; unlock_address = \u0026ldquo;https://objectstorage.ap-seoul-1.oraclecloud.com/p/k5TQIRpJrotyyCVwzQLDw3CqWrI1feMzvPtz7YEwm9I5IdvZwBxDUhGGmAxboJvf/n/cn8wdnkejjgq/b/tf-example-tfstate/o/lock\u0026quot; } }\nterraform init terraform apply 응답 오류 발생 Object Storage의 HTTP API가 terraform state locking 지원 안하는 것으로 판단됨. $ terraform apply ╷ │ Error: Error acquiring the state lock │ │ Error message: Unexpected HTTP response code 404 │ │ Terraform acquires a state lock to protect the state from being written │ by multiple users at the same time. Please resolve the issue above and try │ again. For most commands, you can disable locking with the \u0026#34;-lock=false\u0026#34; │ flag, but this is not recommended. ","lastmod":null,"permalink":"https://thekoguryo.github.io/oci/chapter14/3/8/","tags":null,"title":""},{"categories":null,"contents":"aa\n","lastmod":null,"permalink":"https://thekoguryo.github.io/test/","tags":null,"title":""},{"categories":null,"contents":"","lastmod":null,"permalink":"https://thekoguryo.github.io/search/","tags":null,"title":"Search"}]