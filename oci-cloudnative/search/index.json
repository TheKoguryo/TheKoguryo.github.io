[{"categories":null,"contents":"4.2.1 EFK 설치하기 Elastic Search + Kibana 설치   설치용 namespace를 만듭니다.\nkubectl create ns logging   Helm Chart를 통해 설치하기 위해 저장소를 등록합니다. 본 예제에서는 Bitnami Helm Chart 저장소를 사용합니다.\nhelm repo add bitnami https://charts.bitnami.com/bitnami   설정값 정의\nHelm Chart를 설치시 설정가능한 파라미터 목록을 참고하여 변경하고자 하는 값을 입력합니다.\n https://github.com/bitnami/charts/tree/master/bitnami/elasticsearch/#parameters 아래 예시는 elasticsearch 내장 kibana를 함께 설치하고, kibana 접근 URL을 이전 장에서 설치한 nginx ingress controller를 사용하는 예시입니다.  cat \u0026lt;\u0026lt;EOF \u0026gt; values.yamlglobal:kibanaEnabled:truekibana:ingress:enabled:truehostname:kibana.ingress.thekoguryo.mlannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:letsencrypt-stagingtls:trueEOF  elasticsearch helm chart 설치\nhelm install elasticsearch -f values.yaml bitnami/elasticsearch -n logging   설치\n아래와 같이 설치되며, 실제 컨테이너가 기동하는 데 까지 약간의 시간이 걸립니다.\noke_admin@cloudshell:efk (ap-seoul-1)$ helm install elasticsearch -f values.yaml bitnami/elasticsearch -n logging NAME: my-logging ... Elasticsearch can be accessed within the cluster on port 9200 at elasticsearch-coordinating-only.logging.svc.cluster.local To access from outside the cluster execute the following commands: kubectl port-forward --namespace logging svc/elasticsearch-coordinating-only 9200:9200 \u0026amp; curl http://127.0.0.1:9200/   설치된 elastic search 내부 주소와 포트를 확인합니다. 이후 Fluentd에서 로그 전송을 위해 사용할 주소입니다.\n 주소: elasticsearch-coordinating-only.logging.svc.cluster.local 포트: 9200    Fluentd 구성 참고 문서\n Kubernetes - Fluentd https://gist.github.com/agapoff/77d746b4588ee37a9e8074904533f6bc    Fluentd 설치를 위한 Service Account를 생성하고 관련 권한을 정의합니다.\n   configmap 추가 설정정의\n Fluentd 컨테이너 이미지에는 로그 파싱과 관련된 설정들이 컨테이너 이미지내에 /fluentd/etc/ 하위에 .conf 파일로 모두 정의 되어 있습니다. 이 파일들을 재정의 할 수 있습니다. 여기에서는 다른 설정들은 그대로 두고 Parser만 변경합니다. 기본 Parser는 Docker Engine이 런타임인 경우 잘 동작하지만, 최근 OSS 쿠버네티스의 기본 런타임인 containerd와 OKE에서 사용하고 있는 cri-o에서는 파싱 에러가 발생합니다. 정상 파싱을 위해 파서 설정(tail_container_parse.conf)만 아래와 같이 cri Parser로 변경합니다. https://github.com/fluent/fluentd-kubernetes-daemonset/issues/434#issuecomment-831801690     fluentd damonset 정의\n설정한 configmap 사용을 위해 Fluentd 문서상의 YAML을 일부 변경하셨습니다.\n   FluentD 설치\nkubectl apply -f fluentd-rbac.yaml kubectl apply -f fluentd-configmap-elasticsearch.yaml kubectl apply -f fluentd-daemonset-elasticsearch.yaml   Kibana 설정   설치한 kibana을 웹 브라우저로 접속합니다. ingress로 지정한 주소로 접속합니다.\n 예, https://kibana.ingress.thekoguryo.ml    Welcome 페이지의 Add Data를 클릭하여 홈으로 이동합니다.\n  왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n   Create index pattern을 클릭합니다.\n  인덱스 패턴을 생성합니다.\n오른쪽에서 보듯이 FluentD에서 전송된 로그는 logstash-로 시작합니다.\n Name: logstash-* Timestamp field: @timestamp     인덱스 패턴이 추가된 결과를 볼 수 있습니다.\n   왼쪽 상단 내비게이션 메뉴에서 Analytics \u0026gt; Discover 를 클릭합니다.\n  생성한 인덱스 패턴을 통해 수집된 로그를 확인할 수 있습니다.\n 테스트 앱의 로그를 확인하기 위해 Add filter를 클릭하여 namespace_name=default 로 지정합니다.     테스트 앱을 접속합니다.\n 예, https://blue.ingress.thekogury.ml/ekf-test    로그 확인\n아래와 같이 kibana에서 테스트 앱의 로그를 확인할 수 있습니다.\n   EKF를 통해 OKE 상의 로그를 수집하는 예시였습니다. EKF에 대한 상세 내용은 제품 관련 홈페이지와 커뮤니티 사이트를 참고하기 바랍니다.\n  ","lastmod":"2021-12-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/logging-efk/1.install-efk/","tags":["oss","logging","efk"],"title":"4.2.1 EFK 설치하기"},{"categories":null,"contents":"4.1.1 NGINX Ingress Controller 사용하기 OKE에서 Kubernetes에서 사용가능한 여러가지 오픈 소스 ingress controller를 사용할 수 있습니다. 본 문서에서는 그중에서 가장 많이 사용되며, OKE 문서에서 예제로 설명하고 있는 nginx-ingress-controller를 테스트 해보겠습니다.\n  공식 문서\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm https://kubernetes.github.io/ingress-nginx/deploy/#oracle-cloud-infrastructure    설치 버전\n OKE 문서는 v0.44.0 기준이 NGINX Ingress Controller for Kubernetes 문서는 최신 버전인 v1.1.0 기준입니다. 본 문서는 최신 버전인 v1.1.0 기준이며, 해당 버전은 Kubernetes 1.22, 1.21, 1.20, 1.19을 지원하고 있습니다. NGINX Ingress Controller의 지원 버전  https://github.com/kubernetes/ingress-nginx#support-versions-table      NGINX Ingress Controller 설치 Ingress Controller 설치   kubectl 사용이 가능한 Cloud Shell 또는 작업환경에 접속합니다.\n  다음 명령으로 NGINX Cloud Deployment 설치\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.0/deploy/static/provider/cloud/deploy.yaml   설치 확인\ningress-nginx namespace에 아래와 같이 설치된 것을 확인할 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-x74zf 0/1 Completed 0 65s pod/ingress-nginx-admission-patch-f8x5k 0/1 Completed 0 65s pod/ingress-nginx-controller-69db7f75b4-vb84p 1/1 Running 0 65s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller LoadBalancer 10.96.212.64 132.226.225.240 80:31975/TCP,443:31756/TCP 65s service/ingress-nginx-controller-admission ClusterIP 10.96.95.133 \u0026lt;none\u0026gt; 443/TCP 65s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ingress-nginx-controller 1/1 1 1 65s NAME DESIRED CURRENT READY AGE replicaset.apps/ingress-nginx-controller-69db7f75b4 1 1 1 65s NAME COMPLETIONS DURATION AGE job.batch/ingress-nginx-admission-create 1/1 2s 65s job.batch/ingress-nginx-admission-patch 1/1 2s 65s   Load Balancer IP 확인   Ingress Controller 서비스의 로드밸런서 IP인 EXTERNAL-IP를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx-controller LoadBalancer 10.96.212.64 132.226.225.240 80:31975/TCP,443:31756/TCP 5m31s   PATH 기반 기본 라우팅 테스트 가장 기본적인 라우팅으로 URL PATH에 따라 라우팅 서비스를 달리하는 경우입니다.\n  테스트를 위한 샘플 앱을 배포합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(path-basic.yaml)을 작성합니다.\n /blue 요청은 nginx-blue-svc 로 라우팅 /green 요청은 nginx-green-svc로 라우팅  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-path-basicannotations:kubernetes.io/ingress.class:nginxspec:rules:- http:paths:- path:/bluepathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- http:paths:- path:/greenpathType:Prefixbackend:service:name:nginx-green-svcport:number:80  작성한 path-basic.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f path-basic.yaml ingress.networking.k8s.io/ingress-path-basic created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-basic \u0026lt;none\u0026gt; * 132.226.225.240 80 49s   앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n  /blue 요청\n   /green 요청\n   POD 정보 확인\n정의한 PATH에 따라 각각 blue, green 앱이 배포된 POD로 라우팅 된 것을 웹페이지 배경색 및 POD IP로 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-blue-6fccd8fb49-q6qph 1/1 Running 0 13m 10.244.0.138 10.0.10.139 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-green-7f646c5c7f-snpxf 1/1 Running 0 13m 10.244.0.5 10.0.10.84 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;     Rewrite Target URL PATH 라우팅 결과를 보면 /blue, /green의 Path가 최종 라우팅 되어 실행되는 앱으로 그대로 전달되는 것을 알 수 있습니다. ingress controller에서 라우팅을 위해서만 사용하고, 실제 앱의 동작을 위해는 수정이 필요한 경우에 사용합니다.\n https://kubernetes.github.io/ingress-nginx/examples/rewrite/    ingress 설정 YAML(path-rewrite-target.yaml)을 작성합니다.\n path: /blue -\u0026gt; /blue(/|$)(.*) 로 변경 annotation 추가: nginx.ingress.kubernetes.io/rewrite-target: /$2 예시  ~~/blue -\u0026gt; ~~/ 로 앱으로 전달 ~~/blue/abc -\u0026gt; ~~/abc 로 앱으로 전달    apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-path-rewrite-targetannotations:kubernetes.io/ingress.class:nginxnginx.ingress.kubernetes.io/rewrite-target:/$2spec:rules:- http:paths:- path:/blue(/|$)(.*)pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- http:paths:- path:/green(/|$)(.*)pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  앞선 path-basic.yaml를 삭제하고 path-rewrite-target.yaml를 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl delete -f path-basic.yaml ingress.networking.k8s.io \u0026#34;ingress-path-basic\u0026#34; deleted oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f path-rewrite-target.yaml ingress.networking.k8s.io/ingress-path-rewrite-target created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-path-rewrite-target \u0026lt;none\u0026gt; * 132.226.225.240 80 43s   앞서 확인한 ingress controller의 EXTERNAL IP로 접속하여 결과를 확인합니다.\n  ~~/blue 요청\n 라우팅된 앱에서는 /blue가 빠지고 /로만 수신됨     ~~/blue/abc 요청\n 라우팅된 앱에서는 /blue가 빠지고 /abc로만 수신됨       ","lastmod":"2021-12-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/ingress-controller/1.install-nginx-ingress-controller/","tags":["oss","ingress-controller"],"title":"4.1.1 NGINX Ingress Controller 사용하기"},{"categories":null,"contents":"3.1 컨테이너 이미지 스캔 OCIR는 알려진 Common Vulnerabilities and Exposures (CVE) database를 기반으로 컨테이너 이미지에 대한 취약점 분석 기능을 제공하고 있습니다. 다음은 취약점 분석을 위한 이미지 스캔 기능을 확인해 봅니다.\n공식 문서\n Scanning Images for Vulnerabilities  관련 Policy 설정 스캔 서비스가 OCIR repository에 대한 권한을 부여합니다. 적용해야 하는 Policy는 위 공식 문서를 참조합니다. tenancy 전체 또는 compartment 에 대해서 지정할 수 있습니다.\n  전체 테넌시\n 이름 예, ocir-scanning-images-root-policy  allow service vulnerability-scanning-service to read repos in tenancy allow service vulnerability-scanning-service to read compartments in tenancy   특정 compartment\nallow service vulnerability-scanning-service to read repos in compartment \u0026lt;compartment-name\u0026gt; allow service vulnerability-scanning-service to read compartments in compartment \u0026lt;compartment-name\u0026gt;   설정 예시\n   이미지 스캐너 설정 이미지 스캐너는 repository 단위로 기능 추가, 삭제 할 수 있습니다. 테스트를 위해 먼저 이미지를 배포합니다.\n  OCIR 이미지 사용하여 앱 배포하기를 참고하여 OCIR에 이미지를 배포합니다.\n  OCIR에 이미지 배포\ndocker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, root)를 선택합니다.\n  스캐너를 설정할 Repository(예, nginx)를 선택하고 오른쪽 Actions 메뉴에서 Add scanner를 선택합니다.\n   생성할 스캐너의 compartment 위치를 확인하고, 처음 스캐너 사용이므로 Create new scan configuration으로 선택하여, 스캐너를 추가합니다. 현재는 스캐너별로 따라 추가 설정할 수 있는 기능이 없는 것으로 보여, 한번 만들고 다음부터는 만든 것을 사용하면 됩니다.\n   스캐너 결과 확인\n스캐너가 추가된 nginx repository에 대해서 tag 별로 컨테이너 이미지 상세 정보를 볼 수 있습니다. Scan results 탭을 보면 스캔 결과를 확인할 수 있으며, View details를 통해 상세 정보를 확인할 수 있습니다.\n   취약점 확인\n이미지 스캐닝 결과 확인 알려진 CVE 정보를 기반한 취약점을 확인 할 수 있으며, 취약점를 클릭하면, 실제 CVE 데이터 베이스로 이동하여, 상세 정보를 확인 할 수 있습니다.\n    ","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/ocir/1.scan-image/","tags":["container registry"],"title":"3.1 컨테이너 이미지 스캔"},{"categories":null,"contents":"1. OKE(Oracle Container Engine for Kubernetes) 소개 \u0026hellip;\n","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/1.oke/","tags":["oke"],"title":"1.1 OKE(Oracle Container Engine for Kubernetes) 소개"},{"categories":null,"contents":"1.3.1 Cloud Shell로 OKE 클러스터 연결하기 Cloud Shell Cloud Shell은 공식 문서에서 설명하는 것 처럼 Oracle Cloud 콘솔에서 제공하는 웹 브라우저 기반 리눅스 터미널입니다. 작은 가상머신으로 구동된다고 이해하시면 되며, Oracle Cloud 콘솔에 접속한 유저에 대해 사전 인증된 OCI CLI를 제공하며, 추가적인 cli 및 설정들을 기본 구성하여 제공합니다.\nOKE 클러스터 접속을 위해 Kubernetes CLI인 kubectl로 기본 설치되어 제공합니다.\n무료로 사용할 수 있고, 인증 및 기본 툴 들이 구성되어 있기 때문 바로 사용할 수 있는 이점이 있습니다.\nCloud Shell로 OKE 클러스터 연결하기   생성한 OKE 클러스터 상세 페이지로 이동합니다.\n  Access Cluster를 클릭합니다.\n   두 가지 접근 방법 중에 Cloud Shell Access을 선택합니다.\n Cloud Shell: OCI에서 제공하는 Cloud Shell을 통해 접근합니다. 현재 접속한 사용자가 현 OCI Tenacy 환경에 작업하기 위한 기본 설정 및 관련 cli들이 구성되어 있습니다. Local Access: 로컬 PC 환경에서 처음 접속하기 위해 필요한 작업부터 시작하는 방법입니다.     Step #1. Launch Cloud Shell\nLaunch Cloud Shell를 클릭하거나, 우측 상단에 있는 링크를 클릭하여 Cloud Shell에 접속합니다.\n   접속한 환경에서 다음 명령을 실행해 보면 oci cli가 설치되어 있으며, 접속이 가능한 상태임을 알 수 있습니다.\noci -v oci os get ns    Step #2. kubeconfig 파일 생성하기\n생성된 OKE 클러스터 접속을 위한 kubeconfig을 생성하기 위해 Access Your Cluster의 두 번째 단계 내용을 Cloud Shell에서 실행합니다.\n 명령어에서 보듯이 Cloud Shell에서는 Kubernetes API를 Public Endpoint을 제공하는 경우에만 접근할 수 있습니다.     OKE 클러스터 연결 확인\nkubectl cluster-info 를 실행하면 생성된 kubeconfig를 통해 클러스터에 접속됨을 확인할 수 있습니다.\n   ","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/3.access-cluster/1.cloudshell-access/","tags":["oke"],"title":"1.3.1 Cloud Shell로 클러스터 연결하기"},{"categories":null,"contents":"1.3.2 로컬 환경에서 클러스터 연결하기 OCI CLI 설치 및 환경 구성 OCI CLI 설치 공식 문서를 참고하여 OCI OCI를 설치합니다.\n https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cliinstall.htm  Oracle Linux 기준 예시\n  설치\nbash -c \u0026#34;$(curl -L https://raw.githubusercontent.com/oracle/oci-cli/master/scripts/install/install.sh)\u0026#34;   설치 확인\noci --version   OCI CLI Config File설정   설정을 위해 필요한 정보 사전 확인\n  user OCID\n오른쪽 위 사용자 Profle에서 User Settings 클릭 후 사용자 OCID 확인   Tenancy OCID\n오른쪽 위 사용자 Profle에서 Tenancy 클릭 후 Tenancy OCID 확인   Region: 사용할 Region\n  API Signing Key: 여기서는 편의상 새로운 Private Key, Public Key를 생성하는 것으로 선택하겠습니다.\n      setup config 실행\noci setup config  실행 예시  [opc@bastion-host ~]$ oci setup config This command provides a walkthrough of creating a valid CLI config file. ... Enter a location for your config [/home/opc/.oci/config]: Enter a user OCID: ocid1.user.oc1..aaaaaaaazo6ilmezdaeozjcmsu6rcxnf5sjz2fau76kpdjvsbbakhqtw75sq Enter a tenancy OCID: ocid1.tenancy.oc1..aaaaaaaamd5zq7ohrxkmcsai23pp634k6i2eymnznv4d6ur7n5n4qj7jfhoq Enter a region by index or name(e.g. 1: ap-chiyoda-1, 2: ap-chuncheon-1, 3: ap-hyderabad-1, 4: ap-ibaraki-1, 5: ap-melbourne-1, 6: ap-mumbai-1, 7: ap-osaka-1, 8: ap-seoul-1, 9: ap-singapore-1, 10: ap-sydney-1, 11: ap-tokyo-1, 12: ca-montreal-1, 13: ca-toronto-1, 14: eu-amsterdam-1, 15: eu-frankfurt-1, 16: eu-marseille-1, 17: eu-zurich-1, 18: il-jerusalem-1, 19: me-dubai-1, 20: me-jeddah-1, 21: sa-santiago-1, 22: sa-saopaulo-1, 23: sa-vinhedo-1, 24: uk-cardiff-1, 25: uk-gov-cardiff-1, 26: uk-gov-london-1, 27: uk-london-1, 28: us-ashburn-1, 29: us-gov-ashburn-1, 30: us-gov-chicago-1, 31: us-gov-phoenix-1, 32: us-langley-1, 33: us-luke-1, 34: us-phoenix-1, 35: us-sanjose-1): 8 Do you want to generate a new API Signing RSA key pair? (If you decline you will be asked to supply the path to an existing key.) [Y/n]: Enter a directory for your keys to be created [/home/opc/.oci]: Enter a name for your key [oci_api_key]: Public key written to: /home/opc/.oci/oci_api_key_public.pem Enter a passphrase for your private key (empty for no passphrase): Private key written to: /home/opc/.oci/oci_api_key.pem Fingerprint: a0:e1:fe:79:22:22:f0:b5:6b:29:72:5f:5d:b6:22:32 Config written to /home/opc/.oci/config If you haven\u0026#39;t already uploaded your API Signing public key through the console, follow the instructions on the page linked below in the section \u0026#39;How to upload the public key\u0026#39;: https://docs.cloud.oracle.com/Content/API/Concepts/apisigningkey.htm#How2   API Public Key 등록   OCI Config File에 등록한 사용자 상세 페이지로 이동\n  왼쪽 아래 Resources에서 API Keys 선택\n  Add Public Key 클릭\n    oci setup config 실행 후 생성된 API Public Key 확인\n[opc@bastion-host ~]$ cat ~/.oci/oci_api_key_public.pem -----BEGIN PUBLIC KEY----- MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAycclV86OzQ+x6I13HEbe ... gCO1GdLyJNS08Zv1uiW6j1IYIszrmr6XK482Vf3r48u8ZkFgBBlsPjU03R9H1x52 dwIDAQAB -----END PUBLIC KEY-----   생성된 API Public Key 내용을 API Public Key 추가\n   OCI CLI를 위한 API Public Key 등록 완료   연결 확인   다시 리눅스 호스트로 돌아가 oci os ns get을 실행하여 연결 확인\n[opc@bastion-host ~]$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; }   kubectl CLI 설치 및 환경 구성 kubectl CLI 설치 공식 문서를 참고하여 kubectl OCI를 설치합니다.\n https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/  Linux 기준 예시\n  설치\ncurl -LO \u0026#34;https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\u0026#34; sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl   kubeconfig 파일 생성하기   생성한 OKE 클러스터 상세 페이지에서 Access Cluster를 클릭합니다.\n  Local Access 를 클릭합니다.\n   PRIVATE_ENDPOINT\nKubernetes API 접근도 Private IP를 통해서 할 때 사용합니다. PRIVATE_ENDPOINT, 즉 Private IP로 접근을 해야 하므로, bastion host 등 내부 IP로 접근이 가능한 서버에서 수행할 때 사용합니다.\n bastion host는 외부에서 SSH로 접근 가능하게 22 포트 오픈이 필요하며, 내부적으로는 OKE 클러스터의 Kubernetes API 및 Worker Nodes 들에 접근이 가능해야 합니다. kubeconfig 파일 생성 및 클러스터에 접속 확인  [opc@bastion-host ~]$ oci ce cluster create-kubeconfig --cluster-id ocid1.cluster.oc1.ap-seoul-1.aaaaaaaair2g7crtxvgchbrfkvf5dz6j7ncrjepinzm2infw6cjy33uzxhyq --file $HOME/.kube/config --region ap-seoul-1 --token-version 2.0.0 --kube-endpoint PRIVATE_ENDPOINT New config written to the Kubeconfig file /home/opc/.kube/config [opc@bastion-host ~]$ kubectl cluster-info Kubernetes control plane is running at https://10.0.0.4:6443 CoreDNS is running at https://10.0.0.4:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;.   PUBLIC_ENDPOINT\nKubernetes API 접근도 Public IP를 통해서 할 때 사용합니다. OKE 클러스터 생성시 Kubernetes API에 Public IP를 부여한 경우에 사용 가능한 방법입니다. 실제 kubeconfig 파일 생성하는 명령(oci ce cluster crate-kubeconfig ~~)은 마지막 옵션값인 PUBLIC_ENPOINT이외는 동일한 명령합니다.\n  ","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/3.access-cluster/2.local-access/","tags":["oke"],"title":"1.3.2 로컬 환경에서 클러스터 연결하기"},{"categories":null,"contents":"4.1.2 NGINX Ingress Controller에서 host 기반 라우팅(feat. OCI DNS) Ingress Controller에서 도메인 네임을 기반하여 라우팅하기 위해 OCI DNS를 사용하는 방법을 확인합니다.\nOCI DNS 서비스 사용하기 이미 구입한 Domain Name이 있다는 전제하에 설정하는 과정입니다. 테스트를 위해 freenom 사이트에서 발급받은 무료 Domain Name(thekoguryo.ml)을 사용하였습니다.\nOCI DNS 서비스 설정   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Networking \u0026gt; DNS Management \u0026gt; Zones로 이동합니다.\n  Create Zone 클릭\n   생성정보 입력\n사용한 Domain Name을 바탕으로 Zone을 생성합니다.\n  METHOD: Manual\n  ZONE NAME: 가지고 있는 Domain Name 입력\n  COMPARTMENT: 생성할 대상 Compartment\n  ZONE TYPE: Primary\n     Zone 내부에 NS 유형과 SOA 유형의 레코드가 생성되어 있습니다. NS는 네임 서버 레코드, SOA는 권한 시작 레코드입니다. Add Record를 클릭합니다.   추가할 레코드를 입력하고 제출합니다.\n  Record Type: A - IPv4 Address\n  NAME: *.ingress\n 와일드 카드 형식으로 ingress controller가 사용할 서브 Domain Name을 입력합니다.    TTL: 300, 우측 자물쇠는 클릭하여 잠금 해제 후 TTL 값 입력\n  RDATA MODE: Basic\n  ADDRESS: 매핑할 IP, 여기서는 앞서 만든 nginx ingress controller의 Load Balancer의 IP 입력\n     반영하기 위해 Publish Change 클릭\n  확인 창이 뜨면 한번 더 Publish Change 클릭\n   레코드 추가 및 반영 완료\n   레코드 중 NS 유형인 네임서버 주소를 모두 복사합니다.\n  Domain Name 제공 측에 설정 이제 Domain Name을 구입한 사이트에서 설정이 필요합니다. 아래 과정은 freenom 기준 설정입니다. 구입한 사이트에서 비슷한 방식으로 설정합니다.\n  freenom 사이트에 접속하여 My Domain에서 사용할 도메인 네임 우측의 Manage Domain 클릭   위쪽 Management Tools \u0026gt; Nameservers를 선택한 다음 **Use custom nameservers (enter below)**를 선택합니다.\n  앞서 OCI DNS Zone 에서 복사해둔 네임서버 주소를 차례대로 입력한 후 Change Nameservers 클릭   DNS 테스트   nslookup 툴로 등록한 DNS를 테스트 해봅니다. 잘 등록된 것을 알 수 있습니다.\nC:\\\u0026gt;nslookup *.ingress.thekoguryo.ml 서버: kns.kornet.net Address: 168.126.63.1 권한 없는 응답: 이름: *.ingress.thekoguryo.ml Address: 132.226.225.240   HOST 기반 라우팅 테스트 HOST 이름에 따라 라우팅 서비스를 달리하는 경우입니다.\n  테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\n배경 색깔이 다른 두개의 웹페이지를 배포합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(host-basic.yaml)을 작성합니다.\n blue.ingress.thekoguryo.ml 요청은 nginx-blue-svc 로 라우팅 green.ingress.thekoguryo.ml 요청은 nginx-green-svc로 라우팅  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-host-basicannotations:kubernetes.io/ingress.class:nginxspec:rules:- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  작성한 host-basic.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f host-basic.yaml ingress.networking.k8s.io/ingress-host-basic created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-host-basic \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80 6s   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n  blue.ingress.thekoguryo.ml 요청\n   green.ingress.thekoguryo.ml 요청\n   와일드 카드 주소로 DNS에 등록한 Ingress Controller의 Load Balancer를 거쳐 접속한 host의 FQDN에 따라 대상 서비스에 라우팅 되는 것을 확인할 수 있습니다.\n    ","lastmod":"2021-12-05T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/ingress-controller/2.nginx-ingress-host/","tags":["oss","ingress-controller"],"title":"4.1.2 NGINX Ingress Controller에서 host 기반 라우팅(feat. OCI DNS)"},{"categories":null,"contents":"3.2 Helm Chart Repostory로 사용하기 OCIR은OCI(Open Container Initiative) Registry로 Helm 3에서는 현재 OCI Registry를 실험적(experimental)인 기능으로 지원하고 있습니다.\n https://helm.sh/docs/topics/registries/  Helm CLI 환경 준비   helm cli를 사용할 Cloud Shell 또는 작업환경에 접속합니다.\n  helm cli 3.7 설치\nOCI Registry인 OCIR에 차트를 등록하기 위해서는 helm 3.7에서 제공하는 helm push 명령을 사용해야 원활히 됩니다. 문서 작성일 기준으로 현재 Cloud Shell에 기본 설치된 helm cli 버전은 3.5.4 여서 3.7을 추가 설치합니다.\nwget https://get.helm.sh/helm-v3.7.1-linux-amd64.tar.gz tar -zxvf helm-v3.7.1-linux-amd64.tar.gz linux-amd64/helm mv linux-amd64/helm ~/.local/bin/   .bashrc의 PATH에 등록\nPATH=$HOME/.local/bin:$HOME/bin:$PATH   OCI Support 활성화\nhelm cli에서 OCI Registry 지원은 실험적 기능으로 사용을 위해 다음 환경변수를 설정이 필요합니다.\nexport HELM_EXPERIMENTAL_OCI=1   Helm Chart 생성후 등록하기 샘플 차트 만들기 Helm Chart Template Guide 예제를 따라 만든 샘플 차트를\nOCIR 등록 해봅니다.\n  테스트를 위해 차트를 만듭니다.\noke_admin@cloudshell:helm (ap-seoul-1)$ helm create mychart Creating mychart   차트 작성\n생성된 차트는 nginx를 배포하는 샘플 차트입니다. 실제 차트 작성을 위해서는 앱에 맞게 수정하겠지만, 지금은 배포 테스트로 수정없이 그냥 사용합니다.\n  차트 패키징\nhelm package 명령으로 패키징합니다.\noke_admin@cloudshell:helm (ap-seoul-1)$ cd mychart oke_admin@cloudshell:mychart (ap-seoul-1)$ helm package . Successfully packaged chart and saved it to: /home/oke_admin/works/helm/mychart/mychart-0.1.0.tgz oke_admin@cloudshell:mychart (ap-seoul-1)$ ls charts Chart.yaml mychart-0.1.0.tgz templates values.yaml   OCIR 로그인 및 Helm Chart Push OCIR에 docker cli로 로그인 할때와 동일하게 사용자와 Auth Token을 사용해 로그인합니다. 이전 내용을 참고합니다.\n  앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 helm cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:mychart (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:mychart (ap-seoul-1)$ helm registry login -u cnrlxx3w0wgq/oke-admin ap-seoul-1.ocir.io Password: Login Succeeded   Helm Chart Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식 Push 하면 됩니다. 그러면 repo-prefix/ 을 포함하여 repository 가 생성됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-prefix\u0026gt;   mychart 예시  oke_admin@cloudshell:mychart (ap-seoul-1)$ helm push ./mychart-0.1.0.tgz oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm Pushed: ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart:0.1.0 Digest: sha256:842486615ee4398001092a9b2f931320add0ecd13885e039427ce6f7926b493b   OCIR 확인\nOCI 서비스 콘솔에서 OCIR을 보면 Push한 차트가 정상적으로 등록된 것을 알 수 있습니다. 예제는 편의상 root compartment로 push 하였습니다.\n 하위 compartment로 push하는 경우 사전에 repository를 만들어야 합니다. 예, helm/mychart repository를 push 전에 만들것     Helm Chart를 OKE 클러스터에 배포하기   Cloud Shell 또는 작업 환경에 접속합니다.\n  등록한 Chart로 배포합니다.\nhelm install mychart oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart --version 0.1.0   배포 예시\noke_admin@cloudshell:mychart (ap-seoul-1)$ helm install mychart oci://ap-seoul-1.ocir.io/cnrlxx3w0wgq/helm/mychart --version 0.1.0 --set service.type=LoadBalancer NAME: mychart LAST DEPLOYED: Fri Dec 3 15:17:51 2021 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: NOTE: It may take a few minutes for the LoadBalancer IP to be available. You can watch the status of by running \u0026#39;kubectl get --namespace default svc -w mychart\u0026#39; export SERVICE_IP=$(kubectl get svc --namespace default mychart --template \u0026#34;{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\u0026#34;) echo http://$SERVICE_IP:80 oke_admin@cloudshell:mychart (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/mychart-76677cc888-rl4v6 1/1 Running 0 55s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 32m service/mychart LoadBalancer 10.96.198.136 146.56.152.14 80:30926/TCP 56s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/mychart 1/1 1 1 56s NAME DESIRED CURRENT READY AGE replicaset.apps/mychart-76677cc888 1 1 1 55s   배포 앱 접속 확인\n   ","lastmod":"2021-12-03T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/ocir/2.helm-chart/","tags":["container registry","helm chart"],"title":"3.2 Helm Chart Repostory로 사용하기"},{"categories":null,"contents":"2.1 DevOps 서비스를 이용한 Spring Boot 앱을 OKE에 배포 자동화하기 DevOps 서비스 사용을 위한 IAM Policy 설정 DevOps 서비스를 사용하기 위해서는 DevOps 자원들에 권한 설정이 필요합니다. 공식 문서를 참조하여 권한 설정을 위한 Dynamic Group 및 Group에 대한 Policy를 설정합니다.\n https://docs.oracle.com/en-us/iaas/Content/devops/using/devops_iampolicies.htm#build_policies  아래 Dynamic Group 및 Policy는 위 문서의 예제를 기준으로 작성한 내용으로 요구사항에 따라 일부 변경이 될 수 있습니다.\nDynamic Group 만들기 주어진 Compartment 내에서 DevOps 서비스를 사용할 수 있도록 Compartment에 대한 Dynamic Group을 먼저 생성합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; Identity \u0026gt; Compartments로 이동합니다.\n  DevOps를 사용할 Compartment로 이동하여 OCID를 복사해 둡니다.\n  Identity \u0026gt; Dynamic Groups로 이동합니다.\n  Create Dynamic Group을 클릭합니다.\n  복사해둔 Compartment OCID를 이용해 필요한 Dynamic Group을 만듭니다.\n  oke-labs-DeployDynamicGroup\nALL {resource.type = \u0026#39;devopsrepository\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   oke-labs-CoderepoDynamicGroup\nALL {resource.type = \u0026#39;devopsbuildpipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}   oke-labs-BuildDynamicGroup\nAll {resource.type = \u0026#39;devopsdeploypipeline\u0026#39;, resource.compartment.id = \u0026#39;\u0026lt;YourCompartmentOCID\u0026gt;\u0026#39;}     DevOps 서비스를 위한 Policy 설정하기   Identity \u0026gt; Policies로 이동합니다.\n  Create Policy을 클릭하여 새 Policy를 만듭니다.\n  Compartment 레벨로 다음 Policy를 만듭니다.\n Name: 예, oke-labs-DevOps-policy  Allow dynamic-group oke-labs-CoderepoDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to manage repos in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to read secret-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to manage devops-family in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to manage generic-artifacts in compartment \u0026lt;YourCompartmentName\u0026gt; Allow dynamic-group oke-labs-BuildDynamicGroup to use ons-topics in compartment \u0026lt;YourCompartmentName\u0026gt;   Root Compartment 레벨로 다음 Policy를 만듭니다.\n Name: 예, oke-labs-DevOps-root-policy  Allow dynamic-group oke-labs-CoderepoDynamicGroup to manage all-resources in tenancy Allow dynamic-group oke-labs-CoderepoDynamicGroup to read secret-family in tenancy Allow dynamic-group oke-labs-DeployDynamicGroup to manage all-resources in tenancy   DevOps 서비스를 통한 CI/CD 배포 자동화 하기 DevOps 프로젝트 만들기 Notification Topic 만들기 DevOps 파이프 라인 실행이 발생하는 주요 이벤트를 알려주기 위한 용도로 Notification Topic 설정이 필요합니다. DevOps 프로젝트 생성시 필수 요구 사항이라 미리 만듭니다\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Application Integration \u0026gt; Notifications으로 이동합니다.\n  Create Topic을 클릭하여 Topic을 생성합니다.\n Name: 예, oke-labs-devops-topic     Notification을 위해 생성한 Topic 이벤트를 가져갈 Subscrition을 일단 생략합니다. 필요시 구성하시면 됩니다.\n  DevOps 프로젝트 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; DevOps로 이동합니다.\n  프로젝트 생성을 위해 Projects로 이동하여 Create DevOps project를 클릭합니다.\n  생성 정보를 입력하여 프로젝트를 만듭니다.\n Project name: 예, my-devops-project Notification Topic: 앞서 생성한 Topic 선택     프로젝트 생성완료\n   Enable Logging 프로젝트 생성 직후 Enable Logging 관련 정보가 보이는 것을 볼 수 있습니다. 설명문에서 보는 것 처럼 Logging을 활성화하지 않을 경우, 파이프라인 실행 화면에서 오른쪽에 보이는 실행 로그가 안보입니다. 그래서 Enable Logging은 필수입니다.\n   Project Overview에서 Enable Log을 클릭하거나 왼쪽 메뉴에서 Logs를 클릭합니다.\n   로그를 활성화 버튼을 토글합니다.\n   대상 Compartment에 이미 Log Group이 있는 경우 나열된 것 중에 선택이 가능합니다. 미리 생성된 Log Group이 없는 경우 아래와 같이 자동입력된 정보를 바탕으로 Enable Log 버튼 클릭시 새로 Log Group과 Log가 만들어 지게 됩니다. 필요시 설정을 수정하고 그렇치 않으면, Enable Log 버튼을 클릭합니다.\n   애플리케이션 배포 자동화 하기 Code Repository를 사용하여 애플리케이션 코드 관리하기 샘플로 Spring Boot Helloword 앱을 만들어 테스트하겠습니다.\n  코드 저장소 생성을 위해 왼쪽 메뉴에서 Code Repositories를 클릭합니다.\n  Create repository를 클릭하여 저장소를 만듭니다.\n Repository name: 예, spring-boot-hello-repo     생성된 코드 저장소 입니다. 일반적인 Git Repository입니다.\n   실제 개발 작업은 git 명령을 통해 개발 PC에서 진행하면 됩니다. 저장소 상세정보 위에 있는 Clone 버튼을 하면 Clone 명령어가 아래 그림처럼 뜨게 됩니다. 여기서는 Clone with HTTPS를 사용하겠습니다.\n   개발 PC에 복사한 주소를 사용해 git clone 명령어를 통해 복제합니다.\ngit clone \u0026lt;YourClonewithHTTPS URL\u0026gt;   이때 사용자 인증이 필요합니다. HTTPS기반 사용자 인증시 아래 유저명 형식과 AuthToken을 사용합니다.\n  인증 유저명\n Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-name\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-name\u0026gt;/\u0026lt;username\u0026gt; 이전 가이드들과 달리 tenancy-namespace가 아닌 tenacy-name인 것에 주의합니다.    AuthToken: 생성에 대한 내용은 이전 가이드들을 참고합니다.\n  Code Repository의 HTTPS 인증관련 문서\n https://docs.oracle.com/en-us/iaas/Content/devops/using/clone_repo.htm#https_auth    예시\n  $ git clone https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com/namespaces/cnrlxx3w0wgq/projects/my-devops-project/repositories/spring-boot-hello-repo Cloning into \u0026#39;spring-boot-hello-repo\u0026#39;... Username for \u0026#39;https://devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: thekoguryo/oke-developer Password for \u0026#39;https://oreozz/oke-admin@devops.scmservice.ap-seoul-1.oci.oraclecloud.com\u0026#39;: remote: Counting objects: 2, done remote: Finding sources: 100% (2/2) remote: Getting sizes: 100% (1/1) Unpacking objects: 100% (2/2), done. remote: Total 2 (delta 0), reused 2 (delta 0)   현재 복제된 저장소는 비어 있습니다. 아래 가이드를 통해 spring-boot-hello 샘플 코드를 작성합니다.\n https://spring.io/guides/gs/spring-boot-docker/    작성된 코드를 git 명령어를 통해서 Code Repository에 저장합니다.\n  예시\ngit add . git commit -m \u0026#34;init\u0026#34; git push     코드 작성 및 반영 완료\n   Build Pipeline 만들기 CI/CD 중에 코드를 빌드하여 배포 산출물을 만드는 CI 과정에 해당되는 부분을 Build Pipeline을 통해 구성이 가능합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Build Pipelines로 이동합니다.\n  Create build pipeline을 클릭하여 파이프라인을 생성합니다.\n  Name: 예, spring-boot-hello-build-pipeline\n     생성된 파이프라인을 클릭합니다.\n  그림과 같이 Stage를 추가하여 파이프라인 흐름을 구성할 수 있습니다. Add Stage를 클릭합니다.\n   제공 Stage\n Managed Build: 빌드스펙에 정의된 내용에 따라 빌드 과정을 실행합니다. Delivery Artifacts: 빌드 산출물(예, 컨테이너 이미지)를 Artifact Repository에 저장합니다. Trigger Deployment: 빌드가 끝나고 Deployment Pipeline을 호출합니다. Wait: 일정시간 대기합니다.     Build Stage 만들기   빌드를 위해 먼저 Managed Build Stage를 추가합니다.\n  Managed Build Stage 설정\n Stage name: 예, build-stage Build Spec File Path: 빌드 스크립트 경로를 지정합니다. 기본적으로 소스 루트에 있는 build_spec.yaml을 파일을 사용합니다. Primary Code Repository: 빌드할 메인 소스가 있는 코드 저장소를 지정합니다.     Primary Code Repository 설정 화면\n 대상 소스 코드가 있는 저장소를 지정합니다.       설정된 Stage를 Add를 클릭하여 추가합니다.\n  아래 그림과 같이 build-stage가 추가되었습니다. Start Manual Run을 클릭하면 테스트를 해 볼수 있습니다.\n   테스트처럼 소스 코드상의 Build Spec의 정의가 필요합니다.\n   Build Spec은 다음 문서를 참조합니다.\n  https://docs.oracle.com/en-us/iaas/Content/devops/using/build_specs.htm\n  문서에 있는 Example 2 기준 예시\n steps: 실행할 스크립트를 정의하는 부분입니다. 예제이는 Build Source, Dockerizer 2개의 step이 정의되어 있고 각각 command에서 실행할 스크립트를 정의하고 있습니다. 정의된 순서대로 실행됩니다. env.exportedVariables: 전역으로 선언된 환경변수로 이전 step에서 값을 변경하면 그다음 step에도 적용됩니다. Deployment Pipeline을 호출시에도 전달됩니다. outputArtifacts: 빌드 산출물의 정의하는 부분으로, 이후 Delivery Artifact Stage를 통해 Artifact Storage에 저장할 때 여기서 정의된 이름을 통해 지정 가능합니다.  version:0.1component:buildtimeoutInSeconds:6000shell:bashenv:exportedVariables:- BuildServiceDemoVersionsteps:- type:Commandname:\u0026#34;Build Source\u0026#34;timeoutInSeconds:4000command:|echo $PATH mvn clean install- type:CommandtimeoutInSeconds:400name:\u0026#34;Dockerizer\u0026#34;command:|BuildServiceDemoVersion=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` echo $BuildServiceDemoVersion docker build -t build-service-demo .outputArtifacts:- name:build-service-demotype:DOCKER_IMAGElocation:build-service-demo- name:build-service-demo-kube-manifesttype:BINARYlocation:deployment/app.yml    Build Spec 정의\n  개발한 spring-boot-hello 소스 코드의 root 경로에 build_spec.yaml을 다음과 같이 정의하고 코드 저장소에 저장합니다.\n  build_spec.yaml\nversion:0.1component:buildtimeoutInSeconds:6000shell:bashenv:# these are local variables to the build configvariables:appName:\u0026#34;spring-boot-hello\u0026#34;# exportedVariables are made available to use as parameters in sucessor Build Pipeline stagesexportedVariables:- APP_NAME- OCIR_PATH- TAGsteps:- type:Commandname:\u0026#34;Init exportedVariables\u0026#34;timeoutInSeconds:4000command:|APP_NAME=$appName - type:Commandname:\u0026#34;Build Source\u0026#34;timeoutInSeconds:4000command:|mvn clean install- type:CommandtimeoutInSeconds:400name:\u0026#34;Build Source - Post\u0026#34;command:|mkdir -p target/dependency \u0026amp;\u0026amp; (cd target/dependency; jar -xf ../*.jar)- type:Commandname:\u0026#34;Define Image Tag - Commit ID\u0026#34;timeoutInSeconds:30command:|COMMIT_ID=`echo ${OCI_TRIGGER_COMMIT_HASH} | cut -c 1-7` BUILDRUN_HASH=`echo ${OCI_BUILD_RUN_ID} | rev | cut -c 1-7` [ -z \u0026#34;$COMMIT_ID\u0026#34; ] \u0026amp;\u0026amp; TAG=$BUILDRUN_HASH || TAG=$COMMIT_ID- type:Commandname:\u0026#34;Define OCIR Path\u0026#34;timeoutInSeconds:30command:|TENANCY_NAMESPACE=`oci os ns get --query data --raw-output` REPO_NAME=$appName OCIR_PATH=$OCI_RESOURCE_PRINCIPAL_REGION.ocir.io/$TENANCY_NAMESPACE/$REPO_NAME- type:CommandtimeoutInSeconds:400name:\u0026#34;Containerize\u0026#34;command:|docker build -t new-generated-image . docker images- type:Commandname:\u0026#34;Check exportedVariables\u0026#34;timeoutInSeconds:30command:| [-z \u0026#34;$APP_NAME\u0026#34; ] \u0026amp;\u0026amp; APP_NAME=unknown [-z \u0026#34;$OCIR_PATH\u0026#34; ] \u0026amp;\u0026amp; OCIR_PATH=unknown [-z \u0026#34;$TAG\u0026#34; ] \u0026amp;\u0026amp; TAG=unknownecho \u0026#34;APP_NAME: \u0026#34; $APP_NAME echo \u0026#34;OCIR_PATH: \u0026#34; $OCIR_PATHecho \u0026#34;TAG: \u0026#34; $TAGoutputArtifacts:- name:output-imagetype:DOCKER_IMAGElocation:new-generated-image     Start Manual Run을 통해 다시 실행하면 아래와 같이 스크립트가 수행되는 것을 볼 수 있습니다.\n   ExportVariables 확인\n실행 결과 화면에서 오른쪽 위쪽 점3개를 클릭하여 상세 화면으로 이동하면 Build Output에서 실행결과로 나온 변수값을 볼 수 있습니다. 이 변수들은 이후 Stage 또는 연결되어 호출된 Deployment Pipeline으로 전달되어 사용할 수 있게 됩니다.\n   컨테이너 이미지 OCIR 등록 Stage 만들기   Build Pipeline 탭으로 이동합니다.\n  플러스 버튼을 클릭하여 build-stage 다음에 stage를 추가합니다.\n   Delivery Artifact Stage를 선택합니다.\n  stage 이름을 입력하고 Create Artifact를 선택합니다.\n   Container image 유형으로 Artifact 추가합니다.\n 이미지 경로: docker tag를 달때 사용하는 이미지 경로입니다. 직접 입력해도 되지만 여기서는 build-stage에서 넘어온 exportedVariable을 사용하여 ${OCIR_PATH}:${TAG} 과 같이 입력합니다.     같은 방식으로 하나 더 추가 합니다.\n Name: generated_image_with_latest Image Path: ${OCIR_PATH}:latest    Artifact 매핑\n  Associate Artifact에서 방금 추가한 2개의 Artifact에 실제 컨테이너 이미지 파일을 매핑해 줍니다. 앞서 build-stage에서 build_spec.yaml에서 정의한 outputArtifacts의 이름을 입력합니다.\noutputArtifacts:- name:output-imagetype:DOCKER_IMAGElocation:new-generated-image      이제 delivery stage까지 추가하였습니다.\n  파이프라인을 다시 실행해 봅니다. 실제 소스코드로 빌드된 컨테이너 이미지가 OCIR에 자동으로 등록됩니다.\n   Deploy Pipeline 만들기 CI/CD 중에 빌드된 산출물을 가지고 실제 서버에 배포하는 CD 과정에 해당되는 부분을 Deployment Pipeline을 통해 구성이 가능합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Deployment Pipelines로 이동합니다.\n  Create pipeline을 클릭하여 파이프라인을 생성합니다.\n  Name: 예, spring-boot-hello-deployment-pipeline\n     생성된 파이프라인을 클릭합니다.\n  Add Stage를 클릭하여 Stage를 추가합니다.\n  제공 Stage\n  Deploy: OKE, Compute 인스턴스 배포, Oracle Function에 배포 기능을 제공합니다.\n  Control: 승인 대기, 트래픽 변경, 대기 등을 지원합니다.\n  Integration: 커스텀 로직 수행을 위한 Oracle Function 실행을 지원합니다.\n     Kubernetes에 배포할 manifest 파일 준비 Kubernetes에 배포할 Stage 유형을 사용하기 위해서는 사전에 배포할 manifest yaml 파일을 준비해야 합니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Artifacts로 이동합니다.\n  Artifacts로 앞서 빌드 파이프라인 만들때 등록한 2개가 있는 것을 볼수 있습니다. 여기에 등록된 Artifact는 재사용이 가능합니다.\n   manifest 파일을 등록하기 위해 Add artifact를 클릭합니다.\n  4 가지 등록 유형을 제공합니다. 이중에 Kubernetes manifest를 선택합니다.\n   Kubernetes manifest 유형에는 Artifact Source로 2가지 유형을 제고합니다.\n Artifact Registry Repository: Container Registry로 OCIR을 제공하고 있듯시 Artifact Registry를 서비스로 제공하고 있습니다. 그곳에 있는 자원을 참조할 경우에 선택합니다. Inline: 인라인은 현재 DevOps 프로젝트에 있는 여기 Artifact에 직접 입력하는 것을 말합니다.    Artifact Source로 Inline 유형으로 다음과 같이 등록합니다.\n Name: 예, k8s_spring_boot_deploy_template     Value\n앞 서와 같이 build-stage에서 export한 변수값들을 사용할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:${APP_NAME}name:${APP_NAME}spec:replicas:1selector:matchLabels:app:${APP_NAME}template:metadata:labels:app:${APP_NAME}spec:containers:- name:${APP_NAME}image:${OCIR_PATH}:${TAG}---apiVersion:v1kind:Servicemetadata:name:${APP_NAME}-serviceannotations:service.beta.kubernetes.io/oci-load-balancer-shape:\u0026#34;10Mbps\u0026#34;spec:type:LoadBalancerports:- port:80protocol:TCPtargetPort:8080selector:app:${APP_NAME}    Kubernetes Enviroment 등록하기   my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Enviroments로 이동하여 배포할 OKE 환경을 등록합니다.\n  OKE 유형을 선택합니다.\n   배포할 클러스터를 선택합니다.\n   Kubernetes manifest 배포 Stage 만들기   등록한 Deployment Pipeline(spring-boot-hello-deployment-pipeline) 설정 페이지로 이동합니다.\n  Add Stage를 클릭하여 Apply manifest to your Kubernetes cluster Stage를 추가합니다.\n  배포할 환경 및 manifest 파일을 선택합니다\n   파이프라인 완성\n   Build Pipeline에서 Deployment Pipeline 호출하기 앞서 만든 Build Pipeline에서 컨테이너 이미지 까지 OCIR에 등록하고 나면, OKE에 배포할 Deployment Pipeline을 기동되어야 전체 빌드에서 배포까지가 완료됩니다. 이제 Deployment Pipeline을 등록하였으므로, Build Pipeline에서 호출할 수 있습니다.\n  앞서 만든 **Build Pipelines(spring-boot-hello-build-pipeline)**으로 이동합니다.\n  파이프라인 마지막에 Stage를 추가합니다.\n  Trigger Deployment 유형을 선택합니다.\n  설정한 Deployment Pipeline을 지정합니다.\n   전체 흐름이 완료되었습니다.\n   Trigger 설정하기 지금 까지는 테스트를 하기 위해 Build Pipeline에서 Start Manual Run을 통해 시작하였습니다. 실제로는 개발자가 코드를 코드 저장소에 반영이 될 때 자동으로 빌드, 배포 파이프라인이 동작할 필요가 있습니다. Trigger는 코드 저장소에 발생한 이벤트를 통해 빌드 파이프라인을 시작하게 하는 역할을 하게 됩니다.\n  my-devops-project 프로젝트 페이지로 이동하여 왼쪽 메뉴의 Trigger로 이동합니다.\n  Create trigger을 클릭합니다.\n  Trigger를 설정합니다.\n Name: 예, spring-boot-hello-trigger Source Code Repository: OCI Code Repository, GitHub, GitLab 연동을 지원하며, 예제에서는 앞서 만든 OCI Code Repository상의 spring-boot-hello-repo를 선택합니다. Actions: 트리거링 되었을 때 호출하는 액션으로 작성한 빌드 파이프라인인 spring-boot-hello-build-pipeline을 선택합니다.     설정이 완료되었습니다.\n  테스트   Trigger에서 지정한 spring-boot-hello 소스 코드에 임의의 변경사항을 발생시키고 Code Repository에 반영합니다.\n  저는 Application.java에 있는 응답메시지를 \u0026ldquo;Hello OCI DevOps\u0026quot;로 변경하고 반영하셨습니다.\n  빌드 실행 내역을 보면, 그림과 같이 Trigger 된것은 Commit ID가 함께 보이며, Code Repository와 링크되어 있습니다.\n   Commit ID를 클릭하면 Code Repository상의 코드 변경 분을 확인할 수 있습니다.\n     빌드 파이프라인이 정상적으로 코드 빌드 부터 컨테이너 이미지 생성, 배포 파이프라인 호출까지 실행되었습니다.\n   배포 파이프라인도 정상 실행되었습니다.\n   OKE 클러스터를 조회해 보면 정상 배포 되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/spring-boot-hello-54848fcfd5-5jpxh 1/1 Running 0 5m39s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 16h service/spring-boot-hello-service LoadBalancer 10.96.186.158 146.56.186.172 80:32224/TCP 41m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/spring-boot-hello 1/1 1 1 15h NAME DESIRED CURRENT READY AGE replicaset.apps/spring-boot-hello-54557d9c47 0 0 0 41m replicaset.apps/spring-boot-hello-54848fcfd5 1 1 1 5m39s   서비스 주소로 접속시 정상 동작을 확인할 수 있습니다.\n   ","lastmod":"2021-11-25T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/devops/1.deploy-app-on-oke-using-devops/","tags":["devops"],"title":"2.1 DevOps 서비스를 이용한 Spring Boot 앱을 OKE에 배포 자동화하기"},{"categories":null,"contents":"1.6.1 Block Volume 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\n기본 설치된 Persistent Volume을 위한 StorageClass 확인 OKE는 OCI IaaS를 기반으로 제공되는 서비스로 OCI Block Volume 서비스를 이용하게 Persistent Volume을 제공합니다. 현 버전 기준으로 FlexVolume 볼륨 플러그인과 CSI(Container Storage Interface) 볼륨 플러그인의 두 가지를 사용하고 있습니다.\n  기본 StorageClass 확인\n oci: OCI Block Volume 서비스를 위한 FlexVolume 플러그인 사용 oci-bv: OCI Block Volume 서비스를 위한 CSI 플러그인 사용  oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE oci (default) oracle.com/oci Delete Immediate false 2d oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d   CSI 볼륨 플러그인\n FlexVolume 플러그인 방식은 오픈소스 Kubernetes 1.2 버전때 부터 사용되어 더 오래되었지만, 업스트림 Kubernetes에서 CSI 플러그인 방식을 사용하는 흐름입니다. OKE에서도 Release Note 2020년 8월 1일, Support for the Container Storage Interface (CSI) volume plug-in 에 나와 있는 것 처럼 OKE에서도 신규 기능은 CSI 플러그인에 추가할 예정이며, FlexVolume은 유지 보수만 할 계획입니다. 그래서 이하 설명에서는 CSI 플러그인을 사용하는 oci-bv storageclass를 사용하겠습니다. Flex 플러그인을 사용하는 oci storageclass에 대한 사항은 공식 문서를 참조바랍니다.    OCI Block Volume용 CSI 플러그인을 사용하여 Persistent Volume 만들어 사용하기 Persitent Volume 테스트\n  아래와 같이 PV 요청 yaml을 사용하여 요청합니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:csi-bvs-pvcspec:storageClassName:\u0026#34;oci-bv\u0026#34;accessModes:- ReadWriteOnceresources:requests:storage:50Gi  테스트 앱 배포\n 요청한 Persistent Volume을 컨테이너 상에 마운트한 테스트 앱  apiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-bvs-pvcname:nginx-bvs-pvcspec:replicas:1selector:matchLabels:app:nginx-bvs-pvctemplate:metadata:labels:app:nginx-bvs-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:csi-bvs-pvc  생성 결과\n 아래와 같이 정상적으로 PV 요청에 따라 PV가 생성되고, 테스트 앱로 구동된 것을 볼 수 있습니다.  oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f csi-bvs-pvc.yaml persistentvolumeclaim/csi-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl apply -f nginx-deployment-bvs-pvc.yaml deployment.apps/nginx-bvs-pvc created oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 101s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 118s   Block Volume OCI 서비스 콘솔에서 Storage \u0026gt; Block Volumes 화면에서 보면 아래와 같이 PV용 Block Volume 만들어 졌으며, 특정 Worker Node에 부착된 걸 알 수 있습니다.\n     테스트\n  Persistent Volume에 쓰기\n아래와 같이 컨테이너 내부로 들어가 마운트 된 PV 내에 파일쓰기를 합니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vdp7c -- bash root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# echo \u0026#34;Hello PV\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/# cat /usr/share/nginx/html/hello_world.txt Hello PV root@nginx-bvs-pvc-7b574c9c5c-vdp7c:/#   POD 삭제 후 재생성\nPersistent Volume이 유지되는 것을 확인하기 위해 기존 POD를 삭제하고 재생성되도록 합니다. 이때 RWO 모드라 다른 Node에 POD가 생성되는 경우, Multi-Attach error가 일시적으로 발생합니다. 기존 POD가 삭제되었다는 것을 인지하는 데 시간이 걸리며, 조금 지난 후에 POD가 다시 재생성됩니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vdp7c 1/1 Running 0 6m53s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl delete pod nginx-bvs-pvc-7b574c9c5c-vdp7c pod \u0026#34;nginx-bvs-pvc-7b574c9c5c-vdp7c\u0026#34; deleted oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 0/1 ContainerCreating 0 17s oke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-bvs-pvc-7b574c9c5c-vrptl 1/1 Running 0 75s   신규 POD에서 Persistent Volume 확인\n앞서 변경한 파일을 신규 POD에서 다시 조회해 보면 기존 내용이 남아 있는 걸 확인할 수 있습니다.\noke_admin@cloudshell:persistent-volume (ap-seoul-1)$ kubectl exec -it nginx-bvs-pvc-7b574c9c5c-vrptl -- cat /usr/share/nginx/html/hello_world.txt Hello PV     참고사항\n 앞선 테스트에서 처럼 ReadWriteOnce 접근 모드에서는 단일 Kubernetes Node에 있는 POD만 붙여서 사용할 수 있습니다. 다른 Node에 있는 POD에서 동일한 PV를 사용하려고 하면, 컨테이너 기동시 Multi-Attach 오류가 발생하며, 컨테이너가 기동이 되지 않습니다. 또한 Self-Healing으로 POD 재기동시 기존 POD가 해당 volume을 사용하고 있다고 생각하여 Multi-Attach 오류가 발생하고, 사용중인 POD가 없음을 인지하는 데 약간의 시간이 걸리게 됩니다.    ReadWriteMany 지원 여부 현재 버전 기준 CSI Driver for OCI Block Volume Service는 ReadWriteOnce만 지원합니다. 그래서 단일 Kuberenetes Node에 멀티 Pod까지만 지원됩니다. 또한 위 작업 내용을 accessMode를 ReadWriteMany로 변경후 동일하게 수행하면 pod가 생성되지 않고 아래와 같이 에러가 나게 됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get events LAST SEEN TYPE REASON OBJECT MESSAGE ... 80s Warning ProvisioningFailed persistentvolumeclaim/mynginxclaim failed to provision volume with StorageClass \u0026#34;oci-bv\u0026#34;: rpc error: code = InvalidArgument desc = invalid volume capabilities requested. Only SINGLE_NODE_WRITER is supported (\u0026#39;accessModes.ReadWriteOnce\u0026#39; on Kubernetes) ","lastmod":"2021-11-13T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/6.persistent-volume/1.block-volume/","tags":["oke"],"title":"1.6.1 Block Volume 사용하기"},{"categories":null,"contents":"1.6.2 File Storage 사용하기 컨테이너의 내부 스토리지는 기본적으로 컨테이너가 삭제, 종료되면 사라집니다. 데이터가 사라지는 것을 막고 보존이 필요한 데이터를 저장하기 위해 별도의 Persistent Volume을 사용합니다.\nPersistent Volume으로 파일 공유를 위해 저장소로 많이 사용하는 NFS(Network File System)을 사용할 수 있습니다. 네트워크 파일 시스템인 NFS의 경우 동시 쓰기를 지원하기에 Kubernetes에서 멀티 POD에서 동시에 읽고 쓰는 용도로 사용할 수 있습니다. OCI에서는 OCI File Storage Service(FSS)가 OCI에서 제공하는 NFS 서비스입니다. 이제 OKE에서 OCI File Storage을 Persistent Volume으로 사용하는 RWS 접근 모드로 사용하는 방법을 확인해 보겠습니다.\nFiles Storage 만들기 관련 문서를 참고하여 File Storage를 만듭니다.\n  https://docs.oracle.com/en-us/iaas/Content/File/home.htm\n  https://thekoguryo.github.io/oci/chapter08/\n    OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Storage \u0026gt; File Storage로 이동합니다\n  대상 Compartment를 확인합니다.\n  File Systems에서 Create File System을 클릭합니다.\n  기본 설정화면에서 간단히 아래 정보를 원하는 값이 맞게 알맞게 수정하고 생성합니다.\n File System Information:  Name   Mount Target Information:  New Mount Target Name Virtual Cloud Network Subnet       생성결과 확인\nFile Storage \u0026gt; Mount Target 에서 생성된 Mount Target 상세 정보로 이동하여 다음 정보를 확인합니다.\n Mount Target OCID: \u0026hellip;sc2mia IP Address: 예, 10.0.20.194 Export Path: 예) /OKE-FFS-Strorage     Security List 설정\nFile System 생성시 Mount Target의 서브넷에 Security List에 File Storage 서비스를 위한 규칙을 추가합니다.\n   File Storage 서비스를 이용하여 Persistent Volume 사용하기   Storage Class 만들기\n앞서 확인한 Mount Target OCID로 업데이트 후 적용\napiVersion:storage.k8s.io/v1kind:StorageClassmetadata:name:oci-fssprovisioner:oracle.com/oci-fssparameters:# Insert mount target from the FSS heremntTargetId:ocid1.mounttarget.oc1.ap_seoul_1.aaaaaa4np2szmmn5nfrw4llqojxwiotboawxgzlpovwc2mjnmfsc2mia  Persistent Volume (PV) 만들기\nMount Targe의 IP와 Export Path로 업데이트 후 적용\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumemetadata:name:oke-fss-pvspec:storageClassName:oci-fsscapacity:storage:100GiaccessModes:- ReadWriteManymountOptions:- nosuidnfs:# Replace this with the IP of your FSS file system in OCIserver:10.0.20.194# Replace this with the Path of your FSS file system in OCIpath:\u0026#34;/OKE-FFS-Storage\u0026#34;readOnly:false  Persistent Volume Claime(PVC) 만들기\nReadWriteMany 접근 모드로 지정하였습니다.\napiVersion:v1kind:PersistentVolumeClaimmetadata:name:oke-fss-pvcspec:storageClassName:oci-fssaccessModes:- ReadWriteManyresources:requests:storage:100GivolumeName:oke-fss-pv  PVC를 사용하는 POD 배포하기\n생성한 PVC를 볼륨으로 등록하여 마운트합니다.\n앞선 예제와 달리 replica를 복수개로 지정할 수 있습니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-fss-pvcname:nginx-fss-pvcspec:replicas:3selector:matchLabels:app:nginx-fss-pvctemplate:metadata:labels:app:nginx-fss-pvcspec:containers:- name:nginximage:nginx:latestvolumeMounts:- name:datamountPath:/usr/share/nginx/htmlvolumes:- name:datapersistentVolumeClaim:claimName:oke-fss-pvc  실행 및 결과 예시\n3개 POD가 각각 서로 다른 3개의 Worker Node에 위치하지만 정상 기동된 것을 볼 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oci-fss-storageclass.yaml storageclass.storage.k8s.io/oci-fss created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pv.yaml persistentvolume/oke-fss-pv created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f oke-fss-pvc.yaml persistentvolumeclaim/oke-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get sc,pv,pvc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE storageclass.storage.k8s.io/oci (default) oracle.com/oci Delete Immediate false 2d19h storageclass.storage.k8s.io/oci-bv blockvolume.csi.oraclecloud.com Delete WaitForFirstConsumer false 2d19h storageclass.storage.k8s.io/oci-fss oracle.com/oci-fss Delete Immediate false 34s NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO Delete Bound default/csi-bvs-pvc oci-bv 3h6m persistentvolume/oke-fss-pv 100Gi RWX Retain Bound default/oke-fss-pvc oci-fss 24s NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/csi-bvs-pvc Bound csi-14f32977-eaf6-4eaa-87bd-7c736ec43a52 50Gi RWO oci-bv 3h6m persistentvolumeclaim/oke-fss-pvc Bound oke-fss-pv 100Gi RWX oci-fss 17s oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl apply -f nginx-deployment-fss-pvc.yaml deployment.apps/nginx-fss-pvc created oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-fss-pvc-9fb98454f-bc7hp 1/1 Running 0 24s 10.244.0.5 10.0.10.40 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-bxw4x 1/1 Running 0 24s 10.244.1.18 10.0.10.15 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-qm9tq 1/1 Running 0 24s 10.244.0.153 10.0.10.219 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   파일 쓰기 테스트\n  아래와 같이 첫번째 POD에서 PV로 파일쓰기를 했지만, 모든 POD에서 동일내용을 확인할 수 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- bash -c \u0026#39;echo \u0026#34;Hello FSS from 10.0.10.40\u0026#34; \u0026gt;\u0026gt; /usr/share/nginx/html/hello_world.txt\u0026#39; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bc7hp -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-bxw4x -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl exec -it nginx-fss-pvc-9fb98454f-qm9tq -- cat /usr/share/nginx/html/hello_world.txt Hello FSS from 10.0.10.40     참고 문서 https://blogs.oracle.com/cloud-infrastructure/post/using-file-storage-service-with-container-engine-for-kubernetes\n","lastmod":"2021-11-10T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/6.persistent-volume/2.file-storage/","tags":["oke"],"title":"1.6.2 File Storage 사용하기"},{"categories":null,"contents":"2. OKE 클러스터 만들기 Quick 모드로 클러스터 만들기 처음 OKE 클러스터를 만드는 단계로 실환경에서는 별도의 OKE 사용자 및 VCN 등 커스텀한 환경을 사용하겠지만, OKE를 이해하기 위한 처음 단계로 Administrator 유저를 통해 Quick 모드로 설치합니다.\n  OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts \u0026gt; Kubernetes Clusters (OKE)로 이동합니다.\n  List Scope에서 생성할 Compartment(예, oke-labs)를 선택합니다.\n  클러스터 생성을 위해 Create Cluster 버튼을 클릭합니다.\n  빠른 클러스터 생성을 위해 기본선택된 Quick Create 모드를 이용하여생성된 OKE 클러스터를 통해 기분 구조를 이해하고자 합니다. 아래 Launch Workflow를 클릭합니다.\n   클러스터 생성 옵션   생성할 클러스터 이름을 입력합니다. 예, oke-cluster-1\n  설치될 Compartment를 선택합니다. 예, oke-labs\n  클러스터의 쿠버네티스 버전을 선택합니다. 예, v1.20.11\n  Kubernete API Endpoint\n Public API로 접속할 수 있게 기본 선택된 Public Endpoint를 그대로 사용 Kubernetes API를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Kubernetes API가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.    Kubernetes Worker Nodes\n Worker Nodes를 접속할 수 있는 방법을 선택합니다. 기본 선택된 Private Worker를 그대로 사용 생성되는 Worker Nodes를 Private IP로 오픈할지, Public IP로 오픈 할지를 선택할 지를 선택합니다. Quick Create로 설치하게 되면, 선택에 따라 Worker Nodes가 위치하는 서브넷이 Private Subnet 또는 Public Subnet으로 설정될 지가 정해집니다.     Shape Worker Node로 사용할 VM Shape를 지정합니다. 기본 선택된 Flex Shape에서 필요에 따라 OCPU, Memory를 조정합니다.\n   Number of nodes\nWorker Nodes 갯수를 지정합니다. 기본 값을 3개입니다.\n  Control Planes(Master Nodes)\nControl Plane은 OCI가 관리하는 영역으로 별도 크기 등을 지정하지 못하며, 기본적으로 다중화되어 있습니다.\n  Advanced Options\n Add an SSH Key  트러블 슈팅 등을 위해 Worker Node VM에 접근하기 위해서는 SSH Key 등록이 필요합니다. 사용할 SSH Key의 Public Key를 아래와 같이 등록합니다.       클러스터 생성 정보를 모두 입력하였습니다. 아래 Next를 클릭\n  앞서 입력한 값들을 리뷰한 후 Create Cluster를 클릭합니다.\n  클러스터 생성 및 관련 네트워크 자원\n아래 그림과 같이 Quick Create로 클러스터를 생성시 기본 네트워크 자원이 함께 생성되는 것을 볼수 있습니다.\n   클러스터 생성 확인   생성이 요청되면, 클러스터 생성, 노드 풀 생성, Worker Node 생성 및 구성 순으로 진행됩니다.\n  클러스터 상세정보에서 Resources \u0026gt; Node Pools를 보면 생성된 pool을 볼수 있습니다.\n   생성된 Node Pool인 pool1을 클릭하여 Node Pool 상세 정보로 이동합니다.\n  Node Pool 상세 정보에서 Resources \u0026gt; Nodes 정보를 보면 생성된 Worker Nodes를 확인할 수 있습니다. VM 생성후 쿠버네티스 구성 시간이 있어 Ready 상태가 될 때까지 약간의 시간이 걸립니다. 테스트 환경에서는 노드가 모두 Ready 될때 까지 5~6분 정도 걸렸습니다.\n   클러스터 및 네트워크 구성 확인 Quick Create \u0026amp; Public Endpoint \u0026amp; Private Workers Example Network Resource Configurations에 설명된 예시 처럼 Kubernetes API Endpoint, Worker Nodes, Service Load Balancer에 대해서 Private 또는 Public 서브넷을 조합하는 몇 가지 구성이 가능합니다. 여기서는 앞서 처럼 Quick Create 모드에서 Public Endpoint, Private Workers를 선택하였고, Service Load Balancer는 기본 생성시는 Public이며 Kubernetes에서 Load Balancer 생성시 선택할 수 있습니다.\n","lastmod":"2021-11-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/2.install-quick-oke-cluster/","tags":["oke"],"title":"1.2 OKE 클러스터 만들기"},{"categories":null,"contents":"4.1.3 NGINX Ingress Controller에서 TLS termination(feats. Let\u0026rsquo;s Encrypt) Ingress Controller에서 외부 수신을 SSL로 하기 위한 설정을 확인합니다.\nSelf-Signed 인증서 사용하기 테스트 목적으로 Self-Signed 인증서를 만들어 사용하는 방법을 확인해 봅니다. 실제 환경에서는 공인 인증기관에서 발급받은 인증서를 사용합니다. Self-Signed 인증서 발급 절차만 대체되어 TLS Secret 등록과정부터는 동일하게 수행됩니다.\n참고 문서\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengsettingupingresscontroller.htm  인증서 만들기   Cloud Shell 또는 작업환경에서 다음 명령으로 인증서를 생성합니다. 공인 인증기관에서 발급받은 인증서 사용시 하지 않아도 됩니다.\nopenssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34;   TLS Secret을 만듭니다.\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt   실행결과\noke_admin@cloudshell:~ (ap-seoul-1)$ openssl req -x509 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=nginxsvc/O=nginxsvc\u0026#34; Generating a 2048 bit RSA private key *************************************************************************************************************************************************************+++++ ****************************************************************************************************************************+++++ writing new private key to \u0026#39;tls.key\u0026#39; ----- oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret tls tls-secret --key tls.key --cert tls.crt secret/tls-secret created   TLS Ingress 자원 배포   테스트를 위한 샘플 앱을 배포합니다. PATH 기반 라우팅 때 사용한 앱을 그대로 사용합니다.\nkubectl create deployment nginx-blue --image=thekoguryo/nginx-hello:blue kubectl expose deployment nginx-blue --name nginx-blue-svc --port 80 kubectl create deployment nginx-green --image=thekoguryo/nginx-hello:green kubectl expose deployment nginx-green --name nginx-green-svc --port 80   ingress 설정 YAML(tls-termination.yaml)을 작성합니다.\n spec.tls.secretName으로 앞서 생성한 Self-Signed 인증서 이름을 사용합니다.  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-tls-terminationannotations:kubernetes.io/ingress.class:nginxspec:tls:- secretName:tls-secretrules:- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80  기존 테스트 ingress는 삭제하고, 작성한 tls-termination.yaml을 배포합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f tls-termination.yaml ingress.networking.k8s.io/ingress-tls-termination created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80, 443 5s   TLS 적용 결과 검증   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서로 인한 경고 메시지가 뜹니다.\n   고급을 클릭하고 해당 페이지로 이동을 선택합니다.\n   브라우저 주소창 메뉴를 통해 인증서 정보를 확인합니다. Self-Signed 인증서로 루트 인증서가 신뢰할 수 없다는 경고를 확인할 수 있습니다.\n    Let\u0026rsquo;s Encrypt \u0026amp; Cert Manager 사용하기 Let\u0026rsquo;s Encrypt는 무료 인증서 발급 사이트로 TLS에 사용할 인증서를 발급 받을 수 있습니다. 대신 90일 동안만 유효하며 만료전에 갱신해야 합니다. Kubernetes에서는 Cert Manager를 통해 자동으로 갱신할 수 있습니다.\n https://letsencrypt.org/2015/11/09/why-90-days.html  설치 참고 문서\n https://cert-manager.io/docs/tutorials/acme/ingress/  Cert Manager 배포   Cloud Shell 또는 작업환경에서 Cert Manager를 배포합니다.\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml   설치 확인\ncert-manager namespace에 자원들이 정상 실행중인 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all -n cert-manager NAME READY STATUS RESTARTS AGE pod/cert-manager-55658cdf68-sk5nj 1/1 Running 0 18s pod/cert-manager-cainjector-967788869-b77w2 1/1 Running 0 18s pod/cert-manager-webhook-7b86bc6578-pnxtg 1/1 Running 0 18s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/cert-manager ClusterIP 10.96.100.11 \u0026lt;none\u0026gt; 9402/TCP 19s service/cert-manager-webhook ClusterIP 10.96.212.15 \u0026lt;none\u0026gt; 443/TCP 19s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/cert-manager 1/1 1 1 19s deployment.apps/cert-manager-cainjector 1/1 1 1 19s deployment.apps/cert-manager-webhook 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/cert-manager-55658cdf68 1 1 1 19s replicaset.apps/cert-manager-cainjector-967788869 1 1 1 19s replicaset.apps/cert-manager-webhook-7b86bc6578 1 1 1 18s   Let\u0026rsquo;s Encrypt Issuer 구성 본 예제에서는 Let\u0026rsquo;s Encrypt에서 제공하는 Staging Issuer, Production Issuer을 사용할 수 있습니다. 여기서는 테스트용도로 Staging Issuer를 사용하겠습니다.\n  Let\u0026rsquo;s Encrypt Staging Issuer 설정\nhttps://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml 파일에서 email 부분만 본인 것으로 수정하여 반영합니다.\n네임스페이스에만 사용되는 Issuer가 아닌 전체 쿠버네티스 클러스터에 사용하기 위해 ClusterIssuer 유형을 사용합니다.\napiVersion:cert-manager.io/v1kind:ClusterIssuermetadata:name:letsencrypt-stagingspec:acme:# The ACME server URLserver:https://acme-staging-v02.api.letsencrypt.org/directory# Email address used for ACME registrationemail:user@example.com# Name of a secret used to store the ACME account private keyprivateKeySecretRef:name:letsencrypt-staging# Enable the HTTP-01 challenge providersolvers:- http01:ingress:class:nginx  Let\u0026rsquo;s Encrypt Production Issuer\nProduction Issuer도 https://cert-manager.io/docs/tutorials/acme/example/production-issuer.yaml 파일을 이용해 동일한 방식으로 설치할 수 있습니다. 다만 사용 limit로 인해 삭제, 생성을 반복할 경우 Rate Limit에 걸릴 수 있습니다.\n  설정 적용\noke_admin@cloudshell:$ (ap-seoul-1)$ kubectl create --edit -f https://cert-manager.io/docs/tutorials/acme/example/staging-issuer.yaml issuer.cert-manager.io/letsencrypt-staging created   TLS Ingress 자원 배포   테스트 앱은 이전 그대로 사용합니다.\n  ingress 설정 YAML(tls-termination-cert-manager.yaml)을 작성합니다.\n 문서 작성일 기준 (STAGING) Doctored Durian Root CA의 만료로 인해 staging issuer 사용시에도 유효하지 않은 인증서라고 나올 수 있습니다. cert manager issuer는 production issuer를 사용하겠습니다. 대신 production issuer는 생성을 반복할 경우 limit에 걸릴 수 있습니다. cert-manager.io/cluster-issuer: 방금 생성한 letsencrypt-staging 설정, issuer가 아닌 cluster-issuer를 사용합니다. spec.tls 하위에 tls 저장할 저장소 이름 및 발급받아 사용할 도메인 이름을 지정합니다  apiVersion:networking.k8s.io/v1kind:Ingressmetadata:name:ingress-tls-termination-cert-managerannotations:kubernetes.io/ingress.class:nginxcert-manager.io/cluster-issuer:\u0026#34;letsencrypt-staging\u0026#34;spec:tls:- secretName:ingress-thekoguryo-ml-tlshosts:- green.ingress.thekoguryo.ml- blue.ingress.thekoguryo.mlrules:- host:green.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-green-svcport:number:80- host:blue.ingress.thekoguryo.mlhttp:paths:- path:/pathType:Prefixbackend:service:name:nginx-blue-svcport:number:80  기존 테스트 ingress는 삭제하고, 작성한 tls-termination-cert-manager.yaml을 배포합니다.\noke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl apply -f tls-termination-cert-manager.yaml ingress.networking.k8s.io/ingress-tls-termination-cert-manager created oke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE ingress-tls-termination-cert-manager \u0026lt;none\u0026gt; blue.ingress.thekoguryo.ml,green.ingress.thekoguryo.ml 80, 443 9s   인증서 발급 확인\n지정한 spec.tls.secretName으로 secret이 만들어지고, certificate 상태(READY)가 True가 되면 정상 발급되었습니다.\noke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-jbv7p kubernetes.io/service-account-token 3 3d19h ingress-thekoguryo-ml-tls kubernetes.io/tls 2 15m letsencrypt-staging Opaque 1 16m oke_admin@cloudshell:ingress-nginx (ap-seoul-1)$ kubectl get certificate NAME READY SECRET AGE ingress-thekoguryo-ml-tls True ingress-thekoguryo-ml-tls 16m   TLS 적용 결과 검증   ingress rule에서 적용한 host 명으로 각각 접속하여 결과를 확인합니다.\n아래와 같이 https로 접속되고 Self-Signed 인증서와 달리 경고 없이 유효한 인증서로 표시됩니다.\n    DNS 대체 주소에 요청한 host가 모두 등록되어, blue 앱도 인증에러 없이 접속됩니다.\n   Let\u0026rsquo;s Encrypt Root CA 변경으로 인해 인증오류 해결   Staging Issuer를 사용할 경우, (STAGING) Doctored Durian Root CA X3 만료로 인해 웹브라우저 접속했을 때 인증 오류가 발생하는 경우가 있습니다. Production Issuer는 해당 문제가 발생하지 않습니다.\n   해당 에러가 발생하는 경우 변경된 새 Root CA를 let\u0026rsquo;s encrypt 사이트에서 다운 받아 브라우저에 등록해 줍니다.\n  파일 다운로드\nhttps://letsencrypt.org/certs/staging/letsencrypt-stg-root-x1.pem\n  브라우저에 다운받은 Root CA 추가(크롬 브라우저 기준)\n  크롬 브라우저 \u0026gt; 설정 \u0026gt; 개인정보 및 보안 \u0026gt; 인증서 관리 로 이동\n  인증서 가져오기 클릭\n   다운받은 파일 선택\n   인증서 설치\n   인증서 등록후 확인\n신뢰할 수 있는 루트 인증 기관에 방금 등록한 (STAGING) 인증서가 보임\n   인증서가 등록후 다시 앱의 웹페이지를 접속하면 인증오류가 발생하지 않고, 인증 경로가 아래와 같이 보이게 됩니다.\n       참고 링크\nhttps://github.com/vancluever/terraform-provider-acme/issues/161\n  ","lastmod":"2021-12-07T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oss/ingress-controller/3.nginx-ingress-tls-termination/","tags":["oss","ingress-controller"],"title":"4.1.3 NGINX Ingress Controller에서 TLS termination"},{"categories":null,"contents":"1.4 앱 배포 및 Load Balancer 사용하기 Docker Hub 이미지 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-docker-hub --image=nginx:latest   배포 결과를 확인해보면 정상적으로 배포된 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl create deployment nginx-docker-hub --image=nginx:latest deployment.apps/nginx-docker-hub created oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-docker-hub-78b9999646-xgtjp 1/1 Running 0 17s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 130m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-docker-hub 1/1 1 1 19s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-docker-hub-78b9999646 1 1 1 20s   Load Balancer 테스트 Load Balancer 서비스 사용하기   클라이언트 서비스를 위해 LoadBalancer Type으로 서비스를 생성합니다.\n  서비스 생성 결과를 확인하면 아래와 같이 LoadBalancer 타입으로 생성되어 Public IP가 할당 된 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl expose deployment nginx-docker-hub --port 80 --type LoadBalancer --name nginx-docker-hub-svc service/nginx-docker-hub-svc exposed oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 135m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 49s   서비스 주소인 Public IP로 접속하면, 연결되는 것을 볼 수 있습니다.\noke_admin@cloudshell:~ (ap-chuncheon-1)$ curl http://152.67.200.29 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; html { color-scheme: light dark; } body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;   OCI Service Console에서 Load Balancer 확인하기   콘솔에서 Networking \u0026gt; Load Balancer 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  아래 그림과 같이 kubernetes 상에 생성된 서비스와 동일한 Public IP로 Load Balancer가 생성된 것을 알 수 있습니다.\n   Load Balancer 이름을 클릭하여 상세 화면으로 이동합니다.\n  상세화면에서 좌측 하단 Resources \u0026gt; Listeners로 이동합니다.\nkubernetes에서 Load Balancer 생성시 지정한 80 포트로 Listen 하고 있는 걸 알 수 있습니다.\n   Resources \u0026gt; Backend Set으로 이동합니다. 생성된 Backend Set을 클릭하여 상세화면으로 이동합니다.\n  좌측 하단 Resources \u0026gt; Backends로 이동합니다.\n세 개의 백엔드 노드의 30610 포트로 부하 분산하는 것을 볼 수 있습니다.\n   다시 터미널로 이동하여 서비스와 노드 정보를 조회합니다.\nkubectl get svc kubectl get nodes -o wide   조회결과\n조회 해보면 OCI Load Balancer 가 Worker Nodes 3개로, 각 노드의 Node Port인 30610으로 부하 분산 되는 것을 알 수 있습니다. 이처럼 kubernetes에서 Load Balancer Type 서비스를 생성하면, OCI Load Balancer와 연동되어 자동으로 자원이 생성됩니다.\n  oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 160m nginx-docker-hub-svc LoadBalancer 10.96.44.184 152.67.200.29 80:30610/TCP 25m oke_admin@cloudshell:~ (ap-chuncheon-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.11 Ready node 155m v1.20.11 10.0.10.11 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.44 Ready node 155m v1.20.11 10.0.10.44 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.73 Ready node 155m v1.20.11 10.0.10.73 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   OCI Service Console에서 Security List 확인하기   콘솔에서 Networking \u0026gt; Virtual Cloud Networks 로 이동합니다. OKE 클러스터가 있는 Compartment로 이동합니다.\n  OKE 클러스터가 사용하는 VCN으로 이동합니다.\n  Subnet을 보면 그림과 같이, 3개의 서브넷이 있습니다.\n oke-k8sApiEndpoint-subnet~~: Kubernetes API Endpoint를 위한 서브넷 oke-svclbsubnet-~~: Load Balancer가 사용하는 서브넷 oke-nodesubnet-~~: Worker Node 들이 사용하는 서브넷     Resources \u0026gt; Security Lists로 이동하면, 위 3개의 서브넷이 사용하는 3개의 Security List가 있습니다.\n   Load Balancer용 서브넷 확인\n먼저 oke-svclbsubnet-~~ 의 상세화면으로 이동합니다. Ingress/Egress Rule을 확인해 보면, 외부에서 80 포트로 수신하고, Worker Node들의 Node Port인 30610로 송신할 수 있도록 자동으로 추가된 것을 볼수 있습니다.\n    다시 VCN 상세 화면으로 이동하여, Worker Nodes용 서브넷을 확인합니다.\nOCI Load Balancer에서 Node Port: 30610으로 요청을 수신할 수 있도록 자동으로 규칙이 추가된 것을 볼수 있습니다.\n   위와 같이 OKE 클러스터에 kubernetes 명령으로 Load Balancer 서비스 타입을 생성하면, 그에 따라 OCI Load Balancer가 생성되고, 관련 Security List에도 등록되는 것을 알 수 있습니다.\n  ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/4.deploy-docker-hub-image-with-lb/","tags":["oke"],"title":"1.4 앱 배포 및 Load Balancer 사용하기"},{"categories":null,"contents":"1.5 OCIR 이미지 사용하여 앱 배포하기 OCIR에 이미지 등록하기 Oracle Cloud Infrastructure Registry(OCIR)는 오라클이 제공하는 관리형 컨테이너 레지스트리로 Docker V2 API를 지원하며, Open Container Initiate 호환 컨테이너 레지스트리입니다. docker cli를 통해 이미지를 Push, Pull 해서 사용할 수 있으며, Kubernetes 클러스터에서도 사용할 수 있습니다.\nOCIR에 이미지를 사용하기 위해서는 먼저 등록 작업이 필요하며, 앞서 예제에서 사용한 nginx 이미지를 아래 절차에 따라 등록해 봅니다.\nOCIR Repository 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Developer Services \u0026gt; Containers \u0026amp; Artifacts : Container Registry로 이동합니다.\n  List Scope에서 대상 Compartment(예, oke-labs)를 선택합니다.\n  이미지를 Push하기 전에 먼저 OCIR에 repository를 생성이 필요합니다.\nCreate repositoy를 클릭하여 아래와 같이 nginx repository를 생성합니다. Push, Pull 모두 인증 테스트를 위해 Access 모드를 Private으로 선택합니다.\n   생성 완료\n   Repository 화면에서 Namespace를 복사해 둡니다. Region내에 Tenant를 나타내는 tenancy-namespace: cnrlxx3w0wgq로 이후 로그인시 필요합니다.\n  OCI Auth Token 만들기 docker cli로 docker hub에 이미지를 등록하거나, 가져올때 username/password로 docker login을 통해 로그인을 합니다. OCIR에도 마찬가지로 로그인이 필요하며, password 대신 보안을 위해 Auth Token을 사용합니다.\n  우측 상단 사용자의 Profile 아이콘을 클릭하여 User Settings으로 이동합니다.\n 아래 그림상의 유저는 OCI local 유저로 username이 oke-admin입니다. 유저명이 oracleidentitycloudservice/~~~로 시작하면 Oracle Identiry Cloud Service의 유저입니다.     왼쪽 아래 Resources \u0026gt; Auth Token으로 이동합니다.\n  Auth Token 생성을 위해 Generate Token을 클릭합니다.\n  설명을 입력하고 생성합니다. Auth Token은 생성시에만 볼수 있으므로 복사해 둡니다.\n    OCIR 로그인 및 이미지 Push   앞서 생성한 Auth Token을 통해 Cloud Shell 또는 접속 환경에서 docker cli로 로그인 합니다.\n OCIR 주소: \u0026lt;region-key\u0026gt;.ocir.io  region-key: 서울 Region은 ap-seoul-1 또는 icn 전체 Region 정보: Availability by Region   username:  \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt; 형식 Username: OCI 서비스 콘솔에서 유저 Profile에서 보이는 유저명을 사용합니다.  Oracle Identity Cloud Service상의 유저: \u0026lt;tenancy-namespace\u0026gt;/oracleidentitycloudservice/\u0026lt;username\u0026gt; OCI Local 유저: \u0026lt;tenancy-namespace\u0026gt;/\u0026lt;username\u0026gt;   tenancy-namespace: 앞서 Repository 생성시 확인한 tenancy-namespace 또는 Cloud Shell에서 oci os ns get으로 확인 가능   Password: 앞서 생성한 로그인할 유저의 Auth Token  oke_admin@cloudshell:~ (ap-seoul-1)$ oci os ns get { \u0026#34;data\u0026#34;: \u0026#34;cnrlxx3w0wgq\u0026#34; } oke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   이미지 Push\n OCIR에 생성한 Repository로 Push 하기 위해 아래 형식으로 태그를 한 후 Push 하면 됩니다.  \u0026lt;region-key\u0026gt;.ocir.io/\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;repo-name\u0026gt;:\u0026lt;tag\u0026gt;   nginx:latest 예시  docker pull nginx:latest docker tag nginx:latest ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest docker push ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   OCIR 확인\nOCI 서비스 콘솔로 다시 돌아가서 대상 Compartment 기준으로 Push한 이미지가 정상적으로 등록된 것을 알 수 있습니다.\n   실수를 막기 위한 참고 사항 다음과 같은 상황에서는 docker push하면 어떻게 될까요?\n  Push 전에 OCIR Repository를 사전에 만들지 않은 경우\n  dev/nginx:latest(또는 bitnami/nginx:latest)와 같이 이미지 이름 앞에 추가 적인 경로가 있는 경우에 OCIR Repoistory를 dev 로만 만든 경우\n=\u0026gt; 사전에 OCIR Repository를 만들지 않으면, 기본 설정에 의해 root compartment 쪽에 push 됩니다.\n=\u0026gt; dev/nginx 까지마 Repository 이름으로 해야 합니다. 그렇게 만들지 않는 경우 동일하게 root compartment 쪽에 push 됩니다.\n  Container Registry 우측 상단에 Settings를 클릭하여 설정정보를 보면 아래와 같이 대상 repository가 없는 경우 root compartment에 private repository를 자동으로 새로 만들고 push 하는 것이 기본 값으로 체크되어 있습니다.\n OCIR 이미지로 OKE 클러스터에 배포 OCIR 이미지 배포 테스트   가장 흔한 형태인 Public Container Registry에 이미지를 가져와서 OKE 클러스터에 배포를 해봅니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   배포 결과 아래와 같이 private repository로 인증문제로 이미지를 가져오는 오류가 발생한 것을 알수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod NAME READY STATUS RESTARTS AGE nginx-ocir-6c9d554866-nqgjg 0/1 ErrImagePull 0 10s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl describe pod nginx-ocir-6c9d554866-nqgjg Name: nginx-ocir-6c9d554866-nqgjg ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- ... Warning Failed 11s (x2 over 23s) kubelet Failed to pull image \u0026#34;ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest\u0026#34;: rpc error: code = Unknown desc = Error reading manifest latest in ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx: denied: Anonymous users are only allowed read access on public repos Warning Failed 11s (x2 over 23s) kubelet Error: ErrImagePull   OCIR Private Repository 이미지 배포 테스트 - imagepullsecret Private Repository에서 이미지를 가져와서 사용하려면 인증을 위한 secret을 등록해서 사용해야 합니다. 아래 절차에 따라 secret을 만들어 사용합니다.\n  앞서 Auth Token을 사용하여 docker login을 하였습니다. 로그인 하면 사용자 홈 밑에 .docker/config.json에 인증정보가 저장됩니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ docker login ap-seoul-1.ocir.io Username: cnrlxx3w0wgq/oke-admin Password: WARNING! Your password will be stored unencrypted in /home/oke_admin/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded   위 인증 정보를 통해 로그인 합니다.\nkubectl create secret generic ocir-secret \\  --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\  --type=kubernetes.io/dockerconfigjson   또는 docker login 정보 없이 직접 secret을 만들 수도 있습니다.\nkubectl create secret docker-registry \u0026lt;secret-name\u0026gt; --docker-server=\u0026lt;region-key\u0026gt;.ocir.io --docker-username=\u0026#39;\u0026lt;tenancy-namespace\u0026gt;/\u0026lt;oci-username\u0026gt;\u0026#39; --docker-password=\u0026#39;\u0026lt;oci-auth-token\u0026gt;\u0026#39; --docker-email=\u0026#39;\u0026lt;email-address\u0026gt;\u0026#39;   아래와 같이 imagepullsecret을 사용하여 다시 배포합니다.\napiVersion:apps/v1kind:Deploymentmetadata:labels:app:nginx-ocirname:nginx-ocirspec:replicas:1selector:matchLabels:app:nginx-ocirtemplate:metadata:labels:app:nginx-ocirspec:containers:- name:nginximage:ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latestimagePullSecrets:- name:ocir-secret  아래와 같이 정상 배포되는 것을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create secret generic ocir-secret \\ \u0026gt; --from-file=.dockerconfigjson=/home/oke_admin/.docker/config.json \\ \u0026gt; --type=kubernetes.io/dockerconfigjson secret/ocir-secret created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get secret NAME TYPE DATA AGE default-token-2tvwr kubernetes.io/service-account-token 3 24h ocir-secret kubernetes.io/dockerconfigjson 1 13s oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl apply -f nginx-ocir-deployment.yaml deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-798957d964-9rddt 1/1 Running 0 7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 8s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-798957d964 1 1 1 7s   OCIR Private Repository 이미지 배포 테스트 - default imagepullsecret 매번 imagepullsecret을 지정하는 것이 불편한 경우 기본으로 사용할 Container Repository에 대한 인증을 default로 저장하여 사용할 수도 있습니다.\n  namespace에 default serviceaccount가 있는데, 여기에 아래와 같이 imagepullsecret을 추가합니다.\nkubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39;   그 결과 아래와 같이 default serviceaccount에 기본적으로 사용할 imagesecret이 추가되었습니다\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl patch serviceaccount default -p \u0026#39;{\u0026#34;imagePullSecrets\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;ocir-secret\u0026#34;}]}\u0026#39; serviceaccount/default patched oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get sa default -o yaml apiVersion: v1 imagePullSecrets: - name: ocir-secret kind: ServiceAccount metadata: creationTimestamp: \u0026#34;2021-11-08T13:51:17Z\u0026#34; name: default namespace: default resourceVersion: \u0026#34;277559\u0026#34; uid: f72718f9-135c-46d2-b0ac-a1ea1b990863 secrets: - name: default-token-2tvwr   앞서 배포한 yaml을 삭제하고 인증정보가 없어 처음 실패한 명령으로 다시 배포합니다.\nkubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest   결과확인하면 default imagepullsecret을 사용하여 정상 배포됨을 알 수 있습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl create deployment nginx-ocir --image=ap-seoul-1.ocir.io/cnrlxx3w0wgq/nginx:latest deployment.apps/nginx-ocir created oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get all NAME READY STATUS RESTARTS AGE pod/nginx-ocir-6c9d554866-vmjtb 1/1 Running 0 5s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 24h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx-ocir 1/1 1 1 5s NAME DESIRED CURRENT READY AGE replicaset.apps/nginx-ocir-6c9d554866 1 1 1 5s   ","lastmod":"2021-11-08T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/5.deploy-ocir-image/","tags":["oke"],"title":"1.5 OCIR 이미지 사용하기"},{"categories":null,"contents":"1.7 Kubernetes 지원 버전 및 업그레이드 Kubernetes 지원 버전 Kubernetes 버전은 x.y.z로 표현되며, 각각 x는 메이저, y는 마이너, z는 패치 버전을 뜻하며, 오픈소스 Kubernetes도 현재 버전과 그 이전 2개 버전까지를 지원하고 있습니다. OKE 또한 OKE가 지원하는 최신버전 기준, 그 이전 2개의 마이너 버전까지 지원하고 있습니다. 금일자 기준 지원하는 버전은 다음과 같습니다.\n 1.20.11 1.19.15 1.18.10  신규 버전 출시 후에 30일 동안만 그 이전 버전을 지원하고 있습니다. 예를 들어 OKE에서 1.20.11, 1.19.15가 2021년 10월 8일에 출시되어 기존 버전인 1.20.8, 1.19.12는 각각 30일후인 2021년 11월 7일까지만 지원합니다. 현재 지원 버전은 다음 링크를 참조합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm  버전 관리 단위 OKE 클러스터는 Control Plane과 Worker Node로 구성되며, Control Plane의 버전이 정해지면, Worker Node는 Control Plane과 같거나 그 이하 버전을 사용할 수 있습니다. 물론 호환되는 내에서 말입니다.\nWorker Node는 Node Pool 단위로 버전을 가질 수 있습니다. 여러 Node Pool을 만들어 각각 다른 버전을 가질 수 있습니다.\n버전 업그레이드 OKE 새 버전이 출시되면 버전 업그레이드는 다음 절차를 따릅니다.\n Control Plane 업그레이드  OCI 서비스 콘솔에서 OKE 클러스터 단위를 업그레이드하면 Control Plane만 업그레이드됨 오라클이 관리하는 영역으로 다운타임 없이 자동으로 업그레이드 됨 OKE 클러스터를 업그레이드 하면, 즉 Control Plane이 업그레이드하면 이전 버전으로 다시 다운그레이드 할 수 없음   Worker Node 업그레이드  OCI 서비스 콘솔에서 Node Pool 단위로 업그레이드 업그레이드 방식  in-place 업그레이드  대상이 되는 기존 Node Pool은 OCI 콘솔에서 버전 업그레이드, 실제 Node가 자동으로 업그레이드 되지 않음 kubectl drain 명령으로 특정 노드에 신규 컨테이너가 생기는 것을 방지함 OCI 서비스 콘솔에서 drain한 Node를 종료(Terminate) 시킴 Node 자가치유에 의해 신규 Node가 자동으로 생성되며, 생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전 기존 노드에 대해서 순서대로 모두 진행   out-of-place 업그레이드  신규 버전의 Node Pool 추가 생성 kubectl 명령으로 기존 Node Pool에 있는 Node 제거, Kubernetes에 의해 컨테이너가 모두 이동하면, Node 삭제하는 방식으로 하나씩 진행 기존 Node가 모두 제거되면, 기존 Node Pool 삭제        Node Pool 단위 버전 혼합 테스트를 위해 1.18.10, 1.19.12, 1.19.15, ,1.20.8, 1.20.11 버전이 사용 가능한 상태에서 1.19.12 버전의 OKE 클러스터를 기준으로 테스트를 진행합니다.\n 테스트 환경  OKE 클러스터 - oke-cluster-1 버전: 1.19.12 Control Plane 버전: 1.19.12 Node Pool - poo1 버전: 1.19.12      OKE 클러스터 상세화면으로 이동합니다.\n  왼쪽 아래 Resources \u0026gt; Node Pools 로 이동합니다.\n   현재 pool1이 1.19.12 버전이 있습니다. 추가 Node Pool 생성을 위해 Add Node Pool을 클릭합니다.\n  새 Node Pool 생성을 위한 정보를 입력합니다.\n Name: 새 Node Pool 이름 Version: 일반 버전 혼합을 확인하기 위해 1.18.10 버전은 선택합니다.  사용 가능한 버전을 보면, 현재 OKE 클러스터 버전 이하만 선택 가능한 걸 알 수 있습니다. 현재 OKE 클러스터 생성시 사용 가능한 1.19.15, ,1.20.8, 1.20.11 버전은 보이지 않습니다.      처음 생성시와 비슷하게 생성시 필요한 정보를 입력합니다. 다음은 예시입니다.  Shape: VM.Standard.E3.Flex Placement Configuration: Worker Node가 위치한 AD와 Node용 서브넷 지정 Advanced Options:  Add an SSH Key: Node에 SSH로 접근하기 위한 Publich Key        생성을 요청하면 실제 Node VM이 만들어지고, 준비되는 데 까지 앞서 설치시와 같이 약간의 시간이 걸립니다.\n  Node Pool을 추가 생성하면 그림과 같이 동일 OKE 클러스터에 두 가지 버전의 혼합을 지원하여, 앞서 Node Pool 추가시 본것 처럼 Pool 단위 VM 크기, 위치(AD, 서브넷)을 달리 할 수 있습니다.\n   kubectl로 노드를 조회해도 동일한 결과가 나옵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 23h v1.19.12 10.0.10.226 Ready node 23h v1.19.12 10.0.10.234 Ready node 6m53s v1.18.10 10.0.10.43 Ready node 6m30s v1.18.10 10.0.10.44 Ready node 23h v1.19.12 10.0.10.78 Ready node 6m41s v1.18.10   OKE 클러스터 버전 업그레이드 Control Plane 업그레이드 위와 같이 1.19.12 버전을 사용 중에 새로운 버전이 출시되었다고 가정합니다. 그러면 앞서 설명한 것과 같이 기술지원 정책에 따라 기존 버전은 30일간 지원하기 때문에, 그동안 버전 검증후 업그레이드가 필요합니다.\n https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm    업그레이드가 가능하면, OKE 클러스터 상세 화면에서 Upgrade Available 버튼이 활성화 됩니다.\n   Upgrade Available 버튼을 클릭하면 다음과 같이 안내 문구와 함께 업그레이드를 시작할 수 있습니다. 최신 버전인 v1.20.11을 선택하도록 하겠습니다.\n   버전을 선택하고 아래 Upgrade 버튼을 클릭하여 업그레이드를 시작합니다.\n  클러스터 상태가 UPDATING으로 표시되고 업그레이드가 진행됩니다. 오라클이 관리하는 Control Plane이 내부적으로 순차적으로 업그레이드가 발생합니다. 실제 애플리케이션이 동작하는 Worker Node는 서비스 중지 없이 업그레이드 됩니다.\n  테스트 시점에는 10~15분 후에 업그레이드 완료되었습니다.\n   Worker Node 업그레이드 - in-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. in-place 업그레이드 방식은 Node Pool 업그레이드 -\u0026gt; 기존 Node Drain \u0026gt; POD 이동 확인 -\u0026gt; 기존 Node VM 종료 순으로 업그레이드 합니다.\nNode Pool 업그레이드   업그레이드 하려는 Node Pool의 상세 페이지로 이동합니다.\n  수정을 위해 Edit를 클릭하면, 오른쪽에 수정 페이지가 뜹니다.\nVersion 항목에, 클러스터 버전과 Node Pool의 버전이 표시되며, 업그레이드 가능한 버전이 표시됩니다.\n   클러스터와 동일한 1.20.11로 선택하고 Save Change를 클릭하여 저장합니다.\n  Resources \u0026gt; Work Requests에 가서 보면, 2~3초 뒤에 Node Pool 업그레이드가 완료됩니다.\n   아직 실제 Worker Node가 업그레이드 된 것은 아닙니다.\nResources \u0026gt; Nodes에 가서 보면 기존 버전 그대로입니다.\n   Node Drain 시키기   kubectl 명령으로 Worker Node와 배포된 POD를 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 24h v1.19.12 10.0.10.226 Ready node 24h v1.19.12 10.0.10.44 Ready node 24h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-b46nd 1/1 Running 0 72m 10.244.1.4 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 39m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-7tsjf 1/1 Running 0 39m 10.244.1.8 10.0.10.44 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 40m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl drain 10.0.10.44 --ignore-daemonsets node/10.0.10.44 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-7hc28, kube-system/kube-flannel-ds-b4dp6, kube-system/kube-proxy-f8qmp, kube-system/proxymux-client-6qs4x evicting pod default/nginx-fss-pvc-9fb98454f-7tsjf evicting pod default/nginx-bvs-pvc-7b574c9c5c-b46nd pod/nginx-bvs-pvc-7b574c9c5c-b46nd evicted pod/nginx-fss-pvc-9fb98454f-7tsjf evicted node/10.0.10.44 evicted   아래와 같이 44번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes NAME STATUS ROLES AGE VERSION 10.0.10.185 Ready node 25h v1.19.12 10.0.10.226 Ready node 25h v1.19.12 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-9smhh 1/1 Running 0 3m20s 10.244.0.138 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5t5bh 1/1 Running 0 45m 10.244.0.137 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-8zkn2 1/1 Running 0 3m20s 10.244.0.6 10.0.10.185 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-fzlxj 1/1 Running 0 46m 10.244.0.136 10.0.10.226 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   종료할 Node의 노드 이름을 확인합니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -L displayName NAME STATUS ROLES AGE VERSION DISPLAYNAME 10.0.10.185 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-0 10.0.10.226 Ready node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-1 10.0.10.44 Ready,SchedulingDisabled node 25h v1.19.12 oke-cxdb6ehmxsa-nqi4muu267q-siouvw2ymqq-2   Node Pool 상세 화면에서 종료할 노드 이름을 클릭하여 Compute 인스턴스로 이동합니다.\n   Node에 해당되는 Compute 인스턴스를 종료합니다. boot volume도 같이 종료합니다.\n    Node 자가치유에 의해 신규 Node가 자동으로 생성됩니다. Work Requests를 보면 아래와 같이 재조정(NODEPOLL_RECONCILE) 작업이 발생되어 지정된 갯수에 맞게 노드가 다시 생성됩니다.\n   생성된 신규노드는 Node Pool에서 지정한 업그레이드 된 버전으로 생성됩니다.\n  kubectl 명령으로 노드를 조회하면, 1.20.2 버전으로 신규 노드가 생성되었습니다. docker 런타임의 deprecate 예정으로 인해 참고로 1.20 부터는 컨테이너 런타임이 cri-o 변경되었습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.170 Ready node 7m25s v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.185 Ready node 25h v1.19.12 10.0.10.185 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11 10.0.10.226 Ready node 25h v1.19.12 10.0.10.226 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 docker://19.3.11   기존 노드에 대해서 순서대로 모두 진행합니다.\n  완료 결과\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME 10.0.10.128 Ready node 6m10s v1.20.11 10.0.10.128 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.146 Ready node 35s v1.20.11 10.0.10.146 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2 10.0.10.170 Ready node 23m v1.20.11 10.0.10.170 \u0026lt;none\u0026gt; Oracle Linux Server 7.9 5.4.17-2102.206.1.el7uek.x86_64 cri-o://1.20.2   Worker Node 업그레이드 - out-of-place 업그레이드 OKE 클러스터가 업그레이드로 인해 Control Plane 만 업그레이드 된 상태이며, 이제 Node Pool 단위로 업그레이드 가능한 상태입니다. out-of-place 업그레이드 방식은 업그레이드 버전의 Node Pool 신규 생성 -\u0026gt; 기존 Node Pool의 모든 노드 Drain -\u0026gt; 기존 Node Pool 삭제 순으로 업그레이드 합니다.\n새 버전의 Node Pool 만들기   OKE 클러스터 상세 페이지로 이동합니다.\n  Resources \u0026gt; Node Pools 로 이동합니다.\n  그림과 같이 기존 버전의 Node Pool이 있는 상태에서 신규 Node Pool 추가를 위해 Add Node Pool을 클릭합니다.\n   신규 Node Pool 정보를 입력하여 생성합니다.\n Name Version: 새 버전 선택 Shape: Node VM 유형 Number of nodes: 노드 수 Placement Configuration  Node가 위치할 AD, Subnet   Add an SSH key: Node VM에 SSH 접속시 사용할 키의 Private Key     추가 된 Node Pool을 OCI 서비스 콘솔 확인할 수 있습니다.\n  기존 Node Pool의 모든 노드 Drain   구동 중인 앱들이 기존 Node Pool에서 동작하고 있습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 18m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-2jbp8 1/1 Running 0 18m 10.244.0.5 10.0.10.242 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 19m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 18m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready node 71m v1.19.12 pool1 10.0.10.29 Ready node 71m v1.19.12 pool1 10.0.10.61 Ready node 71m v1.19.12 pool1 10.0.10.12 Ready node 4m25s v1.20.11 pool2 10.0.10.126 Ready node 4m48s v1.20.11 pool2 10.0.10.191 Ready node 4m42s v1.20.11 pool2   아래와 같이 kubectl drain \u0026lt;node_name\u0026gt; --ignore-daemonsets 명령으로 하나의 노드를 스케줄에서 제외시킵니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl drain 10.0.10.242 --ignore-daemonsets node/10.0.10.242 already cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/csi-oci-node-l62xw, kube-system/kube-flannel-ds-7dv8l, kube-system/kube-proxy-zv4ks, kube-system/proxymux-client-24nmv evicting pod kube-system/coredns-c5d4bf466-wv8fb evicting pod default/nginx-fss-pvc-9fb98454f-2jbp8 evicting pod kube-system/coredns-c5d4bf466-h5rrm pod/coredns-c5d4bf466-wv8fb evicted pod/nginx-fss-pvc-9fb98454f-2jbp8 evicted pod/coredns-c5d4bf466-h5rrm evicted node/10.0.10.242 evicted   아래와 같이 242번 노드가 컨테이너 스케줄링에서 제외된 것을 볼 수 있습니다. POD가 다른 Node로 다 이동한 걸 확인후 다음 작업으로 진행합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 76m v1.19.12 pool1 10.0.10.29 Ready node 76m v1.19.12 pool1 10.0.10.61 Ready node 76m v1.19.12 pool1 10.0.10.12 Ready node 9m15s v1.20.11 pool2 10.0.10.126 Ready node 9m38s v1.20.11 pool2 10.0.10.191 Ready node 9m32s v1.20.11 pool2 oke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-bvs-pvc-7b574c9c5c-8jj2d 1/1 Running 0 22m 10.244.0.134 10.0.10.29 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-5k8wg 1/1 Running 0 57s 10.244.3.2 10.0.10.126 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-nwqfd 1/1 Running 0 23m 10.244.1.5 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx-fss-pvc-9fb98454f-twx4h 1/1 Running 0 22m 10.244.1.6 10.0.10.61 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;   나머지 기존 Node Pool에 있는 Node들도 drain합니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.242 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.29 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.61 Ready,SchedulingDisabled node 79m v1.19.12 pool1 10.0.10.12 Ready node 11m v1.20.11 pool2 10.0.10.126 Ready node 12m v1.20.11 pool2 10.0.10.191 Ready node 12m v1.20.11 pool2   기존 Node Pool 삭제   기존 Node Pool에 있는 모든 Node들이 drain되어 더이상 사용되지 않습니다.\n  OCI 서비스 콘솔에서 OKE 클러스터 상세페이지로 이동합니다.\n  Resources \u0026gt; Node Pools로 이동하여 기존 Node Pool을 삭제합니다.\n   업그레이드가 완료되었습니다.\noke_admin@cloudshell:file-storage (ap-seoul-1)$ kubectl get nodes -L name --sort-by=.metadata.labels.name NAME STATUS ROLES AGE VERSION NAME 10.0.10.12 Ready node 18m v1.20.11 pool2 10.0.10.126 Ready node 19m v1.20.11 pool2 10.0.10.191 Ready node 19m v1.20.11 pool2   ","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/7.supported-version-and-upgrade/","tags":["oke"],"title":"1.7 Kubernetes 버전 업그레이드"},{"categories":null,"contents":"1.8 애플리케이션 로그 모니터링 OKE 상에 배포되어 있는 애플리케이션 로그 모니터링을 OCI Logging 서비스를 통해 모니터링할 수 있습니다.\nOCI Logging 서비스 사용 권한 설정 Worker Node에 대한 Dynamic Group 만들기   OCI 콘솔에 로그인합니다.\n  좌측 상단 햄버거 메뉴에서 Identity \u0026amp; Security \u0026gt; **Identity ** \u0026gt; Compartments로 이동합니다.\n  OKE 클러스터가 있는 Compartment의 OCID를 확인합니다.\n   좌측 Dynamic Group 메뉴로 이동하여 아래 규칙을 가진 Dynamic Group을 만듭니다.\ninstance.compartment.id = \u0026#39;\u0026lt;compartment-ocid\u0026gt;\u0026#39;    Dynamic Group에 대한 OCI Logging 서비스 권한 부여하기   좌측 Policy 메뉴로 이동하여 아래 규칙을 가진 Policy을 만듭니다. 방금 생성한 Dynamic Group에 대한 Policy를 만듭니다.\nallow dynamic-group \u0026lt;dynamic-group-name\u0026gt; to use log-content in compartment \u0026lt;compartment-name\u0026gt;    컨테이너를 위한 Custom Log 설정하기 Log Group 만들기 Log Group은 로그들을 관리하는 말 그대로 로그의 묶음 단위 입니다. 커스텀 로그를 만들기 위해 먼저 만듭니다.\n  좌측 상단 햄버거 메뉴에서 Observability \u0026amp; Management \u0026gt; Logging \u0026gt; Log Groups로 이동합니다.\n  Create Log Group을 클릭하여 로그 그룹을 만듭니다.\n   Custom Log 만들기 Custom Log는 커스텀 애플리케이션에서 수집하는 로그에 매핑되는 것입니다. Custom Log를 정의하고, 이에 대한 로그 수집기를 정의합니다.\n  Resources \u0026gt; Logs 메뉴로 이동하여 Create custom log를 클릭합니다.\n  로그 이름과 보관 주기 등을 설정하여 custom log를 만듭니다.\n   이해를 돕고자 Agent는 별도로 설정합니다. 여기서는 일단 Add configuration later 선택\n   Agent Configuration 설정 Agent Configuration는 로그를 수집하는 agent를 설정하는 부분입니다.\n  Logging \u0026gt; Agent Configurations 메뉴로 이동하여 Create agent log를 클릭합니다.\n  Agent 이름 및 대상 Host Group을 앞서 만든 Dynamic Group으로 지정합니다.\n   Agent 설정 부분에서 로그가 위치한 경로 및 수집된 로그의 전달 위치를 지정합니다.\n  log input: /var/log/containers/*.log\n앞서 지정한 Dynamic Group상에 있는 VM, 여기서는 OKE 클러스터 Worker Node VM 상에 수집할 로그의 위치를 지정합니다.입력하고 엔터키를 꼭 칩니다.\n  log destination: 수집한 로그를 전달한 앞서 생성한 custom log 이름을 지정합니다.\n     참고: Worker Node VM상에 컨테이너 로그 위치   Worker Node VM에 SSH로 접속이 가능한 환경, 예, bastion host에서 Worker Node에 접속해 보면 컨테이너 로그 위치는 다음과 같습니다.\n[opc@bastion-host ~]$ ssh opc@10.0.10.175 Last login: Tue Nov 16 06:43:43 2021 from bastion-host.suba22926d1b.okecluster1.oraclevcn.com [opc@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 ~]$ sudo su [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 opc]# cd /var/log/containers/ [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# ls -la total 16 drwxr-xr-x. 2 root root 4096 Nov 15 14:05 . drwxr-xr-x. 13 root root 4096 Nov 15 13:55 .. lrwxrwxrwx. 1 root root 100 Nov 15 13:52 coredns-c5d4bf466-qdgrh_kube-system_coredns-a533d7375a4bd90b894d533e637bae6ce38e2e7d89cd0ff16e34bd120111c7e4.log -\u0026gt; /var/log/pods/kube-system_coredns-c5d4bf466-qdgrh_24b226a0-0fb3-4ede-b02f-17b177e6c248/coredns/0.log ... lrwxrwxrwx. 1 root root 101 Nov 15 14:05 nginx-bvs-pvc-7b574c9c5c-vdpdh_default_nginx-cc996c8fd2281d1bec7fdad75518c66a2ec6f03adc214f8bbd97c26967e8d0e7.log -\u0026gt; /var/log/pods/default_nginx-bvs-pvc-7b574c9c5c-vdpdh_aab712c6-81e1-434c-b051-f3e8fe50fa24/nginx/0.log lrwxrwxrwx. 1 root root 106 Nov 15 13:52 proxymux-client-cb86k_kube-system_proxymux-client-471c1c80fe2dc01c5e1413819af58d968796541cc744384f1b2daa566873d8ba.log -\u0026gt; /var/log/pods/kube-system_proxymux-client-cb86k_153b7b7a-a777-4be9-b971-657eb8ecdddd/proxymux-client/0.log   /var/log/containers/*.log는 위에서 보는 것처럼 링크라서 kubernetes namespace 기준으로 하고 싶다면, 로그 경로를 default namespace인 경우/var/log/pods/default_*/*/*.log 이렇게 해도 되겠습니다.\n  로깅 테스트   애플리케이션 로그 확인을 위해 이전 가이드에 샘플로 배포된 nginx 앱을 접속해 봅니다.\n   발생한 POD 로그는 다음과 같습니다.\noke_admin@cloudshell:~ (ap-seoul-1)$ kubectl logs nginx-bvs-pvc-7b574c9c5c-vdpdh -f ... 10.244.0.128 - - [16/Nov/2021:08:10:46 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   동일한 로그가 Worker Node VM 상에서도 로그를 조회해 보면 /var/log/containers/*.log 위치에 발생하는 것을 확인 할 수 있습니다.\n[root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# pwd /var/log/containers [root@oke-cjy33uzxhyq-nkglpmyewja-siouvw2ymqq-2 containers]# tail -f nginx-bvs-pvc-7b574c9c5c-vdpdh_default_nginx-cc996c8fd2281d1bec7fdad75518c66a2ec6f03adc214f8bbd97c26967e8d0e7.log ... 2021-11-16T08:10:46.961696444+00:00 stdout F 10.244.0.128 - - [16/Nov/2021:08:10:46 +0000] \u0026#34;GET /?customlogtest HTTP/1.1\u0026#34; 304 0 \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36\u0026#34; \u0026#34;10.179.87.76\u0026#34;   OCI 서비스 콘솔에서 Logging 화면으로 다시 돌아갑니다.\n  Agent, Logs, Log Groups 각 화면에서 Resources \u0026gt; Explore Log에서 로그를 조회 할 수 있습니다. 검색을 위해서는 로그목록 오른쪽위에 있는 Explore with Log Search를 클릭합니다.\n   Custom filters 항목에서 POD 이름 또는 앞서 테스트 URL에 있는 customlogtest 같이 검색값으로 조회하면 됩니다. Custom filters에 값을 입력하고 엔터키를 꼭 칩니다.\n   검색된 로그 데이터를 확인할 수 있습니다.\n   ","lastmod":"2021-11-14T00:00:00Z","permalink":"https://thekoguryo.github.io/oci-cloudnative/oke/8.monitor-application-log/","tags":["oke"],"title":"1.8 애플리케이션 로그 모니터링"},{"categories":null,"contents":"","lastmod":null,"permalink":"https://thekoguryo.github.io/oci-cloudnative/search/","tags":null,"title":"Search"}]